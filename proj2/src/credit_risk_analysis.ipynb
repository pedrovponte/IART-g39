{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projeto desenvolvido por:\n",
    "* Mariana Ramos - up201806869\n",
    "* Pedro Ferreira - up201806506\n",
    "* Pedro Ponte - up201809694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [Required Libraries](#Required-Libraries)\n",
    "\n",
    "3. [The Problem Domain](#The-Problem-Domain)\n",
    "\n",
    "4. [Step 1: Answering the question](#Step-1:-Answering-the-question)\n",
    "\n",
    "5. [Step 2: Checking the data](#Step-2:-Checking-the-data)\n",
    "\n",
    "6. [Step 3: Tidying the data](#Step-3:-Tidying-the-data)\n",
    "\n",
    "7. [Step 4: Exploratory Analysis](#Step-4:-Exploratory-Analysis)\n",
    "\n",
    "9. [Step 5: Classification](#Step-5:-Classification)\n",
    "\n",
    "    - [5.1: Decision Trees](#5.1:-Decision-Trees)\n",
    "        \n",
    "        - [5.1.1: Cross Validation](#5.1.1:-Cross-Validation)\n",
    "\n",
    "        - [5.1.2: Parameter Tuning](#5.1.2:-Parameter-Tuning)\n",
    "    \n",
    "    - [5.2: K-Nearest Neighbor](#5.2:-K-Nearest-Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "If you don't have Python on your computer, you can use the [Anaconda Python distribution](http://continuum.io/downloads) to install most of the Python packages you need. Anaconda provides a simple double-click installer for your convenience.\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install numpy pandas scikit-learn matplotlib seaborn\n",
    "    \n",
    "    conda install -c conda-forge watermark\n",
    "\n",
    "`conda` may ask you to update some of them if you don't have the most recent version. Allow it to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem Domain\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Answering the question\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The first step to any data analysis project is to define the question or problem we're looking to solve, and to define a measure (or set of measures) for our success at solving that task. The data analysis checklist has us answer a handful of questions to accomplish that, so let's work through those questions.\n",
    "\n",
    ">Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data?\n",
    "\n",
    "We're trying to design a predictive model in order to evaluate the credit risk of a given loan and decide whether the loan should be granted or not.\n",
    "\n",
    ">Did you define the metric for success before beginning?\n",
    "\n",
    "Let's do that now. Since we're performing classification, we can use [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) — the fraction of correctly classified loans — to quantify how well our model is performing. The accuracy achieved should be, at least, \n",
    "\n",
    ">Did you understand the context for the question and the scientific or business application?\n",
    "\n",
    "TODO\n",
    "\n",
    ">Did you record the experimental design?\n",
    "\n",
    "TODO\n",
    "\n",
    ">Did you consider whether the question could be answered with the available data?\n",
    "\n",
    "TODO\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Checking the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In order to be able to make conclusions and reach our goal, we will need to extract and study the data. To do so, we have to import the panda library and extract the information to be stored in a variable so we can work with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "credit_data = pd.read_csv('data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  855969\n",
      "Number of columns:  73\n"
     ]
    }
   ],
   "source": [
    "# data frame shape\n",
    "print('Number of rows: ', credit_data.shape[0])\n",
    "print('Number of columns: ', credit_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data file has a total of 855969 different results and a total of 73 evaluation criteria. The second parameter of the read_csv function makes all the empty fields be filled with NA so we can easily analyse missing values in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by reading the data into a pandas DataFrame to see if is everything alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>default_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075358</td>\n",
       "      <td>1311748</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1075269</td>\n",
       "      <td>1311441</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1069639</td>\n",
       "      <td>1304742</td>\n",
       "      <td>7000</td>\n",
       "      <td>7000</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>170.08</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1072053</td>\n",
       "      <td>1288686</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>18.64</td>\n",
       "      <td>109.43</td>\n",
       "      <td>E</td>\n",
       "      <td>E1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1071795</td>\n",
       "      <td>1306957</td>\n",
       "      <td>5600</td>\n",
       "      <td>5600</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>21.28</td>\n",
       "      <td>152.39</td>\n",
       "      <td>F</td>\n",
       "      <td>F2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1071570</td>\n",
       "      <td>1306721</td>\n",
       "      <td>5375</td>\n",
       "      <td>5375</td>\n",
       "      <td>5350.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69</td>\n",
       "      <td>121.45</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501    1296599       5000         5000           4975.0   36 months   \n",
       "1  1077430    1314167       2500         2500           2500.0   60 months   \n",
       "2  1077175    1313524       2400         2400           2400.0   36 months   \n",
       "3  1076863    1277178      10000        10000          10000.0   36 months   \n",
       "4  1075358    1311748       3000         3000           3000.0   60 months   \n",
       "5  1075269    1311441       5000         5000           5000.0   36 months   \n",
       "6  1069639    1304742       7000         7000           7000.0   60 months   \n",
       "7  1072053    1288686       3000         3000           3000.0   36 months   \n",
       "8  1071795    1306957       5600         5600           5600.0   60 months   \n",
       "9  1071570    1306721       5375         5375           5350.0   60 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... il_util open_rv_12m open_rv_24m  \\\n",
       "0     10.65       162.87     B        B2  ...     NaN         NaN         NaN   \n",
       "1     15.27        59.83     C        C4  ...     NaN         NaN         NaN   \n",
       "2     15.96        84.33     C        C5  ...     NaN         NaN         NaN   \n",
       "3     13.49       339.31     C        C1  ...     NaN         NaN         NaN   \n",
       "4     12.69        67.79     B        B5  ...     NaN         NaN         NaN   \n",
       "5      7.90       156.46     A        A4  ...     NaN         NaN         NaN   \n",
       "6     15.96       170.08     C        C5  ...     NaN         NaN         NaN   \n",
       "7     18.64       109.43     E        E1  ...     NaN         NaN         NaN   \n",
       "8     21.28       152.39     F        F2  ...     NaN         NaN         NaN   \n",
       "9     12.69       121.45     B        B5  ...     NaN         NaN         NaN   \n",
       "\n",
       "   max_bal_bc all_util total_rev_hi_lim inq_fi total_cu_tl inq_last_12m  \\\n",
       "0         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "1         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "2         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "3         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "4         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "5         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "6         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "7         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "8         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "9         NaN      NaN              NaN    NaN         NaN          NaN   \n",
       "\n",
       "  default_ind  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           1  \n",
       "9           1  \n",
       "\n",
       "[10 rows x 73 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>default_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>855964</th>\n",
       "      <td>36371250</td>\n",
       "      <td>39102635</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>332.10</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855965</th>\n",
       "      <td>36441262</td>\n",
       "      <td>39152692</td>\n",
       "      <td>24000</td>\n",
       "      <td>24000</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>797.03</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855966</th>\n",
       "      <td>36271333</td>\n",
       "      <td>38982739</td>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.99</td>\n",
       "      <td>316.07</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855967</th>\n",
       "      <td>36490806</td>\n",
       "      <td>39222577</td>\n",
       "      <td>12000</td>\n",
       "      <td>12000</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>19.99</td>\n",
       "      <td>317.86</td>\n",
       "      <td>E</td>\n",
       "      <td>E3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855968</th>\n",
       "      <td>36271262</td>\n",
       "      <td>38982659</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>664.20</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  \\\n",
       "855964  36371250   39102635      10000        10000          10000.0   \n",
       "855965  36441262   39152692      24000        24000          24000.0   \n",
       "855966  36271333   38982739      13000        13000          13000.0   \n",
       "855967  36490806   39222577      12000        12000          12000.0   \n",
       "855968  36271262   38982659      20000        20000          20000.0   \n",
       "\n",
       "              term  int_rate  installment grade sub_grade  ... il_util  \\\n",
       "855964   36 months     11.99       332.10     B        B5  ...     NaN   \n",
       "855965   36 months     11.99       797.03     B        B5  ...     NaN   \n",
       "855966   60 months     15.99       316.07     D        D2  ...     NaN   \n",
       "855967   60 months     19.99       317.86     E        E3  ...     NaN   \n",
       "855968   36 months     11.99       664.20     B        B5  ...     NaN   \n",
       "\n",
       "       open_rv_12m open_rv_24m  max_bal_bc all_util total_rev_hi_lim inq_fi  \\\n",
       "855964         NaN         NaN         NaN      NaN          17100.0    NaN   \n",
       "855965         NaN         NaN         NaN      NaN          10200.0    NaN   \n",
       "855966         NaN         NaN         NaN      NaN          18000.0    NaN   \n",
       "855967         NaN         NaN         NaN      NaN          27000.0    NaN   \n",
       "855968         NaN         NaN         NaN      NaN          41700.0    NaN   \n",
       "\n",
       "       total_cu_tl inq_last_12m default_ind  \n",
       "855964         NaN          NaN           0  \n",
       "855965         NaN          NaN           0  \n",
       "855966         NaN          NaN           0  \n",
       "855967         NaN          NaN           0  \n",
       "855968         NaN          NaN           0  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>default_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.559690e+05</td>\n",
       "      <td>8.559690e+05</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>8.559690e+05</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11609.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>7.886560e+05</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>13288.000000</td>\n",
       "      <td>855969.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.224073e+07</td>\n",
       "      <td>3.476269e+07</td>\n",
       "      <td>14745.571335</td>\n",
       "      <td>14732.378305</td>\n",
       "      <td>14700.061226</td>\n",
       "      <td>13.192320</td>\n",
       "      <td>436.238072</td>\n",
       "      <td>7.507119e+04</td>\n",
       "      <td>18.122165</td>\n",
       "      <td>0.311621</td>\n",
       "      <td>...</td>\n",
       "      <td>71.486993</td>\n",
       "      <td>1.354305</td>\n",
       "      <td>2.945515</td>\n",
       "      <td>5840.443332</td>\n",
       "      <td>61.024526</td>\n",
       "      <td>3.216357e+04</td>\n",
       "      <td>0.947772</td>\n",
       "      <td>1.524232</td>\n",
       "      <td>1.841963</td>\n",
       "      <td>0.054286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.271969e+07</td>\n",
       "      <td>2.399418e+07</td>\n",
       "      <td>8425.340005</td>\n",
       "      <td>8419.471653</td>\n",
       "      <td>8425.805478</td>\n",
       "      <td>4.368365</td>\n",
       "      <td>243.726876</td>\n",
       "      <td>6.426447e+04</td>\n",
       "      <td>17.423629</td>\n",
       "      <td>0.857189</td>\n",
       "      <td>...</td>\n",
       "      <td>23.015293</td>\n",
       "      <td>1.483710</td>\n",
       "      <td>2.595313</td>\n",
       "      <td>5108.500262</td>\n",
       "      <td>20.018117</td>\n",
       "      <td>3.769964e+04</td>\n",
       "      <td>1.441667</td>\n",
       "      <td>2.697601</td>\n",
       "      <td>2.975049</td>\n",
       "      <td>0.226581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.473400e+04</td>\n",
       "      <td>7.069900e+04</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.320000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.067986e+06</td>\n",
       "      <td>1.079273e+07</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>9.990000</td>\n",
       "      <td>260.550000</td>\n",
       "      <td>4.500000e+04</td>\n",
       "      <td>11.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2405.000000</td>\n",
       "      <td>47.900000</td>\n",
       "      <td>1.400000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.431355e+07</td>\n",
       "      <td>3.697532e+07</td>\n",
       "      <td>13000.000000</td>\n",
       "      <td>13000.000000</td>\n",
       "      <td>13000.000000</td>\n",
       "      <td>12.990000</td>\n",
       "      <td>382.550000</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>17.610000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4485.500000</td>\n",
       "      <td>62.100000</td>\n",
       "      <td>2.380000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.446311e+07</td>\n",
       "      <td>5.803559e+07</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>571.560000</td>\n",
       "      <td>9.000000e+04</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7701.250000</td>\n",
       "      <td>75.300000</td>\n",
       "      <td>3.990000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.861687e+07</td>\n",
       "      <td>7.351969e+07</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>28.990000</td>\n",
       "      <td>1445.460000</td>\n",
       "      <td>9.500000e+06</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>223.300000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>83047.000000</td>\n",
       "      <td>151.400000</td>\n",
       "      <td>9.999999e+06</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     member_id      loan_amnt    funded_amnt  \\\n",
       "count  8.559690e+05  8.559690e+05  855969.000000  855969.000000   \n",
       "mean   3.224073e+07  3.476269e+07   14745.571335   14732.378305   \n",
       "std    2.271969e+07  2.399418e+07    8425.340005    8419.471653   \n",
       "min    5.473400e+04  7.069900e+04     500.000000     500.000000   \n",
       "25%    9.067986e+06  1.079273e+07    8000.000000    8000.000000   \n",
       "50%    3.431355e+07  3.697532e+07   13000.000000   13000.000000   \n",
       "75%    5.446311e+07  5.803559e+07   20000.000000   20000.000000   \n",
       "max    6.861687e+07  7.351969e+07   35000.000000   35000.000000   \n",
       "\n",
       "       funded_amnt_inv       int_rate    installment    annual_inc  \\\n",
       "count    855969.000000  855969.000000  855969.000000  8.559690e+05   \n",
       "mean      14700.061226      13.192320     436.238072  7.507119e+04   \n",
       "std        8425.805478       4.368365     243.726876  6.426447e+04   \n",
       "min           0.000000       5.320000      15.690000  0.000000e+00   \n",
       "25%        8000.000000       9.990000     260.550000  4.500000e+04   \n",
       "50%       13000.000000      12.990000     382.550000  6.500000e+04   \n",
       "75%       20000.000000      15.990000     571.560000  9.000000e+04   \n",
       "max       35000.000000      28.990000    1445.460000  9.500000e+06   \n",
       "\n",
       "                 dti    delinq_2yrs  ...       il_util   open_rv_12m  \\\n",
       "count  855969.000000  855969.000000  ...  11609.000000  13288.000000   \n",
       "mean       18.122165       0.311621  ...     71.486993      1.354305   \n",
       "std        17.423629       0.857189  ...     23.015293      1.483710   \n",
       "min         0.000000       0.000000  ...      0.000000      0.000000   \n",
       "25%        11.880000       0.000000  ...     58.500000      0.000000   \n",
       "50%        17.610000       0.000000  ...     75.000000      1.000000   \n",
       "75%        23.900000       0.000000  ...     87.500000      2.000000   \n",
       "max      9999.000000      39.000000  ...    223.300000     22.000000   \n",
       "\n",
       "        open_rv_24m    max_bal_bc      all_util  total_rev_hi_lim  \\\n",
       "count  13288.000000  13288.000000  13288.000000      7.886560e+05   \n",
       "mean       2.945515   5840.443332     61.024526      3.216357e+04   \n",
       "std        2.595313   5108.500262     20.018117      3.769964e+04   \n",
       "min        0.000000      0.000000      0.000000      0.000000e+00   \n",
       "25%        1.000000   2405.000000     47.900000      1.400000e+04   \n",
       "50%        2.000000   4485.500000     62.100000      2.380000e+04   \n",
       "75%        4.000000   7701.250000     75.300000      3.990000e+04   \n",
       "max       43.000000  83047.000000    151.400000      9.999999e+06   \n",
       "\n",
       "             inq_fi   total_cu_tl  inq_last_12m    default_ind  \n",
       "count  13288.000000  13288.000000  13288.000000  855969.000000  \n",
       "mean       0.947772      1.524232      1.841963       0.054286  \n",
       "std        1.441667      2.697601      2.975049       0.226581  \n",
       "min        0.000000      0.000000     -4.000000       0.000000  \n",
       "25%        0.000000      0.000000      0.000000       0.000000  \n",
       "50%        0.000000      0.000000      2.000000       0.000000  \n",
       "75%        1.000000      2.000000      3.000000       0.000000  \n",
       "max       15.000000     33.000000     32.000000       1.000000  \n",
       "\n",
       "[8 rows x 52 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the structure of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
       "       'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title',\n",
       "       'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
       "       'issue_d', 'pymnt_plan', 'desc', 'purpose', 'title', 'zip_code',\n",
       "       'addr_state', 'dti', 'delinq_2yrs', 'earliest_cr_line',\n",
       "       'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record',\n",
       "       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
       "       'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
       "       'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint',\n",
       "       'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt',\n",
       "       'tot_cur_bal', 'open_acc_6m', 'open_il_6m', 'open_il_12m',\n",
       "       'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util',\n",
       "       'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util',\n",
       "       'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
       "       'default_ind'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all data frame columns\n",
    "credit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 855969 entries, 0 to 855968\n",
      "Data columns (total 73 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           855969 non-null  int64  \n",
      " 1   member_id                    855969 non-null  int64  \n",
      " 2   loan_amnt                    855969 non-null  int64  \n",
      " 3   funded_amnt                  855969 non-null  int64  \n",
      " 4   funded_amnt_inv              855969 non-null  float64\n",
      " 5   term                         855969 non-null  object \n",
      " 6   int_rate                     855969 non-null  float64\n",
      " 7   installment                  855969 non-null  float64\n",
      " 8   grade                        855969 non-null  object \n",
      " 9   sub_grade                    855969 non-null  object \n",
      " 10  emp_title                    806526 non-null  object \n",
      " 11  emp_length                   812908 non-null  object \n",
      " 12  home_ownership               855969 non-null  object \n",
      " 13  annual_inc                   855969 non-null  float64\n",
      " 14  verification_status          855969 non-null  object \n",
      " 15  issue_d                      855969 non-null  object \n",
      " 16  pymnt_plan                   855969 non-null  object \n",
      " 17  desc                         121812 non-null  object \n",
      " 18  purpose                      855969 non-null  object \n",
      " 19  title                        855936 non-null  object \n",
      " 20  zip_code                     855969 non-null  object \n",
      " 21  addr_state                   855969 non-null  object \n",
      " 22  dti                          855969 non-null  float64\n",
      " 23  delinq_2yrs                  855969 non-null  int64  \n",
      " 24  earliest_cr_line             855969 non-null  object \n",
      " 25  inq_last_6mths               855969 non-null  int64  \n",
      " 26  mths_since_last_delinq       416157 non-null  float64\n",
      " 27  mths_since_last_record       131184 non-null  float64\n",
      " 28  open_acc                     855969 non-null  int64  \n",
      " 29  pub_rec                      855969 non-null  int64  \n",
      " 30  revol_bal                    855969 non-null  int64  \n",
      " 31  revol_util                   855523 non-null  float64\n",
      " 32  total_acc                    855969 non-null  int64  \n",
      " 33  initial_list_status          855969 non-null  object \n",
      " 34  out_prncp                    855969 non-null  float64\n",
      " 35  out_prncp_inv                855969 non-null  float64\n",
      " 36  total_pymnt                  855969 non-null  float64\n",
      " 37  total_pymnt_inv              855969 non-null  float64\n",
      " 38  total_rec_prncp              855969 non-null  float64\n",
      " 39  total_rec_int                855969 non-null  float64\n",
      " 40  total_rec_late_fee           855969 non-null  float64\n",
      " 41  recoveries                   855969 non-null  float64\n",
      " 42  collection_recovery_fee      855969 non-null  float64\n",
      " 43  last_pymnt_d                 847107 non-null  object \n",
      " 44  last_pymnt_amnt              855969 non-null  float64\n",
      " 45  next_pymnt_d                 602998 non-null  object \n",
      " 46  last_credit_pull_d           855919 non-null  object \n",
      " 47  collections_12_mths_ex_med   855913 non-null  float64\n",
      " 48  mths_since_last_major_derog  213139 non-null  float64\n",
      " 49  policy_code                  855969 non-null  int64  \n",
      " 50  application_type             855969 non-null  object \n",
      " 51  annual_inc_joint             442 non-null     float64\n",
      " 52  dti_joint                    442 non-null     float64\n",
      " 53  verification_status_joint    442 non-null     object \n",
      " 54  acc_now_delinq               855969 non-null  int64  \n",
      " 55  tot_coll_amt                 788656 non-null  float64\n",
      " 56  tot_cur_bal                  788656 non-null  float64\n",
      " 57  open_acc_6m                  13288 non-null   float64\n",
      " 58  open_il_6m                   13288 non-null   float64\n",
      " 59  open_il_12m                  13288 non-null   float64\n",
      " 60  open_il_24m                  13288 non-null   float64\n",
      " 61  mths_since_rcnt_il           12934 non-null   float64\n",
      " 62  total_bal_il                 13288 non-null   float64\n",
      " 63  il_util                      11609 non-null   float64\n",
      " 64  open_rv_12m                  13288 non-null   float64\n",
      " 65  open_rv_24m                  13288 non-null   float64\n",
      " 66  max_bal_bc                   13288 non-null   float64\n",
      " 67  all_util                     13288 non-null   float64\n",
      " 68  total_rev_hi_lim             788656 non-null  float64\n",
      " 69  inq_fi                       13288 non-null   float64\n",
      " 70  total_cu_tl                  13288 non-null   float64\n",
      " 71  inq_last_12m                 13288 non-null   float64\n",
      " 72  default_ind                  855969 non-null  int64  \n",
      "dtypes: float64(39), int64(13), object(21)\n",
      "memory usage: 476.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# data frame summary\n",
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features have missing values. Let's take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification_status_joint    99.948363\n",
      "annual_inc_joint             99.948363\n",
      "dti_joint                    99.948363\n",
      "il_util                      98.643759\n",
      "mths_since_rcnt_il           98.488964\n",
      "                               ...    \n",
      "pub_rec                       0.000000\n",
      "open_acc                      0.000000\n",
      "inq_last_6mths                0.000000\n",
      "earliest_cr_line              0.000000\n",
      "id                            0.000000\n",
      "Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# percentage of missing values per feature\n",
    "print((credit_data.isnull().sum() * 100 / credit_data.shape[0]).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit_data.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "plt.ylabel('Null values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tidying the data\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to all features and the respective percentage of missing values, we can conclude that some of them (like *verification_status_joint*, *annual_inc_joint*, *dti_joint*, *il_util*, *mths_since_rcnt_il*, *total_bal_il*, *inq_last_12m*, *open_acc_6m*, *open_il_6m*, *open_il_24m*, *open_il_12m*, *open_rv_12m*, *open_rv_24m*, *max_bal_bc*, *all_util*, *inq_fi*, *total_cu_tl* have almost all entries missing. As this feature is not crucial for the project, we are dropping it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns\n",
    "credit_data.drop(['verification_status_joint', 'annual_inc_joint', 'dti_joint', 'il_util', 'mths_since_rcnt_il',\n",
    "                   'total_bal_il', 'inq_last_12m', 'open_acc_6m', 'open_il_6m', 'open_il_24m', 'open_il_12m', 'open_rv_12m',\n",
    "                   'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the number of unique values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_code                         1\n",
       "default_ind                         2\n",
       "application_type                    2\n",
       "term                                2\n",
       "pymnt_plan                          2\n",
       "initial_list_status                 2\n",
       "next_pymnt_d                        3\n",
       "verification_status                 3\n",
       "home_ownership                      6\n",
       "grade                               7\n",
       "acc_now_delinq                      8\n",
       "inq_last_6mths                      9\n",
       "emp_length                         11\n",
       "collections_12_mths_ex_med         12\n",
       "purpose                            14\n",
       "delinq_2yrs                        29\n",
       "pub_rec                            31\n",
       "sub_grade                          35\n",
       "addr_state                         51\n",
       "open_acc                           77\n",
       "last_pymnt_d                       97\n",
       "last_credit_pull_d                102\n",
       "issue_d                           103\n",
       "mths_since_last_record            123\n",
       "total_acc                         134\n",
       "mths_since_last_delinq            155\n",
       "mths_since_last_major_derog       167\n",
       "int_rate                          520\n",
       "earliest_cr_line                  697\n",
       "zip_code                          931\n",
       "revol_util                       1338\n",
       "loan_amnt                        1368\n",
       "funded_amnt                      1368\n",
       "dti                              4074\n",
       "total_rec_late_fee               5320\n",
       "funded_amnt_inv                  8818\n",
       "tot_coll_amt                    10138\n",
       "collection_recovery_fee         20386\n",
       "total_rev_hi_lim                20793\n",
       "recoveries                      22627\n",
       "annual_inc                      48163\n",
       "title                           60954\n",
       "installment                     67349\n",
       "revol_bal                       72779\n",
       "desc                           120335\n",
       "last_pymnt_amnt                230321\n",
       "out_prncp                      236739\n",
       "total_rec_prncp                250385\n",
       "out_prncp_inv                  254328\n",
       "emp_title                      290910\n",
       "total_rec_int                  317683\n",
       "tot_cur_bal                    322286\n",
       "total_pymnt                    493510\n",
       "total_pymnt_inv                494744\n",
       "member_id                      855969\n",
       "id                             855969\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique observations per column\n",
    "credit_data.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature *policy_code* only has one value, so we can conclude that this one is not important for our analysis and we can also drop this collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column \"policy_code\"\n",
    "credit_data.drop('policy_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*id* and *member_id* features are randomly generated fields by bank for unique identification purposes only, so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"id\" and \"member_id\"\n",
    "credit_data.drop(['id', 'member_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features like *funded_amnt*, *funded_amnt_inv*, *mths_since_last_record*, *out_prncp*, *out_prncp_inv*, *total_pymnt*, *total_pymnt_inv*, *total_rec_prncp*, *total_rec_int*, *total_rec_late_fee*, *recoveries*, *collection_recovery_fee*, *last_pymnt_d*, *last_pymnt_amnt*, *next_pymnt_d* can be dropped because they leak data from future, after the loan has already started to be funded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "credit_data.drop(['funded_amnt', 'funded_amnt_inv', 'mths_since_last_record', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "                   'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', \n",
    "                   'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*emp_title* feature requires other data and a lot of processing to become potentially useful, so we opt to drop that one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"emp_title\" column\n",
    "credit_data.drop('emp_title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also *desc*, *initial_list_status*, *total_rev_hi_lim* features doesn't add value to our model, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"desc\", \"initial_list_status\", \"total_rev_hi_lim\" columns\n",
    "credit_data.drop(['desc', 'initial_list_status', 'total_rev_hi_lim'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*zip_code* feature is mostly redundant since only the first 3 digits of the 5 digit zip code are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"zip_code\" column\n",
    "credit_data.drop('zip_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*addr_state* collumn seems like to contain categorical values. Let's explore the unique value counts of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA    125172\n",
      "NY     71114\n",
      "TX     68708\n",
      "FL     58639\n",
      "IL     34379\n",
      "NJ     32061\n",
      "PA     30250\n",
      "OH     28651\n",
      "GA     28043\n",
      "VA     25234\n",
      "NC     23787\n",
      "MI     22175\n",
      "MD     20209\n",
      "MA     19835\n",
      "AZ     19693\n",
      "WA     18816\n",
      "CO     18211\n",
      "MN     15424\n",
      "MO     13743\n",
      "IN     13348\n",
      "CT     13042\n",
      "TN     12392\n",
      "NV     11972\n",
      "WI     11199\n",
      "AL     10732\n",
      "OR     10562\n",
      "SC     10302\n",
      "LA     10186\n",
      "KY      8299\n",
      "OK      7759\n",
      "KS      7693\n",
      "AR      6399\n",
      "UT      6055\n",
      "NM      4757\n",
      "HI      4380\n",
      "WV      4247\n",
      "NH      4156\n",
      "RI      3753\n",
      "MS      3653\n",
      "MT      2466\n",
      "DE      2402\n",
      "DC      2382\n",
      "AK      2134\n",
      "WY      1974\n",
      "VT      1753\n",
      "SD      1745\n",
      "NE      1124\n",
      "ME       491\n",
      "ND       452\n",
      "ID         9\n",
      "IA         7\n",
      "Name: addr_state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(credit_data['addr_state'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *addr_state* column contains too many unique values, so it’s better to drop this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"addr_state\" column\n",
    "credit_data.drop('addr_state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at *pymnt_plan* feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    855964\n",
      "y         5\n",
      "Name: pymnt_plan, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(credit_data['pymnt_plan'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this column has two unique values, *y* and *n*, with *y* occurring only 5 times. Let’s drop this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"pymnt_plan\" column\n",
    "credit_data.drop('pymnt_plan', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at *application_type* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIVIDUAL    855527\n",
      "JOINT            442\n",
      "Name: application_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(credit_data['application_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this column only has two unique values, *INDIVIDUAL* and *JOINT*, with *JOINT* occurring only 442 times. Let's drop this column too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"application_type\" column\n",
    "credit_data.drop('application_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s look at the unique value counts for the *purpose* and *title* columns to understand which columns we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in column: purpose\n",
      "\n",
      "debt_consolidation    505392\n",
      "credit_card           200144\n",
      "home_improvement       49956\n",
      "other                  40949\n",
      "major_purchase         16587\n",
      "small_business          9785\n",
      "car                     8593\n",
      "medical                 8193\n",
      "moving                  5160\n",
      "vacation                4542\n",
      "house                   3513\n",
      "wedding                 2280\n",
      "renewable_energy         549\n",
      "educational              326\n",
      "Name: purpose, dtype: int64 \n",
      "\n",
      "Unique Values in column: title\n",
      "\n",
      "Debt consolidation                     398089\n",
      "Credit card refinancing                159228\n",
      "Home improvement                        38633\n",
      "Other                                   30522\n",
      "Debt Consolidation                      15469\n",
      "                                        ...  \n",
      "Looking to consolidate credit cards         1\n",
      "BestLoan2012                                1\n",
      "intravenous sedation equipment              1\n",
      "Contract Capital Loan                       1\n",
      "Auto Loan 1                                 1\n",
      "Name: title, Length: 60954, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in ['purpose','title']:\n",
    "    print(\"Unique Values in column: {}\\n\".format(name))\n",
    "    print(credit_data[name].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the *purpose* and *title* columns do contain overlapping information. *title* column contains too many different values with distinct information, so it is hard to analyse its values. On the other hand, *purpose* has many unique values and they are nominal so, to convert them to numerical in order to analyse them will create many new collumns and difficult our job.\n",
    "Taking this into account, we opt to drop both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"title\" and \"purpose\" column\n",
    "credit_data.drop(['title', 'purpose'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the features that we still have, we see that we have two that are very similar: *grade* and *sub_grade*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in column: grade\n",
      "\n",
      "B    247998\n",
      "C    236855\n",
      "A    145665\n",
      "D    132802\n",
      "E     66448\n",
      "F     21328\n",
      "G      4873\n",
      "Name: grade, dtype: int64 \n",
      "\n",
      "Unique Values in column: sub_grade\n",
      "\n",
      "B3    54958\n",
      "B4    54116\n",
      "C1    51588\n",
      "C2    50457\n",
      "C3    48337\n",
      "B2    47589\n",
      "B5    47333\n",
      "C4    46941\n",
      "B1    44002\n",
      "A5    43957\n",
      "C5    39532\n",
      "D1    34667\n",
      "A4    33991\n",
      "D2    28385\n",
      "D3    25289\n",
      "D4    24253\n",
      "A3    23095\n",
      "A1    22516\n",
      "A2    22106\n",
      "D5    20208\n",
      "E1    17230\n",
      "E2    15994\n",
      "E3    13294\n",
      "E4    11011\n",
      "E5     8919\n",
      "F1     6702\n",
      "F2     4979\n",
      "F3     4099\n",
      "F4     3168\n",
      "F5     2380\n",
      "G1     1698\n",
      "G2     1259\n",
      "G3      877\n",
      "G4      570\n",
      "G5      469\n",
      "Name: sub_grade, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in ['grade','sub_grade']:\n",
    "    print(\"Unique Values in column: {}\\n\".format(name))\n",
    "    print(credit_data[name].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that *sub_grade* contains redundant information that is already in the *grade* column, so we can also drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"sub_grade\" column\n",
    "credit_data.drop('sub_grade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "loan_amnt                           0\n",
      "term                                0\n",
      "int_rate                            0\n",
      "installment                         0\n",
      "grade                               0\n",
      "emp_length                      43061\n",
      "home_ownership                      0\n",
      "annual_inc                          0\n",
      "verification_status                 0\n",
      "issue_d                             0\n",
      "dti                                 0\n",
      "delinq_2yrs                         0\n",
      "earliest_cr_line                    0\n",
      "inq_last_6mths                      0\n",
      "mths_since_last_delinq         439812\n",
      "open_acc                            0\n",
      "pub_rec                             0\n",
      "revol_bal                           0\n",
      "revol_util                        446\n",
      "total_acc                           0\n",
      "last_credit_pull_d                 50\n",
      "collections_12_mths_ex_med         56\n",
      "mths_since_last_major_derog    642830\n",
      "acc_now_delinq                      0\n",
      "tot_coll_amt                    67313\n",
      "tot_cur_bal                     67313\n",
      "default_ind                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = credit_data.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*mths_since_last_major_derog* column has to many null values, in the order of 75%, so we will drop this one to. We also drop *mths_since_last_delinq* column as it has a high  percentage of null valuess too, in order of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"mths_since_last_major_derog\" and \"mths_since_last_delinq\" columns\n",
    "credit_data.drop(['mths_since_last_major_derog', 'mths_since_last_delinq'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now investigate columns that are of the **object** data type and figure out how we can make those values numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                    36 months\n",
      "grade                           B\n",
      "emp_length              10+ years\n",
      "home_ownership               RENT\n",
      "verification_status      Verified\n",
      "issue_d                01-12-2011\n",
      "earliest_cr_line       01-01-1985\n",
      "last_credit_pull_d     01-01-2016\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "object_columns_df = credit_data.select_dtypes(include=['object'])\n",
    "print(object_columns_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns seem to represent categorical values:\n",
    "    * term\n",
    "    * grade\n",
    "    * emp_length\n",
    "    * home_ownership\n",
    "    * verification_status\n",
    "    * earliest_cr_line\n",
    "    * last_credit_pull_d\n",
    "    * issue_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the other categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term :\n",
      " 36 months    600221\n",
      " 60 months    255748\n",
      "Name: term, dtype: int64 \n",
      "\n",
      "grade :\n",
      "B    247998\n",
      "C    236855\n",
      "A    145665\n",
      "D    132802\n",
      "E     66448\n",
      "F     21328\n",
      "G      4873\n",
      "Name: grade, dtype: int64 \n",
      "\n",
      "emp_length :\n",
      "10+ years    282090\n",
      "2 years       75986\n",
      "< 1 year      67597\n",
      "3 years       67392\n",
      "1 year        54855\n",
      "5 years       53812\n",
      "4 years       50643\n",
      "7 years       43204\n",
      "8 years       42421\n",
      "6 years       41446\n",
      "9 years       33462\n",
      "Name: emp_length, dtype: int64 \n",
      "\n",
      "home_ownership :\n",
      "MORTGAGE    429106\n",
      "RENT        342535\n",
      "OWN          84136\n",
      "OTHER          144\n",
      "NONE            45\n",
      "ANY              3\n",
      "Name: home_ownership, dtype: int64 \n",
      "\n",
      "verification_status :\n",
      "Source Verified    318178\n",
      "Verified           280049\n",
      "Not Verified       257742\n",
      "Name: verification_status, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['term', 'grade', 'emp_length', 'home_ownership', 'verification_status']\n",
    "for name in cols:\n",
    "    print(name,':')\n",
    "    print(credit_data[name].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*grade*, *emp_length* and *term* columns contain ordinal values, i.e. they are in natural order and we can sort or order them either in increasing or decreasing order. For this reason, we can change the values of this columns to the appropriate numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip months from 'term' and make it an int\n",
    "credit_data['term'] = credit_data['term'].str.split(' ').str[1]\n",
    "\n",
    "\n",
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0\n",
    "    },\n",
    "    \"grade\": {\n",
    "        \"A\": 1,\n",
    "        \"B\": 2,\n",
    "        \"C\": 3,\n",
    "        \"D\": 4,\n",
    "        \"E\": 5,\n",
    "        \"F\": 6,\n",
    "        \"G\": 7\n",
    "    },\n",
    "    \"term\": {\n",
    "        \"36\": 36.0,\n",
    "        \"60\": 60.0\n",
    "    }\n",
    "}\n",
    "credit_data = credit_data.replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emp_length</th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emp_length  grade  term\n",
       "0        10.0      2  36.0\n",
       "1         0.0      3  60.0\n",
       "2        10.0      3  36.0\n",
       "3        10.0      3  36.0\n",
       "4         1.0      2  60.0\n",
       "5         3.0      1  36.0\n",
       "6         8.0      3  60.0\n",
       "7         9.0      5  36.0\n",
       "8         4.0      6  60.0\n",
       "9         0.0      2  60.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data[['emp_length', 'grade', 'term']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*verification_status* and *home_ownership* features contain nominal values, so we can't order them. In this case, we will have to convert them to numerical values using dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   verification_status_Not Verified  verification_status_Source Verified  \\\n",
      "0                                 0                                    0   \n",
      "1                                 0                                    1   \n",
      "2                                 1                                    0   \n",
      "3                                 0                                    1   \n",
      "4                                 0                                    1   \n",
      "\n",
      "   verification_status_Verified  home_ownership_ANY  home_ownership_MORTGAGE  \\\n",
      "0                             1                   0                        0   \n",
      "1                             0                   0                        0   \n",
      "2                             0                   0                        0   \n",
      "3                             0                   0                        0   \n",
      "4                             0                   0                        0   \n",
      "\n",
      "   home_ownership_NONE  home_ownership_OTHER  home_ownership_OWN  \\\n",
      "0                    0                     0                   0   \n",
      "1                    0                     0                   0   \n",
      "2                    0                     0                   0   \n",
      "3                    0                     0                   0   \n",
      "4                    0                     0                   0   \n",
      "\n",
      "   home_ownership_RENT  \n",
      "0                    1  \n",
      "1                    1  \n",
      "2                    1  \n",
      "3                    1  \n",
      "4                    1  \n"
     ]
    }
   ],
   "source": [
    "# Converting nominal features into numerical features by encoding them as dummy variables\n",
    "nominal_columns = [\"verification_status\", \"home_ownership\"]\n",
    "dummy_loan = pd.get_dummies(credit_data[nominal_columns])\n",
    "print(dummy_loan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the dummy variables into the original DataFrame, drop nominal columns\n",
    "credit_data = pd.concat([credit_data, dummy_loan], axis=1)\n",
    "credit_data = credit_data.drop(nominal_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>default_ind</th>\n",
       "      <th>verification_status_Not Verified</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>home_ownership_ANY</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>01-12-2011</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>01-12-2011</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>01-12-2011</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>01-12-2011</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.69</td>\n",
       "      <td>67.79</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>01-12-2011</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  term  int_rate  installment  grade  emp_length  annual_inc  \\\n",
       "0       5000  36.0     10.65       162.87      2        10.0     24000.0   \n",
       "1       2500  60.0     15.27        59.83      3         0.0     30000.0   \n",
       "2       2400  36.0     15.96        84.33      3        10.0     12252.0   \n",
       "3      10000  36.0     13.49       339.31      3        10.0     49200.0   \n",
       "4       3000  60.0     12.69        67.79      2         1.0     80000.0   \n",
       "\n",
       "      issue_d    dti  delinq_2yrs  ... default_ind  \\\n",
       "0  01-12-2011  27.65            0  ...           0   \n",
       "1  01-12-2011   1.00            0  ...           1   \n",
       "2  01-12-2011   8.72            0  ...           0   \n",
       "3  01-12-2011  20.00            0  ...           0   \n",
       "4  01-12-2011  17.94            0  ...           0   \n",
       "\n",
       "   verification_status_Not Verified  verification_status_Source Verified  \\\n",
       "0                                 0                                    0   \n",
       "1                                 0                                    1   \n",
       "2                                 1                                    0   \n",
       "3                                 0                                    1   \n",
       "4                                 0                                    1   \n",
       "\n",
       "   verification_status_Verified  home_ownership_ANY  home_ownership_MORTGAGE  \\\n",
       "0                             1                   0                        0   \n",
       "1                             0                   0                        0   \n",
       "2                             0                   0                        0   \n",
       "3                             0                   0                        0   \n",
       "4                             0                   0                        0   \n",
       "\n",
       "   home_ownership_NONE home_ownership_OTHER  home_ownership_OWN  \\\n",
       "0                    0                    0                   0   \n",
       "1                    0                    0                   0   \n",
       "2                    0                    0                   0   \n",
       "3                    0                    0                   0   \n",
       "4                    0                    0                   0   \n",
       "\n",
       "   home_ownership_RENT  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*earliest_cr_line*, *issue_d*, *last_credit_pull_d* columns contain date values. Let's convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-04-01</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-02-01</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  earliest_cr_line    issue_d last_credit_pull_d\n",
       "0       1985-01-01 2011-12-01         2016-01-01\n",
       "1       1999-04-01 2011-12-01         2013-09-01\n",
       "2       2001-11-01 2011-12-01         2016-01-01\n",
       "3       1996-02-01 2011-12-01         2015-01-01\n",
       "4       1996-01-01 2011-12-01         2016-01-01"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date inputs\n",
    "cols = [\"earliest_cr_line\",\"issue_d\",\"last_credit_pull_d\"]\n",
    "for col in cols:\n",
    "    credit_data[col] = pd.to_datetime(credit_data[col],format=\"%d-%m-%Y\")\n",
    "credit_data[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check if still exist columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "loan_amnt                                  0\n",
      "term                                       0\n",
      "int_rate                                   0\n",
      "installment                                0\n",
      "grade                                      0\n",
      "emp_length                             43061\n",
      "annual_inc                                 0\n",
      "issue_d                                    0\n",
      "dti                                        0\n",
      "delinq_2yrs                                0\n",
      "earliest_cr_line                           0\n",
      "inq_last_6mths                             0\n",
      "open_acc                                   0\n",
      "pub_rec                                    0\n",
      "revol_bal                                  0\n",
      "revol_util                               446\n",
      "total_acc                                  0\n",
      "last_credit_pull_d                        50\n",
      "collections_12_mths_ex_med                56\n",
      "acc_now_delinq                             0\n",
      "tot_coll_amt                           67313\n",
      "tot_cur_bal                            67313\n",
      "default_ind                                0\n",
      "verification_status_Not Verified           0\n",
      "verification_status_Source Verified        0\n",
      "verification_status_Verified               0\n",
      "home_ownership_ANY                         0\n",
      "home_ownership_MORTGAGE                    0\n",
      "home_ownership_NONE                        0\n",
      "home_ownership_OTHER                       0\n",
      "home_ownership_OWN                         0\n",
      "home_ownership_RENT                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = credit_data.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are some missing values, let's deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values for 'revol_util' with the median 'revol_util'.\n",
    "credit_data.revol_util.fillna(credit_data.revol_util.median(), inplace=True)\n",
    "\n",
    "# Fill in the missing values for 'collections_12_mths_ex_med' with the median 'collections_12_mths_ex_med'.\n",
    "credit_data.collections_12_mths_ex_med.fillna(credit_data.collections_12_mths_ex_med.median(), inplace=True)\n",
    "\n",
    "# Fill in the missing values for 'tot_coll_amt' with the median 'tot_coll_amt'.\n",
    "credit_data.tot_coll_amt.fillna(credit_data.tot_coll_amt.median(), inplace=True)\n",
    "\n",
    "# Fill in the missing values for 'tot_cur_bal' with the median 'tot_cur_bal'.\n",
    "credit_data.tot_cur_bal.fillna(credit_data.tot_cur_bal.median(), inplace=True)\n",
    "\n",
    "# Fill in the missing values for 'emp_length' with the 0.\n",
    "credit_data.emp_length.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "loan_amnt                               0\n",
      "term                                    0\n",
      "int_rate                                0\n",
      "installment                             0\n",
      "grade                                   0\n",
      "emp_length                              0\n",
      "annual_inc                              0\n",
      "issue_d                                 0\n",
      "dti                                     0\n",
      "delinq_2yrs                             0\n",
      "earliest_cr_line                        0\n",
      "inq_last_6mths                          0\n",
      "open_acc                                0\n",
      "pub_rec                                 0\n",
      "revol_bal                               0\n",
      "revol_util                              0\n",
      "total_acc                               0\n",
      "last_credit_pull_d                     50\n",
      "collections_12_mths_ex_med              0\n",
      "acc_now_delinq                          0\n",
      "tot_coll_amt                            0\n",
      "tot_cur_bal                             0\n",
      "default_ind                             0\n",
      "verification_status_Not Verified        0\n",
      "verification_status_Source Verified     0\n",
      "verification_status_Verified            0\n",
      "home_ownership_ANY                      0\n",
      "home_ownership_MORTGAGE                 0\n",
      "home_ownership_NONE                     0\n",
      "home_ownership_OTHER                    0\n",
      "home_ownership_OWN                      0\n",
      "home_ownership_RENT                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = credit_data.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_ownership_RENT                    0\n",
      "home_ownership_OWN                     0\n",
      "term                                   0\n",
      "int_rate                               0\n",
      "installment                            0\n",
      "grade                                  0\n",
      "emp_length                             0\n",
      "annual_inc                             0\n",
      "issue_d                                0\n",
      "dti                                    0\n",
      "delinq_2yrs                            0\n",
      "earliest_cr_line                       0\n",
      "inq_last_6mths                         0\n",
      "open_acc                               0\n",
      "pub_rec                                0\n",
      "revol_bal                              0\n",
      "revol_util                             0\n",
      "total_acc                              0\n",
      "last_credit_pull_d                     0\n",
      "collections_12_mths_ex_med             0\n",
      "acc_now_delinq                         0\n",
      "tot_coll_amt                           0\n",
      "tot_cur_bal                            0\n",
      "default_ind                            0\n",
      "verification_status_Not Verified       0\n",
      "verification_status_Source Verified    0\n",
      "verification_status_Verified           0\n",
      "home_ownership_ANY                     0\n",
      "home_ownership_MORTGAGE                0\n",
      "home_ownership_NONE                    0\n",
      "home_ownership_OTHER                   0\n",
      "loan_amnt                              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "credit_data = credit_data.dropna()\n",
    "print(credit_data.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 855919 entries, 0 to 855968\n",
      "Data columns (total 32 columns):\n",
      " #   Column                               Non-Null Count   Dtype         \n",
      "---  ------                               --------------   -----         \n",
      " 0   loan_amnt                            855919 non-null  int64         \n",
      " 1   term                                 855919 non-null  float64       \n",
      " 2   int_rate                             855919 non-null  float64       \n",
      " 3   installment                          855919 non-null  float64       \n",
      " 4   grade                                855919 non-null  int64         \n",
      " 5   emp_length                           855919 non-null  float64       \n",
      " 6   annual_inc                           855919 non-null  float64       \n",
      " 7   issue_d                              855919 non-null  datetime64[ns]\n",
      " 8   dti                                  855919 non-null  float64       \n",
      " 9   delinq_2yrs                          855919 non-null  int64         \n",
      " 10  earliest_cr_line                     855919 non-null  datetime64[ns]\n",
      " 11  inq_last_6mths                       855919 non-null  int64         \n",
      " 12  open_acc                             855919 non-null  int64         \n",
      " 13  pub_rec                              855919 non-null  int64         \n",
      " 14  revol_bal                            855919 non-null  int64         \n",
      " 15  revol_util                           855919 non-null  float64       \n",
      " 16  total_acc                            855919 non-null  int64         \n",
      " 17  last_credit_pull_d                   855919 non-null  datetime64[ns]\n",
      " 18  collections_12_mths_ex_med           855919 non-null  float64       \n",
      " 19  acc_now_delinq                       855919 non-null  int64         \n",
      " 20  tot_coll_amt                         855919 non-null  float64       \n",
      " 21  tot_cur_bal                          855919 non-null  float64       \n",
      " 22  default_ind                          855919 non-null  int64         \n",
      " 23  verification_status_Not Verified     855919 non-null  uint8         \n",
      " 24  verification_status_Source Verified  855919 non-null  uint8         \n",
      " 25  verification_status_Verified         855919 non-null  uint8         \n",
      " 26  home_ownership_ANY                   855919 non-null  uint8         \n",
      " 27  home_ownership_MORTGAGE              855919 non-null  uint8         \n",
      " 28  home_ownership_NONE                  855919 non-null  uint8         \n",
      " 29  home_ownership_OTHER                 855919 non-null  uint8         \n",
      " 30  home_ownership_OWN                   855919 non-null  uint8         \n",
      " 31  home_ownership_RENT                  855919 non-null  uint8         \n",
      "dtypes: datetime64[ns](3), float64(10), int64(10), uint8(9)\n",
      "memory usage: 164.1 MB\n"
     ]
    }
   ],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Analysis\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNIAAAWRCAYAAABUtLqlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl4Def///FXRBI0ltoFVaUotUaCaqtVLbVWlNopV/WjoeWri35Quri0pZvoph+qltoiStFSVNVSse+kKJKoSCSRECLJOb8/8nOunJNtMoMkPB/XlYszZ95z33POnEnyyj33uNntdrsAAAAAAAAA5KhIfncAAAAAAAAAKAwI0gAAAAAAAAADCNIAAAAAAAAAAwjSAAAAAAAAAAMI0gAAAAAAAAADCNIAAAAAAAAAAwjSAACAZQMGDFDdunUdX2PHjs3vLuEO5XqshYSE3NL2UlJStH///iyfGzt2rFNfgoKCbmlfCouQkBCn16Vu3bq51ri+lgMGDLgNPbVu165dpmtdX6MbX++8806OdU899VSWdYXlNQOAwo4gDQAAAMjC5s2b1aVLFy1cuDC/u4IC5uTJkxo6dKjefPPNm77tv/76K9vnIiMjFRERcdPbBAAYR5AGAAAAZBAREaH//Oc/eumll/TPP//kd3dQgCQlJemjjz5St27dtGXLllvSxpkzZ3Tu3Lksn9u+ffstaRMAYFzR/O4AAAAAUJAsX75cv//+e67rPf/88/L393c8fuihh25lt1AAHDx4ULNnz77l7Wzfvl09evTItDyn0WoAgNuDIA0AAAAwoXnz5mrevHl+dwN3iCJFishms0nKPkjbsWOH4//u7u5KS0u7bf0DAKTj0k4AAAAAyGcZRzRmNfLs5MmTunDhguNxvXr1bku/AADOGJEGAAAKpLi4OK1cuVLr1q1TZGSkYmJiVKpUKdWsWVNt2rTRCy+8oNKlS+e4jYSEBC1cuFB//PGHTp06pcuXL0uSSpYsqRo1auiRRx7RoEGDstzO2LFjtXz5csfj4OBgNWzYUFu2bNH8+fN18OBBJSQkqFKlSnrsscc0dOhQVatWzdI+X7t2TcHBwdq4caPCwsIUHx8vDw8PVatWTX5+furTp48efPDBHLdx/fp1rV+/XitXrtSpU6d0/vx5eXl5ycfHR61atdILL7ygmjVrZlkbERGhp556yvH4iSee0IwZMzR9+nQtX75c8fHxKlu2rB577DFNnjxZISEhevvttx3rv/nmm+ratasmT56sLVu2KCUlRZUrV9bAgQPVr18/x3p2u12rV6/Wzz//rEOHDunSpUsqWbKkatWqpbZt2+qFF17QPffcY+m1jIqK0o8//qht27bpzJkzunLlitzc3FSmTBnVrFlTTzzxhPr166dixYo5alz354bly5c7joURI0Zo5MiRkjIfIxmfc5WWlqY///xTy5cvV1hYmM6fP68iRYqoUqVK8vf3V48ePdSwYcNs96dt27aKjIyUJJUoUUJ79+5VWlqagoODtXz5cp08eVKpqam677771KFDBw0cODDb1/Dy5csKDg7Wpk2bdOLECV26dEl2u13e3t66//771apVK/Xp00cVK1bM5VXOH3FxcZo7d642bdqks2fPKiUlReXLl1fTpk3VtWtXtWnTJsf6a9euaenSpdqwYYNOnDih+Ph4x/5XrVpVLVq00KBBg1S5cmVHjetn44bIyEjHXUn9/f01b9480/vl5+enw4cPS5Kio6N14sQJ1a5d2/G8a7jWokULx/o5SUtL08qVK/Xrr7/q2LFjiouLU2pqqkqUKKEqVarI19dXAwYMUK1atTLVBgUFacaMGY7Hn376qTp16qRNmzZpwYIFOnz4sK5cuSIfHx899dRTGjp0qO69916zLwEAFAoEaQAAoMD56aef9O677yopKclp+cWLF3Xx4kXt2rVL3333nSZOnKjOnTtnuY0jR45o2LBhio6OzvRcbGysYmNjtXfvXi1ZskQzZ85U/fr1c+yT3W7XxIkTtWjRIqfl4eHh+vHHH7V8+XJ999138vPzy+Pepjtw4IBGjhyp8+fPOy1PSUlRWFiYwsLCtGjRIo0aNUrDhg3LchsHDx7U//3f/+ns2bNOy5OTk5WQkKBjx45p7ty5Gjx4sMaMGSN3d/cc+2S32zVq1CitX7/esSwqKkpFi2b9I2R8fLz69Omj8PBwx7LTp087/WIdGxurESNGaPfu3U61N96TnTt3avbs2Zo+fbqaNWuWY/+ys2XLFr366qu6cuVKpueio6MVHR2t0NBQLV68WHPmzJGPj4+pdow6c+aMRo8enWXocfnyZZ08eVILFy5U9+7dNWnSJKdwLzuxsbF65ZVXtHfvXqflx44d07Fjx/TTTz9p/vz5qlChQqbnX3755UzHmZQeUMXFxWnv3r2aM2eOPv/881xDqdtt8+bNGjNmjBISEpyWR0ZGKjIyUqtWrdITTzyhTz75RN7e3pnqIyMjNWTIEJ0+fTrTc/Hx8YqPj9fhw4e1ZMkSff7553rsscdu1a5k4ufnpzlz5jgeb9++3SlIy3hZZ5EiReTr65vrfG2XLl3SsGHDtG/fvkzPJSYmKjExUWFhYQoODtZ7772ngICAHLdns9k0fvx4LV261Gn5qVOndOrUKS1dulRff/216c8uABQGXNoJAAAKlG+++UZvvfVWphDNVUJCgsaMGaO5c+dmeu7y5csaPnx4liGaq+joaL322muOuYmyM3Xq1EwhWkZXr17VW2+9ZWrOomPHjmngwIFZhhsZpaWl6ZNPPsn0S6yU/kt2nz59MoVoWW1j1qxZGj16tOx2e47rbtu2zSlEu6FLly5Zrv/99987hWhS+uipJ598UlL6SKBBgwZlCtFcRUdHa/DgwTp69GiO62Xl3Llzeu2117IM0VydOXNGb731Vp7byIuTJ08qICDA0Mih5cuXa/DgwUpOTs5xvbS0NP3nP//JFKJldPr0aX388cdOy65evarhw4fnepxJ6XenfO211wrUXUt37Nih4cOHZwrRXG3atEnDhg3L9Fm02WwaOXJkliGaq8uXL2v06NFKTEy00uU8ad68udzc3ByPM96h0263OwVp9erVU8mSJXPd5ttvv51liOYqJSVF77zzjs6cOZPjejNnzszy/HNDfHy8hg4dWqCOGwC42RiRBgAACoxdu3Zp+vTpTsvKli2rHj16qHr16oqIiFBwcLBiY2Mdz3/44Ydq0KCBfH19HcsWLlzoFBZ4e3urV69eqlmzpq5evarNmzdry5YtjufPnj2rsLCwHOccCg0NlZR+iV2rVq10+fJl/fjjj05hXWRkpPbs2ZOnUWl2u11vvPGGrl696ljm5eWl5557TvXr11dMTIyWLl3qtD9Tp05Vhw4dHL9Ix8XF6fXXX1dKSopjHQ8PD3Xr1k0PP/ywEhIStGrVKoWFhTmeX7t2rb777rtsR7dJcmyvefPm6tChg6KiorR161an1zqr9Z977jk1atRIhw4dkpubm4oXLy5J+uKLL5z64OHhoS5duqhBgwaKj4/XihUrHEFgcnKy3nrrLa1cudLYC/n/zZw503EJryRVqFBBPXv2lI+PjxITE7V27VqnYGHnzp1KTExUyZIl5evrqylTpmj9+vXasGGDY51mzZqpZ8+ekvJ2Z87r169r9OjRTv1xc3NT+/bt5e/vr+TkZK1bt84pENu7d68mT56s9957L9vtJicna//+/SpWrJi6deumevXq6fTp01q4cKGuX7/uWG/t2rWaPHmyPD09JUkbN27UuXPnHM/fc8896tq1qx544AEVLVpUJ0+eVEhIiCPEvnr1qr788ktNmzbN8D5n58blj2Zdv35db7zxhlJTUx3LqlSpoueee04VKlTQ8ePHtXz5csf+7969W3PnztWLL77oWP/XX391CjQ9PDzUq1cv1alTR6mpqdqzZ49Wr17teD4xMVGhoaF66qmndO+992rKlCk6deqUvvvuO8c6ZcqUcYSx5cuXt7SPZcqUUd26dXXs2DFJ6eectLQ0ubu76+jRo4qPj3esm/Fusdk5cOCA03Hs5uambt26qXHjxpKko0ePatmyZY7AMSUlRX/88YcGDhyY7TZvfH6bNGmiDh06yMPDQxs2bNC2bdsc6yQlJemdd96xdJkrABRkBGkAAKDA+OKLL5xGkdSuXVtz585VuXLlHMsGDx6sgQMH6sSJE5LSR+dMmzZNCxcudKxTpEgRNW3aVGFhYbpy5YqmT5+u1q1bO54fOHCgunXrpuPHjzuWRURE5Dp594QJE9S/f3/H444dO6pz585OAdaJEyfyFKRt3rzZKVwqWrSovv/+e6ewqk+fPurWrZsjtLt06ZJWr16t3r17S5IWLFjgNAl5iRIl9MMPP6hRo0aOZUOHDtWbb77pFBR888036tWrl8qUKZNt/+rVq6c5c+bIw8NDkjRmzBinUTOu+vbtq4kTJzoe3xj1lpiY6DSiz93dXf/73//UsmVLx7Jhw4apT58+OnTokCTp+PHj2r59u1q1apVte66KFy+uBg0aOOYM++GHH5zmfurbt6/atm2rixcvOvoXGRmpevXqqUaNGqpRo4YiIyOdAogaNWrkeslbVlavXu10jLm7uysoKMhprq0hQ4bo448/1qxZsxzLgoODNWjQoCznrLrBy8tL8+fPd5pXrV69ek5zvCUnJys8PNyxnYiICKdtjBgxQkOGDHFa1qlTJ7344ouqUaOGateurYcffjiPe31rrFixQlFRUY7HderU0eLFi1WiRAnHsoCAAPXv39/xeZw3b54GDRqkIkXSL8JJTk6Wv7+/Y/7Bd955R7169XLU9+/fX0lJSfr9998dy268Zvfcc48CAgK0Y8cOpyDtxvKbxc/PzxGkJSYm6vDhw2rUqFGm+dGMnGPi4uLUunVrHT9+XDExMXr55Zc1evRop3U8PT01f/58x2PXYyQrPXv21Pvvv+84D/Tv31/Tpk1zel1CQ0N17NgxbogA4I7EpZ0AAKBACA8P186dO52WTZ482SlEk6Ry5cpp8uTJTsv27NnjdEnh0KFDtWjRIu3evVu///67U4gmyTEJfka5XUr6wAMPOE2YL0n3339/pqAhr5eC/fHHH06P27Vrl2nEV7ly5Ryhx0cffaQVK1aoR48ejudDQkKc1h8+fLhTiCalB3QffPCBU2h25coVp8AoK71793aEaJJyDNEkOQWNGdcPDQ11eo2bNGniFKJJ6b/Uu9bn1j9Xb731lkJCQrRnzx6tX78+Uxhls9kyjRwychmoGcuWLXN6/Pzzz2c5Yf3rr7/u1M+0tDSnwDMrzz33XKabE3Tq1MkRGt2Q8XisVKmS03Pz58/X0qVLnUZVNmvWTPv27dPKlSv16aefavDgwTn243Zx/ZwMHDjQKUST0o+ppk2bOh5HRkY6QilJ6t69u+bNm6cdO3Zo06ZNTp8hKf3YcL3BQm7nhZvNdaTZjcs7MwZpbm5uat68ea7batOmjWbPnq2tW7dq27ZtGj58eKZ1XD8Lue1vyZIlNW7cuEzngVGjRmWajy/jpakAcCdhRBoAACgQDh486DRnV5UqVdSkSZMs123SpImqVKmif//917Fs9+7dql69utN6bm5u8vHx0aVLl7R//34dOnRI+/fv186dOzOFJ7nNbdakSZMsQyTXX7wzXlpnhOs8YBmDgIxujD5zFRMT47ib4w3PPvtsluuWKFFCbdq00YoVKxzLdu/enSlQyMg1kMvJPffck+0oKtc5wo4dO6YOHTpkWu/atWs51hnl7u6uKlWqKCYmRvv27dOhQ4e0b98+7dmzJ9McZGbmtTPi4MGDTo+ze1+KFCmi9u3b66uvvnIsy20euayOEy8vL5UqVcrpEsCMx2O7du1UoUIFR3AWGRmp8ePHS5Jq1qyppk2bys/PT61bt84Uulk1ZcqUHJ9funSp9uzZk+3zrsfBjBkznEbx3eA6L+Lhw4ezvJFIlSpVlJSUpAMHDjjOC6GhoU6vnXTrjo3s3Jgn7ca5cPv27Ro6dKh27drlWKdOnTo5jiLNSrly5XT9+nXt3r1bhw4d0oEDBxQaGuo0klXKfX+bN2/uuFQ7o6JFi6pVq1ZOl2JnHGkLAHcSgjQAAFAgZJy7SVKmUMxV9erVnYK0mJgYp+eTkpK0YMECrVq1SsePH891Yv3cni9btmyWy728vPK0HVeuv7hn1052XF83Nzc3Va1aNdv1XV9X19fNlesoE7PrZpzXTkofBWZkQnIjE+Nn1da8efO0Zs0aQxPL5/U9M9oH11Awp2M6r+9LdseJ6x0/M+6bt7e3vvnmGwUGBmZ6Xf/55x/9888/CgkJkZubm3x9ffXSSy/piSeeyLEfRuV2+WNoaGiOQZrr8WP0uMh4jpDSR6MuXbpUK1eu1IEDB3INjm7FsZGTsmXLqnbt2vr7778lpY+23bVrl1Pwb2R+tBvsdrtWrVqlZcuWadeuXU6XoWe3fk5c/3CQkeso30uXLhnuJwAUJgRpAACgQLgxIbpRrr/wZbyk7d9//1X//v2d5vtxc3NTrVq11KhRIzVt2lTr16/PdLmYmf65u7vnqd+5yesIGNd+5XbpZU6vW1a8vb0N9yWndc2O7MnrpbJHjhzRkCFDFBcX51jm7u6uunXrqmHDhmrWrJnmzp1reqSbUVkdLzm9N3l9X8wejw8//LDWrl2rlStXavXq1dq9e3emcMVut2vXrl3atWuXhgwZcsvvbGrEzTh+EhMTNWjQoEzv/X333adGjRqpSZMmOnLkSKZLpW83Pz8/R5CWnJysr7/+2ul5o0FaSkqKXnnlFW3evNlpeeXKldW4cWM1btxYcXFxTnObGdlmdlyP4Zt9bgSAgoIgDQAAFAiul5JlnPMsK67PZxwpMXbsWKcQrW/fvhoxYoTTfGs7duzIU/9yCzbMuvfee50eZ5xQPaPLly/r+vXrmUYiub5uNptNkZGR2Y5+yul1y4rriDuz65YuXdrpcfv27TPdodUqm82m0aNHO4Vor776qgYOHOi4w6kkp5se3Cre3t4qUaKE05xTZ8+ezXa0YF7fFyvHY7FixdSrVy/16tVLSUlJ2rVrl/bs2aO9e/dq7969Tpe+zp49W126dMny8sjbqXTp0k6j9L799ts8j5abMmWKU4j2zDPP6O2335aPj49j2WeffWa5r1b5+fnpxx9/dDw2Mz+alH4H24whmq+vr95//32ny6+XLFmSp77ldF52vUzU9TMPAHcKbjYAAAAKBNe7u/3777/at29fluvu2bMn06VdN365jIqKcvrF09vbWxMmTMh004LsAqvbzXW/Q0NDs1xvwYIFatWqlVq2bKn+/ftrzpw5ktKDONdLqn755Zcst3HlypVMo/By+6U8L4FNTuvWrVvX6fGRI0eyHGW0fft2rVy5UseOHcvzfHN79+51upSzTp06CgwMdArRpMy/8LtyHTlm9vI+1/c2u/fFZrNp7dq1TsuMhiV5kZKSopMnT2r9+vWaOXOmIiIiVKJECT3++OMaNWqUfvjhB23dulUPPfSQU112x+Tt5Hr8uM4/d8OPP/6oTZs2KTIy0ul9S0tL06pVq5zWfe+995xCNCn3S0Zv1rGRk5xGnD344IOGL//OOBeiJP33v//NNIdhXi+d3rdvX5aXHdtstkzHCXfsBHCnIkgDAAAFQo0aNdS4cWOnZePHj9fFixedlsXGxmrChAlOy1q2bOkIk1znDEtKSso0imLTpk1Ok3dLt38upBvatGnj9Hjbtm3atGmT07KEhATHKKq4uDjt3LnTKWTq0qWL0/pff/21Dhw44LQsNTVVEyZMcJq3qHTp0nryySdz7F9ul4oaXbd169ZOd/8MDw/Xt99+67TO5cuXNWHCBL3xxhvq1q2bmjRpkmmdnLjOh3X+/Hmn0WlSeiDpenMG1/e+aFHnizauXr1quA8Zde3a1elxcHBwlnchnTZtmk6dOuXUfufOnU21mZPOnTurY8eOCgwM1CeffKLPPvss0757e3tbnvfvVnAdfTZ79mydPHnSadnvv/+ud999Vy+//LLatm0rX19fnT17VpJ08eLFTDeZyHhHT0k6dOiQfv31V6dlt+rYyEn58uVVs2bNLJ/z8/MzvB3Xc6Hr/oaHh2vx4sVOy3J7r1NSUjRu3LhMl3jOnDkz0+fvkUceMdxXAChMuLQTAADcdGfOnDE8z5Cvr69q1KghSXr55Zf1yiuvOJ77+++/1blzZz3//POqVq2aIiIiFBwc7DTxuIeHh8aMGeN47Drhvc1mU79+/dS3b195e3srNDRUGzZsyPQLo+vE8LfL448/rjp16jjucGe32xUYGKiuXbuqcePGio2N1bJly5x+KS5evLief/55x+O+fftq8eLFSkhIkJQeHvbt21fdunVTw4YNlZCQoJ9//jnTXfReeeWVTKO1bpWyZcuqa9euWrZsmWPZF198oV27dql169aSpJCQEKfQs0iRIlne2TM7ru99QkKCevfurZ49e8rd3V2bN2/Wtm3bMtW5Biyuc7398ccfmjlzplJSUlS1alU999xzhvrTpUsXfffdd47gLi0tTYGBgerQoYP8/f2VnJystWvXau/evU51ffv2zfVmG2b06NFDn3zyiePxqlWrdPbsWbVr105lypTRxYsXtX79+kxziPn6+t70vuRVt27d9OWXXzpuzpGUlKSAgAAFBASodu3aOnv2bKbLFBs2bKj77rtPklSmTBl5eno6BdAjR45Uv379VLFiRe3fv19r1qzJFBC5nhdcj424uDhNmjRJ999/v+Lj4zVq1Kibsr/+/v5Z3owjL0FaxYoVnULj9957T8ePH1fNmjUVFhamFStWOF16LBk7D27atEndu3dX165d5e3trT///FMbN250WufRRx/VAw88YLivAFCYEKQBAICbbs+ePTnegS+jKVOmOIK0p556SkOHDtWsWbMcz8fGxmrmzJlZ1rq5uWnChAlq1KiRY1m1atXUpEkTp8tCo6Oj9cUXX+TYD9eRb7eLm5ubpk6dqt69eztGt6SmpiokJCTbMPKNN95wurzLx8dHH330kQIDA2Wz2SSljxwJDg5WcHBwltvo0qWLBg8efHN3Jhevv/66QkNDncKyrVu3auvWrVmu/+qrrzqODSOaNWsmHx8fp9Dx9OnTmjp1ao51rpeq1alTx+nxtWvXHAFU9+7dDQdp3t7emj59uvr16+cIKOx2u3755ZdsL/P09/fXm2++aWj7eTVo0CCtXbtWhw4dciw7cOBAptGLGXXo0MHp85VfSpcurXfffVejR492HOPXrl1zmkssI29vb02cONHx2NPTU08//bRWr17tWJaYmKhvvvkmx3Zdzwv333+/vLy8nMLXhQsXSpKqVq1604I0Pz+/TKPFpLzdsbNTp05O587k5GTNnTs3x5rczoMlS5ZUYmKi/v77b6dQNiNvb29NmjTJcD8BoLDh0k4AAFCgvPnmm3r//fdVokSJHNcrXbq0Pv/8c73wwguZnpsyZUqmOdEycnd3V/fu3Z2WZTcf2+1Qr149/fDDD5luHOCqaNGiGjNmjPr165fpubZt22r+/PmOETg5beM///mPPv74Y0t9NqNs2bKaM2dOrhPXFylSRIGBgRo2bFietu/h4aGpU6fmeOwUL15cnTp1clrm+t77+vpmmpPrBtfL13Lz8MMPKzg4WA0aNMhxPTc3N/Xo0UOzZs1yugT2ZvLy8tKsWbMcIwBz8+yzz+bLcZKdDh066MMPP9Q999yT43oVKlTQrFmzMo2I+u9//6v7778/x9oePXo4PXY9Njw9PTOtc0NUVJQj5LMqq8CsVq1aOZ7XXL3yyitq2rRpjut06tRJxYsXdzw+fPhwjnMTtmvXToGBgdk+X65cOX3//fe3ZEQlABQUjEgDAAAFTq9evdSuXTutXr1a69atU3h4uGJiYuTt7a0HHnhATzzxhHr37q1SpUplWf/AAw9oxYoVjrvWRUZGqkiRIqpcubJ8fX01YMAA1a1bV5s2bXLMobVjxw5FRUXlGmbdKo0bN9a6deu0ZMkSbdiwQX///bcuXbokLy8vVa9eXf7+/urdu3emycIz8vX11apVq7RhwwatWbNGYWFhioqKkoeHh3x8fNS6dWv17t07T6O8brZq1app2bJl+vnnn7Vu3TodOXLEMQqmSpUq8vf3V79+/UxPVN68eXP99NNPmjlzprZt26bo6GgVLVpUPj4+atmypQYNGqRSpUpp7dq1Sk1NlSStW7dO48aNU7FixSSlB61z5szRjBkztHHjRsexV6NGDbVr1y7PfXrwwQe1dOlSbdmyRStXrtSRI0cck7xXrlxZLVq0UM+ePXMN226GMmXKaPbs2frzzz+1atUqHThwQOfPn1dycrKKFy+uKlWqqFGjRnruuefyNPrpdunWrZseeeQRLV68WJs3b1Z4eLgSEhJUvHhx1a5dW23btnVcxu2qfPnyWrZsmWbNmqXffvtNZ8+eld1uV4UKFdSoUSP17dtX/v7+OnLkiI4ePSopfUTj/v37neZvHDdunHx8fLRixQqdOXNGnp6eqlKlilq2bKnU1FR5enpa3s9KlSrpvvvuc8zxJuXtsk4pPTSeO3eu5s2bp9WrV+uff/5RSkqK7r33XjVo0EDPP/+8Ixhbv369pPRLZtevX6+OHTtmu91XX31Vfn5+mjVrlg4cOKBr166pWrVqateunYYMGaIyZcqY22kAKCTc7AVh9lAAAAAAQIESFBSkGTNmOB53795dH374YT72CADyH5d2AgAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGcNdOAAAAAAAAwABGpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAACsv5ZKAAAgAElEQVQAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAKDAu3z5sjp37qyIiIhMzx09elQBAQFq3769xo0bp9TUVEnSuXPn1K9fP3Xo0EHDhw/XlStXLPWBIA0AAAAAAAAF2v79+9WnTx+dPn06y+ffeOMNvfPOO1q7dq3sdruWLFkiSXr33XfVt29f/frrr3r44Yf11VdfWeoHQRoAAAAAAAAKtCVLlmjixImqWLFipuciIyN17do1NWnSRJIUEBCgX3/9VSkpKdq5c6fat2/vtNyKopaqAQAAAAAAAJMSEhKUkJCQaXmpUqVUqlQpx+PJkydnu40LFy6oQoUKjscVKlRQVFSU4uLi5O3traJFizott4IgDQAAAAAAoACz29LkVsQ9v7txS3h6eiogIECXLl1yWj5ixAiNHDnS0DZsNpvc3Nwcj+12u9zc3Bz/ZuT6OK8I0nBXSYmLkGxpea7zKFdDKRfPmG732rQJpmtLTpmrxLcHmq7/d6v5k22dzd8r7PEXTdXW+HGM6XYlyavaw0qOOGSq9my/aabbffCPOfq7zWDT9Smp5l/v+ltn6UjroabrfRpcNl1bZuZixQ97wXT9oT3lTdc+uutLbWkeaKo21eIMBU/sCtKm5sa+Obu6bqHdZ3YFaZ3JdiXpofJxpmur/zpf4R36m67fGVvOdG1A6BcK8X/NVG3bJudNtytZO8bPHfY23a7Vz3Xpe6+arq266kdFdu5ruv7IefPv9dO7gvSbhWPcw3Sltc+13UK7kvTkriD9brLtJo3N/3X83llLFDe0l+n6ffsrma61ss+StK2Y6VKN2xKkyY+ab7tDsvkzue/Ob7Xb72VTtRXLWZvk2sp5/GTMvabbtfLZkqSa5eJN19ZYO09n2g8wXf/PxTKma63ut5XzipXPV6qFdq2ew095mv95+OVtn+vbR0aZqvWuXFb9Qt4x3XZB51bEXanxkbKb+H22IHMr4q5iZapqxYoVSktz3reMo9FyU7lyZUVHRzsex8TEqGLFiipbtqwSExOVlpYmd3d3RUdHZ3lpaF4QpOHuYkuTbCa/rZitk2S/aG3oqJX6lEhrH/OUyAvmCtOsxAzWtmG6zzehPiXF2l+JUiLMt22rmHk4dJ7qo80HFdfCrf36eS08OveVspByE6b6vGqy7euy9pesq+ExpmtTr5uvlaTUc+bPKVdirL3XVyLM9d1W1VqQJpk/xlMijP8Ql3W9+c912rUkS22n/Wv+vb4aYe29tnSMW4y0zH6urQZpVtq2VbF2jNsumK+/Gm7tXGp2nyUprrilphUXYb7t5GvWflZJNrnfqcmJltqVzJ/Hr0ZZiVesvdepybGW2rbyvevqhRRLbVvZb6vnFbNtp+bjzykJXtZ+Hk4w+bPC3cBu5ffZAurGZ6RKlSqWtlO1alV5eXlp9+7d8vX11YoVK/T444/Lw8NDzZs315o1a9SlSxf99NNPevzxxy21xc0GAAAAAAAAUOi89NJLOnjwoCRp2rRpmjJlijp06KCkpCQNHJh+ZdfEiRO1ZMkSdezYUbt27dKoUeZGPd7AiDQAAAAAAAAUChs3bnT8/7vvvnP8v169egoODs60ftWqVTVv3ryb1j5BGgAAAAAAQEFnSzM153fBZu0y5PzApZ0AAAAAAACAAQRpAAAAAAAAgAEEaQAAAAAAAIABzJEGAAAAAABQ0Nlt6V93kkK4P4xIAwAAAAAAAAwgSAMAAAAAAAAMIEgDAAAAAAAADGCONAAAAAAAgILObpdshW9OsRy52fO7B3nGiDQAAAAAAADAAII0AAAAAAAAwACCtLvUjh07NGDAgPzuhmlLlizRqlWr8rsbAAAAAADgLsIcaSiU9uzZI39///zuBgAAAAAAt4XdbpPsd9gcaYVwfwjS7nL//POP3nnnHcXHx6tEiRIaN26cGjVqpLCwML3//vtKSkpSbGyshg0bpj59+igoKEhRUVE6c+aMIiMj1bNnTw0fPjzb7aempmrSpEn6+++/FRMTo7p16+rTTz9VTEyMAgMD9cADD+jEiROqX7++mjZtquXLl+vSpUv68ssvVatWLbVt21Zdu3bVli1bdPXqVX300UdKSEjQxo0b9ddff6lChQp67LHHbuMrBgAAAAAA7lZudru98N0iAZbt2LFDM2bM0NWrVzVs2DA988wz2rdvn0aPHq21a9dq6tSpatu2rVq1aqXw8HB17dpVe/fuVVBQkDZv3qwFCxYoMTFR7dq10x9//KFSpUpl2c7OnTu1Zs0aTZw4UTabTYMGDVL//v3VoEEDtWvXTiEhIapXr57at2+vDh06aMyYMZoxY4YSEhL03//+V23bttXAgQM1ePBgzZs3T6GhoQoKCtLYsWPl7++vgICA2/zKAQAAAABw+12PCpPSUvK7GzeXu4c8K9XJ717kCSPS7mJXrlxRRESEnnnmGUlSkyZNVLp0aZ06dUpjx47Vn3/+qW+//VZhYWFKSkpy1LVo0UKenp4qV66cypQpo8TExGyDND8/P5UpU0YLFizQqVOndPr0ace2ypcvr/r160uSKleurFatWkmSfHx8FBER4djGjRFnDz74oNatW2dpn1MunpFsqXmu86hQSynRJ023e3XcK6ZrS81cq4Rh7U3Xh28w/zFvcHK1DtfqZKq29sb3TbcrSV41min5zB5TtSefGm+63fon1uhI7Y6m61NS3E3XNj7zs/bX6GK6vnqzBNO1ZZf/odjubUzX79lWyXRtu6glWl+pl6naFItTfT4btUi/VOptqva63Ey32y1qoVZU6mO6vlGlGNO1NQ/8pn8aPW26fktMRdO1A84t0DyffqZqO7WIyH2lHFg5xsP3ZP09zgirn+t7yyflvlI27tu9QWd9nzJdvy/C/HvdNWqhVlo4xj1k/u+8Vj7XVv+63DFqkdaYbLuF/znT7Zb7ebMudnncdP2OUB/TtVb2WZI2FjddqmmnF+n1+823HXDtuunaR86HaFtlc3/QrVIx0XS7krXz+LGocqbbtfLZkqQ6FWNN19Y6uE4nGz5juj7sQlnTtVb328p5xcrnK9XCzylWz+HHvcz/PPzG2fmael9/U7WlqpXXy9s+N902YBQ3G7iLZTUY0W63Ky0tTaNGjdJvv/2mWrVqadSoUU7reHl5Of7v5uaW5XZu2LBhg15//XUVK1ZMAQEB8vPzc6zv6enptK67e9Yn3BvtubmZ/2YAAAAAAABgFUHaXczb21vVqlVzjPLat2+fYmJi9OCDD2rr1q169dVX1a5dO23evFmSlJaWluc2tm/frmeffVY9evRQqVKltGPHDlPbceXu7n5TtgMAAAAAQKFgs92ZX4UMl3be5aZOnapJkyYpKChIHh4eCgoKkqenp0aOHKm+ffvKy8tL9erVU9WqVZ0utzSqZ8+eev3117V69Wp5eHioWbNmprbj6pFHHtGnn36qkiVLqkOHDpa3BwAAAAAAkBuCtLtUixYt1KJFC0nSvHnzMj3/4osv6sUXX8y0fOTIkU6PN27cmGM7devW1c8//5zlcxlrM/YhICDAcROBjOtk7HOnTp3UqZO5ubsAAAAAAADMIEiDZbt27dL772c9sfzMmTNVqZL5ScgBAAAAAAAKCoI0WNa8eXOtWLEiv7sBAAAAAMCdy25L/7qTFML94WYDAAAAAAAAgAEEaQAAAAAAAIABBGkAAAAAAACAAcyRBgAAAAAAUNDZbJItLb97cXO5ued3D/KMEWkAAAAAAACAAQRpAAAAAAAAgAEEaQAAAAAAAIABzJEGAAAAAABQ4Nkkuy2/O3GTFb79YUQaAAAAAAAAYAAj0nBXuTZtguwXo/Jc5zFzra6Oe8V0u8Unf2W61mp99aIjLLVdvb25vxCcaDvBUrsNTq42vY37OrlZart6R/P1R4KtnVavpZivP7K9vOnaRy3We7lZ+0uS2foUe/79PchT9nyrj465x3RtTYv1pdKsvddm60+F3mup3bIWtmGz9lYrOcX83aiio7xN195nsb6oxWPcav3d5sy+MqZry1msLyZrd4CzUl/e5mmp7fI2858vT3dr5zOz9Rcvmj8HS+nncavbyA+X4ovlW73Vs1FhPJsVsdhrK/XlLN5U0my99x12M0sUXIxIAwAAAAAAAAwgSAMAAAAAAAAM4NJOAAAAAACAgs5mS/+6kxTC/WFEGgAAAAAAAGAAQRoAAAAAAABgAEEaAAAAAAAAYABzpAEAAAAAABRwdrtdshe+OcVyZLfndw/yjBFpAAAAAAAAgAEEaQAAAAAAAIABBGkAAAAAAACAAcyRBgAAAAAAUNDZbOlfd5JCuD+MSAMAAAAAAAAMIEgDAAAAAAAADCBIAwAAAAAAAAxgjjQAAAAAAICCzm5L/7qTFML9YUQabpvExEQFBgbmdzcAAAAAAABMIUjDbXPp0iUdPXo0v7sBAAAAAABgCpd24rb54IMPdOHCBQUGBurpp5/WDz/8IJvNpgYNGmjixIny8vJSy5Yt9fDDDys6Olpvvvmm/ve//8nDw0MRERFq27atSpQoofXr10uSZs6cqfLly+fzXgEAAAAAgLsFI9Jw24wfP14VK1bUqFGjtGTJEi1atEgrVqxQuXLlNGvWLElSXFycXnrpJa1YsUJFixbV/v379e6772rZsmVasGCBypYtq5CQENWtW1erV6/O5z0CAAAAAAB3Eze73W7P707g7hAREaGBAwdqyJAh+vrrrx2jyVJSUlS/fn1NmzZNdevW1f79+1WsWDHt2LFDn332mRYtWiRJatu2rb7//nvVqFFDQUFBkqSRI0fm2/4AAAAAAHC7JJ/8S0pJzu9u3FweXvKq1TK/e5EnXNqJ2y4tLU3PPvusxo8fL0m6cuWK0tLSHM8XK1bM8X8PDw+nWnd3d0ttJ749UPaLUXmuKzVzrRKGtTfdbvHJX5mu9ahQSynRJ03XX504wnRtqa9+UcIrz5qqDV9rbcBrg5OrdbhWJ1O195krkySVnL5aia+a38CRYE/TtS3OLdcOn+6m61Ns5l/zR88v05bKPUzX2+Vmuvax88H6s/Lzpmov2619G3s2apF+qdTb0jbyo91y7tdM1/qf+0mhPs+Zrv83rbjp2m5RC7WiUh9TtVWLXjXdriQ1j/xJu6qa22+bhT85Wn293YuYb9w3YoV2V+tmuj4qxfx73TFqkdZYOMbNn1Gsfb6s/nXZyn5X9kwy3W6z8JXaU72r6fr4616ma9tGLdXGSj1N14d6mf++OfbsAn14Xz/T9e3SrpiutXJOscpK29GpxXJfKRtWv3dVysdj/Pz1EqZrrZ7PrLDStpX7IHaOWqRVFvb5QlHzP58NiZyv2VX7m6r1rlZevXZ8brptwCgu7cRtU7RoUaWmpqpFixb67bffdPHiRdntdk2aNEk//PBDfncPAAAAAAAgR4xIw21Trlw5+fj4aPLkyRoxYoQGDRokm82mhx56SMOGDcvv7gEAAAAAAOSIIA23jYeHh2O+M0nq2TPzZQDHjx93/L9FixZq0aKF4/HGjRsd/2duNAAAAADAXcVul+xWLtwtgArhtP1c2gkAAAAAAAAYQJAGAAAAAAAAGECQBgAAAAAAABjAHGkAAAAAAAAFnc2W/nUnKYT7w4g0AAAAAAAAwACCNAAAAAAAAMAAgjQAAAAAAADAAOZIAwAAAAAAKOjstvSvO0kh3B9GpAEAAAAAAAAGEKQBAAAAAAAABhCkAQAAAAAAAAYQpAEAAAAAAAAGcLMBAAAAAACAgs5mS/+6kxTC/SFIw13l363uSonM+2HfQFL4BvMfl+pFR5iu9fjqF12daL6++LszTNdaqU9b85qldiUpLc3NVJ3X6x9Yatfr9cmma+MXT7HUdnyap+naGHfzx+ijks4UKWa6vraSTNdKkmeRNFN1JW12S+1KUkm3FFN1NarHWWr34fsumK7df7aipbYvpJl/r8uafL2s1oenlbDUbnML22hVN9JS2/c9aP5Y2Xq8qulaX0mnU+8xXV9K5j6XN3jI/OfTzUJtetvmfihPuQkXa5j7ziUdTitpus1mFuv9SsearpUkn9KXTdemJJe11HaK2RdcUnSq+XOhlfqHqsRYaleSylcw95pH/2ttn60IS7V2jFupL23xfGbhMLPMbNvuFtu1Ul8n7aqlts3We9muWWoXMIpLOwEAAAAAAAADCNIAAAAAAAAAA7i0EwAAAAAAoKCz22S3W7tUuaBxsxe+OdIYkQYAAAAAAAAYQJAGAAAAAAAAGECQBgAAAAAAABjAHGkAAAAAAAAFnk0qhHOK5azw7Q8j0gAAAAAAAAADCNIAAAAAAAAAAwjSAAAAAAAAAAOYIw0AAAAAAKCgs9nSv+4khXB/GJEGAAAAAAAAGECQBgAAAAAAABhAkAYAAAAAAAAYQJAGAAAAAAAAGMDNBgAAAAAAAAo6u12yF77J+XNkt+d3D/KMEWnIs4MHD2rcuHHZPv/777/r+++/vyltHThwQFOnTr0p2wIAAAAAALCCEWnIs4YNG6phw4bZPn/o0KGb1taJEyd08eLFm7Y9AAAAAAAAswjSkGc7duzQjBkzJKWHart371ZsbKzGjx+vqlWratGiRZIkHx8f9ejRI8tthISEaPny5YqPj9eTTz6pzp076/3331dSUpJiY2M1bNgwderUSdOnT1dSUpK+/vprDRs2TB9//LFCQ0OVlpamgIAADR48+HbtNgAAAAAAuMu52e2F8IJU5KuMQVq9evU0btw4bdy4UTNmzFBISIiCgoIkSSNHjsx2GyEhIfrmm2+0Zs0aFS1aVJMnT1bbtm3VqlUrhYeHq2vXrtq7d69CQkIUGhqqDz/8UAsXLtTp06f19ttv6/r16xo6dKhee+01NW/e/LbsNwAAAAAA+SV5/y+yX0/K727cVG6eJeTV+Nn87kaeMCINljz22GOSpAcffFDx8fF5qq1fv76KFk0/BMeOHas///xT3377rcLCwpSUlPnksH37dh09elR//fWXJCkpKUnHjx/PU5AW9viLSom8kKd+SlKDk6t1uFanPNfdUL29+QkhS331ixJeMX9iKf7uDNO1HhVqKSX6pKnao36vmW5XkhqdXqUD93c2VVtv8wem2/W8r4mun91nuv53vymma9tHLdbaSi+Yro9xN39K73dugRb49DNdX1vmv6G3OLdcO3y6m6pNsVmb6vPR88u0pXLWI2dzU6N6nOl2q+/cqHC/tqbr95+taLq2c9QirarU23R9abdU07WPnQ/Wn5WfN1UbI0/T7UpS9/M/annlvqZqW9WNNN1u5T/+0Pk2bUzXbz1e1XRtj/M/apnJfZakUvY007VPRy3WbxbOZ24y/3fedlFLtL5SL1O1KRanD342apF+Mfn5inF3N93ugHMLNM/COdzPO9Z0bb2wX3SsjvmfU5YmlzVdO+HMAr1fw/x+N79m/hi38l4/VCXGdLuSdP++9TrdpJ2p2qP/ljfdrpV9lqRLRcwf473/XaBFVcy/16Vt+fNeW5VfbVttt6RbiulaKz+beVWvIL+d35huGzCKIA2WeHl5SZLc3NzyXFusWDHH/0eNGqVSpUrpySefVMeOHbVq1apM66elpemNN97QM888I0mKjY3VPffcY7LnAAAAAAAAecNdO3HTubu7KzU1byMWtm7dqldffVXt2rXT5s2bJaUHZxm31bJlSy1ZskQpKSm6cuWK+vbtq337zI8cAgAAAAAAyAtGpOGm8/Pz01tvvaXy5ctrwIABhmpGjhypvn37ysvLS/Xq1VPVqlUVERGhRo0aacaMGZo2bZpee+01nTlzRt27d1dqaqoCAgLUokWLW7w3AAAAAAAUAHZb+tedpBDuD0Ea8qxFixaZAqxq1app48aNktKDtBv/z05AQIACAgIcj1988UW9+OKLWa7722+/Of4/fvx4s90GAAAAAACwhCANt8yaNWv07bffZvncihUrbnNvAAAAAAAArCFIwy3TsWNHdezYMb+7AQAAAAAAcFMQpAEAAAAAABR0dptkK3xziuWoEM6Rxl07AQAAAAAAAAMI0gAAAAAAAAADCNIAAAAAAAAAAwjSAAAAAAAAAAO42QAAAAAAAEBBZ7cXysn5c2S353cP8owRaQAAAAAAAIABBGkAAAAAAACAAQRpAAAAAAAAgAHMkYa7So0fx0hp103V1t74vul2T7SdYLq2gaTwteYz77Q1r5mubXR6lY76mat/aOcXptu1ug2zfZbS9/nY4+NN11fwtPb3iQqeV03XNm92yVLbHfwiTNfu+auypbYT0jxN1d2MGSKu2M19Kzx0tqLpNqtbrK9XPtZ0rdX6nbEVLLUd4eZlqu6Z5uGW2pWkx00e45EHS5lus7KkC2dKmq5vVjHadK3V+sNR5Sy1nSw307UellqW0ky2fTNmaTG7jfYNrB3jVup3H/AxXVtP0j+XSpuuTypm7UyeZOE7QQmlWmrbav3dpklJa9+7rNRbOUatsnpeMVufauEcLEkpFur3eBQ3XfuohfqSRYvJz3TLhYTNlv51JymE+8OINAAAAAAAAMAAgjQAAAAAAADAAII0AAAAAAAAwADmSAMAAAAAACjomCOtQGBEGgAAAAAAAGAAQRoAAAAAAABgAEEaAAAAAAAAYABzpAEAAAAAABRwdrtNdntafnfj5rIzRxoAAAAAAABwRyJIAwAAAAAAAAwgSAMAAAAAAAAMIEgDAAAAAAAADOBmAwAAAAAAAAWd3SbZCt/k/DniZgMAAAAAAADAnYkgDQAAAAAAADCAIO0udPDgQY0bNy5PNYmJiQoMDMx1vbp160qSgoKCFBQUZKp/Rhw4cEBTp069ZdsHAAAAAABwxRxpd6GGDRuqYcOGeaq5dOmSjh49eot6lHcnTpzQxYsX87sbAAAAAADcHnZboZxTLEeFcH8I0u5CO3bs0IwZMySlh2q7d+9WbGysxo8frzZt2ujnn3/W//73P7m7u6tatWqaOnWqPvjgA124cEGBgYH68ssv9dlnn2n79u26dOmSKlasqM8++0zly5fPsr3WrVvrqaee0oEDB1S+fHn16NFD8+bN0/nz5/Xhhx/K399fZ86c0aRJkxQfH69ixYppwoQJql+/vsaOHStvb28dPnxYUVFRCgwM1NNPP63p06crKSlJX3/9tYYPH347Xz4AAAAAAHCX4tLOu1xKSooWL16st99+W1988YUk6fPPP9fs2bMVEhKiqlWr6tSpUxo/frwqVqyoL7/8UmfOnNGpU6e0aNEirV27Vv+PvfuOj6rK+zj+HSbNSJWEAFJEVHoRgQjSBSN9CSBNxN1n8VEXcFl1CWVXFsGlY8FVdF1lEQwtIRBAikhREBSkKyoiVYYSaQlpM/f5Iw9ZWAWHc6LJhM/79ZrXK5nM9/7umczcmfxy7ply5cpp8eLFV61x6tQptWjRQosWLVJGRoZWr16tOXPmaPDgwZo5c6YkadiwYXr22WeVmJio559/XkOHDs3NHz9+XHPmzNFrr72miRMnqnjx4hoyZIjatGlDEw0AAAAAAPxqmJF2g2vevLkk6c4779SZM2ckSa1bt1afPn3Utm1bxcTEqEaNGjpy5EhupnLlyho2bJjmz5+vAwcOaPv27apUqdI167Ro0UKSdOutt+qee+6RJJUvX17nzp1Tamqqdu/ereHDh+fePi0tTT/88IOknBltLpdLd911V+4+mgqtUNs8W7mBcbbW/qXG2bzI26j7Xck6plIAACAASURBVHK+1Q6OrGqUs93n/Bxzg8NXb0r/0konrTPOtrOs3c4z13IL5mLyqXZ7T3y+1JWkO/asNM9a1u7z/WzLLZgrvWS9Wc6ybn4eU6rsXGWetazdKR8f4/n1vJakDvk07jKrzB7fktTesrbN8cy29t8Pvme5BXMtPQvyrfZt21eb5Szr5udrV/WvlptnLWvn57jz65jSxZN/z60hh9/Nt9qAP2ik3eBCQ0MlSS6XK/e6UaNG6csvv9S6dev07LPPatCgQbnNL0navXu3nn76aT366KOKiYlRkSJF5DjONeuEhITkfu12u6/4mc/nU0hIiJKSknKvO378uEqWLHnVfTSVcWS35M287lxo5QbKOLjNuO43bf5inK21f6n2VO1onPd6ze+3ut8la+dtnYyyNT59ybiulNNEyzq53yj7RaOnjOvajFmSsr3mE30bHF6sbRW7GOcrNzhrnC2dtE6nu7Y0zm/7pKxxtp1nrlZF9TLK2q6oEOOZqxXGtc2fW+098Voe1ds4f2dEinH2jj0r9U2tB4zzn6ZEGmf7fD9b75XrZ5R9oOFh47pSThPtdOcWRtmju4ob17U9phQrnmGcrbJzlQ7UNW9z7/GYtxA7eeKVbPEYD9a131dci83z2mvxvJZy/uBdZjjuhnWPGdcts2q9TrQze3xL0tad5Y2ztsez9WHm9/nfD76n4ZX7GOcfTM8yzrb0LNC6qB5G2crl7P4xfNv21fqufluj7Bff//RSLP6w/V1XKWH+PqX6V8v15V3mbdcDZ0sYZ23HbX40szumZFscz7p43tPiKPPn1nch7p+/0VUMOfyuXq74sFG2WIUI/XbTi8a1A4LPl3MpTAJwPJzaiStkZ2frgQceUKlSpfS///u/6tq1q7744gsFBQUpOztbkvTpp5+qcePG6tOnj2677TatXbtWXq/XuGaxYsV022235TbSPv74Y/Xrd+0/tNxud+7+AAAAAAAA/BpopOEKQUFBGjJkiH73u98pNjZWO3bs0MCBA1W6dGmVL19e/fv3V4cOHfTll1+qc+fOeuSRR1S7du0rTv00MWnSJC1YsECdO3fWlClTNG3atGvOQKtbt6527NihyZMnW9UFAAAAAADwF6d23oCio6MVHR19xXUVKlTQmjVrJEmdOnVSp04/Pg0lPv4/5+fPnz//J7e9b98+SdLgwYN/dJ0kjR8//if3o2rVqpo1a9aPtnf57S/fVpUqVbRqlfm6LwAAAAAAANeLRhoAAAAAAEBB5ziSE3hril3Tz6y3XhBxaicAAAAAAADgBxppAAAAAAAAgB9opAEAAAAAAAB+oJEGAAAAAAAA+IEPGwAAAAAAACjofL6cS2ESgONhRhoAAAAAAADgBxppAAAAAAAAgB9opAEAAAAAAAB+YI00AAAAAACAgs7x5VwKkwAcD400AAAAAAAAFHhLlizRa6+9puzsbA0YMED9+vXL/dkXX3yhuLi43O9TUlJUokQJJScnKzExUVOmTFHp0qUlSa1atdLQoUON9oFGGgAAAAAAAAo0j8ejadOmKSEhQSEhIerdu7eio6N1xx13SJJq1KihpKQkSdLFixfVs2dPjR49WpK0e/duxcXFqVOnTtb7wRppAAAAAAAAyDfff/+9jhw5csXl3LlzV9xm48aNuvfee1WyZEmFh4crJiZG77///k9ub8aMGWrUqJEaNmwoSdq1a5cSExPVuXNnPfPMMzp79qzxvjIjDTeUQ/0mK+voievO1fxmmfbfP8q4bqWOLuNsTt48G/rMWKva1deb5b9o9JRV3brfJRtvo8anL1nVtsmvrj3SqrYnM9w4+9WWYsbZ3pJWbalgnK/iTjPOSlJxd6ZRLstn//+gm13ZRrmK5c5Y1a1Z/qRxdu+xSOPsHZK+PnWLcb6iK904K0kVHbP8R1tutarb1WIbjaoet6odUT7VOLv523LG2SqStp8wf6wUk9lz45Iwma95UkSOVe1gw9o+ua3qSpLpK/6qPRWNa/azzNcOO/fzN7qG8mHmj/GbVdyq9s0W8wKyLOcUmOazMu0fZ3mxjV/b9vPmrz3VLfMl5DXO2rL7K8A8H2pxDLbNN8gye1/3n/xFo1xott17lIDg+CRf4K0pdk3/v0Zav379dPTo0St+NGjQIA0ePDj3+xMnTigy8j/vbcqUKaOdO3f+aJPnz5/XvHnztGTJktzrIiMj9bvf/U4NGjTQ1KlTNWbMGE2ZMsVol2mkAQAAAAAAIN/Mnj1bXu+VTe/ixa/8Z4vP55PL9Z/2suM4V3x/yeLFi9W2bdvc9dAk6dVXX839+ve//73atWtnvK+c2gkAAAAAAIB8U65cOVWoUOGKy3830sqWLauTJ/9zVsfJkydVpkyZH21r9erV6tChQ+7358+f1zvvvJP7veM4crvNZ/bSSAMAAAAAAECB1rRpU23atEkpKSm6ePGiVq5cqRYtWlxxG8dxtGfPHt19992514WHh+uf//ynduzYIUl69913rWakcWonAAAAAABAQedzCt8aaT7/10WNiorS0KFD9cgjjygrK0s9evRQ3bp1NXDgQA0ZMkR16tRRSkqKgoODFRoamptzu9168cUXNXr0aKWnp+u2227TxIkTjXeZRhoAAAAAAAAKvM6dO6tz585XXPfmm2/mfl26dGl9/PHHP8o1bNhQiYmJebIPnNoJAAAAAAAA+IFGGgAAAAAAAOAHGmkAAAAAAACAH1gjDQAAAAAAoKBzfDmXwiQAx8OMNAAAAAAAAMAPNNIAAAAAAAAAP9BIAwAAAAAAAPzAGmkAAAAAAAAFnc+XcylMAnA8zEgDAAAAAAAA/EAjDQAAAAAAAPADjTQUaHFxcUpISMjv3QAAAAAAAGCNNAAAAAAAgALP8eVcCpMAHA+NNPwipkyZohUrVqhUqVKKjIxUmzZt9MYbb6hUqVIKCwvTK6+8ohEjRsjj8ejEiRNq0qSJxo0bJ0kaP3681q5dqzJlysjr9apx48aSpEWLFmnmzJny+XyqVauWnnvuOYWGhubnMAEAAAAAwA2ERhry3Jo1a7R161YlJyfr4sWL6tatm9q0aaMDBw7on//8pypUqKDk5GTVqFFDL7/8sjIzM9WxY0ft2bNHR44c0d69e5WcnKzz58+rS5cukqSvv/5a8+bNU3x8vEJDQzVlyhS99dZbevLJJ/N5tAAAAAAA4EbhchzHye+dQOEyduxYVa5cWf3795ckvfDCC6pevbomT56sjRs35t5u586d2rZtm7799lstX75cr776qt5//31VqVIlNztixAg1bNhQaWlpeu211xQRESFJysrKUs2aNTV58uRff4AAAAAAAPzK0le+LufiufzejTzluqm4wh54PL9347owIw15rkiRIvL5fnyec1hYWO7Xs2bN0ooVK/TQQw+padOm+uqrr+Q4jlwuly7v7QYF5TxEvV6v2rdvr1GjRkmSUlNT5fV6r3vfvm75qLKOnrjuXM1vlmnvHR2uO3dJxQ4u42yxl5fq/JCOxvnQZ8YZZ0Mq1Vfmoe1G2S9bjDKuK0l1v0vWzts6GWVrfPqScd3gyKrKOrnfOL+69kjjbHtPvJZH9TbOny3iNs72/n624sv1M85XcaUZZ6OPJWpz+W5G2Syf3WfmNDu+UB+V7W6UrVjujHHdyp9/oIN332+c33ss0jhr+zgr5soyztrc36edEOO6ktTV856SovoYZRtVPW5ct/zGD3WsaWvj/OZvyxlnux2fo8SyfY3zxZxs42xbzzytjnrIOF9E5v/nbeOZrzVRPY2yGTI/jkp2z68Ut3ntfsdma3Z582N47WDzP8rqHVyiHZU7G+eXqLhxdtTB2Rpb2Xzc96abH89sHuOVS581ritJd+5doa9rxhhlvzldyrhuIL9PKeG7/r8bLrEdtw2b2jbH0RjPXK2I6mWcv9ll/vph814htGKkGn36unHtgOA40k/8rR3QAnBuF5/aiTzXtGlTrVy5UpmZmbpw4YLWrl2rY8eOXXGbjz/+WL169VKXLl2UkZGhL7/8Uj6fT02aNNHy5cuVmZmps2fPasOGDZKk6OhorVq1SqdPn5bjOBo9erRmzpyZH8MDAAAAAAA3KGakIc+1atVKn3/+ubp166YSJUqoTJkyP/pQgAEDBmj06NF64403VLRoUd199906cuSIevbsqV27dqlTp06KiIhQ1apVJUnVq1fXoEGDNGDAAPl8PtWoUUOPPfZYfgwPAAAAAADcoGikIc99/vnnuu2227R06VJlZWWpV69eat68uQYOHJh7myZNmmjFihU/mR86dKiGDh36o+t79uypnj3NTtUAAAAAAACwRSMNea5KlSqaPn263n77bTmOo9/85jeqXr16fu8WAAAAAACAFRppyHMlS5bUW2+9ld+7AQAAAABA4eH4ci6FSQCOhw8bAAAAAAAAAPxAIw0AAAAAAADwA400AAAAAAAAwA+skQYAAAAAAFDQ+Xw5l8IkAMfDjDQAAAAAAADADzTSAAAAAAAAAD/QSAMAAAAAAAD8wBppAAAAAAAABR1rpBUIzEgDAAAAAAAA/EAjDQAAAAAAAPADjTQAAAAAAADAD6yRhhtKVrZbWVlus6xhTpL2LjB/qkW/LO1dEGKcPzP378bZGM9cfdjILB8ZYt+nz/aabWN17ZHGNdt74q3ybXePM87a5j+v97RV7dtdqcbZH7yhVrXPeM0e48GyX1Mh0zF7nO05Fmlcs7Jlvpgr2zgrSUUt8hcdu7cOpvlbgy5a1bXZxtf7SxvXLG+ZrxRs/ryUpEpB5nlP1k1WtTMt/l/rlmNVOysA/1dcK+h8vuUPp99snK1nmW8gr3FWkhqk2+Xzw77Tpazyd1psw/zdrL17Sp3Kt/w3lve5DbujmXneK5dVXZv8phDz149mFvniwWFqZFw5QDhOzqUwCcDxBN67DAAAAAAAACAf0EgDAAAAAAAA/EAjDQAAAAAAAPADjTQAAAAAAADAD3zYAAAAAAAAQEHnOJLP/sO2ChQ+bAAAAAAAAAAonGikAQAAAAAAAH6gkQYAAAAAAAD4gTXSAAAAAAAACjqfr/CtkRaA42FGGgAAAAAAAOAHGmkAAAAAAACAH2ikAQAAAAAAAH5gjTQAAAAAAICCzvHlXAqTABwPM9IAAAAAAAAAP9BIQ56Ji4tTQkJCnm5z3rx5Sk5O/sW2DwAAAAAA4C8aaSjQtm3bpszMzPzeDQAAAAAAANZIuxG88cYbWr58ubxer5o1a6Y+ffpo0KBBuv322/XNN9+oZs2auvvuu5WYmKizZ8/q1VdfVdWqVdWmTRs9+OCD2rhxoyTphRdeUM2aNf2quWjRIs2cOVM+n0+1atXSc889p9DQUDVr1kwxMTHaunWr3G63XnzxRVWsWFGbN2/W2LFj5Xa7Vb9+fe3fv19PPPGE1qxZo08++USRkZGSpLVr12rOnDk6ffq0Hn/8cfXq1esXu98AAAAAACgwfL6cS2ESgONhRloht379eu3evVsLFizQokWL5PF4tGTJEu3bt08DBw5UUlKStm3bpqNHj2ru3Lnq1KmT5s6dm5sPDw/XokWLNGTIEA0bNsyvml9//bXmzZun+Ph4JSUlqXTp0nrrrbckSSdPnlSTJk20aNEiNWrUSLNnz1ZWVpb+/Oc/a9KkSVq0aJGCgnL6u02bNlWbNm00ZMgQNW/eXJKUmZmp+fPna8aMGZo2bVoe31sAAAAAAABX53Icx8nvncAvZ8KECVq+fLlKlCghSUpPT1etWrW0ZcsWffTRR5Kk/v3764knnlDTpk2VkJCgLVu2aPz48WrTpo3i4+NVpkwZSVLjxo31/vvv65ZbbvnJWnFxcWrcuLHS0tL02muvKSIiQpKUlZWlmjVravLkyapWrZp27typ0NBQLVy4UJ999pkefvhh/eUvf8ld/+zLL7/UuHHjNGvWrNxtxsbGKi4uTvfcc4969uwpx3FUvXp17du375e+CwEAAAAAyHfpCyfIST2T37uRp1w3l1RYd/8m7RQUnNpZyHm9Xg0YMEC//e1vJUnnzp3T8ePHtX379itu53a7fzJ/aXaYJPl8vqve7r9rtm/fXqNGjZIkpaamyuv15v48NDRUkuRyueQ4jtxut3x+Tue8VN/lcvl1+/+2977/UdaRE9edq3dwiXZU7mxUU5LSs8yfatHHErW5fDfj/BlviHE2xjNXK6LMTp+NDLloXFeSGhxerG0VuxhlPZnhxnXbe+K1PKq3cb7t7nHG2eDIqso6ud84/3m9p42zjY8t0pbyvzHO/+ANNc7aPM6CZTcVvI1nvtZE9TTKpuvnj4dX08ETr2UWj7NirmzjbPPjC7ShbA/jfLpjPu52nrlaZfi7LhWUYVxXkhoeXaTPbjV7jKdmmx/DW3oWaF2U+f1dNDjLOHvPkSRtrdDVOO/Jusk4a/sYd8v8/7w2xxSfzN5fXGLzGlIuNM24bv1Di7W9ktlrpiQdyTB/3ezkiVeyxe/a5vQY28dZiMVrSFvPPK2Oesgom255UpDNfW5+BLd/j3RH6R+Ms3fuXaGva8YY5785Xco4aztum1krto/x/Kq7J9T8dfPZQ+9qUqWHjbLFK0Tofze+aFwb8BendhZy9957r5KSkpSamqrs7Gz94Q9/0O7du/3OL126VJK0atUqVa1aNXdm27VER0dr1apVOn36tBzH0ejRozVz5syr3v7222/XuXPncmeXLVmyJPdnbrf7iiYcAAAAAABAfmFGWiHXpk0bffnll3rooYfk9XrVvHlzNWrUyO/8tm3btGDBAt10000aP368X5nq1atr0KBBGjBggHw+n2rUqKHHHnvsqrcPCQnRxIkTNWzYMBUpUkRVqlRRWFiYpJx10qZOnapixYr5vc8AAAAAABQ6jqTCtjpXAA6HRtoN4Mknn9STTz55xXVr1qzJ/XrWrFm5X8fGxio2Njb3+6effloVKlTwq87ljbaePXuqZ88fny51+Zpml2r5fD6tWbNGc+bMUXh4uN5++215PB5JUseOHdWxY0dJ0oMPPnjVbQEAAAAAAPzSaKThukyYMEEbN2780fW1a9fWuHFma0MVKVJEJUuWVI8ePRQcHKxbb73VeFsAAAAAAAC/FBppuKrLZ61dMmzYL/NpGo899tg1T/8EAAAAAADIbzTSAAAAAAAACjrHJ/nsPrW+wHECbzx8aicAAAAAAADgBxppAAAAAAAAgB9opAEAAAAAAAB+YI00AAAAAACAgs5XCNdIC8DxMCMNAAAAAAAA8AONNAAAAAAAAMAPNNIAAAAAAAAAP7BGGgAAAAAAQEHn+HIuhUkAjocZaQAAAAAAAIAfaKQBAAAAAAAAfuDUTtxQyte6IF+Zc0bZig3McpK0d1OEcVaSsnzmPe9TbrunuWm+YYOzVnUlqbLhNr7aUsyq7tkibuPs5/WeNs42PrbIKn/3jinGWdt8fL2/WtU+Yfg4q+TNsKorSW45Rrlwea3q2uRLhV+0qm2TP5Rq9/zKkssoV7l2ilVdm23s2R5lVdcxHLMklShh97u2yXtO3WRV24btf3pN8/l5csntD9odz2zyBxYXtaqd5TL/jV0oYv78kKTTbvPX7ErebKvawYaPmCDD153LFTV8Dbko8/vLljfb7pltm88vdo9w83y6xfNSkjIs8uGWB1PT/E2Bd4YgAlRgHo0AAAAAAACAXxkz0gAAAAAAAAo4x+fI8dnPai1QAnA8zEgDAAAAAAAA/EAjDQAAAAAAAPADjTQAAAAAAADAD6yRBgAAAAAAUNA5juQrZB9P6rBGGgAAAAAAAFAo0UgDAAAAAAAA/EAjDQAAAAAAAPADa6QBAAAAAAAUdI4v51KYBOB4mJEGAAAAAAAA+IFGGgAAAAAAAOAHGmkAAAAAAACAH1gjDQAAAAAAoKDzOTmXwiQAx8OMNAAAAAAAAMAPNNIAAAAAAAAAP9BIw68iLi5OCQkJ17xN165df6W9AQAAAAAAuH400lBgJCUl5fcuAAAAAAAAXBUfNnCDyc7O1ujRo/X111/r1KlTqlatmp5++mk9/fTTuvPOO/XFF1+odOnSeumll1SyZEk1a9ZMMTEx2rp1q9xut1588UVVrFhRbdq00b///W9VqFBBmzdv1vTp0zVr1ixt2bJF06ZNU3p6us6dO6fhw4erbdu2fu1btWrVtG/fPr3yyivyeDw6ePCgjh49qp49e+qJJ55QRkaG/va3v2nr1q0KDg7Wk08+qQ4dOvzC9xgAAAAAAAWAz5dzKUwCcDwux3EC7yMSYOzTTz/VsmXL9Nxzz8nn82nAgAFq3bq1Jk6cqISEBNWsWVODBw9W48aN1b9/f1WrVk2vvvqq2rZtq/Hjx0vKOU3zao20IUOG6KmnnlLVqlW1adMmvfDCC1qyZIni4uLUuHFjxcbGXnXfLm+krV+/XrNnz9b58+fVtm1brVu3TvPmzdPu3bs1depUnT59Wo8++qgSExMVEhLya919AAAAAADki4vvjJJzPiW/dyNPuYrdopseHZvfu3FdmJF2g2nUqJFKliyp2bNn69tvv9V3332ntLQ0lS5dWjVr1pQk3XnnnTp79mxupnnz5rnXf/bZZ9fc/qRJk/Thhx/q/fff144dO5Sammq0n9HR0QoJCVHp0qVVsmRJnT9/Xp9++qkeeughFSlSRJGRkVq6dOl1b/fMY73kO3n8unO3JK5TSreW1527ZO+mCONss+ML9VHZ7sb5g0XCjLP9js3W7PL9jLIPNjpiXFeSSiet0+muZvf5qi0VjOv2/n624suZjVmSbneZPeYlqfGxRdpS/jfG+bt3TDHOBkdWVdbJ/cb5+Hp/Nc72PzZbswwfZ5W8GcZ1JamlZ4HWRfUwyjpyGddt5ZmvtVE9jfMRN6cZZ2t/u1S7b+9onD+UWsw428ETr2VRvY2yjeofM64rSZEr1utkTAuj7J7tUcZ1bX/XFSLO/vyNruKOPSv1Ta0HjPNfnbrFOGvzu5akYJn/n7edZ65WRfUyymZbPK8lqb0nXssNx31f1x+M6xZ/Y4XOPRZjnP9gcWnjbLfjc5RYtq9x/kIR8/vc5vVDsnsNya/XD8nuuHJRbuO6No9vSapSwvx4Vv2r5fryrvbG+QNnSxhnbcdtw6Z2ust8FSfb5/WxIPPafzjyrl6t8LBRtliFCD3yyYvGtQF/sUbaDeaDDz7QM888o7CwMMXGxqpRo0YqX768QkNDc2/jcrl0+UTFSz/77+svfZ2dnZ17Xd++fbVz507Vrl1bjz/+uPF+/tT+BAUFyeX6zxuPgwcPKjMz07gGAAAAAADA9aCRdoPZtGmT2rdvr+7du6t48eLavHmzvF7vdW+nVKlS+uabbyTlNOck6cyZM/ruu+/01FNPqUWLFvrggw+Mtn01jRo10rJly+Q4jk6fPq2HH36YRhoAAAAA4Mbgc/6zTlqhuQTeamM00m4wPXv21NKlS9W5c2c99dRTatCggTZv3nzd2xkyZIjGjRun7t27q1ixnNN8SpYsqR49eqhjx45q3769UlNTlZ6errQ081OQLte3b1+Fh4erS5cuevTRR/WXv/xFRYsWzZNtAwAAAAAA/BzWSLvBVKtWTUuWLLnmbQYPHpz79b59+3K/jo2Nzf2wgJYtW6plyx+vXzV8+HANHz489/vRo0dLUu4HFVzLpVqX15ekNWvW5H49ZsyYn90OAAAAAADAL4FGGn416enp6tXrpxf/HTJkiO6///5feY8AAAAAAAD8RyMNv5qwsDAlJSXl924AAAAAABCAHMkJvDXFri3wxsMaaQAAAAAAAIAfaKQBAAAAAAAAfqCRBgAAAAAAAPiBNdIAAAAAAAAKOp8v51KYBOB4mJEGAAAAAAAA+IFGGgAAAAAAAOAHGmkAAAAAAACAH2ikAQAAAAAAAH7gwwYAAAAAAAAKOp+TcylMAnA8zEgDAAAAAAAA/MCMNNxQdm+LUPrh6+94t5W0bWOUcd1Ql91H+jpyGWfvUJpVbdP8tk/KWtVtZ7GNKm67MVdxmed/8IZa1bbJx9f7q3G2/7HZVvneO8YYZ23yq2qPtKorSan59FKYJrdx1mV+SLDOZ1scj2zyX++IsKobabGNcHeWVW2b/LlzYVa1bfKWDzOrfH49zvLTx0mljLPt37DLh8nufUqYY5H32f1fv6jFTIbzlsd/03wxZVvVlSSXAm8Gx7dnSxhnq1vm8/OIYPubMs0XdbxWdW3yVbLsjilVssyeIzdl240Z8Bcz0gAAAAAAAAA/MCMNAAAAAACgoHN8OZfCJADHw4w0AAAAAAAAwA800gAAAAAAAAA/0EgDAAAAAAAA/MAaaQAAAAAAAAWdz8m5FCYBOB5mpAEAAAAAAAB+oJEGAAAAAAAA+IFGGgAAAAAAAOAH1kgDAAAAAAAo4BzHkePz5fdu5C2HNdIAAAAAAACAQolGGgAAAAAAAOAHGmkAAAAAAACAH2ikAQAAAAAAAH6gkYaAEBcXp4SEBPXv3z/3uq5du+bjHgEAAAAA8CvyOYXzEmBopCGgbNmyJffrpKSkfNwTAAAAAABwownK7x0AforjOBo/frzWrl2rMmXKyOv1auXKlZKknj17av78+apWrZr27duXz3sKAAAAAABuFMxIQ4G0YsUK7d27V8nJyXrppZd06NAhjRo1SpI0f/78fN47AAAAAABwI3I5jhN4J6Si0BszZoyqVKmSuybaiBEj1LBhQw0fPjx3Fhoz0gAAAAAAN4q0V/4o5+yp/N6NPOUqEaHwwS/m925cF07tRIHkcrl0eY83KChvHqofNfyD0g+fvO5cW888rY56yLhuqMtnnG1+fIE2lO1hnA8p4jXORh9L1Oby3Yyy57whxnUlqZ1nrlZF9TLKFndnGte1GbMknbEYd4xnrlYYjlmSTrjNnyf9j83WrPL9jPO9d4wxzgZHVlXWyf1G2VW1RxrXlaQO3lgBXQAAIABJREFUnngti+pttY38qFu56HnjbK39S7Wnakfj/P4LxY2zXTzvaXFUH6NshCvDuK4kNT2eoI1lY42yQUXMj+GNjy3SlvK/Mc4Huc1rNzi8WNsqdjHOezLDjbPtPfFang/PrRu1diCPOd1lfoJMt+NzlFi2r3E+2DF/fnXyxCvZcNzFlG1cV5JaehZoXZTZe8M0iz//bH/XNrM3bF83XRa1A3XcwRaVbd6HS1KWxT1uM+abKkaq9WevGNcG/MWpnSiQmjRpouXLlyszM1Nnz57Vhg0bJElut1vZ2XZvPgAAAAAAAEwwIw0FUtu2bbVr1y516tRJERERqlq1qiTp/vvvV9euXZWQkJDPewgAAAAAAG40NNJQYA0dOlRDhw694rrY2P+clsP6aAAAAACAG4bPybkUJgE4Hk7tBAAAAAAAAPxAIw0AAAAAAADwA400AAAAAAAAwA+skQYAAAAAAFDQOT7J58vvvchbTuCNhxlpAAAAAAAAgB9opAEAAAAAAAB+oJEGAAAAAAAA+IFGGgAAAAAAAOAHPmwAAAAAAACgoPNJ8jn5vRd5K/A+a4AZaQAAAAAAACj4lixZog4dOuiBBx7Q7Nmzf/Tz6dOnq3Xr1uratau6du2ae5svvvhCsbGxiomJ0ciRI5WdnW28D8xIAwAAAAAAQIHm8Xg0bdo0JSQkKCQkRL1791Z0dLTuuOOO3Nvs3r1bU6dO1d13331F9tlnn9XYsWNVv359jRgxQvPmzVPfvn2N9oMZaQAAAAAAACjQNm7cqHvvvVclS5ZUeHi4YmJi9P77719xm927d2vGjBnq3LmzxowZo4yMDB09elTp6emqX7++JCk2NvZHuevBjDQAAAAAAICCzvHlXAqT/x/P999/L6/Xe8WPihcvruLFi+d+f+LECUVGRuZ+X6ZMGe3cuTP3+9TUVNWoUUPPPvusKleurLi4OP3jH/9Qq1atrshFRkbK4/EY7zKNNNxQslVEWYYTMU1zkpTl2E3+vOCYP1WLWS5GmeUz2/e8OLybbsN0n/MiH2w5cpt8JW+GVW2b/KraI42zHTzxxvl2u8cZ17Xdxrraw63qhlj8rlNSb7KqbZMvIrtjimneozCrujbbcHvtxnzca35/Vw6+YFXbXcR830u4Mq1q2+SLhtnVvvUms/vt6MWiVnVt5Of9neKEWNX2/vxNrirU8g9Cm3yw5fEs1DB/Pg/+BDPdhtu6srliLvP1iGzzNu+lbbnyKZ9lWdkmn+ayey9unHfZ3tvIT/369dPRo0evuG7QoEEaPHhw7vc+n0+uy37PjuNc8f3NN9+sN998M/f73/3udxoxYoRatGhxzdz1opEGAAAAAACAfDN79uyfnJF2ubJly+qzzz7L/f7kyZMqU6ZM7vfHjh3Txo0b1aNHD0k5DbOgoCCVLVtWJ0+ezL3dqVOnrshdL9ZIAwAAAAAAQL4pV66cKlSocMXlvxtpTZs21aZNm5SSkqKLFy9q5cqVatGiRe7Pw8LCNGnSJB0+fFiO42j27Nlq166dbr31VoWGhmrr1q2SpKSkpCty14sZaQAAAAAAAAWdz8m5FCbXMZ6oqCgNHTpUjzzyiLKystSjRw/VrVtXAwcO1JAhQ1SnTh2NGTNGTzzxhLKystSgQQP99re/lSRNnjxZo0aN0oULF1SrVi098sgjxrtMIw0AAAAAAAAFXufOndW5c+crrrt8XbSYmBjFxMT8KFe9enUtWLAgT/aBUzsBAAAAAAAAP9BIAwAAAAAAAPzAqZ0AAAAAAAAFnOPzyfH58ns38lYAjocZaQAAAAAAAIAfaKQBAAAAAAAAfqCRBgAAAAAAAPiBNdIAAAAAAAAKOseRfE5+70XecgJvPMxIAwAAAAAAAPxAIw0AAAAAAADwA400AAAAAAAAwA800gAAAAAAAAA/0Ei7AcXFxSkhIeGqP69WrZok6b333tN7772Xp7VPnDih//mf/1HXrl3VrVs3bdq0KU+3DwAAAABAoeRzCuclwPCpnbiqPn365Pk2J06cqDZt2qhfv3769ttv1b9/f61fv15utzvPawEAAAAAAOQlGmk3AMdxNH78eK1du1ZlypSR1+tV48aNtWjRIs2cOVM+n0+1atXSc889p9DQ0NzcK6+8IkkaPHiwmjVrppiYGG3dulVut1svvviiKlasqA0bNmjChAkKCQlRdHS0du/erVmzZl11X9q1a6d7771XklS5cmVlZGQoLS1NXbt21VtvvaUqVaooLS1N7du318qVK9WyZUvVrl1bJ0+e1Ouvv65nn31WaWlpKlKkiEaNGqX69ev/snceAAAAAADA/+PUzhvAihUrtHfvXiUnJ+ull17SoUOHdPHiRc2bN0/x8fFKSkpS6dKl9dZbb111GydPnlSTJk20aNEiNWrUSLNnz1ZGRoaGDRumqVOnKiEhQWfOnPnZfYmJiVGJEiUkSW+99ZZq1KihYsWK6Te/+Y0WL14sSVq5cqVatWql0NBQ/fDDDxo4cKCSkpK0cOFCtWrVSgkJCRoyZIi2bt2aN3cQAAAAAACAH1yO4wTeCam4LmPGjFGVKlXUv39/SdKIESNUs2ZNvfbaa4qIiJAkZWVlqWbNmpo8ebKqVaumffv2XTEjrVq1atq5c6dCQ0O1cOFCffbZZ+rTp4+ee+45JSYmSpK2bdumadOmXXNG2iXvvPOOZs2apXfffVflypXTkSNH9Nvf/larVq3SgAED9Kc//Un16tVTtWrVtGPHDoWFhWnr1q0aPHiwmjRpopYtWyomJuaKGXQAAAAAABRWqeMGyvnhRH7vRp5ylSqjm0e+md+7cV04tfMG4HK5dHm/NCgoSF6vV+3bt9eoUaMkSampqfJ6vdfczqWm1aXthYWFXfHzoCD/Hk4TJ07UunXrNHv2bJUtW1aSVKFCBZUvX14rV67U6dOnVa9evdzbX6pzzz33aOnSpVq7dq2WLVumxMREvf32237VvGRtw8G6ePjkdWUkqb0nXsujel93Li/Y1i7myjLONju+UB+V7W6UTXXsDi8xnrlaEdXLKHuzK9u4rs2YJSnTMZ/o28YzX2uiehrn3TL/v0hLzwKti+phnE+1eDnp4InXMsPHeLvd44zrSlJwZFVlndxvlF1Xe7hx3baeeVod9ZBxPtTlM842P75AG8qa/67PWjy3O3nilWz4u85y2U2i73Z8jhLL9jXKui3+59jF854WR5mvOVo57IJxtt7BJdpRubNxPjUj2Djb9HiCNpaNNc4XDcs0ztb9Llk7b+tklD16sahxXcnuNbuEy3zMtvd3ihNinLV5Xkt2p8fYvH5IUrDF62Y7z1ytMnyfkiGXcV3J7j63WZXY9j1pUYv3Z7avXRcsXrsC9e8Am9kyts+tNIvX7B7H52iB4et1eMUIdfj0ZePagL84tfMG0KRJEy1fvlyZmZk6e/asNmzYIElatWqVTp8+LcdxNHr0aM2cOfO6tlulShVduHBBe/fulSQlJyf/bOadd97R5s2b9d577+U20S7p3r27xo4dqy5duvxkduLEiVq8eLG6deumv/71r7l1AQAAAAAAfg3MSLsBtG3bVrt27VKnTp0UERGhqlWrqlixYho0aJAGDBggn8+nGjVq6LHHHruu7QYHB2vq1Kn661//Kp/PpwoVKlzz9o7j6NVXX1XRokVzTzOVpDfeeENRUVF64IEH9Je//EVdu3b9yXz//v319NNPKyEhQW63WxMmTLiu/QUAAAAAALBBI+0GMXToUA0dOvRH1/fs+ePTyPbt2ycpZ220/75OkmJjYxUbm3P6QJ06dbRgwQJJ0ubNmzV9+vSr7oPL5dKnn376kz9zHEebN29WkyZNFBUV9ZN1y5Urpzlz5lx1+wAAAAAAFFo+J+dSmATgeGikIc9NmDBBGzdu/NH1tWvX1rhxP72e0QsvvKAPP/xQb74ZWIsMAgAAAACAGweNNOSZ6OhoRUdHG2VHjhypkSNH5vEeAQAAAAAA5B0+bAAAAAAAAADwAzPSAAAAAAAACjjHceQE4Jpi1+QE3niYkQYAAAAAAAD4gUYaAAAAAAAA4AcaaQAAAAAAAIAfaKQBAAAAAAAAfuDDBgAAAAAAAAo6n5NzKUwCcDzMSAMAAAAAAAD8QCMNAAAAAAAA8AONNAAAAAAAAMAPrJEGAAAAAABQ0Pl8OZfCJADHQyMNN5RMSZlyGWbNcpIUovxbQLFyxR/yJb/7UBmrupLkM7zPK5Y7Y1XXJr/nWKRV7XS5jbPh8lrVdiwe4/llXe3hVvm2nnnG22i5++9WtW3ye+4ZalW7WEimcdaVaXc8K+nKMsqlOCFWdSXJ7Zjt+xm33QR+m3zVILs3l0EW+fPpwVa1zzvm+TMXzX/fdSUduVjUKJufR8F0x/z4b5svXcT8mGCbP6wwq9oXipg/v4It355dcJnd5+WLXLQrLCnSnWGUS/GGWtc25XXsnmG2efx67I5m5nnbuoC/OLUTAAAAAAAA8AONNAAAAAAAAMAPnNoJAAAAAABQ0PmcnEthEoDjYUYaAAAAAAAA4AcaaQAAAAAAAIAfaKQBAAAAAAAAfmCNNAAAAAAAgILOUUCuKXZNATgcZqQBAAAAAAAAfqCRBgAAAAAAAPiBRhoAAAAAAADgBxppAAAAAAAAgB/4sAEAAAAAAIACznEcOU4Ars5/LQE4HmakAQAAAAAAAH6gkQYAAAAAAAD4gUZaAbdr1y6NHDkyz7aXkJCguLi4684dPnxYI0aM+Nnbff7553rooYfUsWNH/elPf1JmZqbfNebNm6fk5GRJUlxcnBISEq57PwEAAAAAAH4pNNIKuDp16mjcuHH5vRs6duyYDh8+fM3bXLhwQYMHD9aYMWO0dOlSSdKCBQv8rrFt27brarwBAAAAAHDD8DmF8xJg+LCBAm7z5s2aPn26pJym2tatW5WSkqJRo0apZcuWOnz4sP785z8rNTVVDRs21PLly7Vp0ya/tr18+XK9/fbbSk9PV2Zmpl544QU1aNBAb7/9thITE1WkSBHVrVtXY8aM0dixY3XkyBH97W9/03PPPfeT2/v4449Vv359Va9eXZI0atQoeb1eSdJ9992n+++/Xzt37lRERIS6d++uWbNm6fjx4xo/fryys7O1Zs0affLJJ4qMjJQkrV27VnPmzNHp06f1+OOPq1evXtq0aZMmTZokSSpRooSmTJmiW265xeo+BgAAAAAA8Acz0gJIVlaW5s6dq+HDh+ull16SJI0ZM0Zdu3bV4sWLVadOHaWkpPi1LZ/Pp/j4eL3++utavHixfv/73+uNN96Q1+vVjBkztHDhQiUkJCgrK0sej0ejRo1S7dq1r9pEk6SDBw8qPDxcQ4cOVdeuXfXKK6+oePHikqRTp06pRYsWWrRokTIyMrR69WrNmTNHgwcP1syZM9W0aVO1adNGQ4YMUfPmzSVJmZmZmj9/vmbMmKFp06ZJkv7xj39o9OjRSkhIUNOmTbV3716buxQAAAAAAMBvLqfQfXZq4XL5jLSBAweqRYsWOnz4sAYMGKA1a9aofv36+uijj1S0aFE5jqO6detq165dV91eQkKCtmzZovHjx+vChQtas2aNDhw4oC1btqhIkSKaNWuWnnjiCR07dkz333+/HnzwQd111125+zFr1qyrbvu1117TrFmzNHfuXJUvX14jR47UrbfeqsGDB6tatWratWuXQkJCFBcXp3vuuUc9e/a8YrtxcXFq3LixYmNjr7iN4ziqXr269u3bp3feeUf/+te/1LZtW91///2677778vw+BwAAAACgoDkf11/OaU9+70aecpWOUrHxV+8zFESc2hlAQkNDJUkul+uK6y71Ql0ul4KC/PuVpqamqkePHurSpYsaNWqkatWqafbs2ZJyZn1t375d69ev1+9//3tNnjzZr21GRESoXr16qlixoiSpffv2evfdd3N/HhISkvu12+3+2e1dus3l43300UfVunVrffjhh5o0aZJ27typJ554wq/9k6SVDQfr4uFTft/+kq6e95QU1ee6c5eEyLxf3d4Tr+VRvY3ztSudMM5W/HSNDjdqY5TdfaiMcV3Jbtw1y580rlv58w908O77jfN7jkUaZzt44rXM4ncdLq9xtpVnvtZG9TTOp+nnn9NXYzPuEPmM60pSW888rY56yCjbcvffjesGR1ZV1sn9xvk99ww1ztY/tFjbK3Uxzl/IDDbONju+UB+V7W6UTXFCfv5G19DF854WGx7Hz7jNJ/A/cmy2/l2+n3H+npvOGGdr7V+qPVU7GuePXChqnI3xzNWKqF7Gea9cP3+jq7A5pphXzWHz2hVqcQxv45mvNRbH8JuKmNdu8n2CNpWLNc4fVphx9qHv52heub7G+WCL6QTdjs9RYlmz2uWLXDQvLCn6WKI2l+9mlE3xhhrXtX1PelM+vk+5aPE+xXbcNmxq28yWsX1PmuEyf920eW6FV4xQzKcvG9cOCAG6ptg1BeB4OLUzwDVr1kyJiYmSpNWrVystLc2v3HfffSeXy6XHH39c0dHRWrVqlbxer1JSUtShQwfdddddeuqpp3Tfffdp3759crvdys7O/tl92bNnj77//ntJ0ocffqhatWr5PRa32527ptrV9OzZU6mpqXr00Uf16KOPcmonAAAAAAD41TAjLcCNGDFCI0eO1MKFC1WzZk2/c9WrV1eNGjXUvn17uVwuNWvWTFu3btUtt9yiXr16qUePHrrppptUpUoVde/eXRkZGTp//ryeffbZ3MX+/1u5cuU0ZswYPf7448rIyFCNGjU0bNgwv/epadOmmjp1qooVK3bV2/zpT39SXFycgoKCFB4errFjx/q9fQAAAAAAABs00gq46OhoRUdHX3FdhQoVtGbNGklS6dKl9frrr+f+LCEh4Zrbi42NVWxszvT7qVOnXvGzUaNGSVLubK/LhYeHKzk5+Wf3t1WrVmrVqtWPrt+3b1/u1+PHj8/9+vLxdezYUR075pyC8uCDD/5kvkmTJlq8ePHP7gcAAAAAAEBeo5FWCC1btkwzZsz4yZ8lJSVZbfvQoUMaPHjwT/5s7NixqlOnjtX2AQAAAADAjzk+R04Aril2TQE4HhpphcylmVsdOnT4RbZfqVIl62YcAAAAAABAIOLDBgAAAAAAAAA/0EgDAAAAAAAA/EAjDQAAAAAAAPADa6QBAAAAAAAUdI4TkIvzX5MTeONhRhoAAAAAAADgBxppAAAAAAAAgB9opAEAAAAAAAB+YI00AAAAAACAgs73/5fCJADHw4w0AAAAAAAAwA800gAAAAAAAAA/0EgDAAAAAAAA/MAaabih1Ij4QdmZp4yydaPMcpJ08tTNxllJKu1ON87uOFTGOFvRIl89IsW47iV3Gm5j77FI45qVLfPFXNnGWdt8qfCLVrUjbk4zzrpcVqVVueh5o1xK6k12hSWFuswWZthzz1DjmvUPLbbK19o6zThrm19We5RV7dNOiFGuQSWPVV1JuttwG0nHy1vVPec2f4JUGd/EqrZN/sigXVa1bQTLyZd8tiwPZhaqVTF/n2Gb33qgrFXtk75Q42yw5V0ebPFQCXXsFuYxzad73VZ182obv7ZSYebvZ23zF9Pt3ovbsDua2efzw4kgu/k6pvmi7htgnpDPkeMLxEfF1bkCcDw3wCMNAAAAAAAAsEcjDQAAAAAAAPADjTQAAAAAAADAD6yRBgAAAAAAUND5nJxLYRKA42FGGgAAAAAAAOAHGmkAAAAAAACAH2ikAQAAAAAAAH6gkQYAAAAAAAD4gQ8bAAAAAAAAKOgcSb783ok8FnifNcCMNAAAAAAAAMAfNNIAAAAAAAAAP9BIAwAAAAAAAPzAGmkAAAAAAAAFnOOTHF8ALip2DU4ArvnGjDQAAAAAAADADzTSAAAAAAAAAD/QSAMAAAAAAAD8wBppAAAAAAAABZ3v/y+FSQCOh0Yacr3++utavHix3G637rvvPvXt21d/+MMfdPvtt+ubb75R+fLlNWnSJJUsWVLr16/Xyy+/rOzsbFWoUEHPP/+8SpUqpTZt2qhLly766KOPdPHiRU2YMEG1a9e+as2vvvpKzz//vNLS0pSSkqLHHntMffr00ZkzZzRy5Eh9++23CgkJUVxcnJo0aaIlS5botddek8vlUp06dfT8888rODj4V7yXAAAAAADAjYpTOyFJWrdundasWaOFCxcqMTFRBw8e1IYNG/TVV1+pb9++Wrp0qapWrarp06crJSVFU6ZM0VtvvaVFixapWbNmmjx5cu62SpYsqQULFqh3796aMWPGNevOnz9fTz75pBYuXKh///vfmjhxoiTppZdeUqVKlbR8+XJNnDhRL774ojwej/7+97/rX//6l5YuXSqv16t169b9ovcLAAAAAADAJS7HcQrXZ6fCyIQJE1S2bFkNGDBAkrR69WotWrRIX3/9tVasWCFJ+vLLL/XMM8/o6aef1rBhw1SuXDlJks/nU4kSJfTuu++qTZs2evPNN1W1alV98sknevXVVzVr1qyr1vV6vdqwYYP27dunr776SsnJydq3b586deqkyZMnq3r16rm3ff/995WcnKzp06f/gvcEAAAAAAAFz9nHe8t38nh+70aeKhJZViVej8/v3bgunNoJSTnNsP+WnZ2toKD/PEQcx5Hb7ZbX61WDBg30+uuvS5IyMjKUmpqae7vQ0FBJksvl+tm6f/zjH1W8eHG1bt1aHTp0UHJysiQpKCjoivz+/ft/dF1KSook6ZZbbvF7nIcffFjZxzx+3/6SKjtX6UDddtedu+TkqZuNs42PLdKW8r8xzp/whhlnO3nilRzV2yhbPSLFuK4k3bFnpb6p9YBR9utT/j8m/lt7T7yWG45Zkoq6so2zzY8v0IayPYzzpcIvGmdrf7tUu2/vaJz34+l+VbX2L9Weqma1U1JvMi8su/u8WEimcd36hxZre6UuxvlaW6cZZ4Mjqyrr5H7j/LLao4yzXT3vKSmqj1G2QaXrP3ZfruKna3S4URujbNLx8sZ1Bx1+V9MrPmyc/93U6j9/o6sI7zlKafPHGuc3DNplnI3xzNWKqF7GeZtTJtp55mqVYe1sWRzMZPcaUreK+WP81k8+1NF7Wxvntx4oa5zt4nlPiw2f15LktXgB6XZ8jhLL9jXOhzrmC/N08MRrmeHv+maZv1eQpJaeBVoXZfbalWbx55/te6TyYak/f6OrqHdwiXZU7mycP5Zu/l7cdtw2s1ZsHmc2bOseDjZ/nP3vkXc1o4LZ62bRChHq98mLxrUDgeNz5PgK11yoQBwPp3ZCknTvvfdq6dKlSk9PV3Z2thYuXKh7771XBw4c0BdffCFJWrhwoVq0aKF69epp+/btOnDggCTpH//4R+4pmdfr448/1pAhQ9S2bVutX79eUs4stYYNG2rp0qWScppoAwcOVJ06dbR9+3adPHlSkvTCCy/ogw8+sB06AAAAAACAX5iRBklS69at9cUXX6h79+7Kzs5Ws2bN1Lp1a82YMUMvv/yyDh06pGrVqmns2LEKDw/XCy+8oD/+8Y/y+XyKiorSpEmTjOoOHjxYffv2VWhoqKpXr65bb71VR44c0ZAhQzRq1Ch16dJFQUFBmjhxoqKiojRy5Mj/Y+/Ow6Oo8v2PfzoLJggJYUkwbKKMOENAYIAgcAWi7NuwKCgS5KdwlQS3EYEJm7IogoZFBxccFQj7jhJBFh0GmAAqIsp4GRAkLB1yGfas3fX7gyGXDAEqpwJJ4/v1PP086e761vecrurqzrdPndKTTz4pr9er+vXrq0ePHkX8SgAAAAAAABSMQhryDB48WIMHD867n5qaquDgYM2cOfOKZWNiYhQTc+UpMhs3bsz7Ozo6WtHR0dfMOWDAAA0YMKDA56ZPn37FY+3bt1f79u2vuU4AAAAAAIAbgUIabrhJkyZp69atVzweFRWlCRMmFEOLAAAAAAAACo9CGq6qatWq+UaYmRo2bFgRtAYAAAAAgF8xS5L5dVJKJt+71gAXGwAAAAAAAADsoJAGAAAAAAAA2EAhDQAAAAAAALCBOdIAAAAAAABKOMt78XYr8cX+MCINAAAAAAAAsIFCGgAAAAAAAGADhTQAAAAAAADABuZIAwAAAAAAKOm8/77dSnywP4xIAwAAAAAAAGygkAYAAAAAAADYQCENAAAAAAAAsIE50vCrsuNkBZ1PtwodV1PS39LDjfOGeJyd+H3ME2wcW96V4yh3qCvXKG7HyUqO8tZysI5qrkxHucs6eM0yLGeH1UzL3zj2l/NljWOjHMbnymUcW0fS/nMhRrF+Kvz7+T+dNtxmrmxnuc9lBxrHrokaaRzbzT3fUXzHPeONY53EJ9Uf4yjvE5I2HI00ig2znG3rMI95/Ob4741j2z3sLN7f4fvLSXyOw996TY9JRXFMMV3HXw+Z7Z+S9KjD+NuLcYKaHPOPD8fx5S2z7ziX3C6z+EyZf9YX5TputiOZtxvH3ucw3innR4abz+Fby1H8nTnO/v8wjQ/Kdfae9gWW9+LtVuKL/WFEGgAAAAAAAGADhTQAAAAAAADABgppAAAAAAAAgA0U0gAAAAAAAAAbuNgAAAAAAABASWdJxXh9mBvDB6/mwYg0AAAAAAAAwAYKaQAAAAAAAIANFNIAAAAAAAAAG5gjDQAAAAAAoISzvBdvtxJf7A8j0gAAAAAAAAAbKKQBAAAAAAAANlBIAwAAAAAAAGxgjjQAAAAAAIASjjnSSgZGpAEAAAAAAAA2UEjDDVG7du3ibgIAAAAAAECRopAGAAAAAAAA2MAcabAlJSVFf/7znxUQEKDU1FTVq1dPzzzzjJ566ilt3LhRkjRjxgxJ0pAhQyRJo0aN0u7duxUWFqaJEycqMjLyqusfPny4Tp06pUOHDmno0KGqWLGiXnvtNWVmZiosLEyvvPKKqlWrpr1792r06NHKzMxUaGiopkyZosqVK9/4FwD0Ly05AAAgAElEQVQAAAAAgGLEHGklAyPSYNu3336rhIQEff7558rKytJXX311zeUbN26slStXqk2bNpowYcJ111+uXDklJyerRYsWGjlypN58800tX75cAwYM0KhRoyRJL730kgYPHqzVq1erY8eO+uSTT4qkbwAAAAAAANfjsizLKu5GoORLSUnR+++/rw8//FCStGHDBi1atEj79u0rcETafffdp++++06SdO7cObVu3Vo7duy46vqHDx+uevXq6bHHHtP//M//qHfv3qpevXre8+fOndPixYvVvn17bd++/UZ1EwAAAACAEulEn97yuo8XdzOKlF9EZVVasLC4m1EonNoJ2/z9/fP+tixLFy5c0OV12NzcXAUEXNyl/Pz88i176fFrCQoKkiR5vV5VrVpVK1eulCR5PB6lp6crMDBQLpcrb/msrCylpaWpWrVqtvuwrMlzOp+abnv5S/odTdKcyL6FjrskxGM+XrWbe75WRjxqHF/elWMc+1/Hl2hz5V5Gsamu24zzStKjx5I0/w6z17yalWmct8Xxpfpb5Z7G8RmW+WG1jXuhvojobRyfI9f1F7qKju4FWhPRxzg+10Huru75WmW4j/vJ2W9Bnd0L9Klhv8s5eG853c/+1yplHOv0mNJxz3jj2MBKdyvnxH6j2KT6Y4zzStITR+bq4yqPG8UGOvjNse/RJCU5+Pyo6Mk1jm3nXqi1Do4p/g7eXw+5F2l9xCPG8TkOTpro4F6gZMP3tdNjipPX/KSf+eeHk89MSbrda/49xckxXJIy/cw/Px45Nk+L7njMOD7Cm20c29K9RF9FmH1HypT/9Re6Bif7mdfB57WT95YkR+8up99TnPDV3OZb2vm2dnIsdbJ/B1WrpJY73zbODdjFqZ2w7euvv5bb7ZbX69WKFSv00EMP6dSpUzp58qSys7O1efPmvGUvXLigDRs2SJKWLl2qZs2a2c5z11136fTp09q5c2de/EsvvaSyZcsqIiJCf/vb3yRJK1eu1LRp04qwhwAAAAAAAFfHiDTYFh4erpdffllut1vNmzfX448/rnPnzqlXr16qXLmy6tatm7dsSEiI1q9fr2nTpikiIkKvvfaa7TylSpXStGnTNGHCBGVlZalMmTKaNGmSJGny5MkaO3asJk+erLCwML3xxhtF3k8AAAAAAEoel2Q5GW9YEvlefyikwbaKFSteMbl/XFyc4uLirlj2WvOhFeT111/Pd79BgwZasmTJFcvVrl1b8+fPL9S6AQAAAAAAigKFNNw0kyZN0tatW694PCoqytZVPQEAAAAAAIoThTTYEh0drejoaEfrGDZsWBG1BgAAAAAA4OajkAYAAAAAAFDSeSXL/ELLJZMP9oerdgIAAAAAAAA2UEgDAAAAAAAAbKCQBgAAAAAAANjAHGkAAAAAAAAlnOWVLK+ruJtRpHxxzjdGpAEAAAAAAAA2UEgDAAAAAABAibd69Wp17NhRbdu2VVJS0hXPr1+/Xt26dVPXrl01ePBgnT59WpK0fPlytWjRQt26dVO3bt2UmJho3AZO7QQAAAAAAECJ5na7lZiYqGXLlqlUqVLq06ePoqOjVatWLUnSuXPnNHbsWC1dulQRERGaNm2aZsyYoZEjR2rPnj0aPny4Onfu7LgdjEgDAAAAAAAo4S7OkXbr3ezaunWrmjZtqnLlyql06dJq166dPv/887znc3JyNGbMGEVEREiSateurWPHjkmSvv/+ey1fvlxdunTRSy+9lDdSzQSFNAAAAAAAABSbY8eOKTU1Nd/tzJkz+ZZJS0tTpUqV8u6Hh4fL7Xbn3Q8LC1ObNm0kSZmZmXr//ff10EMPSZIqVaqkwYMHa9WqVbrjjjv06quvGreVUzsBAAAAAABQbPr27asjR47keyw+Pl5DhgzJu+/1euVy/d9VSy3Lynf/krNnzyouLk733nuvunfvLkl655138p5/6qmn8gpuJiik4Vclpv5xeascN4rtFJ1qnPfA9jDjWEmqEpBhHHvYU9pR7nSVMopr2+iwo7xO1vG37VUc5f1fy6zPkrNtJUlhAVnGsTWiTjrK3bj+UePYfd9VdJS7osus324FOcorSTkus8HZJx3sJ07jG1Z3X3+hGxSfVH+McewTR+Yax/fd9YpxXqfrONp2kKO8LSLMPnckKSfb31Huuyv+yzj2x/QKjnJfkHnbA2U5ym3Koyu/jN+sdXTuab6tnMZvWhzqKLcTWQX8A3Sz4jMd7KNO4kP8chzldbKOU15nn11O3Fn2zPUXukHxh86GOMrtZC8tnqOZ5Ocws5P4vaUCjWPbOYgPCQxQS+PMKG5JSUnyeDz5HgsJyf/erVy5snbu3Jl3/8SJEwoPD8+3TFpamp588kk1bdpUf/rTnyRdLKwtXbpUTzzxhKSLBTh/f/PPAE7tBAAAAAAAQLG54447VLVq1Xy3/yykNWvWTNu2bdPJkyeVkZGhdevW6YEHHsh73uPx6Omnn1aHDh2UkJCQN1qtdOnSmjVrlr777jtJ0ty5cxmRBgAAAAAAcCuzLJcsy/no6ZKkMP2JiIjQCy+8oNjYWOXk5KhXr16qV6+eBg4cqGeffVbHjx/Xjz/+KI/Ho7Vr10qSoqKiNGHCBE2dOlVjx45VZmam7rzzTr3xxhvGbaaQBgAAAAAAgBKvS5cu6tKlS77HPvjgA0lS3bp19Y9//KPAuEaNGmn58uVF0gZO7QQAAAAAAABsoJAGAAAAAAAA2MCpnQAAAAAAACWcZUmWt7hbUbSs4rq0rQOMSAMAAAAAAABsoJAGAAAAAAAA2EAhDQAAAAAAALCBOdIAAAAAAABKOMvrkuV1FXczipQv9ocRaQAAAAAAAIANFNIAAAAAAAAAGyikAQAAAAAAADYwRxoAAAAAAEAJZ1kXb7cSX+wPI9IAAAAAAAAAGyikAQAAAAAAADZQSMNN0a9fP6WkpFz1+dTUVMXExBTpOgEAAAAAAIoShTQAAAAAAADABi42gCukpKRo8uTJ8nq9qlKlikqXLq19+/bJ4/Fo4MCB6ty5s7p3765x48YpKipKHo9HrVu31vLly3X48GFNmDBBWVlZCgsL06uvvqoaNWrYypuVlaXnnntOP//8s6pXr64JEyYoNDRUycnJ+uijj5SZmans7GxNnDhRDRs2vMGvAgAAAAAAJYfldcnyuoq7GUXKF/vjsixfvEYCbqSUlBTFxcVp06ZNeu+99xQeHq7Y2FidO3dOffr00cyZM7Vhwwa53W4NGzZMW7Zs0SeffKK3335b7du319SpU1WvXj0lJydr1qxZWrp0qfr166f4+HhFR0cXmDM1NVUPPfSQ5s6dq0aNGmnSpEnyeDwaPny4BgwYoMTERJUvX15LlizR+vXr9e677153nQAAAAAA3CpSO/WV55i7uJtRpPzviFDVz5KKuxmFwog0FKhmzZoqW7astm7dqszMTC1dulSSdOHCBe3bt0+dOnVS79699fLLL+vTTz9V165ddfDgQYWEhKhevXqSpA4dOmj06NE6e/as7ZyNGjWSJHXr1k3Dhw+Xn5+f3nnnHW3cuFE///yztm/fLj8/8zOSTw3qLe+J44WOK7/8K53s3tI474HtYcaxjY6s0M4qfzCOP+wpbRzb/fg8La/8mFHsA41TjfNKUoXVf9X/dnnAKPZv26sY5+3mnq+VEY8ax1cJyDCOdbqta0SdNI6ttPavOtHO7PWWpH3fVTSObXZ8mbZW7mEU61aQcV7J2T7u7+B3qK7u+VrlYD9rUN38C1S1HRt1uHHh5qS83IajkcaxTxyZq4+rPG4U23fXK8Z5JSmw0t3KObHfKPZo20HGeWt8u0GHGjxoHJ+T7W8cW+uHdfpnnbbG8T+mVzCOdbqPB8r8/dXBvUDJEX2MYp3+utzRvUBrDHP/18NnjPOWfXuNzsZ3NI7ftDjUONbptj7tb/69rt/RJM2J7GscH+7JNY5t516otRG9jWJD/HKM80rS/ceWadsdZp+bp7yljPM6eW9JUo2y5vv47/65Rj/WMt/HD50NMY512m8nxxUnxxR/B5md7N+StLdUoHHs84fnamo1s+8KIVUr6v9tm2qcG7CLQhoKFBR08Z9Tr9eryZMnq06dOpKk9PR0hYaGKjAwUDVr1lRKSoq2bdum0aNH69ChQ1esx7IseTweWzkDAv5vd7QsSwEBATp//rx69eqlrl27qnHjxqpdu7aSknyrWg0AAAAAAG4NXGwA19S0aVPNnz9fkpSWlqauXbvq2LFjki6OGps0aZKio6MVHBysu+66S6dOndLu3bslSWvWrFFkZKTKlStnK9f+/fv1448/SpKWLl2qZs2a6eDBg3K5XHr66acVHR2tL774wnZhDgAAAACAW4blypsn7Va5yfK9OdIYkYZrio+P19ixY9W5c2d5PB4NHTpU1atXlyS1adNGY8aM0UsvvSRJKlWqlBITEzVu3DhlZGQoNDRUiYmJtnNVr15d77zzjn755Rfdc889euGFFxQUFKTf/va36tChg1wul1q0aKGvv/76hvQVAAAAAADgWiik4QrR0dF5E/iXKVNGU6ZMKXC54OBgffvtt/kea9CggRYvXnzFsnPmzLlmzqpVq+rzzz8v8Lm33nor3/2RI0faWicAAAAAAEBRopCGm+aXX37RkCFDCnxu/Pjxqlu37k1uEQAAAAAAgH0U0nDTVK9eXStXrizuZgAAAAAA4HMs6+LtVuKL/eFiAwAAAAAAAIANFNIAAAAAAAAAGyikAQAAAAAAADYwRxoAAAAAAEAJZ3ldsryu4m5GkfLF/jAiDQAAAAAAALCBQhoAAAAAAABgA4U0AAAAAAAAwAYKaQAAAAAAAIANXGwAAAAAAACghLMslyzL9ybnvxZf7A8j0gAAAAAAAAAbGJGGX5WjP5RRTmpIoePKSzr8TeHjLvFaxqGO4++vfcRRbtP4I9+bv16SVMHBOhrffdxRbifx+/ZXcJT7fK75YfmHXRHGsa0cxpf2zzGOlaQAP69RnL/H4ZtLkr9lto5T/s5+i3ISv/J4pHFsvMP4MMPX65JAw/ijbQc5ylvj2w3G64hc976j3E7iP6k/xji2lqQvT4Ubx1dwmb0vL/G4zH9hdribKVtmuQPl/Jhi6sdltxnHRr/tLD5Uzo7hTuJPy7zdTvk73N6m8Z4iGH1RFOu42Y6eLWMc+zuH8cXJ6ZYyjT/n8neU10n8b7OdHVNM44Nych3lBexiRBoAAAAAAABgAyPSAAAAAAAASjjLe/F2K/HF/jAiDQAAAAAAALCBQhoAAAAAAABgA4U0AAAAAAAAwAbmSAMAAAAAACjhvJZLXh+8au+1+GJ/GJEGAAAAAAAA2EAhDQAAAAAAALCBQhoAAAAAAABgA3OkAQAAAAAAlHCWJMsH5xS7Fqu4G2CAEWkAAAAAAACADRTSAAAAAAAAABsopAEAAAAAAAA2UEgDAAAAAAAAbOBiAwAAAAAAACWd1yXLe2tdbEA+2B9GpOGm6devn1JSUoxiR4wYoSNHjkiSBg4cKLfbrWXLlmn48OFF2UQAAAAAAICropAGn5CSkiLLunhh3A8++EARERHF3CIAAAAAAPBrw6mdKFBKSoomT54sr9erKlWqqHTp0tq3b588Ho8GDhyozp07q3v37ho3bpyioqLk8XjUunVrLV++XIcPH9aECROUlZWlsLAwvfrqq6pRo8Z1c6ampio2NlYbN26UJM2YMUOSdNtttyktLU2DBg1SUlKSevbsqdmzZ9/Q/gMAAAAAAPwnl3VpmA9wmZSUFMXFxWnTpk167733FB4ertjYWJ07d059+vTRzJkztWHDBrndbg0bNkxbtmzRJ598orffflvt27fX1KlTVa9ePSUnJ2vWrFlaunSp+vXrp/j4eEVHRxeY82qFtCFDhigmJkazZ89W1apV8/7evn27tm/frtdff/2mvS4AAAAAABSHf7Z6QjlH0oq7GUUqsEq4an35cXE3o1AYkYarqlmzpsqWLautW7cqMzNTS5culSRduHBB+/btU6dOndS7d2+9/PLL+vTTT9W1a1cdPHhQISEhqlevniSpQ4cOGj16tM6ePVucXcnzY/MnlZNa+APPfYdW67saXYzzZuX4G8c2ObpC2yP/YBxf/Tf/Mo6t/NVXOt6ypVFs2qGyxnklqd7BT7X7zs5GsRUjzxvnjdy6SUebtTaO37e/gnFsS/cSfRXRyzjekvlEna3ci/VlxMPG8aX9c4xjnezjxz3Bxnklqat7vlZFPGoUe8rffHaE2KNJmh3Z1zj+jL/5to4/PFdvV3vcOD7MY/77W9+jSUoy7HeLiOPGeSWpxrcbdKjBg0axkeveN84bWOlu5ZzYbxz/Sf0xxrFPHZmrWVXMt3UFj9c4tvvxeVpe+THjeD8Hv/N2c8/XSsP3daCc/b7c0b1AayL6GMVW8M8yzht9dLlSIrsbx2d6zL+nOP3s+sX/NuPYfkeTNMfBsfQOj/ln10PuRVof8YhRbJDLY5xXklocX6q/Ve5pFHvWCjTO28G9QMmG+7ckBcr8mOLk9ZakHAczGjnttxNOcl9wmfe55/F5WurgGF7GMt/H27kXam1Eb6PYoGqV1HLn28a5AbsopOGqgoKCJEler1eTJ09WnTp1JEnp6ekKDQ1VYGCgatasqZSUFG3btk2jR4/WoUOHrliPZVnyeK5/MHW5XLp8gGRubq4CAthFAQAAAABAycDFBnBdTZs21fz58yVJaWlp6tq1q44dOyZJ6tatmyZNmqTo6GgFBwfrrrvu0qlTp7R7925J0po1axQZGaly5cpdN09ISIhOnTqlkydPKjs7W5s3b857zt/f31YxDgAAAAAA4EZhuA+uKz4+XmPHjlXnzp3l8Xg0dOhQVa9eXZLUpk0bjRkzRi+99JIkqVSpUkpMTNS4ceOUkZGh0NBQJSYm2spTtmxZPfXUU+rVq5cqV66sunXr5j3XqlUrDRo0SLNmzSr6DgIAAAAAUMJZXpcsr/kUHyWRL/aHQhoKFB0dnXdRgDJlymjKlCkFLhccHKxvv/0232MNGjTQ4sWLr1h2zpw5180bFxenuLi4Kx5PSEhQQkKCJOVdjKBq1arq0aPHddcJAAAAAABQFCik4ab65ZdfNGTIkAKfGz9+fL5RaAAAAAAAACUJhTTcVNWrV9fKlSuLuxkAAAAAAACFRiENAAAAAACghPNaLnkt35tT7Fp8sT9ctRMAAAAAAACwgUIaAAAAAAAAYAOFNAAAAAAAAMAGCmkAAAAAAACADVxsAAAAAAAAoISzLJcsH5yc/1p8sT+MSAMAAAAAAABsoJAGAAAAAAAA2EAhDQAAAAAAALCBOdIAAAAAAABKOEuSZRV3K4qWL3aHQhp+VULDMuTJvGAUG1bRLE6STrjLGMdKkr+f+eFly09VjGN7OohvGH7COO8lZUOyjOJSDtxhnLO7w/jqgeeNYyWpTGCOcWxoaIaj3FUrnjaOPXMmyFHuAH+vUVyNwHOO8kpSjSCzddwdYNbmS34ffMo4tubr9zvK/f/eutc4dnP8945yV/TkGsXlZPs7yutkHZ/UH2Oc86kjcx3F99/1inGs0/iNUX9ylLu05TGODZSz91dZGe5nRXCyhr/hvwR33/e/jvI6id/6jfl3BUk6rUDj2GCvs3+hnMRnOtzepvGlDffPy/n54L+ev4l0to87if/xaCVHuZ1wuqVM42+znB1HncTvCDIvM7RzEB96m79aGmcG7OPUTgAAAAAAAMAGCmkAAAAAAACADZzaCQAAAAAAUMJZlktey1XczShSlg/2hxFpAAAAAAAAgA0U0gAAAAAAAAAbKKQBAAAAAAAANjBHGgAAAAAAQAlnWS6fnFPsWnyxP4xIAwAAAAAAAGygkAYAAAAAAADYQCENAAAAAAAAsIFCGgAAAAAAAGADFxsAAAAAAAAo4Szr4u1W4ov9YUQaAAAAAAAAYAOFNAAAAAAAAMAGCmkAAAAAAACADcyRBgAAAAAAUMJ5LZe8lqu4m1GkfLE/jEjDFc6ePau4uLhrLjNixAgdOXLkmsv069dPKSkpRdk0AAAAAACAYkMhDVc4ffq09u7de81lUlJSZPni5TUAAAAAAAAMcWonrjB+/HilpaUpLi5OMTEx+uijj+RyuVSnTh2NGjVKSUlJSktL06BBg5SUlKS///3v+uijj5SZmans7GxNnDhRDRs2tJUrMTFR27Zt0+nTpxUeHq7ExERVrFhRq1ev1syZM+VyuVS3bl2NGzdO58+fV0JCgg4cOKBSpUpp+PDhuv/++2/wqwEAAAAAAHCRy2JYEf5DamqqYmNjNXPmTMXHx2vRokUKCwvTK6+8oqCgIA0bNkwxMTGaPXu2IiMjNWDAACUmJqp8+fJasmSJ1q9fr3fffVf9+vVTfHy8oqOjC8xz6NAhTZkyRdOmTZOfn59efvll3XvvverUqZO6d++uZcuWqXLlyho6dKjatWunLVu25OX/6aefNHr0aC1cuPAmvzoAAAAAANx830UPUnbqieJuRpEqVbWS7kt5v7ibUSiMSMNV7dixQ61bt1ZYWJgkqXfv3hoxYkS+Zfz8/PTOO+9o48aN+vnnn7V9+3b5+dk7Y7hGjRoaNmyYFi9erJ9//lm7du1S9erV9e2336phw4aqXLmyJGny5MmSpKlTp2rKlCmSpNq1axsV0Y50fkyeY+5Cx1X/eoN++f2DhY675IS7jHHs71NX6uuq3YzjD+bebhzb8/g8La38mFFsw3BnB/iau7/Qz/XaGMXuSqtknLf78XlabthnSaoecN441um2Dg3NMI6t9cM6/bNOW+P4M2eCjGMbHl6lb6p1NYr193P2W9B9h1bruxpdjGIDArzGeevs/0w/3N3JOL7m6+ajcUs/PFIXFo83jt8c/71xbDv3Qq2N6G0Ue3fFfxnnlZzt41+eCjfO+9SRuZpV5XHj+P67XjGODax0t3JO7DeO3xj1J+NYJ9takgJl/v6KcS/WxoiHjWJzHM564qTfv294zDhvxeS/Kr3DA8bxW7+pYhzb1T1fqyIeNY7PdplPMt3r+DwtcfCZHWSZ72ed3Qv0aUQfo9jyrmzjvJLU7Pgyba3cwyj2tFXKOG8H9wIlG/ZZkn4Xaf7dsMa3G3Sogfl38R+Pmn83dNpvJ99UOroXaI1hbvO929n+LUm7gvyNY0ceStL4Gn2NYkOrVtSQLdOMcwN2MUcarsrrzX/4tSxLubm5+R47f/68evXqpdTUVDVu3Fj9+vWzvf49e/boySeflNfrVbt27fTQQw/JsiwFBATIddmXqpMnT+rkyZNXPL5///4r2ggAAAAAAHCjUEjDFQICApSbm6smTZpo48aNOnXqlCRp0aJFeadp+vv7y+Px6ODBg3K5XHr66acVHR2tL774Qh6Px1aeHTt2qEmTJnr00Ud155136ssvv5TH41HdunW1a9cunThx8VeriRMnasOGDWrUqJE+++wzSReLaAMHDsxXWAMAAAAAALiROLUTV6hQoYIiIyM1YcIE/fd//7f69eunnJwc1alTR6+8cvEUk1atWmnQoEH64IMP9Nvf/lYdOnSQy+VSixYt9PXXX9vK07FjR8XHx6tLl4unVUVFRSk1NVURERFKSEjIG61Wv3599ejRQ+fPn9fIkSPVtWtXBQQE6I033qCQBgAAAAD4VfBaLnmtW+t/YF/sD4U0XCEwMFALFizIu//ww1fOM5KQkKCEhARJ0ltvvZXvuZEjR0qS5syZc808ERERWrx4cYHPtW/fXu3bt8/3WEhIiKZPn379DgAAAAAAANwAFNJwQ/3yyy8aMmRIgc+NHz9edevWvcktAgAAAAAAMEMhDTdU9erVtXLlyuJuBgAAAAAAgGNcbAAAAAAAAACwgRFpAAAAAAAAPsAq7gaAEWkAAAAAAACAHRTSAAAAAAAAABsopAEAAAAAAAA2MEcaAAAAAABACee1XPJaruJuRpHyxf4wIg0AAAAAAACwgUIaAAAAAAAAYAOFNAAAAAAAAMAG5kgDAAAAAAAo4Sy5ZPngnGLXYsn3+kMhDb8qPx6voIxUq9Bx1SXtSg03zhugwue8nDsn2Dg2RB5HuUMss/gf3BUc5a3pYB1llesod1nLPN7JtnIa7043j60l6X/SyxvHO/34c2eXNooLdWU7zCydzwo0ijubaRYnSXUkpZ4rYxyfGv+9cWy7h6XNDuL9HR7PTON/THd2TKnlYB0VXF5HuSt4zOM3Rv3JOLade6Gj+Jg9E41jncZviRruKLcvnnKx/ZtI49iODuP9HL6vncSXcpZapSzzFQTJ2XvbNP60VcpR3qJax832w9FKxrE1HMYX57/pTnObxvs7zOsk/veZzv7/MI0PznL2ngbs8sXvGQAAAAAAAMBNRyENAAAAAAAAsIFTOwEAAAAAAEo4779vtxJf7A8j0gAAAAAAAAAbKKQBAAAAAAAANlBIAwAAAAAAAGygkAYAAAAAAADYwMUGAAAAAAAASjhLLllyFXczipQv9ocRaQAAAAAAAIANFNIAAAAAAAAAGyikAQAAAAAAADYwRxoAAAAAAEAJZ1mS1yruVhQtywf7w4g0AAAAAAAAwAYKaQAAAAAAAIANFNIAAAAAAAAAG3yqkDZ8+HAtW7ZMqampiomJMVrHiBEjdOTIEUnSwIED5Xa7i7KJ17Rlyxb1798/7/758+f13HPPqUuXLurSpYs+++yzIslzeR9jYmKUmppaJOstSWbMmKEZM2YUdzMAAAAAALgpvHLdkjdf41OFtKKQkpIi69+z2X3wwQeKiIi44Tm9Xq/+8pe/6MUXX5TX6817/P3331dkZKRWr16tjz/+WK+99prS09Md57u8jwAAAAAAACgaxX7VTsuyNGXKFK1fv17+/v7q3bu3HnjgAY0ePVqnTp1S6dKllZCQoHr16hUYn56ertGjR+v48eNyuVz64x//qOaRSO8AACAASURBVGbNmunUqVNKSEjQgQMHVKpUKQ0fPlzff/+90tLSNGjQICUlJalnz56aPXu2IiMjNXHiRG3btk0ul0tdu3bVoEGDlJKSovfee09BQUHav3+/ateurSlTpig7O1svvvhiXtErLi5ODz744FX7uH//fu3fv1/jxo3TnDlz8h5v0qSJatasKUmqUKGCypUrp/T0dFWsWLHA9aSkpOjdd99VYGBg3qi80qVLa/369ZIuFuaWLVuWr4+S9M4772jv3r3KyMjQG2+8ofvuu08fffSRli9fLj8/P9WrV0+vvvrqNbfT+++/r+TkZHk8HrVo0UJDhw7Vxo0b9cYbb2jVqlU6fvy4+vXrp8WLF1+1ODljxgwdPXpUBw8e1MmTJ/XMM89o27Zt+u6773TvvfcqMTFRLperwFwul0uzZs3SokWLFBYWppCQkKvuEwAAAAAAADeCyyrmoUvJycmaPXu2PvnkE+Xk5Oixxx5TZmam/vjHP6pt27batWuXXnjhBa1du1ajR49WkyZN1KRJE8XGxmrjxo164YUX1LlzZz344INKS0vTY489phUrVujNN99UUFCQhg0bpp9++kmjR4/WwoULFRMTo9mzZ6tq1ap5f3/11VfaunWrpk+fruzsbPXr10/x8fEKDg7W008/reTkZIWHh+uRRx7R4MGDdfr0ae3evVtjxozR3r17tWrVKg0bNuy6fU1JSdHbb7+dr5h2yZo1azR16lStWbNGAQEF1zdTUlL0zDPP6LPPPlO5cuXUrFkzDRs2TH369NGIESN07733qn///lf0sW/fvnryySc1d+5cbd++XYmJiWrevLk2b94sf39/JSQk6Pnnn79qAeyvf/2rlixZklfoGjp0qB544AF169ZNw4YNU2RkpFJSUtS3b1916tTpqv2fMWOGvvzySy1cuFDffPON+vfvr9WrV+vOO+9Ux44dNX36dKWlpRWY66677tLLL7+sJUuWyOVyqXfv3mrbtq2GDBly3dcdAAAAAABft7VRnDIPnyjuZhSpoGqV1GznO8XdjEIp9hFpO3bsUIcOHVSqVCmVKlVK8+bNU+vWrdW2bVtJUv369RUaGqoDBw4UGL9161YdOHBA06dPlyTl5ubq8OHD2rFjh6ZMmSJJql27thYuXHjVNqSkpKh79+7y9/dXcHCwunTpom3btikmJka/+c1vVLlyZUnS3XffrdOnT6tBgwZ666235Ha71apVK8XFxTl6DZKTkzVx4kTNmjXrqkW0S+655x7dcccdkqSwsDDdf//9kqTIyEidOXOmwJiHHnpIklSrVi2tXbtW/v7+atCggXr16qUHH3xQAwYMuOYprtu2bdPu3bvVo0cPSVJmZqYiIyMlSQkJCerYsaMaNmx4zSLaJc2bN1dAQIAiIyNVqVIl1apVS5IUERGh06dPXzVXenq6WrZsqdtvv12S1L59+3ynydr1RaMhyjhc+NNnu7rna1XEo4WOuyRA5vXqju4FWhPRxzg+0EHuNu6F+iKit1FslsNz3Tu7F+hTw34HqfD7xiUPuRdpfcQjxvHZDs6Yd7qtnXCa28nW7uBeoGTD3KGubAeZpWbHl2lr5R5GsWetQOO87dwLtdbwveWU09z+Do4pTt5fF+RvnFdydhz3uMz38O7H52l55ceM40tbHuNYp9s6Zs9E49jASncr58R+4/gtUcONY1u5F+vLiIeNYrMcznri5DX3ODiSOj2Gm39qOvu8vpjbvN9Ov5+Vlvn7y8nxLMfhfubkc7M48zoZveGr31Oc8tVtXVy5g6tVUqudt/Y82pZcsnxwTrFr8cX+FHshLSAgQK7LvqAePnz4ivm9LMuSx1PwB53X69Unn3yicuXKSZLS0tJUoUKFK9a7f//+vNMoC1rH1fLddttteY+7XC5ZlqU777xTycnJ2rx5szZt2qS//OUvWrNmjfz8Cv+hOGfOHH344Yf68MMPVbt27esuHxiY/x83f//r/2NxaZnLX48///nP2rVrl/7617/qqaee0pQpU9SkSZMC4z0ej/r3768BAwZIks6cOZO3zvT0dPn7++vAgQPKysrK93pdr/0FFQ2vlmvhwoX59ouAgABlZzv7BxoAAAAAAKAwiv1iA40bN9a6deuUk5OjjIwMPf/883K5XFq3bp0kadeuXUpPT9dvfvObAuObNm2qefPmSZL++c9/qkuXLsrIyFCjRo3yroK5f/9+DRw4UC6XS/7+/lcU5Zo2baoVK1bI4/EoIyNDq1evVnR09FXbPHfuXM2YMUMdOnTQmDFjdPLkSZ07d67QfV+/fr0+/vhjzZ8/31YRza6C+ni5kydPqmPHjrrnnnv03HPPqXnz5vrpp5+uunzTpk21cuVKnT9/Xrm5uYqLi9PatWvl8Xg0YsQIJSQkqEmTJpo2bZrjtl8t1/33369Nmzbp7NmzysrK0hdffOE4FwAAAAAAQGEU+4i0Nm3aaM+ePerRo4e8Xq9iY2MVHR2tsWPHasaMGQoMDNSMGTNUqlSpAuNHjhyp0aNHq0uXLpKkN954Q2XKlNGzzz6rkSNHqmvXrgoICNAbb7whl8ulVq1aadCgQZo1a1beOnr37q2DBw+qW7duysnJUZcuXdSmTRulpKQUmPMPf/iDXnzxRXXp0kX+/v4aOnSoQkJCCt336dOnKysrS08//XTeY+PHj1fdunULva7LFdTHy5UvX169e/dWr169FBwcrJo1a6pnz55XXV9MTIz+8Y9/6JFHHpHH49F//dd/qXv37po1a5YqVKigtm3bqlmzZurcubPatm2r+vXrG7f9arlcLpf69++vXr16KSQkJO/UUgAAAAAAgJul2AtpkvTCCy/ohRdeyPdYQRPyv/7663l/b9y4UdLFubXee++9K5YNCQnJmzftcgkJCUpISMi3DuliQe4/RUdH5xuZdnn+999//6r9uZr/XN+qVascxV/e/ssn3b9aHy+Pf+KJJ/TEE0/Yzj148GANHjw432MDBw7M+7tMmTL68ssvr7mOy9tYtWrVfG27fHsXlEuS+vbtq759+9puMwAAAAAAQFEqEYW0W8GkSZO0devWKx6PiorShAkTbK9n586dGjduXIHPvf/++9e8KIATRZX3448/1vLly694PDw8XB988IGjNgIAAAAA8GvllbMLxJREvtgfCmlFZNiwYUWynkaNGmnlypVFsq7iyFvYkW4AAAAAAAC+otgvNgAAAAAAAAD4AgppAAAAAAAAgA2c2gkAAAAAAFDCWXLJkqu4m1GkfLE/jEgDAAAAAAAAbKCQBgAAAAAAANhAIQ0AAAAAAAAl3urVq9WxY0e1bdtWSUlJVzy/d+9e9ejRQ+3atVNCQoJyc3MlSUePHlXfvn3Vvn17PfPMMzp//rxxGyikAQAAAAAAlHCWJO8tdrMK0X+3263ExETNmzdPK1as0MKFC/XPf/4z3zJDhw7V6NGjtXbtWlmWpUWLFkmSXnnlFT322GP6/PPPFRUVpT//+c+FyJwfhTQAAAAAAACUaFu3blXTpk1Vrlw5lS5dWu3atdPnn3+e9/yRI0eUmZmp+vXrS5J69Oihzz//XDk5OdqxY4fatWuX73FTXLUTAAAAAAAAxebYsWPyeDz5HgsJCVFISEje/bS0NFWqVCnvfnh4uHbv3n3V5ytVqiS3261//etfKlOmjAICAvI9bopCGgAAAAAAAIpN3759deTIkXyPxcfHa8iQIXn3vV6vXC5X3n3LsvLdv9rz/7mcpCvuFwaFNPyqBErKLdRZ2JfHmsUVBfO3uORy2G7T+EBHWS+twyy3n8M+O4n3d5jbSbzTc/Wd7OO5jvZSc2WCsottHacySjnK63Hwmjk9HjnZV3Ic7mmm8UVxDDZdh+UwtZ+DFQTK6yi3k/gtUcONY1u5FzuKb77ndeNYJ/HroxIc5ZUkr+F7u5TDbe0kPtvh+9pZdPF9Zjs5DjuJd/o9xck6TPfPouD0OO6L31N+rXIcvt6m8b+G4salecVuJZf6k5SUVOCItMtVrlxZO3fuzLt/4sQJhYeH53v+xIkTeffT09MVHh6u8uXL6+zZs/J4PPL3978irrCYIw0AAAAAAADF5o477lDVqlXz3f6zkNasWTNt27ZNJ0+eVEZGhtatW6cHHngg7/kqVarotttu09dffy1JWrlypR544AEFBgaqUaNGWrNmjSRpxYoV+eIKi0IaAAAAAAAASrSIiAi98MILio2N1R/+8Ad17txZ9erV08CBA/X9999LkqZMmaLXXntN7du314ULFxQbGytJGjNmjBYtWqSOHTtq586dev75543b8WsY/QgAAAAAAAAf16VLF3Xp0iXfYx988EHe3/fee6+WLFlyRVyVKlU0Z86cImkDI9IAAAAAAAAAGxiRBgAAAAAAUMJZkqxb7OIZxXdJP3OMSAMAAAAAAABsoJAGAAAAAAAA2EAhDQAAAAAAALCBOdIAAAAAAABKOK/r4u1W4ov9YUQaAAAAAAAAYAOFNAAAAAAAAMAGCmkAAAAAAACADcyRBgAAAAAAUMJ55ZJXPjip2DX4Yn8YkQYAAAAAAADYQCENAAAAAAAAsIFCGm6o2rVrX/P5ZcuWafjw4ZKkgQMHyu1234xmAQAAAAAAFBpzpKHE+OCDD4q7CQAAAAAAlFhWcTcAFNJKktzcXI0dO1b79u1Tenq6ateurbfeeksLFizQ/Pnz5e/vr9atW2vo0KE6cuSIRowYoZMnTyooKEjjx4/Xvffee9V1t2jRQu3atdPXX38tf39/TZ06VdWqVdOuXbs0YcIEZWVlKSwsTK+++qr+8Y9/KDk5WVOnTtXPP/+s9u3ba8uWLapYsaKefPJJPffcc6pXr16BeVJTUzV06FBduHBB9913X97j58+f16uvvqp9+/bJ4/Fo4MCB6ty5c77YmJgYzZ49W9u3b9fmzZt1+vRpHT58WM2bN9fYsWNlWZZef/11ffnllwoPD1f58uXVsmVL9ejRo2g2AAAAAAAAwDVwamcJ8u233yowMFALFy7UF198obNnz2r27NmaN2+elixZolWrVumHH37Qnj179Morr6hdu3b69NNPNWTIEM2cOfOa6z5x4oTuv/9+rVixQo0bN1ZSUpKys7P14osvatSoUVq1apX69OmjF198Uc2bN9fXX38ty7L097//XRUqVND27duVmZmpn3/+WXXr1r1qnnHjxqlHjx5auXKlGjZsmPf4zJkzVadOHS1btkxJSUl69913dfjw4Wu+FtOnT9eqVau0adMm/fTTT0pOTtYPP/ygTz/9VImJidq5c2fhX2QAAAAAAABDLsuyGBlYguzbt0/bt2/XgQMHtG7dOvXu3Vtnz57ViBEj8i3XoEEDbd68WWXKlLG13tq1a2v37t267bbbtHTpUu3cuVMDBgzQyy+/rBUrVuQt17hxY23cuFFxcXH605/+pJkzZ+p3v/udjh8/rlatWmnt2rWaOHHiVfNc3i6v16t69eppz5496tGjhzIzMxUYGChJOnv2rEaOHKlTp05p+/btev31168YkZaYmChJevzxx/Xss88qOTlZNWvWVGxsrCRp1KhRatCgASPSAAAAAAC3vHWNn9WFw+nF3YwiVbpaRbXdMb24m1EonNpZgmzYsEHTp09XbGysevTooX/9618qW7aszp07l7eM2+1WcHCwAgL+b9NZlqX9+/erVq1a11z/bbfdJklyuVyyLEter/eKZSzLksfjUatWrbRlyxYdOHBAY8eOVWxsrPz8/NS6devr9uNSbdblcsnP7+KgR6/Xq8mTJ6tOnTqSpPT0dIWGhmr16tXXbOvl7Q0KCsq3zOWvgV1fNhqijMMnCh3Xwb1AyRF9Ch1XFJzmDtSV29muh9yLtD7iEaNYj1zGeSWpnXuh1kb0Nop10ucY92JtjHjYOD7HwUBfJ32WnA0xbuNeqC8c5M51sL2d7ONVgs9df6FrqHfwU+2+s/P1FyxAaoa9HzIK0tG9QGscva/NfwPz1W3tlJPc2Q763M09XysjHjWOL6tc41inxzMnx5RW7sX60kHu5nteN44NrHS3ck7sN4pdH5VgnFdytp8V1+e1JGU72NpOj2fmvZY6uxfo02I6ljr9zHbCSW5vMR7DA/js8pncTvPmOHi9u7rna5Xh52ZwtYpqs3OGcW7ALk7tLEG2bdumDh06qGfPngoJCVFKSoo8Ho+++uornT9/Xrm5ufrjH/+oPXv2qFGjRvrss88kSVu3btWoUaMKne+uu+7SqVOntHv3bknSmjVrFBkZqXLlyqlly5ZasGCBatWqpbCwMAUGBmrTpk1q1qzZNdfZrFkzrVq1SpK0bt06ZWVlSZKaNm2q+fPnS5LS0tLUtWtXHTt2rFDtbdGihdasWaPs7GydPXtWX331VWG7DAAAAACAT/Leojdfw4i0EuThhx/WSy+9pM8++0yBgYFq2LChTp8+rccff1x9+vSR1+tVmzZt1KxZM9WsWVMjR47UvHnzFBwcrPHjxxc6X6lSpZSYmKhx48YpIyNDoaGheadT3n333bIsS02aNJEkNWnSRPv27dPtt99+zXWOHj1aQ4cO1cKFCxUVFZW3fHx8vMaOHavOnTvL4/Fo6NChql69eqHmOWvevLl++OEHde/eXSEhIapUqVKh+wwAAAAAAGCKQloJUrt27aue6ti3b9989++44w59+OGHttf9008/5f3do0ePvHnFGjRooMWLFxcYs379+ry/hw8fbitPRESEZs+enXf/0nxqZcqU0ZQpU65Y/vK2bNy4UZJUtWrVfPOezZkzJ+/vQYMGadCgQYVqEwAAAAAAQFGgkHaLyMzMVO/eBc8b8Oyzz+rBBx8sslyTJk3S1q1br3g8KipKEyZMKLI8AAAAAAAAJQmFtFtEUFCQVq5ceVNyDRs27KbkuZ7XXzefiBgAAAAAAF/ilUtel7OLupU0Ti6CUly42AAAAAAAAABgA4U0AAAAAAAAwAYKaQAAAAAAAIANzJEGAAAAAABQwln/vt1KfLE/jEgDAAAAAAAAbKCQBgAAAAAAANhAIQ0AAAAAAACwgTnSAAAAAAAASjjvv2+3El/sDyPSAAAAAAAAABsopAEAAAAAAAA2UEgDAAAAAAAAbGCONPyqWP++mcYWFye5cxzWy03ji+L18shlFOeVv6O8WQ7jnfAa9vlirDO5DnIXlyMZZRzF13OwDqevlpN4p9vKSbyfw3e3abzp8eBypi0PdNhnJ/HFdQwvClkOcq+PSjCO7eBeYBz/0J4JxnmdrsNJn6Xi3dZOOG21k3gnn7lFEe+ruU0V52cXbq4Ah5+bpvEUN3CzsK8BAAAAAACUcJZL8t5iNWXLB/vjmz9dAQAAAAAAADcZhTQAAAAAAADABgppAAAAAAAAgA3MkQYAAAAAAFDCeeXyyYuNXIsv9ocRaQAAAAAAAIANFNIAAAAAAAAAGyikAQAAAAAAADYwRxoAAAAAAEAJZ/37divxxf4wIg0AAAAAAACwgUIaAAAAAAAAYAOFNAAAAAAAAMAG5kgDAAAAAAAo4byui7dbiS/2hxFpAAAAAAAAgA0U0gAAAAAAAAAbKKTdos6ePau4uLhrLjNixAgdOXKkyHKmpqYqJiZGkjR8+HAtW7asyNb9nzZt2qSPPvrohq0fAAAAAADgP1FIu0WdPn1ae/fuveYyKSkpsizrJrWoaO3Zs0fnzp0r7mYAAAAAAHBTWJK8t9jNFysSXGzgFjV+/HilpaUpLi5OMTEx+uijj+RyuVSnTh2NGjVKSUlJSktL06BBg5SUlKSwsLAC17N3716NHj1amZmZCg0N1ZQpU1S5cmW9++67WrVqlfz9/dW8eXMNHTq00G2cO3euVq5cqYyMDAUGBurNN9/UXXfdpZiYGHXq1ElbtmxRQECABg8erL/85S86dOiQhg0bpnvuuUcLFiyQJEVGRqpnz56OXisAAAAAAAA7XJavDknCNaWmpio2NlYzZ85UfHy8Fi1apLCwML3yyisKCgrSsGHDFBMTo9mzZ6tq1apXXU+nTp300ksvqXXr1po3b54OHz6spk2b6p133tEnn3yiwMBADRkyRC1atFDLli0VGxurjRs3avjw4WrSpIl69OhR4HrPnTun+Ph4vfvuuwoKCtK0adN05swZjRo1SjExMerfv7/69++vESNG6NChQ5o9e7a++eYbTZw4UStWrNCMGTMkSUOGDLkhrx8AAAAAACXJkujndS41vbibUaTKVK2oXilTi7sZhcKItFvcjh071Lp167wRZ71799aIESNsxZ48eVInTpxQ69atJUmPPfaYJGnSpEnq1KmTgoODJUk9e/bUihUr1LJlS9vtKlOmjN5880199tlnOnjwoDZv3qzf/va3ec8/8MADki6OOAsPD1dAQIAiIyN15swZ2zkKsqnREGUcPlHouI7uBVoT0cdRblNOczu5mnAH9wIlG+Z2WqF30u/i6rNT5P715PblPvs5eHe3cy/U2ojeRrEeR+/s4juOO83rX0yvt1NOc3sdbG8n+/hDeyYY55WkwEp3K+fEfqPY9VEJxnmdvq+dfGbzHenm47OL3CU9b3EdU4KrVVLrnTMcZAfsYY60W5zX681337Is5ebm2ooNDAzU/2fvzuOirPf//z8HHEEFFTdySz12oiw9mSlii4IWIgKKWi5pdjTLQMudEhXLcrefSx/TMktzSQ031DoamlQKHc2THf1SWhocExcUFwRxZn5/kJMk0HhdJqKPu7e53caL6/V+vd/XXLPw4j3vy2L5/SNGbm6u0tLSrmpTksttXvbrr7/qqaee0tmzZ/XYY4+pc+fOBdZrs1qtzvtlylDvBQAAAAAAJY9C2i2qTJkyunTpklq0aKHExESdPn1akrRixQr5+/tLktzd3WWz2Ypsw9vbW76+vvryyy8lSWvXrtXMmTPVsmVLbdiwQTk5Obp06ZI++eQTtWzZ8pr6t3fvXtWrV099+/ZV48aNtWXLlmL78kfu7u7XXLwDAAAAAKC0ctyit9KGQtotqmrVqqpVq5beeOMNPf/88+rdu7fat2+vM2fO6OWXX5YktWnTRgMGDFBaWlqR7UydOlVvv/22IiIitHHjRo0cOVKBgYFq06aNunTpotDQUNWqVUtPP/30NfXv4Ycflt1uV4cOHdS5c2c1aNBA6enpLsc3b95c69ev1+LFi68pLwAAAAAAgFF8Z+4WZbVanVe2lKRu3bpdtc/o0aM1enTx63H4+flp2bJlV21/8cUX9eKLLxbYVqdOHSUmJkqSJk2aVGy7FSpU0MKFCwv92eU2pIIXE7iy/ebNmxfYDwAAAAAA4K9GIQ0aNmyYDhw4cNX2oKAgvfTSS4bbzcnJ0VNPFb7Q8ODBg9W2bVvDbQMAAAAAANxoFNKg6dOn/yXtenp6au3atX9J2wAAAAAA3E7slvzbraQ0joc10gAAAAAAAAAXUEgDAAAAAAAAXEAhDQAAAAAAAHABa6QBAAAAAADc5Oy/3W4lpXE8zEgDAAAAAAAAXEAhDQAAAAAAAHABhTQAAAAAAADABayRBgAAAAAAcJNjjbSbAzPSAAAAAAAAABdQSAMAAAAAAABcwFc7cVt54B8Zstc8aijWv8URw3kP76lsOFaS7iibbTj2vzZvU7lPuLsbigu+L81UXkl6qImxY775v3VN5c00OGZJuq/MWVO5a3oYf6z/1j7XVO6HI04Zjv1qrY+p3EZVslwssTZyHMbPE0nykM1wrF+DE6ZyN2mQYTh2++FapnJnuhn76NGxi/Hz87JHu50xFLcv3sNU3qruxp+bDf9x0lTuZg/+ajg2Zbe5x9omi+HYsia/6GE1GL/l/tGm8oZkLDfcRrvv3zCV20z82sZjTOXOtpTc3+bN5C7vMHeeOQzGVbTkmcorSd4G2zjrsJrObdSjfc29Z5uJT/qgrKncZhg9T8zGZ7mZ+5xy2kT8cmuW4dgOkuZ5nDMUW6NseQUazgy4jhlpAAAAAAAAgAuYkQYAAAAAAHCzs0gO4xO+b06lcDzMSAMAAAAAAABcQCENAAAAAAAAcAGFNAAAAAAAAMAFrJEGAAAAAABwk7P/druVlMbxMCMNAAAAAAAAcAGFNAAAAAAAAMAFFNIAAAAAAAAAF7BGGgAAAAAAwE2ONdJuDsxIAwAAAAAAAFxAIQ0AAAAAAABwAYU0AAAAAAAAwAWskQYAAAAAAHCTc/x2u5WUxvEwIw0AAAAAAABwAYU0AAAAAAAAwAUU0m4jZ8+eVVRUVLH7vPLKK/rf//53g3r0u+TkZPXu3fuaYoKCgpSenv4X9QgAAAAAAKAgCmm3kaysLO3fv7/YfZKTk+VwlMZvKQMAAAAAAPy1uNjAbWTChAk6duyYoqKiFBQUpIULF8pisei+++7TmDFjtGTJEh07dkwDBgzQkiVL5OPjU2g7+/fv19ixY5WTk6NKlSpp2rRpOnz4sObMmaPFixdLkmJiYtSiRQu1aNFC/fv3l4+Pjzw9PbVw4cIi+3fq1Cn169dPx44dU5MmTTRu3DiVLVtWH330kdauXasLFy7IarVq+vTp+tvf/vaXHCMAAAAAAG5Gdkv+7VZSGsdjcTD96LaRnp6uPn36aO7cuYqOjtaKFSvk4+Oj8ePHy9PTU6NGjVJQUJAWLVqkOnXqFNlOaGiohg8frsDAQC1dulRpaWlq06ZNkYW0tm3b6vPPPy+2zeTkZD3//PNas2aN6tWrpyFDhqhp06bq0qWLoqOj9c4778jT01MzZ87UmTNnNGbMGJf6CgAAAADAreD9Vi/rbPqJku7GdeVdp5r++fX/V9LduCbMSLsNffPNNwoMDHTOOHvqqaf0yiuvuBSbmZmp48ePKzAwUJLUs2dPSfmFsKJUrVrVpWLXQw89pPr160uSwsLCFB8fr2eeeUbTp0/Xhg0bdOjQISUlJenee+91qa+FOdXvSdmPHb3muKrrHS+0cwAAIABJREFUt+tk2GOG8x7eU9lw7INp67S7brjh+P/avA3H9j6yRItr9TIUG3xfmuG8klRj83Yde9zYMd/837qG8/Y6skRLDI5Zku4rc9Zw7AO/rNOeO40/1n9rn2s4tuL8z3RmQLDh+K/WFj6D1RUhGcu1ybe7odhKlouG80pSq6Px+vqOSEOxOQ53w3mDMlYq0beb4Xi/BsY/QNXeuVX/axloOH774VqGY3v8ukTLahp7fnXscspwXknynrNRZ6M7GIrdF+9hOK//kdVKrtXZcHzDf5w0HFtt03adCDH+3pWy2/hj3SFjuTYafF5LUlnZDce2y1ihLb5PGorNM7nqiZnXs3bfv2E4r7V6Q+UdP2g4fm3jMYZjux5dqlV39DQcb4bZ3OUdxs8zM+d4RUue4byS9MjRT/TlHV0MxZ51WA3nNXN+S9KjfY2/Z3tNjte5UcberyUp6YOyhmPNjtvMrBUz51mWm/HPKWberyVpuTXLcOzaXxIUcWdHQ7E16tTQu1+/bzg34CrWSLsN2e0FPzQ4HA5dunTJpVir1SqL5fe5l7m5uUpLS5PFYimwtlpe3u8fEDw9PV1qu0yZ3+u6DodDZcqU0a+//qqnnnpKZ8+e1WOPPabOnTuzhhsAAAAAACgRFNJuI2XKlNGlS5fUokULJSYm6vTp05KkFStWyN/fX5Lk7u4um81WZBve3t7y9fXVl19+KUlau3atZs6cKR8fH6WlpSk3N1enT5/Wrl27rrl/u3bt0pEjR2S327VmzRq1atVKe/fuVb169dS3b181btxYW7ZsKbZ/AAAAAADcihyS7LfYrTROk+GrnbeRqlWrqlatWnrjjTf0/PPPq3fv3srLy9N9992n8ePHS5LatGmjAQMG6L333lPduoV/PW7q1KmKi4vT1KlT5ePjoylTpqhGjRpq3bq1QkNDVbt2bTVr1uya+3fXXXfp1Vdf1fHjx9WyZUt17dpVOTk5WrZsmTp06CCHw6HmzZvrxx9/NHUcAAAAAAAAjKCQdhuxWq1avny58//dul29Ps/o0aM1evToYtvx8/PTsmXLrtr+2muvFbp/YmLin/bN39+/0DYrVKhQ5JU+XWkXAAAAAADgeqGQhkINGzZMBw4cuGp7UFCQXnrpJUNt/vvf/9brr79e6M/mz58vX19fQ+0CAAAAAADcCBTSUKjp06df9zYfeughrV279rq3CwAAAADAre7yumK3ktI4Hi42AAAAAAAAALiAQhoAAAAAAADgAgppAAAAAAAAgAtYIw0AAAAAAOAm5/jtdispjeNhRhoAAAAAAADgAgppAAAAAAAAgAsopAEAAAAAAAAuoJAGAAAAAAAAuICLDQAAAAAAANzk7Jb8262kNI6HGWkAAAAAAACAC5iRhtvKnv/46kLatdePO0hKTqllOK+nbIZjJen0RQ/Dsc0rZZrK3dzLWPyu74wfL0kKMdHG/Z5nTOW+32o8Pi2nguHYBySl55Y3HP/zOi/DsZ3nS5+vq2o43lN2w7FmZDrKllgbVd0umspbzs3468Kun+8wHFvbZHwFk491Bbux+K0rK5nKGz7HeBuVlGcqd47N3XDs17trG44NNxnvZvKC9GbOlIsm/9ZrNt4Mo0dtbeMxhnN2PbrUVHzE3tcNx5qN33R/rKncZR3Gz9Nci7nzxGi8Q+anX1yPNm60zz8w/nk2YrK5+LImX8/McDeZ22h8Jbu53z/MxA/JLWcqt9F4z4uepvICrmJGGgAAAAAAAOACZqQBAAAAAADc5BwyN+P7ZlRy80WNY0YaAAAAAAAA4AIKaQAAAAAAAIALKKQBAAAAAAAALmCNNAAAAAAAgJucQ6VzTbHilMbxMCMNAAAAAAAAcAGFNAAAAAAAAMAFFNIAAAAAAAAAF7BGGgAAAAAAwE3OLofspXJVsaKVxvEwIw0AAAAAAABwAYU0AAAAAAAAwAUU0gAAAAAAAAAXUEgDAAAAAAAAXMDFBgAAAAAAAG5y9t9ut5LSOB5mpN3mYmJiFB8fX+TPV6xYoUcffVSTJ0821Xbv3r3/dP/nnntOGRkZhtoHAAAAAAD4qzEjDcVKSEjQxIkT9cgjj5hqJyUl5U/3effdd03lAAAAAAAA+CsxI+0243A4NHHiRAUHB6t379765ZdfJElr1qxR586dFRERoVdffVW5ubmaM2eO9u7dq/Hjx+uLL77Qpk2b9OSTTyo8PFzt27fX7t27JeXPNktOTpYkpaenKygoqEDOCRMmSJK6detWbN+CgoKUnp6u+Ph4DRkyRP/85z/1+OOPKy4urti+AwAAAAAA3AgWh8PhKOlO4Mb59NNPtWTJEr3//vs6e/aswsPDNXDgQG3YsEELFy6Uh4eHpk+frnLlyunFF19U7969FR0drebNm+vZZ5/VW2+9pSpVqmjVqlXasmWL3nnnHec+/v7+Sk9PV58+fZSYmKiYmBi1aNFCkZGR8vPzU2pqarF9CwoK0qJFi5SSkqJZs2YpISFB7u7uat++vebPn6+ff/75qr4PHTpUkZGRN+joAQAAAABQMmY+/JKy0k+UdDeuq0p1qumlr2aWdDeuCV/tvM2kpKToiSeekNVqVZUqVfTYY4/J4XDo8OHDevLJJyVJeXl5atSoUYE4Nzc3vf3220pMTNTPP/+slJQUubn9dRMamzZtKi8vL0lS3bp1lZWVVWjfr9XWhwbpQtrxa47rkLFcG327X3PcZZ6yGY4NylipRN/iZ/MVp1alc4Zj7/lhk/7f3SGGYn/OqmQ4rySFZCzXJoPHvJbnecN5/3F4vf5TL8xwfFpOBcOxHTOWK8HEeZZnMf6c7Hx0qVbf0dNwvKfD+DKhZh5r48+sfGaOeVW3i4bzBvwarx01jf8R4Ljdw3BseMYyrfPtYTjejNKau5LyDOdtnbFKX/h2NRyfJavhWLPH203G/9Zq9vXMzCcMs+/ZZpjJnW3iNbzr0aVaZeI1PGLv64ZjrdUbKu/4QcPxm+6PNRxr9hy3WSyGY828b1aT8fcPSXr06Col3WHsdeWcw/ivf2beryXpoowf74iMZVpr4rEua+L1zOy4zbyWBmd8rM98nzIUazNxvM2+jpY38QmtTcZKbTP4u49n3epq+e//M5wbcBWFtNuMxWLRlZMQy5QpI5vNppCQEMXG5n+QOX/+vGy2gi9+58+fV9euXRUeHq7mzZvLz89PS5Yscf78cpuXLl26Lv308Pj9l8TLfS6s7wAAAAAAADcKa6TdZgICArRp0yZdvHhRWVlZSkpKkiRt3rxZJ0+elMPhUFxcnD788MMCcYcOHZLFYtELL7wgf39/bd682Vls8/Hx0YEDByRJW7ZsKTSvu7u76SJbUX0HAAAAAAC4EZjSc5tp166d9u7dq44dO6patWpq2LChvL29FR0drWeeeUZ2u1333nuvBgwYUCDunnvu0b333quQkBBZLBY98sgj2rVrlySpf//+iomJ0SeffKK2bdsWmrdt27aKiIhQfHx8gdlmZvsOAAAAAMDtwCHJ+IIqN6fSuGg/hbTb0JAhQzRkyJCrthd2Vc3Fixc778+YMaPAzy5/FbRJkybauHGjc3t0dLQkadKkSc5ts2fP/tN+JSYmSpLq1KlT4AICV/ahqL4DAAAAAAD81Sik4Ybq3bu3zpw5c9X27t27q0ePklmAGgAAAAAAwBUU0nBDXTm7DAAAAAAAoDShkAYAAAAAAHCTs1vyb7eS0jgertoJAAAAAAAAuIBCGgAAAAAAAOACCmkAAAAAAACAC1gjDQAAAAAAAKXSkSNHNGLECJ08eVINGjTQtGnTVKFChQL7HDt2TK+88opOnDghNzc3jRw5UgEBAcrLy5O/v7/q1q3r3Dc+Pl7u7u5F5qOQBgAAAAAAcJOzyyG7HCXdjevqeoxn/Pjx6tmzp0JDQ/X222/r//7v/zRixIgC+0yZMkVBQUHq1auXfvrpJ/Xu3Vvbt29XamqqmjZtqgULFricj692AgAAAAAAoNTJy8vTN998o+DgYElSZGSkPv3006v2e/zxx9WxY0dJUr169ZSbm6vs7Gzt3btXmZmZioyM1JNPPqmUlJQ/zcmMNAAAAAAAAJSYX3/9VTabrcC2ihUrqmLFisXGnTp1Sl5eXipTJr+8Vb16dWVkZFy13+VCmyQtWLBA9957r7y9vWWxWNS2bVs9//zz+vHHH/Xcc89p/fr1qlKlSpE5KaQBAAAAAACgxPTq1Uv/+9//CmyLjo7WoEGDnP/ftGmTJk6cWGCfevXqyWKxFNj2x/9f6YMPPtDHH3+sjz76SJLUvXt3588aNWqkJk2aaPfu3WrXrl2RbVBIAwAAAAAAuMk5frvdSi6PZ8mSJYXOSLtSSEiIQkJCCmy7fLEAm80md3d3HT9+XDVq1Cg015QpU/TFF19oyZIluuOOOyRJa9as0YMPPqg777wzvz8Oh6xWa7F9ppCG28rXntKpctce10FSooG4y6rZyxqODZKU4mE8Pi+36Cmpf2aMpJUG47M97YbzSlKIpO2eRf8loTgVVPz03+L8Q9J6E/EPyvbnOxXDzMKV59yMHa/rEm83t+RmjsVYvIfD3HkmGT/mafI0nDPAZLzV3EMtWzF/pfszeSZz5xg8z3JN9PmyLHdjj3aWPEzl/cXdeHw5u7mPyxdNHLeyJj+p22XmMTOX3Ogrw+26ePCm+2MNx4ZnLDMVH/L9BMOxZuODmw40HNtZ0uwyJwzFlrOY+xXsUUmTrNmGYqMvGv+MY5bV5PPabDxunFyTr6ZG4y237av4raFmzZqG4qxWqx566CFt3LhRYWFhWrNmjR577LGr9vvggw+UnJysZcuWFSjQpaamas+ePYqLi9NPP/2k/fv3q1mzZsXmpJAGAAAAAACAUmncuHGKiYnR3LlzVbNmTc2YMUOStGzZMh07dkyDBw/W22+/LS8vL/Xu3dsZN3/+fEVFRenVV19Vx44dZbFYNHnyZHl5eRWbj0IaAAAAAAAASqXatWtr8eLFV23v0aOH8/4333xTZPysWbOuKR+FNAAAAAAAgJucXcaXL7hZlcbx8CViAAAAAAAAwAUU0gAAAAAAAAAXUEgDAAAAAAAAXMAaaQAAAAAAADc5hxyyy1HS3biuHKVwPMxIAwAAAAAAAFxAIQ0AAAAAAABwAYU0AAAAAAAAwAUU0gAAAAAAAAAXcLEBAAAAAACAm5zjt9utpDSOhxlpAAAAAAAAgAsopAEAAAAAAAAuoJAGAAAAAAAAuIA10gAAAAAAAG5y9t9ut5LSOJ5SMSPtueeeU0ZGhmw2m/r166fg4GAtWLBAo0ePNtRe7969nfcjIiKuVzeLtWLFCiUkJBS7z3fffaepU6de17wxMTHq1auXHI7fl/CLj49XTExMsXFbt27VwoULC2w7d+6cmjZtqoyMjALbU1JS1LlzZ5f7tHfvXudj99///ldt2rRRr169NHPmTH3++ecut+PKOAAAAAAAAK6XUjEj7d1335UkHTlyRKmpqfryyy9NtZeSkuK8v3btWlNtuWr37t1q0aJFsfscOHBAJ0+evO65//Of/2jRokV65plnXI75/vvvr9rm5eWlxx9/XBs2bNA///lP5/Y1a9aoa9euLrfduHFjNW7cWFJ+wS48PFxDhw51OR4AAAAAAKAk3JBCWnR0tMLCwhQcHCxJioyM1IQJE1ShQgXFxcXp9OnT8vT01JgxY9SoUSPFxMTo9OnTOnz4sEaMGKEJEyZo0aJFGjhwoE6fPq3IyEiNGjVKc+bM0eLFi7V//36NHTtWOTk5qlSpkqZNm6Zq1aopLi5OP/74o06cOCE/Pz/NmDFD06ZNkyR169ZNK1eulJ+fn1JTU3XhwgXFxsYqNTVVFotF/fr1U6dOnRQfH6+kpCRlZWUpLS1NDz/8sOLi4ooc67lz5zR06FCdOHFCkhQVFaVy5copMTFRO3fuVPXq1eXr66vXX39d2dnZyszM1IABAxQaGqpZs2YpOztbc+fOla+vr1JSUjRp0iRJ+bPooqOjVa9ePQ0fPlzZ2dlyc3NTbGysHnjggWKPf79+/TR37ly1adNG9erVK/Czn3/+WWPHjtXp06dVvnx5jR49WuXLl9fy5cslSbVq1VKXLl2c+0dGRmrKlCnOQlpubq62bdumUaNGScovqn344Yey2+267777NG7cOHl4eKhly5a6//77dfz4cY0cOVLvvPOO+vfvr2XLlkmSypYtq/T0dLVo0UKRkZFFtrNmzRrNnTtXXl5eql27tsqXL+/SOQgAAAAAAGCWxXHld/7+Ips3b9b69es1a9YsHTp0SFFRUdqwYYO6d++usWPHqlGjRjpw4ICioqL02WefOb+ud7mIFBQUpEWLFkmS+vTpo8TERCUnJzsLaaGhoRo+fLgCAwO1dOlSpaWlKSgoSBs3btS4ceNkt9v1zDPP6Omnn1ZwcLCzeCbJeX/KlCm6ePGiYmNjlZmZqW7duuntt9/Wvn37NGvWLCUkJMjd3V3t27fX/Pnz5efnV+hYV69ere+++07jxo3T/v37tW7dOo0aNUoxMTHOItEbb7yhoKAgBQQEKC0tTeHh4fr2228VHx/vLJ5deV/6vZD2zTffyNPTU/3799f27dv1448/ql+/fkUe+8t5z5w5o82bN+ujjz7S6tWrnW137dpVAwYM0BNPPKE9e/ZoyJAh+uyzzzRv3jxJ0qBBgwq053A49Pjjj2v+/Pn629/+pg0bNigxMVHTp0/Xjz/+qHHjxmnhwoXy8PDQ9OnTVa5cOb344ovy8/PTokWL5O/vX+Cxmz17tjPP5b42bty40Ha6dOmiLl26aM2aNapcubKef/55Va9e3XmMAAAAAAC4Vb3+yCCdSj9e0t24rnzqVNeYL2eXdDeuyQ2Zkda6dWu99tprOnfunBISEhQeHq7z58/r+++/1yuvvOLcLzs7W6dOnZIkNWnSxKW2MzMzdfz4cQUGBkqSevbs6fxZ5cqVtWTJEv300086dOiQsrOzi2xn586devPNNyVJVapUUdu2bZWSkiIvLy81bdpUXl5ekqS6desqKyuryHaaNm2qGTNmKCMjQ23atFFUVNRV+8TExCgpKUnz5s3TDz/8UGy//iggIECDBg3S/v371bp1az399NMuxfXp00f/+te/tGjRInl7e0uSzp8/r19++UVPPPGEJOmBBx5QpUqV9NNPPxXZjsViUadOnZSQkKDBgwdr7dq16tu3ryQpOTlZhw8f1pNPPilJysvLU6NGjZyx//jHP1zqa1HtfPvtt2ratKmqVasmSQoLC9POnTtdavOyNwy+8Ew7tFzD63e/5rjLqtndDcfG/LJEk+7sZTg+z2I4VGMOL9Hr9Yzlzja5bOTEw8v0Sr0ehmIrmFj+MfbwEk0wOGZJejDHZji2Q8ZybfQ1fp6ddDd+nvU+skSLaxkft5fd+N9kOh9dqtV39PzzHQvh4TB3npk55ufcjJ9nT/66VCtqGhuzJFlN/AnMzPGWzL2mmBl3rsVEYpk/x0sqbzkTz62uR5dqlYnHuqyJv7WGZyzTOl9jr+GS5CbjuTtmLFeCwee12cWDzbymZFuMZy/Nj3XI9xMMx1qrN1Te8YOG44ObDjQcm5j+LwXVecJQbDmLuV/BNqRtVGjdDoZioy9WNJw3JGO5Npn4nGJm9obZz0hm3kHMjtvM61lwxsf6zPcpQ7E2E6M2e7zdS2jMnnWrq/W/5xjODbjqhhTSypYtq8DAQCUmJurTTz/VvHnzZLfbVbZs2QJrlB09elSVK1eWJHl6errUttVqleWKD9e5ubk6duyYfvjhB82aNUt9+vRRZGSkTp06peIm3/3xZw6HQzZb/i/EHh4ezu0Wi6XYdurXr69NmzYpKSlJW7du1fvvv6+NGzcW2Ofll19WxYoVFRgYqA4dOhR6EYI/5snLy5MkNWvWTBs2bNC2bdu0ceNGrV69+qqLAhTGzc1Nb775prp3766nnnqq0DH/cdxFiYyM1D//+U/17NlThw4dUkBAgCTJZrMpJCREsbGxkvILdVe25epjWlQ7O3bsKNDnMmVKxRJ/AAAAAADgFnHDrtoZERGhhQsXqnLlyqpdu7a8vb1Vv359ZyHtq6++Uq9e1/4XW29vb/n6+jovQLB27VrNnDlTO3bsUEhIiLp06aKKFSsqOTnZWdRxd3fXpUuXCrTTsmVLrVq1SlL+LLfPP//8Ty8OUJiPPvpIs2fPVkhIiMaNG6fMzEydO3dO7u7uzvxfffWVBg8erHbt2mn79u2S8otHV/bLx8dHBw8elMPhUFpamvOrqFOmTNG6devUuXNnjR07Vvv27XO5b/Xr19cLL7ygBQsWSMq/eECdOnX0r3/9S5K0Z88enThxQn//+98LPUaX1apVSzVr1tSsWbMUHh7uLGT6+/tr8+bNOnnypBwOh+Li4vThhx9e8zEsqp1mzZppz549ysjIkN1uv6pACQAAAAAA8Fe6YVN6mjVrprNnz6pHj9+nfk+dOlVxcXF67733ZLVa9dZbbxWYXeaqy+1MnTpVPj4+mjJlik6dOqXhw4drw4YNslqtevDBB5Weni5Jatu2rSIiIhQfH+9sIyoqSnFxcQoLC5PNZtMLL7yg++67z1nAclWnTp00dOhQhYWFyd3dXSNGjFDFihXVqlUrzZgxQ97e3ho0aJB69uwpDw8P3XPPPapdu7bS09PVpEkTzZkzR9OmTdPgwYP1ySefqH379mrQoIGaNWsmKX+ttGHDhik+Pl7u7u6aPHnyNfXv8lc8/3jsZs+eLavVqtmzZ6ts2bJq3ry5Ro0apWrVqql3795XtdOlSxeNHDlSmzdvdm675557FB0drWeeeUZ2u1333nuvBgwYcE39K64dDw8PxcbGqm/fvipXrpzuuuuua24bAAAAAIDSyCFzX5O+GZXG8dzQ78Zt2bKlwP8bNmyoxYsXX7XfHxePT0xMvOq+v7+//P39JeVfMODy1R8vq1GjhtavX19oPy4vcC/JWSjz8vJyXtHzSpGRkYqMjHT+v7D+XsnLy0vz58+/antoaKhCQ0Od/3/22WcLjb+yMHVlP6+0dOnSYvtwpT8eSzc3twLxRT0GzZs3L3Dc/6hjx47q2LHjVdu7deumbt26XbX9yoLklY/dlRczuLKvRbXTvn17tW/fvsh+AQAAAAAA/FVYZMqAX3755aqrWV42YcIENW7c+Ib2Z9iwYTpw4MBV24OCgvTSSy/d0L4AAAAAAADcqiikGXDnnXcWuEhCSZs+fXpJdwEAAAAAAOCWd8MuNgAAAAAAAACUZsxIAwAAAAAAuMk5JNlLuhPXWWm82AAz0gAAAAAAAAAXUEgDAAAAAAAAXEAhDQAAAAAAAHABa6QBAAAAAADc5By//buVlMbxMCMNAAAAAAAAcAGFNAAAAAAAAMAFFNIAAAAAAAAAF7BGGgAAAAAAwE3O/tvtVlIax0MhDbeV9rkXlZtz0VBspME4SSrrbu7loZ3tvOHY45c8TeV+KMdmKK68LpnKK0ntc/IMxeWZnGzb0mDe66GsibeSO23mjvmdtlzDsWdNvp1YHcbGbb0Oi5MabcNqMrWZeA+Dx+t6xFdxmDvPfO3GXktz5G4qryTVMPgccTd5ntW0GX9NyTH5euZp4rH2NPnRtryMvX9Ikk0WU7mNPq/tJvNKMtxCeZPPazPxuRZz55nNYvy4BTcdaDg2Mf1fpuI/+3au4Vgz8a3/0d9UXkk6ZbtgMLKi6dxGmT3PzMSbfd808y5g9vXMaLzZ9y4z8SWV22xewFV8tRMAAAAAAABwAYU0AAAAAAAAwAV8tRMAAAAAAOAmZ5dD9lvsK6ylcTzMSAMAAAAAAABcQCENAAAAAAAAcAGFNAAAAAAAAMAFFNIAAAAAAAAAF3CxAQAAAAAAgJuc47fbraQ0jocZaQAAAAAAAIALKKQBAAAAAAAALqCQBgAAAAAAALiANdIAAAAAAABucg45ZC+Vq4oVzVEKx8OMNAAAAAAAAMAFFNIAAAAAAAAAF1BIAwAAAAAAAFzwlxbSnnvuOWVkZMhms6lfv34KDg7WggULNHr0aEPt9e7d23k/IiLienWzWCtWrFBCQkKx+3z33XeaOnXqdc175swZDRs2TGFhYQoLC1O/fv106NCh65rjWn399ddq3779VdvnzJmjiRMnutzOsmXLtGzZMkn5x/fRRx/V5MmTneeLq2JiYhQfH+/y/gAAAAAAlFb2W/RW2vylFxt49913JUlHjhxRamqqvvzyS1PtpaSkOO+vXbvWVFuu2r17t1q0aFHsPgcOHNDJkyeva97p06fr7rvv1vTp0yVJCQkJGjJkiFavXn1d81yLgIAAXbx4Ud9//73uv/9+5/Z169Zpzpw5LrfTo0cP5/2EhARNnDhRjzzyyHXtKwAAAAAAwPVWbCEtOjpaYWFhCg4OliRFRkZqwoQJqlChguLi4nT69Gl5enpqzJgxatSokWJiYnT69GkdPnxYI0aM0IQJE7Ro0SINHDhQp0+fVmRkpEaNGqU5c+Zo8eLF2r9/v8aOHaucnBxVqlRJ06ZNU7Vq1RQXF6cff/xRJ06ckJ+fn2bMmKFp06ZJkrp166aVK1fKz89PqampunDhgmJjY5WamiqLxaJ+/fqpU6dOio+PV1JSkrKyspSWlqaHH35YcXFxRY713LlzGjp0qE6cOCFJioqKUrly5ZSYmKidO3eqevXq8vX11euvv67s7GxlZmZqwIABCg0N1axZs5Sdna25c+fK19dXKSkpmjRpkqT8WXTR0dGqV6+ehg8fruzsbLm5uSk2NlYPPPBAkf05ceKEqlatKrvdLjc3N3Xo0EHly5eXJNntdr355pvasWOHLBaLwsPDNWDAACUnJzuPrZQ/Y6tFixZq0aKF+vfvLx8fH3l6euqdd97R+PHjtWvXLlmtVr344ovq0KGDvvvuO02cOFE5OTny8fHR+PFwlAOOAAAgAElEQVTjVbduXWefLBaLOnXqpISEBGchbffu3apUqZLuvvtu2Ww2TZkyRSkpKbLZbIqMjFTfvn2VnJysqVOnym636+9//7vq1KnjbG/v3r0aP368YmNjNX78eC1atEg1a9YstB2Hw6FJkyZp27ZtqlGjhmw2258WOQEAAAAAAK6XYgtpERERWr9+vYKDg3Xo0CHl5uaqUaNG6t69u8aOHatGjRrpwIEDioqK0meffSZJqly5st555x1J0oQJEyRJc+fOVZ8+fRQfH6/k5GRn+8OHD9fw4cMVGBiopUuX6sMPP1RQUJCsVqs+/vhj2e12PfPMM/riiy8UGxurxYsXa+XKlQX6OHv2bPn4+CghIUGZmZnq1q2b7rnnHknSt99+q4SEBLm7u6t9+/bq0aOH/Pz8Ch3r5s2bVbt2bc2fP1/79+/XunXrNGrUKAUFBalFixZ69NFH9cYbb+jFF19UQECA0tLSFB4erh49emjw4MFKSUnRwIEDi/yq4apVq9SmTRv1799f27dv165du4otpA0cOFBRUVFaunSpWrZsqYcffljh4eGS8r8a+euvv2rdunW6ePGievfurbvvvlvlypUrsr2ff/5Z7733nurUqaP33ntP2dnZ2rRpk06ePKm+ffuqXbt2io2N1TvvvKNatWopKSlJY8aM0QcffFCgncjISPXq1UsjR46Um5ub1qxZo65du0rK/5qmJK1evVoXL15Uv379nAW3Q4cOaevWrfL29tbs2bMl5Rdqk5OTFR0dLX9/f2eOoto5ceKE9u3bp4SEBJ09e9Z5PK5Fs2/mXXPMZa2OltzXSB/635oSyx2SsbzEcrfOWFUiedtlrCiRvCWdu6SOtyR1LMHz7PGMj0skb+ejS0skryR1uA2f15IUXEKPdUk+r0vyuVWS4y6px1oquffNknxem3k962wyd2L6v0y2YJy1ekNDcV8f2Wo69/Vow4iS/Fx4u75vllTuknwdLcn3D8AVxRbSWrdurddee03nzp1TQkKCwsPDdf78eX3//fd65ZVXnPtlZ2fr1KlTkqQmTZq4lDgzM1PHjx9XYGCgJKlnz57On1WuXFlLlizRTz/9pEOHDik7O7vIdnbu3Kk333xTklSlShW1bdtWKSkp8vLyUtOmTeXl5SVJqlu3rrKysopsp2nTppoxY4YyMjLUpk0bRUVFXbVPTEyMkpKSNG/ePP3www/F9uuPAgICNGjQIO3fv1+tW7fW008/Xez+999/vz7//HPt3r1bX3/9td5//30tX75cH3/8sZKTk9W5c2e5u7urXLlyCgsL044dOxQUFFRke1WrVnXOBPvmm2/05JNPys3NTdWrV9eGDRv0ww8/KC0tTQMHDnTGnDt37qp26tSpo3r16iklJUUPPvigtm3bppEjR0qSduzYof3792vnzp2S8s+L1NRU3XXXXWrQoIG8vb1dOlZFtXPw4EE98cQTslqtqlKlih577DGX2rvSrubPKzft+DXHtToar6/viLzmuMvKuhv/5vdD/1ujf9fuZDj++CVPw7EhGcu1ybe7odjyumQ4r5T/y/YXvl0NxeaZWP6xXcYKbfF90nC8GWZzW02sMGDmeEvSWRMrBXTMWK4Eg+eZhxyG80r5RbTNvk8Zij1ncTect/PRpVp9R88/37EIHg7jj3WHjOXaaPB4S1IFE89tM+dZjowfbyn/F4LPDD7W7ibOM7PP6xwTr2dmnluS5GniNcXsuG2yGI4181jbTeSVzL1vmnk1M/u8zrUYP8/Mvp7NLnPCcGxi+r8UVOcJw/GffTvXcKy1ekPlHT9oKLb1P/obzivlF9Fa1Qo0FDvG5ms4r5nzW5JySvA8K8n3TTPM5Dbz3mXmddRsbjPvH551q+uRf79tOHdp4Pjt362kNI6n2N98ypYtq8DAQCUmJurTTz/VvHnzZLfbVbZs2QJrlB09elSVK1eWJHl6uvZLu9VqlcXy+4eV3NxcHTt2TD/88INmzZqlPn36KDIyUqdOnZLDUfSB/ePPHA6HbDabJMnDw8O53WKxFNtO/fr1tWnTJiUlJWnr1q16//33tXHjxgL7vPzyy6pYsaICAwPVoUOHQi9C8Mc8eXl5kqRmzZppw4YN2rZtmzZu3KjVq1dr4cKFRY4pLi5Or776qvOrmVFRUQoODta+fftkt9uv2t9msxWZWyr4uJQpU6bAsT98+LDsdrvq1KnjfFxtNpvza65/1KVLFyUkJOjMmTMKCAhwFittNptGjBihJ57I/0CTmZmpChUqaM+ePS6fF8W1M2XKlALjK1PmL13iDwAAAAAAoIA//bNARESEFi5cqMqVK6t27dry9vZW/fr1nQWXr776Sr169brmxN7e3vL19XVegGDt2rWaOXOmduzYoZCQEHXp0kUVK1ZUcnKyszDm7u6uS5cK/iW8ZcuWWrUq/2simZmZ+vzzzw2tm/XRRx9p9uzZCgkJ0bhx45SZmalz587J3d3dmf+rr77S4MGD1a5dO23fvl1SftHnyn75+Pjo4MGDcjgcSktLU2pqqiRpypQpWrdunTp37qyxY8dq3759RfbFYrHo4MGDWrBggbNolp6erkuXLunOO+9Uy5YttWbNGtlsNl24cEHr16+Xv7+/fHx8lJaWptzcXJ0+fVq7du0qtP3mzZtr48aNcjgcOnnypJ5++mnVrl1bWVlZ+ve//y1J+uSTTzR8+PBC44ODg7Vz504lJCSoS5cuzu0tW7bUihUrlJeXp/Pnz6tnz57as2fPtTwMxbYTEBCgTZs26eLFi8rKylJSUtI1tw0AAAAAAGDUn07padasmc6ePVvgSotTp05VXFyc3nvvPVmtVr311lsFZji56nI7U6dOlY+Pj6ZMmaJTp05p+PDh2rBhg6xWqx588EGlp6dLktq2bauIiIgC65BFRUUpLi5OYWFhstlseuGFF3Tfffc5C1iu6tSpk4YOHaqwsDC5u7trxIgRqlixolq1aqUZM2bI29tbgwYNUs+ePeXh4aF77rlHtWvXVnp6upo0aaI5c+Zo2rRpGjx4sD755BO1b99eDRo0ULNmzSTlX3Rg2LBhio+Pl7u7uyZPnlxsf2bMmKGJEyeqbdu2KleunLy9vTV9+nRVrlxZTz31lA4dOqSIiAjl5eUpLCxMjz/+uKT8r+OGhoaqdu3aztx/1LNnT02YMMG5xtiYMWPk7e2tmTNn6o033lBubq68vLyK7KOnp6datWql5ORkNW/e3Lm9e/fuOnz4sDp37qxLly4pMjJS/v7+BdbFc0VR7UjS3r171bFjR1WrVk0NGxpblwIAAAAAAMAIl74bt2XLlgL/b9iwofPKkFe6fKXKyxITE6+67+/v7yyK+Pn5admyZQViatSoofXr1xfaj8uL1EtyFsq8vLycV/S8UmRkpCIjf1/TqrD+XsnLy0vz58+/antoaKhCQ0Od/3/22WcLjd+8eXOh/bzS0qWuL5BZo0YNvfXWW4X+zGq1KjY2ttCfvfbaa4Vuv/KxKFu2bKH7NW3a1Dm7788UFl9Uv658zCVp0KBBzvtXPi5X9rGo8Q0ZMkRDhgxxqY8AAAAAAADX0221yNQvv/xSoIhzpQkTJqhx48Y3tD/Dhg3TgQMHrtoeFBSkl1566Yb2BQAAAAAA3Lzsv91uJaVxPLdVIe3OO+8scJGEkjZ9+vSS7gIAAAAAAABcZPwaxAAAAAAAAMBthEIaAAAAAAAA4ILb6qudAAAAAAAApZHjt3+3ktI4HmakAQAAAAAAAC6gkAYAAAAAAAC4gEIaAAAAAAAA4ALWSAMAAAAAALjJOSTZS7oT11npWyGNGWkAAAAAAACASyikAQAAAAAAAC6gkAYAAAAAAAC4gDXScFupUfW8LuWeNRRbs4axOEk6ebKC4Viz7q15okTjzahX87ShuLyL7ubyVs0yHJt60sdU7hwTf98oY3KFAYcshmO9dclUbqPxZ6/D21iuwXHXcrtgKq+Z+BybuXO8gonHK0fmchuNr+iWZyqvmTZsDuPPDUnytNgMx5Y3+dyqYrloODbLUdZU7jwTr2dupXLFFHMqWsyd42bizbz+S1I1GT/PylnMvY6biW/9j/6GY78+stVw/Bf/ec9wXrNtbLl/tOncRvk4zJ3jZuIvmHzvMvcMMcdobpvJXpuJz7aYO95nDT6vbSbzlgZ2h0N2x631Hlkax8OMNAAAAAAAAMAFFNIAAAAAAAAAF1BIAwAAAAAAAFxAIQ0AAAAAAABwARcbAAAAAAAAuMk5frvdSkrjeJiRBgAAAAAAALiAQhoAAAAAAADgAgppAAAAAAAAgAtYIw0AAAAAAOAmZ5dD9lK5qljRSuN4mJEGAAAAAAAAuIBCGgAAAAAAAOACCmkAAAAAAACAC1gjDQAAAAAA4CbnkOQohWuKFac0joYZaQAAAAAAAIALKKQBAAAAAAAALqCQBgAAAAAAALjgtiukPffcc8rIyJDNZlO/fv0UHBysBQsWaPTo0Yba6927t/N+RETE9epmsVasWKGEhIRi9/nuu+80derU65bz66+/Vvv27a/aPmfOHE2cONHldpYtW6Zly5ZJyh/Ho48+qsmTJzsfF1fFxMQoPj7e5f0BAAAAACjNHJLst9itNK6RdttdbODdd9+VJB05ckSpqan68ssvTbWXkpLivL927VpTbblq9+7datGiRbH7HDhwQCdPnrxuOQMCAnTx4kV9//33uv/++53b161bpzlz5rjcTo8ePZz3ExISNHHiRD3yyCPXrZ8AAAAAAAB/FYvD4Sh1BcDo6GiFhYUpODhYkhQZGam4uDi99dZbOn36tDw9PTVmzBg1atRIMTExOn36tA4fPqwRI0ZowoQJWrRokQYOHKiff/5Zd999t0aNGqU5c+Zo8eLF2r9/v8aOHaucnBxVqlRJ06ZNU7Vq1RQXF6cff/xRJ06ckJ+fn2bMmKFp06Zp8eLFatKkiVauXCk/Pz+lpqbqwoULio2NVWpqqiwWi/r166dOnTopPj5eSUlJysrKUlpamh5++GHFxcUVOc5z585p6NChOnHihCQpKipK5cqV08svv6zy5cvr9ddfl6+vr15//XVlZ2crMzNTAwYMUGhoqMLDw5Wdna1nn31Wvr6+SklJ0aRJkyTlz6KLjo5WvXr1NHz4cGVnZ8vNzU2xsbF64IEHiuzPrFmzlJ2drZiYGEn5Bb2JEydq5cqVstlsmjJlilJSUmSz2RQZGam+ffsqOTlZU6dOld1u19///nfVqVNHkmSxWLRgwQJVq1ZNsbGxGj9+vBYtWqSaNWsW2o7D4dCkSZO0bds21ahRQzabTV27dlVkZOT1OKUAAAAAALipRT88QMfTj5V0N66r6nVqaM5X80u6G9ekVM5Ii4iI0Pr16xUcHKxDhw4pNzdXb775psaOHatGjRrpwIEDioqK0meffSZJqly5st555x1J0oQJEyRJc+fOVZ8+fRQfH6/k5GRn28OHD9fw4cMVGBiopUuX6sMPP1RQUJCsVqs+/vhj2e12PfPMM/riiy8UGxurxYsXa+XKlQX6N3v2bPn4+CghIUGZmZnq1q2b7rnnHknSt99+q4SEBLm7u6t9+/bq0aOH/Pz8Ch3n5s2bVbt2bc2fP1/79+/XunXrNGrUKAUFBalFixZ69NFH9cYbb+jFF19UQECA0tLSFB4erh49emjw4MFKSUnRwIEDi/wK5KpVq9SmTRv1799f27dv165du4otpEVGRqpXr14aOXKk3NzctGbNGnXt2lVS/tc0JWn16tW6ePGi+vXr55y5dujQIW3dulXe3t6aPXu2pPxiaHJysqKjo+Xv7+/MUVQ7J06c0L59+5SQkKCzZ88qPDy8yH4WJ63907p0xPWvkF7W4LvN+rnJ44ZyStLJkxUMxz70vzX6d+1OhuOrVT9nOLb+ni069EA7w/FmmMmdd9HdcN6/7/tMPzYKNhyfetLHcGzHjOVK8O1uON5LNsOxbTJWaptvN8PxFhOTsltnrNIXvl0NxZ41+TZm5phXd881nNf/yGol1+psOD7HZvwcN3O8JSlHxnMHZ3ysz3yfMhRb0S3PcF5JCvg1XjtqGvvji81hMZz3kaOf6Ms7uhiOdzPx3Gp1NF5f32H8D05ZjrKGY0MylmuTidczM+M2c57ZZfyxlsyN29ti/Bw3e545TIz70aOrlHSH8deUSdZsw7Eb0jYqtG4Hw/GnbBcMx359ZKta1Qo0FPvFf94znFeSrNUbKu/4QUOxW+43tpyNZP55Xa4EP6dcMPHeZXbcZpjJbWa2TIeM5dpoYsy5FuMrSHU+ulSr7+hpKLZ83WoK/maW4dyAq0plIa1169Z67bXXdO7cOSUkJKhDhw6aO3euXnnlFec+2dnZOnXqlCSpSZMmLrWbmZmp48ePKzAw/02xZ8/fn8CVK1fWkiVL9NNPP+nQoUPKzi76TX/nzp168803JUlVqlRR27ZtlZKSIi8vLzVt2lReXl6SpLp16yorK6vIdpo2baoZM2YoIyNDbdq0UVRU1FX7xMTEKCkpSfPmzdMPP/xQbL/+KCAgQIMGDdL+/fvVunVrPf3008XuX6dOHdWrV08pKSl68MEHtW3bNo0cOVKStGPHDu3fv187d+6UlH/8U1NTddddd6lBgwby9vZ2qU9FtXPw4EE98cQTslqtqlKlih577DGXxwkAAAAAAHA9lMpCWtmyZRUYGKjExER9+umnmjdvnhYsWFBgjbKjR4+qcuXKkiRPT0+X2rVarbJYfv+LXG5uro4dO6YffvhBs2bNUp8+fRQZGalTp06puG/E/vFnDodDNlv+X2A8PDyc2y0WS7Ht1K9fX5s2bVJSUpK2bt2q999/Xxs3biywz8svv6yKFSsqMDBQHTp0KPQiBH/Mk5eX/xfPZs2aacOGDdq2bZs2btyo1atXa+HChUX2R5K6dOmihIQEnTlzRgEBAc6ioM1m04gRI/TEE09Iyi9KVqhQQXv27HH5+BfXzpQpUwqMoUyZUnnqAgAAAABgiF0O2Uvl8vxFK43jKbVX7YyIiNDChQtVuXJl1a5dW/Xr13cW0r766iv16tXrmtv09vaWr6+v8wIEa9eu1cyZM7Vjxw6FhISoS5cuqlixopKTk52FMXd3d126dKlAOy1bttSqVask5ReCPv/88z+9OEBhPvroI82ePVshISEaN26cMjMzde7cObm7uzvzf/XVVxo8eLDatWun7du3S8ovRl3ZLx8fHx08eFAOh0NpaWlKTU2VJE2ZMkXr1q1T586dNXbsWO3bt+9P+xQcHKydO3cqISFBXbr8/hWCli1basWKFcrLy9P58+fVs2dP7dmz55rHXFQ7AQEB2rRpky5evKisrCwlJSVdc9sAAAAAAABmlNppPc2aNdPZs2edV4GcOnWq4uLi9N5778lqteqtt94qMLvMVZfbmTp1qnx8fDRlyhSdOnVKw4cP14YNG2S1WvXggw8qPT1dktS2bVtFREQUWIcsKipKcXFxCgsLk81m0wsvvKD77rvPWcByVadOnTR06FCFhYXJ3d1dI0aMUMWKFdWqVSvNmDFD3t7eGjRokHr27CkPDw/dc889ql27ttLT09WkSRPNmTNH06ZN0+DBg/XJJ5+offv2atCggZo1ayYp/6IDw4YNU3x8vNzd3TV58uQ/7ZOnp6datWql5ORkNW/e3Lm9e/fuOnz4sDp37qxLly4pMjJS/v7+Bdafc0VR7UjS3r171bFjR1WrVk0NGza8pnYBAAAAAADMKrWFNEnasmWL837Dhg21ePHiq/a5fKXKyxITE6+67+/v7yzW+Pn5admyZQViatSoofXr1xfah8uL50tyFsq8vLw0bdq0q/aNjIwscJXJwvp7JS8vL82ff/XVK0JDQxUaGur8/7PPPlto/ObNmwvt55WWLl1abB8K89prr121zWq1KjY29qrtVx5bSRo0aJDz/pXjv/JxKawdSRoyZIiGDBlyzf0FAAAAAAC4Hkp1Ie1W8MsvvxQoLl1pwoQJaty48Q3tz7Bhw3TgwIGrtgcFBemll166oX0BAAAAAAD5HL/9u5WUxvFQSCthd955Z4GLJJS06dOnl3QXAAAAAAAAbkql9mIDAAAAAAAAwI1EIQ0AAAAAAABwAV/tBAAAAAAAuMnZf7vdSkrjeJiRBgAAAAAAALiAQhoAAAAAAADgAgppAAAAAAAAgAtYIw0AAAAAAOAm53A45HA4Srob11VpHA8z0gAAAAAAAAAXUEgDAAAAAAAAXEAhDQAAAAAAAHABa6ThtnLwhI8uZFy65rgGkv5fRtXr3yEXHb/kaTz2V+Ox9SXt/7Wa4Xgz/n/27j2uqir///j7iFxMtCAvE4qaNOFDs98YjmAKJHhBEkYpDXM0rSi+kmmP7zjqqITmrdTxq6ZTmqNGFzNCUZQc89KQXxV1NKUm5luWkiaJIhhegMP5/WGePHLxuLeJyOvpg4ewz/rsz1r7HA74ca21ayr3byV9fcrLcLyLyfxm4s+bzG423oxzBn8c3YgeGz3Haau7qbxm480wer1vhHJZDMWdKXcznftGnMOIszbXGskrSYW2mhmzWUZfJzcqviaYfZ3U5OvsJ5vx95QXShqbym0u3lzuydbmhuI+eWCiqbx981YZPkfP7OmmcpuJ32xy3OdM/NSvfe8I5pkds5l4D1u5qdxG491r4V5bqJ0opAEAAAAAANzibLKpXLdXwdBWC8fD0k4AAAAAAADACRTSAAAAAAAAACdQSAMAAAAAAACcwB5pAAAAAAAAt7jynz9uJ7VxPMxIAwAAAAAAAJxAIQ0AAAAAAABwAoU0AAAAAAAAwAnskQYAAAAAAHCLs/3853ZSG8fDjDQAAAAAAADACRTSAAAAAAAAACdQSAMAAAAAAACcwB5pAAAAAAAAt7hy2VReC/cUq05tHA8z0gAAAAAAAAAnUEiTtHv3bg0dOrSmu3FL8ff3r/R4XFyc8vLyDJ+3oKBAHTt21N///neH4+PHj9eQIUNks/1SjU5NTdX48eP1/fff6/e//73+/e9/2x+z2WwaPHiw3nnnHcN9AQAAAAAAuB4U0nBdli5dqubNmxuOX79+vcLCwvTBBx84FM0k6fPPP9fbb79dIaZly5YaPXq0Jk2aJKvVKkl677335ObmpiFDhhjuCwAAAAAAwPWgkPaz06dPKy4uTn369FF8fLxKSkr00UcfqV+/foqKitL48eNVXFwsSerWrZsSExPVv39/Pfvss8rIyNCTTz6psLAwZWVlSZKOHDmiESNGaMCAARo8eLC+/PLLavOfP39e//3f/23Pt3btWpWVlalr16766aefJEmxsbFasmSJJCk9PV1TpkxRamqqXnrpJT399NPq1auXkpKS7OdcsmSJBgwYoOjoaL322muy2Wz6/vvvFRERocGDB2vEiBH66quvNGjQIMXExGjw4MH67rvv7PGJiYmKjo5WdHS0jhw5IkkKCwvT999/r9TUVL344osaMmSIevfurZkzZ1YojFUmNTVVTz75pNzc3LRr1y6Hx5555hn97W9/s+e60pAhQ9SgQQO9++67ysvL05IlSzRz5kxZLJZr5gQAAAAAALgRKKT97Pjx40pMTFRGRoby8/P1/vvv64033lBycrLWr1+vBg0a6PXXX5ck5efnKyQkRGvXrtXFixf1ySef6L333tOoUaO0cuVKSdK4ceM0duxYrVmzRq+88opeeumlavMvXLhQXl5eSk9P18qVK7Vw4UJ9/fXXCgoK0p49e1RcXKzjx49rz549kqTMzEw98sgjkqT9+/drwYIFWrdunbZt26acnBz985//VHZ2tlJSUrR27Vrl5eVp3bp1kqRvv/1Ws2fP1vLly7Vy5UqNGDFCqampGjRokA4cOGDv08MPP6x169apW7duWrVqVYU+79u3T/Pnz1d6ero+//xzbd68udoxfvXVV8rPz1fnzp3Vt29fffDBBw6Pt27dWvHx8frLX/5SoShnsVg0ffp0LVmyRJMnT9aLL74oHx+favMBAAAAAHC7sNlst+VHbcNdO3/Wrl07+fr6SpL8/Px09uxZ9ejRQ15eXpKkJ554QhMmTLC3DwkJkSS1aNFCAQEBkiQfHx8VFRWpuLhY2dnZDu3PnTungoIC+/mutmvXLs2YMUOS5O3trfDwcGVlZSk0NFQ7d+5UvXr1FBUVpY0bN6q0tFR79+7V1KlTtWHDBnXq1Emenp6SJF9fXxUWFmrnzp06ePCgYmJiJEkXLlyQj4+PAgICdPfdd6tly5aSpNDQUE2dOlWZmZkKCwtTjx497H3q2bOnJOm+++7T3r17K/Q5PDxcTZo0kSRFRkZq165d6t27d5XXOCUlRREREXJxcVFkZKQWL16s/Px8+zkkadiwYfrHP/6ht99+W40aNXKIb926tWJjY7V792499thjVeapziN7FxqKk6S+eRWLiTcLuetGXnLXrdx1ccx1NXddHDO5605ectet3K5N/QzHRprss9l4M+ric10Xxww4i0Laz+rX/+VSWCwWNW7cWEVFRfZjNptNZWVl9q/d3Nzsn7u4uDicq7y8XG5ubkpLS7MfO3HihO66664q819dhbXZbLJarQoJCdHy5cvl4uKirl276vDhw0pJSdH9998vd3d3SbL/fbnvl2OfeuopjRgxQpJUVFQkFxcXFRQUyMPDw94+IiJCnTp10rZt27RixQpt375d06ZNc7gml895tSvHXV5eXuE6XKmkpETp6emqX7++tm7daj+empqq5557zv51vXr1NGPGDMXGxuqJJ56ocB4fHx+1aNGiyjzXsr3zKJ3PPXndcX3zVimjeazhvGaQu27kJXfdyl0Xx1xXc9fFMZOb1xm5b93cPbOnG87r2tRPpSe/MRy/+YGJhmMj81Zpo4nrbWYzmNr6XNfGvGZzN/BtamriBOAslnZWY+vWrTpz5owkafXq1QoMDD0h8xwAACAASURBVHQqrlGjRmrTpo29kLZjx45rboofFBSklJQUSZf2a9uyZYu6dOkib29veXh4aNu2bQoICFBQUJAWL17sMHOsqvOlpaWpuLhYZWVlSkhI0KZNmyq0GzNmjA4dOqTY2FiNHj36mnu5XSkzM1Nnz57VxYsXtWHDBvssvcps27ZNXl5e+uyzz7R161Zt3bpVU6dOrfSmA23atFF8fLyWLVvmdF8AAAAAAAB+bRTSquDp6annn39eQ4cOVUREhIqKijRmzBin42fPnq2UlBRFRUVp7ty5mjdvXrUb4yckJOjMmTOKiorSH//4R8XHx6tDhw6SLi0jbdy4sRo2bKigoCD9+OOPCg0NrTZ/WFiYevfurUGDBqlfv35q166dBgwYUKFdfHy8/va3v2nAgAGaPXu2w80KrsXb21txcXGKjo7WI488ouDg4CrbXr7JwJX69eunixcvKjMzs0L7YcOG6cEHH3S6LwAAAAAA3M5skspvs4/at0MaSzslSYGBgQ6zzWbNmmX/fODAgRXa5+TkVNr2yvP4+fkpOTnZ6T54enpqzpw5lT6WkJCghIQESZK/v79D/piYGPs+aJIcco4cOVIjR450OFfLli0dlla2a9dOH330UYWcVeW4HJuVlaW2bds6jL86b775ZoVjbm5u+uyzzySpwmy2evXq6b333qsQc/V4AQAAAAAAbhYKaTfRihUrtGbNmgrHmzVrpqVLl9ZAj26so0ePatSoUZU+Nm3aNHXs2PEm9wgAAAAAAODGoZB2Ew0fPlzDhw+v6W7cEJXNDGvVqpXDDRYAAAAAAABuJxTSAAAAAAAAbnE2SbZauatY1WrjaLjZAAAAAAAAAOAECmkAAAAAAACAEyikAQAAAAAAAE5gjzQAAAAAAIBbXLlsKq+Vu4pVrTaOh0IaAAAAAAAAaqXjx49r7NixOnXqlO69917NmTNHDRs2dGhz7Ngx9evXT61atZIkNWnSRMuWLVNJSYkmTpyo7OxseXh4aM6cOfLz86s2H0s7AQAAAAAAUCtNmTJFTz75pD7++GM98MADWrx4cYU22dnZioqKUlpamtLS0rRs2TJJUnJysho0aKCMjAz95S9/0YQJE66Zj0IaAAAAAAAAap3S0lLt2bNHffr0kSTFxMTo448/rtDu0KFD+s9//qM//OEPGjZsmHJyciRJ27dvV3R0tCTp97//vU6fPq3jx49Xm5OlnQAAAAAAAKgxP/zwg6xWq8Oxxo0bq3HjxtXGFRQUyNPTU/XrXypvNW3aVHl5eRXaubu7Kzo6WrGxscrMzFRCQoI2btyoH3/8UU2bNrW3a9q0qU6cOCEfH58qc1JIAwAAAAAAuMXZbDbZbLVvc/7qXB7PkCFDdOzYMYfHXnjhBY0aNcr+dUZGhmbOnOnQpnXr1rJYLA7Hrv5aksN5QkNDNXfuXB0+fFg2m82hvc1mU7161S/epJAGAAAAAACAGvPuu+9WOiPtSn379lXfvn0djpWWliowMFBWq1UuLi46efKkmjVrVuH8ycnJ6tevn7y8vCRdKpjVr19fzZs3148//mi/CUF+fn6l8VeikIY65d67z6js4mlDsfc3MxYnSYVnPAzHSlJzt3OGY/9T1shU7sJ6LobiftfI+PW67N47Cw3FHTjrbSqv0TFLUoBXvqnc991dYDjWWmZu20uj11uSDhfeaSq30f9Xa2QpM5VXkjwNnsNqq/g/XdejgazXblQFL48LpnL7eBQbjj12oeG1G1XD6HPdplGRqbyS1NrgOY6f9TSV11XlhmN/63PKVO72PicNx35xvOm1G1XDzP+Xu5qKluobjC+Tue9rM4KHl9RY/JYV7qZyl5i4bmafazPRFy3mfm5eMBjvZSs1lVcy/jNk8wMTDeeMzFtlKr5X9nTDsWbjzfRbMvc6M/4T4BKjvy2kexj/PaOvpDQT8UuP7zAca5XU7/Q/DcW2btRShw1nRk275557DMW5urqqc+fO2rhxo6KiorR27VqFhIRUaLdnzx5duHBBcXFxysrKUnl5udq2bavQ0FClpaWpc+fO2rt3r9zd3atd1ilxswEAAAAAAADUUi+//LJWr16tyMhI7d27V2PGjJEkvf/++5o/f74kaeLEifrf//1f9evXT6+++qrmzp2revXqaejQoSopKdGjjz6q6dOn67XXXrtmPmakAQAAAAAA3OLKZVO5yZm8t5obMZ4WLVooOTm5wvHBgwfbP2/evLmWL19eoY27u7teffXV68rHjDQAAAAAAADACRTSAAAAAAAAACdQSAMAAAAAAACcwB5pAAAAAAAAtzybbLfZHmnm7otbM5iRBgAAAAAAADiBQhoAAAAAAADgBAppAAAAAAAAgBPYIw0AAAAAAOAWV26zqdxW+/YUq05tHA8z0gAAAAAAAAAnUEgDAAAAAAAAnEAhDQAAAAAAAHAChTQAAAAAAADACdxsAAAAAAAA4BZn+/njdlIbx/OrF9J2796t119/XcnJyb92qlrD399fOTk5FY7HxcVp2rRpat68uaFzdu/eXcuWLbMfO336tIKDgxUfH69Ro0ZJktatW6e33npLVqtV9erVU0REhJ5//nnVr19fu3fvVnx8vFq1aiVJKi8vV3FxseLi4jR48GAlJCTo+++/17lz55Sfn29v96c//UnBwcHKzc3VnDlz9MUXX8jFxUXe3t7605/+pICAAHufysrK9Mgjj6hPnz6aPHmywxiuFR8WFiYPDw+5urraY9q3b6+ZM2de9/UCAAAAAAC4XsxIu4UsXbrUVPy3336rM2fO6K677pIk/eMf/1Djxo3tj6empmr58uVatGiRWrVqpZ9++knjx49XYmKiZsyYIUl64IEHHIqe//73v/X4448rKipKixYtklR5cbSgoEBPPvmkXnzxRc2fP1+StH//fo0aNUpr165VkyZNJEmffvqpOnbsqIyMDP3pT39SgwYNrit+yZIlatmypanrBAAAAAAAYMRNKaSdPn1acXFxOnr0qO69914tWLBA69ev1/Lly2WxWNShQwdNnjxZDRs2VLdu3RQeHq6DBw+qSZMmeuyxx5ScnKwTJ05o1qxZ6tKli44cOaKkpCSdOXNGHh4emjx5stq3b19l/vPnz2vSpEnKycmRxWLRM888o379+ik4OFibN2+Wp6enYmNjFRYWpueee07p6enat2+fOnbsqMzMTBUWFio3N1fdunVTUlKSpEsFnYyMDFmtVnXv3l1jx47VsWPH9Oyzz8rLy0seHh4aN26cEhMTVVZWJnd3d82cOVNt2rSRJCUmJurAgQOSpIULF6p169YKCwvT22+/raysLG3fvl2nTp3SyZMn1aNHD40fP14Wi6Xa6xweHq4tW7bosccekyR9/PHH6tWrl/3x119/XTNnzrTPJPP09NT06dMVHByshISESs957NgxNWjQQG5ubtXm/uCDD/TQQw9p4MCB9mOdOnXS+PHjdf78efux1NRU9erVSzabTRs2bNDjjz9+XfEAAAAAAAA1xWKz2X7VJamXlwuuW7dOLVq00KBBgxQVFaV33nlHq1evlpeXl6ZMmWIvPPn7+2vRokXq2bOnhg4dqmbNmmnu3Llas2aNPvnkEy1atEixsbFKTExU+/bt9fXXXyshIUGbNm2qsg+vvfaaSkpKNGnSJJ0+fVoDBw7UokWL9Oabbyo6OlpdunRR37595e/vr6VLl2rcuHGKjIzUqVOntGDBAqWnp8vFxUURERFasmSJ8vLylJKSonnz5slisWjs2LEKCQlRQECAvZjVsmVLTZgwQSEhIerbt6/WrFkji8Wi/v37y9/fX/Pnz1dERIReffVVSdK4ceMcCmlz585VWlqaGjdurGHDhunpp59W7969qxyjv7+/3n//fb3xxhtasmSJ8vPzHZZFDhkyRF27dtWBAwfss8Aui4mJUXx8vO6880770s7z58+rsLBQgYGBevbZZ/Xggw86PKdXz0iLj49X9+7d9cc//rHKPp4+fVrh4eHavn27tm/frnfeeUcffvih0/GVLe0cNmyYvXAIAAAAAMDt6rHAwTrxfV5Nd+OG+k3L5vpo9/s13Y3rclNmpLVr106+vr6SJD8/P509e1Y9evSQl5eXJOmJJ57QhAkT7O1DQkIkSS1atLAXgnx8fFRUVKTi4mJlZ2c7tD937pwKCgrs57varl277EsXvb29FR4erqysLIWGhmrnzp2qV6+eoqKitHHjRpWWlmrv3r2aOnWqNmzYoE6dOsnT01OS5Ovrq8LCQu3cuVMHDx5UTEyMJOnChQvy8fFRQECA7r77bvvSw9DQUE2dOlWZmZkKCwtTjx497H3q2bOnJOm+++7T3r17K/Q5PDzcvpwxMjJSu3btqraQJl2awfXtt9/q7Nmz+vjjj9WnTx/l5+c7tLFarRXiSktL7bPdLi/tLCkp0dixY+Xp6elQRKvOlTPm/vznPysnJ0fnzp1TbGysnnnmGa1bt05BQUG68847FR4ersmTJ+vLL7+0zya8VrxkfmnnkT5DVXb8+t94/A79Q990rP76V6fwjIfh2Idy1+lfvtGG4/9T1shwbOwP72rVPUMMxf6u0WnDeSWp3X8y9NX9fQ3FHjjrbTivmTFLUoBX/rUbVeG3X27S/7XvYzjeWmb8RsxmrrckHS6803BsZN4qbWweayi2kaXMcF5JCj6RoszfPG4o1mqrfpZwdR7J+1Dbmw+8dsMqeHlcMBz7/46s1+etowzHH7vQ0HCsmee6TaMiw3klqf3XG/XlfZGGYo+f9TSct2fean3SfJDh+N/6nDIc23r/Fh3pFG44/ovjTQ3HmnmuJcnVxNbDvfI+0ObmTxiKLZPx72tJ6pu3ShkGxx08vMRwXs9XU/XTuBjD8VtWuBuO/UPe+0prPthwvJnn2uzr7KLF+M/NASfe05rfPGko1stWajivZO5nyDm5GM5r9nr3yp5uONa1qZ9KT35jOH7zAxMNx5odd7nhSKlf3iqlG8yd7lHx333OeuPIh4pvbfz3lKXHdxiOtZYel4urj6HY1q1b6vDXWYZzA84y/tPjOtSv/0u9zmKxOOzbJUk2m01lZb/8Y+jKZYQuLo5v9uXl5XJzc1NaWpr948MPP7TvC1aZqyfd2Ww2Wa1WhYSEaPfu3dq1a5cCAwPVrl07paSk6P7775e7+6VfKC7/fbnvl2Ofeuoph/zx8fGSJA+PXwomERERWrNmjR588EGtWLFCL7/8coVrcvmcV7ty3OXl5RWuQ2UsFot69OihLVu2aNOmTerT55d/kHt7e6tVq1bav3+/Q8zp06eVm5tbYWmsm5ubpk2bpm3btmnjxo3XzN2xY0f961//sn/92muvKS0tTdHR0Tp37pykS8s69+/fr7CwMEVHR6tevXpatWqV0/EAAAAAAAA16aYU0iqzdetWnTlzRpK0evVqBQYGOhXXqFEjtWnTRmlpaZKkHTt2aMiQ6mePBAUFKSUlRdKlwtGWLVvUpUsXeXt7y8PDQ9u2bVNAQICCgoK0ePFih5ljVZ0vLS1NxcXFKisrq3Jp6ZgxY3To0CHFxsZq9OjR+vLLL50aoyRlZmbq7NmzunjxojZs2GCfpXctffv21XvvvSc3Nzd5ezvOyhkzZoxmzJih3NxcSVJxcbEmTZqkyMhItWjRosK5GjVqpFGjRum1117ThQvVz34YPHiw9u3bp9TUVHthMD8/XwcOHFC9evWUnZ2tEydOaPv27dq6dau2bt2qN998U+vXr9dPP/10zXgAAAAAAICaViN37fT09NTzzz+voUOHqrS0VB06dNCUKVOcjp89e7aSkpL01ltvydXV1b5XWVUSEhKUlJSkqKgoWa1WxcfHq0OHDpIuLSP99NNP1bBhQwUFBWnGjBkKDQ2tNn9YWJi++uorDRo0SFarVcHBwRowYICOHTvm0C4+Pl4TJ07UokWL5Orqar9RgTO8vb0VFxengoICRUdHKzg42Km43/3udzp58qTDpv2XPfroo3JxcdHo0aNVUlIiq9WqRx991D6brjIDBw5UcnKyli9frv/6r/+qtr+rVq3S3LlztWzZMlmtVrm6uio6OlrDhg3Tq6++qpiYGIcZe4GBgbr33nu1fv16DR48uNr4y5577jmHPdIaNGhgn9UGAAAAAMDtqlw2lZtYEn8rqo3j+dULaYGBgQ6zzWbNmmX/vLJiT05OTqVtrzyPn5+fw0b31+Lp6ak5c+ZU+lhCQoL9jpX+/v4O+WNiYuz7oElyyDly5EiNHDnS4VwtW7bU1q1b7V+3a9dOH330UYWcVeW4HJuVlaW2bds6jP9aLp/TYrFo27Zt9uOjRo1yaBcREaGIiIhKz3H1cyVdWoJ69dLOytpJ0m9+8xvNnj270nMnJiZWejw1NdWpeEkO1xYAAAAAAOBmq5EZab+GFStWaM2aNRWON2vWTEuXLq2BHt1YR48erVAUu2zatGnq2LHjTe4RAAAAAABA3XLbFNKGDx+u4cOH13Q3boirZ8JJUqtWrez7wgEAAAAAAODmu20KaQAAAAAAALctm+w357tt1MLhcDtEAAAAAAAAwAkU0gAAAAAAAAAnUEgDAAAAAAAAnEAhDQAAAAAAAHACNxsAAAAAAAC4xZXLpvLauDt/NWrjeJiRBgAAAAAAADiBQhoAAAAAAADgBAppAAAAAAAAgBPYIw0AAAAAAOAWZ/v5z+2kNo6HGWkAAAAAAACAE5iRhjrl21N36fyPpdcd5yfpPz96G85rtsZ+ouQOw7F3ymoq953lxuK/LbzTVN52Js5RU2OWpK9PeRmO/a3JeDPMXG9JspjMbzT+J5v5H2M34hxGnJeL8dgLDQ3H/j9Jx03E15QjZxubim9/A85hVKmJ/7f88nhTw7GtTcbX1Pe1JJWZzG42viZkrnAzHNv3VXPxbiZ/UzEbb4aZZ9rdVm4qt9F4M+//Zs9Rk9/Xmx+YaDg2Mm+Vqfhe2dMNx5qN/8REvyUZfrVEXzD3OjMTH+0dYir3eoPxDe4y/jMPuB7MSAMAAAAAAACcwIw0AAAAAACAW5zNZpPNVvv2FKtObRwPM9IAAAAAAAAAJ1BIAwAAAAAAAJxAIQ0AAAAAAABwAnukAQAAAAAA3OLKZVN5Dd4p+ddQG8fDjDQAAAAAAADACRTSAAAAAAAAACdQSAMAAAAAAACcQCENAAAAAAAAcAI3GwAAAAAAALjF2WSTzVb7Nuevjo2bDQAAAAAAAAC3JwppAAAAAAAAgBMopAEAAAAAAABOuO0Labt379bQoUNruhu3FH9//0qPx8XFKS8vz/A5P/zwQ4djQ4cO1e7duyVJpaWlmjdvnnr37q2oqCg9/vjj2rhxo73twoUL1adPH124cMF+7MrnbuHCherWrZv+8Ic/OHz88MMPhvoLAAAAAEBtYpNN5bfZR23cI42bDcBu6dKlpuLnzp2r7t2765577qnw2OTJk3Xx4kWlpqbK09NTubm5iouLU0lJifr37y9JOnbsmP7617/qL3/5S6Xnj42N1ahRo0z1EQAAAAAAwKg6UUg7ffq04uLidPToUd17771asGCB1q9fr+XLl8tisahDhw6aPHmyGjZsqG7duik8PFwHDx5UkyZN9Nhjjyk5OVknTpzQrFmz1KVLFx05ckRJSUk6c+aMPDw8NHnyZLVv377K/OfPn9ekSZOUk5Mji8WiZ555Rv369VNwcLA2b94sT09PxcbGKiwsTM8995zS09O1b98+dezYUZmZmSosLFRubq66deumpKQkSdKSJUuUkZEhq9Wq7t27a+zYsTp27JieffZZeXl5ycPDQ+PGjVNiYqLKysrk7u6umTNnqk2bNpKkxMREHThwQNKl2V6tW7dWWFiY3n77bWVlZWn79u06deqUTp48qR49emj8+PGyWCzVXuennnpKkyZN0rJlyxyO5+bmatOmTdqxY4fuuOMOSZKvr68mTJigV155xV5Ie+KJJ7Rx40b17t1bnTt3NvJUAwAAAAAA/Gpu+6WdknT8+HElJiYqIyND+fn5ev/99/XGG28oOTlZ69evV4MGDfT6669LkvLz8xUSEqK1a9fq4sWL+uSTT/Tee+9p1KhRWrlypSRp3LhxGjt2rNasWaNXXnlFL730UrX5Fy5cKC8vL6Wnp2vlypVauHChvv76awUFBWnPnj0qLi7W8ePHtWfPHklSZmamHnnkEUnS/v37tWDBAq1bt07btm1TTk6O/vnPfyo7O1spKSlau3at8vLytG7dOknSt99+q9mzZ2v58uVauXKlRowYodTUVA0aNMheOJOkhx9+WOvWrVO3bt20atWqCn3et2+f5s+fr/T0dH3++efavHnzNa9zXFycCgoKKizxzM7Olp+fn72Idlnnzp2Vm5urM2fOSJLuuusuJSUlaeLEiQ5LPC9btWqVw7LOhISEa/YJAAAAAADgRqkTM9LatWsnX19fSZKfn5/Onj2rHj16yMvLS9KlmVATJkywtw8JCZEktWjRQgEBAZIkHx8fFRUVqbi4WNnZ2Q7tz507p4KCAvv5rrZr1y7NmDFDkuTt7a3w8HBlZWUpNDRUO3fuVL169RQVFaWNGzeqtLRUe/fu1dSpU7VhwwZ16tRJnp6eki7N4iosLNTOnTt18OBBxcTESJIuXLggHx8fBQQE6O6771bLli0lSaGhoZo6daoyMzMVFhamHj162PvUs2dPSdJ9992nvXv3VuhzeHi4mjRpIkmKjIzUrl271Lt372qvc/369TVr1iwNGzZM3bt3tx+3WCyyWq0V2peVldkfv7JfGRkZ+utf/6rw8HCH9jdiaecjexcaju2bV7HgeLNE1mDumhx3TeWui2Mmd93JW9O56+L7WV19rsldN/LW5dy8n91cNXm9XZv6GY41e81q6prX5PWuydy3Olst3VOsOrVxPHWikFa//i/DtFgsaty4sYqKiuzHbDabvagjSW5ubvbPXVxcHM5VXl4uNzc3paWl2Y+dOHFCd911V5X5bTZbha+tVqtCQkK0fPlyubi4qGvXrjp8+LBSUlJ0//33y93dXZLsf1/u++XYp556SiNGjJAkFRUVycXFRQUFBfLw8LC3j4iIUKdOnbRt2zatWLFC27dv17Rp0xyuyeVzXu3KcZeXl1e4DlW5//777Us8L3vwwQf13XffqbCwUHfeeaf9+P79++Xr6+twTJImTZqkqKioaq+pUds7j9L53JPXHdc3b5UymscazmvmrSEyb5U2mshd/YLc6pkdtxk1lbsujpncvM6uR029n5l5L5N4rsl9e+eui2O+Ebl5P7u5eWvy9+Fe2dMNx7o29VPpyW8Mx3/ywETDsWaueU1ebzPM5G7g21Q9TEycAJxVJ5Z2Vmbr1q32JYWrV69WYGCgU3GNGjVSmzZt7IW0HTt2aMiQIdXGBAUFKSUlRdKl/dq2bNmiLl26yNvbWx4eHtq2bZsCAgIUFBSkxYsXO8wcq+p8aWlpKi4uVllZmRISErRp06YK7caMGaNDhw4pNjZWo0eP1pdffunUGKVLy0vPnj2rixcvasOGDfZZes64vMRz//79ki7N5ouKitLEiRNVXFwsSTp69KhmzpypF154oUK8l5eXkpKStHjxYqdzAgAAAAAA/NrqxIy0q3l6eur555/X0KFDVVpaqg4dOmjKlClOx8+ePVtJSUl666235Orqqnnz5lW7EX9CQoKSkpIUFRUlq9Wq+Ph4dejQQdKlZaSffvqpGjZsqKCgIM2YMUOhoaHV5g8LC9NXX32lQYMGyWq1Kjg4WAMGDNCxY8cc2sXHx2vixIlatGiRXF1d7TcqcIa3t7e9IBYdHa3g4GCnYy8v8by89FSSXn75Zb355pt6/PHH5eLiIjc3N40ePVqRkZGVnqNnz57q06ePfvzxR/uxVatW6ZNPPnFoN27cOD388MNO9w0AAAAAAMCo276QFhgY6DDbbNasWfbPBw4cWKF9Tk5OpW2vPI+fn5+Sk5Od7oOnp6fmzJlT6WMJCQn2TfP9/f0d8sfExDgUo67MOXLkSI0cOdLhXC1bttTWrVvtX7dr104fffRRhZxV5bgcm5WVpbZt2zqM/1quPKd0aYlndna2/WsXF5dK+3xZZXufzZ071+Fxs/ujAQAAAABQW5XbpPJKtmaqzcpr4XBu+0LazbJixQqtWbOmwvFmzZpp6dKlNdCjG+vo0aNVFrKmTZumjh073uQeAQAAAAAA3FwU0m6Q4cOHa/jw4TXdjRvi6plwktSqVSuHGywAAAAAAADUNXX2ZgMAAAAAAADA9aCQBgAAAAAAADiBpZ0AAAAAAAC3ONvPf24ntXE8zEgDAAAAAAAAnEAhDQAAAAAAAHAChTQAAAAAAADACeyRBgAAAAAAcIsrt9lUbqt9e4pVpzaOhxlpAAAAAAAAgBMopAEAAAAAAABOoJAGAAAAAAAAOIE90gAAAAAAAG55NtlU+/YUq17tGw+FNNQpNhn/Nq19394160ZcL6PnsNyA3EaZHbeZ+Lo47pocc02qyddZTeE95ebH19Xvr5pSk8+1i8ns9WrwXcVMbiuv8puqvAbjP3lgouHYvnmrTMX3zJ5uONZM/GYTfTarpp5rs3kBZ7G0EwAAAAAAAHAChTQAAAAAAADACSztBAAAAAAAuMWV22wqt9XGTTqqVhvHw4w0AAAAAAAAwAkU0gAAAAAAAAAnUEgDAAAAAAAAnEAhDQAAAAAAAHACNxsAAAAAAAC4xdl+/nM7qY3jYUYaAAAAAAAA4AQKaQAAAAAAAIATKKQBAAAAAAAATmCPNAAAAAAAgFtcuc2mclvt21OsOrVxPMxIAwAAAAAAAJxAIQ0AAAAAAABwAoU0AAAAAAAAwAm3zR5pu3fv1uuvv67k5OSa7sotw9/fXzk5ORWOx8XFadq0aWrevPl1n7O0tFSvv/66MjIy5O7uLnd3dz399NOKjIxUQUGBhg8fZJ53MAAAIABJREFULknKz8+XJDVp0kSStGLFCr344ot64YUXFBgYaD/f+PHj1aVLF8XExCgsLEweHh5ydXW1P96+fXvNnDlT48eP165du3TnnXdKks6fP6+77rpLM2fOlJ+f33WPAwAAAACA2sT285/bSW0cz21TSIPzli5dajh28uTJunjxolJTU+Xp6anc3FzFxcWppKRE/fv3V1pamiRp4cKFkqRRo0Zd1/mXLFmili1bVvrYiy++qJiYGPvX06dP18KFC/U///M/BkcDAAAAAADgvNuqkHb69GnFxcXp6NGjuvfee7VgwQKtX79ey5cvl8ViUYcOHTR58mQ1bNhQ3bp1U3h4uA4ePKgmTZroscceU3Jysk6cOKFZs2apS5cuOnLkiJKSknTmzBl5eHho8uTJat++fZX5z58/r0mTJiknJ0cWi0XPPPOM+vXrp+DgYG3evFmenp6KjY1VWFiYnnvuOaWnp2vfvn3q2LGjMjMzVVhYqNzcXHXr1k1JSUmSLhWWMjIyZLVa1b17d40dO1bHjh3Ts88+Ky8vL3l4eGjcuHFKTExUWVmZ3N3dNXPmTLVp00aSlJiYqAMHDki6VNxq3bq1wsLC9PbbbysrK0vbt2/XqVOndPLkSfXo0UPjx4+XxWKpdHy5ubnatGmTduzYoTvuuEOS5OvrqwkTJuiVV15R//79b9yTeQ0lJSU6efKkfYYaAAAAAADAr81is9XCe41WYvfu3YqPj9e6devUokULDRo0SFFRUXrnnXe0evVqeXl5acqUKfbCk7+/vxYtWqSePXtq6NChatasmebOnas1a9bok08+0aJFixQbG6vExES1b99eX3/9tRISErRp06Yq+/Daa6+ppKREkyZN0unTpzVw4EAtWrRIb775pqKjo9WlSxf17dtX/v7+Wrp0qcaNG6fIyEidOnVKCxYsUHp6ulxcXBQREaElS5YoLy9PKSkpmjdvniwWi8aOHauQkBAFBAQoPDxcW7ZsUcuWLTVhwgSFhISob9++WrNmjSwWi/r37y9/f3/Nnz9fERERevXVVyVJ48aNcyikzZ07V2lpaWrcuLGGDRump59+Wr179650fBkZGVq2bJlSUlIcjhcXF+uhhx7S7t27ddddd0mqfEba0KFDdeLECXsRTpJ++OEHjR8/vsqlncOGDdNjjz1mX9rZuHFjnTlzRu7u7urZs6cSEhLk6el5na8WAAAAAABql+BOkTqW+0NNd+OGauF7jzL3b6zpblyX22pGWrt27eTr6ytJ8vPz09mzZ9WjRw95eXlJkp544glNmDDB3j4kJESS1KJFCwUEBEiSfHx8VFRUpOLiYmVnZzu0P3funAoKCuznu9quXbs0Y8YMSZK3t7fCw8OVlZWl0NBQ7dy5U/Xq1VNUVJQ2btyo0tJS7d27V1OnTtWGDRvUqVMne0HI19dXhYWF2rlzpw4ePGhfznjhwgX5+PgoICBAd999t30JZGhoqKZOnarMzEyFhYWpR48e9j717NlTknTfffdp7969FfocHh5u38csMjJSu3btqrKQZrFYZLVaKxwvKyuzP34t06ZNq7BH2pWcWdp5+PBhPf300woODr7uItq2zqN0PvfkdcVIUmTeKm1sHnvdcTeC2dzXflaq1jdvlTIM5jZboTcz7poas2Ru3LX1uZZqbtxmxiyZH3dN5a3J15kZtTV3Xfzekmp23GbU1tw1+Vy7mMjeJ+8DbWr+hOF4M8zmtpp4lfOz6/pV/FeE8/rlrVK6qde4cWbH3TN7uuFY16Z+Kj35jaHYzQ9MNJzX7HtKueFIc891A9+mCt+70ET2W5/NZpPNZuYK33pq49yu26qQVr/+L8OxWCxq3LixioqK7MdsNpu96CNJbm5u9s9dXBzfXsvLy+Xm5mbf80uSTpw4YZ9xVZmrXwA2m01Wq1UhISFavny5XFxc1LVrVx0+fFgpKSm6//775e7uLkn2vy/3/XLsU089pREjRkiSioqK5OLiooKCAnl4eNjbR0REqFOnTtq2bZtWrFih7du3a9q0aQ7X5PI5r3bluMvLyytchys9+OCD+u6771RYWOiwpHL//v3y9fW9acss27Ztqz/96U/685//rIyMDDVq1Oim5AUAAAAAAHVbvZruwK9t69atOnPmjCRp9erVDrOhqtOoUSO1adPGXkjbsWOHhgwZUm1MUFCQfdnj6dOntWXLFnXp0kXe3t7y8PDQtm3bFBAQoKCgIC1evNhh5lhV50tLS1NxcbHKysqqXFo6ZswYHTp0SLGxsRo9erS+/PJLp8YoSZmZmTp79qwuXryoDRs22GfpVcbHx0dRUVGaOHGiiouLJUlHjx7VzJkz9cILLzid80bo16+fWrRoocWLF9/UvAAAAAAAoO66rWakXc3T01PPP/+8hg4dqtLSUnXo0EFTpkxxOn727NlKSkrSW2+9JVdXV/teZVVJSEhQUlKSoqKiZLVaFR8frw4dOki6tIz0008/VcOGDRUUFKQZM2YoNDS02vxhYWH66quvNGjQIFmtVgUHB2vAgAE6duyYQ7v4+HhNnDhRixYtkqurq/1GBc7w9vZWXFycCgoKFB0dreDg4Grbv/zyy3rzzTf1+OOPy8XFRW5ubho9erQiIyOdzlmd5557zmGPtAYNGmjVqlWVtv3zn/+s4cOH68knn7Qv6QUAAAAAAPi13DaFtMDAQIfZZrNmzbJ/PnDgwArtc3JyKm175Xn8/PyUnJzsdB88PT01Z86cSh9LSEhQQkKCJMnf398hf0xMjH0fNEkOOUeOHKmRI0c6nKtly5baunWr/et27drpo48+qpCzqhyXY7OystS2bVuH8V+Li4tLpX262pU3Gbissmt5Ze4rx1Rdu8sCAgJ06NChavsBAAAAAMDtwCabyk3vRn1rsdXC8dw2hbSbZcWKFVqzZk2F482aNdPSpUtroEc31tGjRystgkmXbhTQsWPHm9wjAAAAAACAWwOFtOs0fPhwDR8+vKa7cUNcPRNOklq1auVwgwUAAAAAAABcctvfbAAAAAAAAAC4ESikAQAAAAAAAE5gaScAAAAAAMAtzmazyWarfZvzV6c2jocZaQAAAAAAAIATKKQBAAAAAAAATqCQBgAAAAAAADiBPdIAAAAAAABuceWyqVy1b0+x6tTG8TAjDQAAAAAAAHAChTQAAAAAAADACRTSAAAAAAAAACewRxoAAAAAAMAtzmazyWarfXuKVac2jodCGuqUMkllshiMNRYnSfVMbqBYbiLWxVRm48xcL7PncDd1xcw9X9YbMG6jLljMTTI2E+9ps5rK7WrwmpfegOtdUz+6a9+vDJeYveJG482+j0qSi8Fz/GQx9056zsT3lrvN3PtZbfz5UVcV1jN3xc3E31lu7j28Jn/2mclt9D3BbPyNuF5Ge15zz5SU7mH8ddbPZHz0BXPfX2ZeKZsfmGg4NjJvleH4XtnTDec1Gz+p8yTDsf0kZXoYe6V6uUvhhjMDzmNpJwAAAAAAAOAECmkAAAAAAACAE1jaCQAAAAAAcIsrl03ltXBPseqU18INT5iRBgAAAAAAADiBQhoAAAAAAADgBAppAAAAAAAAgBMopAEAAAAAAABO4GYDAAAAAAAAtzybbLVwc/7q1b7xMCMNAAAAAAAAcAKFNAAAAAAAAMAJFNIAAAAAAAAAJ7BHGgAAAAAAwC3OZpNsttq3p1h1auNwmJEGAAAAAAAAOIFCGgAAAAAAAOCEOllI2717t4YOHVrT3bil+Pv7V3o8Li5OeXl5hs5ZWlqqefPmqXfv3oqKitLjjz+ujRs3SpLy8vLUpUsX+7RUm82mrl27atKkSfb4zMxMDR06VN9//738/f21Y8cOh/OHhYXp+++/N9Q3AAAAAACA68UeaajW0qVLDcdOnjxZFy9eVGpqqjw9PZWbm6u4uDiVlJSof//+8vLy0tdff63f/va3+uKLL+Tv76+dO3fa4/fu3auHH35YkuTq6qrJkydr3bp18vT0ND0uAAAAAABqk3LZVK5auKlYNWrjeOrkjDRJOn36tOLi4tSnTx/Fx8erpKREH330kfr166eoqCiNHz9excXFkqRu3bopMTFR/fv317PPPquMjAw9+eSTCgsLU1ZWliTpyJEjGjFihAYMGKDBgwfryy+/rDb/+fPn9d///d/2fGvXrlVZWZm6du2qn376SZIUGxurJUuWSJLS09M1ZcoUpaam6qWXXtLTTz+tXr16KSkpyX7OJUuWaMCAAYqOjtZrr70mm82m77//XhERERo8eLBGjBihr776SoMGDVJMTIwGDx6s7777zh6fmJio6OhoRUdH68iRI5J+mfWVmpqqF198UUOGDFHv3r01c+bMajc5zM3N1aZNmzR9+nR74cvX11cTJkzQ66+/Lknq2rWr/vWvf0mSPvvsM/Xq1UtNmjTRN998I0nat2+funXrJklq1qyZHn74Yb366qvXfnIBAAAAAAB+BXW2kHb8+HElJiYqIyND+fn5ev/99/XGG28oOTlZ69evV4MGDewFn/z8fIWEhGjt2rW6ePGiPvnkE7333nsaNWqUVq5cKUkaN26cxo4dqzVr1uiVV17RSy+9VG3+hQsXysvLS+np6Vq5cqUWLlyor7/+WkFBQdqzZ4+Ki4t1/Phx7dmzR9KlZY6PPPKIJGn//v1asGCB1q1bp23btiknJ0f//Oc/lZ2drZSUFK1du1Z5eXlat26dJOnbb7/V7NmztXz5cq1cuVIjRoxQamqqBg0apAMHDtj79PDDD2vdunXq1q2bVq1aVaHP+/bt0/z585Wenq7PP/9cmzdvrnJ82dnZ8vPz0x133OFwvHPnzsrNzdWZM2cUFBTkUEgLDg5Wt27dlJmZqZKSEh05ckQPPPCAPXb8+PH67LPPKizxBAAAAAAAuBnq7NLOdu3aydfXV5Lk5+ens2fPqkePHvLy8pIkPfHEE5owYYK9fUhIiCSpRYsWCggIkCT5+PioqKhIxcXFys7Odmh/7tw5FRQU2M93tV27dmnGjBmSJG9vb4WHhysrK0uhoaHauXOn6tWrp6ioKG3cuFGlpaXau3evpk6dqg0bNqhTp04Os7wKCwu1c+dOHTx4UDExMZKkCxcuyMfHRwEBAbr77rvVsmVLSVJoaKimTp2qzMxMhYWFqUePHvY+9ezZU5J03333ae/evRX6HB4eriZNmkiSIiMjtWvXLvXu3bvS8VksFlmt1grHy8rK7I8HBgZq3rx5+umnn5Sfn69WrVrp4Ycf1t///nc98MADeuihh1Sv3i+1Xk9PT73yyiv2JZ5G9Nq70FCcJEXnvW841qx+eRULmzdL3xrMXVPXvE/eBzWSV5Iia/B6DzjxXo3l7lUHr3lNPtc1mbsm31Nq6nv7sRr83qqrPz/qYu7BP7xbI3mluvt+Vhd/XzDz+u5rMvcbRz40eQbj6uJr3LWpn+HYV4+Y+x3ebDzwa6uzhbT69X8ZusViUePGjVVUVGQ/ZrPZ7EUfSXJzc7N/7uLi4nCu8vJyubm5KS0tzX7sxIkTuuuuu6rMf/WySJvNJqvVqpCQEC1fvlwuLi7q2rWrDh8+rJSUFN1///1yd3eXJPvfl/t+Ofapp57SiBEjJElFRUVycXFRQUGBPDw87O0jIiLUqVMnbdu2TStWrND27ds1bdo0h2ty+ZxXu3Lc5eXlFa7DlR588EF99913Kiws1J133mk/vn//fvn6+tqP3XHHHcrIyFBgYKAk6Xe/+52++eYbh2WdV+revbupJZ6bO4/S+dz8646Lzntf65oPNpRTkuqZWPfdL2+V0pvHGo6v+lm6tr55q5RhMHepLCYym7vm7io3nLdP3gfa1PwJw/FWE+OOzFuljSae64sW45OMB5x4T2t+86TheE9bxcK5s3rlfaDNBq+52deZ2Wte2/LeiNxmrriZ9xQz76OSue/tnyzG30kfO/GePjLxveVuM/5+Vlt/fphVW3OfqWf8ig/+4V29f88Qw/F3lht/D6/N72cuJt5XzLynmPldQTI37pp6D5ekNA/jr7M3jnyo+NYDDcdHXzD+/VVbX+O9sqcbzuva1E+lJ78xHD+p86RrN6rCq0fe17jWxv4N4NWyicbvMD5xojaw2WzVbrFUG9XG8dTZpZ2V2bp1q86cOSNJWr16tb24cy2NGjVSmzZt7IW0HTt2aMiQ6n+ZCQoKUkpKiqRL+7Vt2bJFXbp0kbe3tzw8PLRt2zYFBAQoKChIixcvdpg5VtX50tLSVFxcrLKyMiUkJGjTpk0V2o0ZM0aHDh1SbGysRo8efc293K6UmZmps2fP6uLFi9qwYYN9ll5lfHx8FBUVpYkTJ9r3mjt69KhmzpypF154waHfK1asUPfu3SVdKua1bdtW6enplRbSpF+WeP74449O9x0AAAAAAMAsCmk/8/T01PPPP6+hQ4cqIiJCRUVFGjNmjNPxs2fPVkpKiqKiojR37lzNmzdPFkvV/+eTkJCgM2fOKCoqSn/84x8VHx+vDh06SLq0jLRx48Zq2LChgoKC9OOPPyo0NLTa/GFhYerdu7cGDRqkfv36qV27dhowYECFdvHx8frb3/6mAQMGaPbs2Q43K7gWb29vxcXFKTo6Wo888oiCg4Orbf/yyy+rffv2evzxx9WvXz+NGTNGo0ePVv/+/e1tgoKC9N133ykoKMh+rHv37iopKVGLFi0qPe/lJZ6lpaVO9x0AAAAAAMCsOrm0MzAw0GG22axZs+yfDxxYccpwTk5OpW2vPI+fn5+Sk5Od7oOnp6fmzJlT6WMJCQlKSEiQJPn7+zvkj4mJse+DJskh58iRIzVy5EiHc7Vs2VJbt261f92uXTt99NFHFXJWleNybFZWltq2besw/mtxcXGptE9XCg0N1RdffOFwbMiQIQ4z+q4eg3Sp2HZlnwEAAAAAAH5tdbKQdrOsWLFCa9asqXC8WbNmWrp0aQ306MY6evSoRo0aVelj06ZNU8eOHW9yjwAAAAAAAH49FNJ+RcOHD9fw4cNruhs3xNUz4SSpVatWDjdYAAAAAAAAv45ym03ltXBz/urUxvGwRxoAAAAAAADgBAppAAAAAAAAgBMopAEAAAAAAABOYI80AAAAAACAW5zNZpOtFu4pVp3aOB4KaQAAAAAAAKiVjh8/rrFjx+rUqVO69957NWfOHDVs2NChTXx8vH744QdJUnl5uf7zn/8oJSVF7dq1U2BgoHx9fe1tU1NT5eLiUmU+CmkAAAAAAAColaZMmaInn3xSjz76qBYtWqTFixdr7NixDm3eeOMN++fz58/X7373O3Xs2FHZ2dnq1KmTli1b5nQ+9kgDAAAAAABArVNaWqo9e/aoT58+kqSYmBh9/PHHVbY/fPiw1q5dq3HjxkmSDh06pNOnTysmJkaDBg1SVlbWNXMyIw0AAAAAAOAWZ5NN5ap9e4pVx/bzeH744QdZrVaHxxo3bqzGjRtXG19QUCBPT0/Vr3+pvNW0aVPl5eVV2X7x4sV65pln5OnpKUmyWCwKDw/X888/r//7v/9TXFyc1q9fL29v7yrPQSENAAAAAAAANWbIkCE6duyYw7EXXnhBo0aNsn+dkZGhmTNnOrRp3bq1LBaLw7Grv76ssLBQO3bs0PTp0+3HYmNj7Z+3b99eDz74oP71r3+pZ8+eVfaVQhoAAAAAAABqzLvvvlvpjLQr9e3bV3379nU4VlpaqsDAQFmtVrm4uOjkyZNq1qxZpTk+/fRThYSEyN3d3X5s7dq1euihh9SqVStJl+4i6urqWm1f2SMNAAAAAAAANeaee+5Ry5YtHT6utaxTklxdXdW5c2dt3LhR0qXCWEhISKVtDxw4oM6dOzscy8nJ0d///ndJl/ZP+/e//62AgIBqczIjDXXKYTcXFblXfRvb6uQYjJOku63XblOdH+sb/1a933reVO5GllJDcf9ybWAqryR952bsmj9UWmIqb0NLmeHYnW7Gxx0p6Qv3/8/efYdFcXZ/A/8uRUCxoVixoDEaWxJFRUBBNAgqCBKxAYqVxIYNFBe7RoolEKPgE7sRCAgiYiNYsGDBgqJoYqKCBVSKCsoCu+8fvDu/XZpk8Z6xnM9zPVdg1tkzu+xOOXPf56j+t64tVXlVAMBjDdXvrRgW1Sx4ESoefv0uBaKa3w9S9TlU3yOUKqzBtmfV4G8FAOmaqn/O2haptk+QU1OxrsftWlXfGXyXwTV4jq8kNXvNujLVDwKXtFX/Ww0DcE1b9U9qz7c1PHjVgKr7hJqur/Ee6s6o+gyhmnkqxxxTw/XnFNbsmF0bqn9WCmt4X1+9Bn+zmqxbk/ULRDU9gqh+DNGW1fBkoQa2Pj6r8rpbari+nV7FF9V8qOk7rur6YiOxyjF9H+yr0fqrLq9Sed0ara/26ac3ZDIZZLJPrEbae3g9S5cuxcKFC7F582Y0b94c69evBwDs27cPWVlZmD17NgAgPT0dFhYWSutOnz4d3t7eGDZsGEQiEXx9fbn6aZX59D9phBBCCCGEEEIIIeST1LJlS+zevbvc8jFjxij9vnXr1nL/RldXF4GBgf8pHk3tJIQQQgghhBBCCCGkGiiRRgghhBBCCCGEEEJINVAijRBCCCGEEEIIIYSQaqAaaYQQQgghhBBCCCEfOKlMBukn1mzgY3w9NCKNEEIIIYQQQgghhJBqoEQaIYQQQgghhBBCCCHVQIk0QgghhBBCCCGEEEKqgWqkEUIIIYQQQgghhHzgZABk+PhqilXlY3w1NCKNEEIIIYQQQgghhJBqoEQaIYQQQgghhBBCCCHVQIk0QgghhBBCCCGEEEKqgWqkEUIIIYQQQgghhHzgpDIZpLKPsapY5T7G10Mj0gghhBBCCCGEEEIIqQZKpBFCCCGEEEIIIYQQUg2fzdTOCxcu4JdffsHu3buF3pQPRseOHXHnzp1yy6dMmYJVq1ahadOmKj1np06dAAAymQyvXr1Cv379sHTpUqirqys9LmdhYYE5c+bAxcUFTZs2RUBAAPdYUFAQAOCbb77hlj98+BCNGzdG7dq1YWBggE2bNv3n7SSEEEIIIYQQQgj5rz6bRBqpvq1bt9Zo/QMHDnA/v379GsOGDcOZM2dgbm5e7vGyjhw5AmtrawwaNEhpeb9+/dCvXz8AgIuLC2bMmIE+ffrUaDsJIYQQQgghhJCPhwyyj7CmWNU+vtfzWSXSsrOzMWXKFDx8+BCGhoYIDAzEwYMHsX37dohEInTp0gU+Pj6oU6cOTE1NMXDgQKSkpKBx48ZwdHTE7t278fTpU6xduxa9e/fGgwcPsGzZMuTm5kJbWxs+Pj7o3LlzpfHfvHkDsViMO3fuQCQSYdKkSRg2bBj69euH48ePQ1dXF6NHj4alpSWmTp2K2NhYJCcno1u3bkhMTEReXh7S09NhamqKZcuWAQBCQkJw+PBhlJSUwMzMDAsWLMCjR48wefJkNGzYENra2vDy8sKSJUtQXFwMLS0t/PTTT2jbti0AYMmSJbh27RqA0tFfbdq0gaWlJXbt2oWLFy/i5MmTePHiBZ49e4YBAwZg4cKFEIlE1X7Pc3Jy8ObNGzRo0KBa//6HH37A8uXLYWRkVO11CCGEEEIIIYQQQvggkn166cwKXbhwAe7u7oiJiUHLli3h5OQEW1tb7NmzB+Hh4WjYsCGWL1/OJZ46duyITZs2YdCgQXBxcUGTJk2wbt06REVFIT4+Hps2bcLo0aOxZMkSdO7cGX///TemT5+Oo0ePVroNfn5+kEgkEIvFyM7OxsiRI7Fp0yYEBwfDzs4OvXv3ho2NDTp27IitW7fCy8sLQ4YMwYsXLxAYGIjY2Fioq6vD2toaISEhyMzMREREBDZs2ACRSIQFCxagf//+6NmzJwYOHIg///wTBgYGWLRoEfr37w8bGxtERUVBJBLB3t4eHTt2xM8//wxra2v4+voCALy8vJQSaevWrcOBAwdQr149uLq6YuLEibCysqr0NcqnbhYXF+PFixdo3749Ro8eDVtbW6XHFc2fPx/9+vXjRprFx8cjJycHAQEB3NTOmTNncv+eRqQRQgghhBBCCPncdOxoggcPMoTejPeqTRsD3LlzTujN+E8+qxFpnTp1QqtWrQAA7du3x6tXrzBgwAA0bNgQADBq1CgsWrSI+/f9+/cHALRs2RI9e/YEALRo0QIvX75Efn4+bt68qfTvCwoKkJOTwz1fWUlJSVizZg0AQE9PDwMHDsTFixdhbm6O8+fPQ01NDba2toiLi0NRUREuX76MFStW4NChQ/j222+hq6sLAGjVqhXy8vJw/vx5pKSkYMSIEQCAt2/fokWLFujZsycaNWoEAwMDAIC5uTlWrFiBxMREWFpaYsCAAdw2yadQfvHFF7h8+XK5bR44cCAaN24MABgyZAiSkpKqTKQB/zd1c8eOHdi/fz8GDhxY4eOVmTt3LoYPH474+Pgq/50qgk088DLj+X9eb8HDPfBv7axy3EYlKq+KiY/2YFtL1WN/WfJG5XXNnkbiTDNHlda9oqmjclwAmJW+B4GtVHvdPYqEec0AcL6W6q+7pp+z2lKVV8X0jD3YZKB6bMOiYpXXHZIZirimo1Vat0BUs5453z/9HRHNxqq0rnoN4jo8/R1RKsYFgCwN1V/3tIw9CK7B37ptUZHK6w7ODMPRpqNUWvd2LU2V4wKAR/oebFRxn/KVRJjXDACXtFU/VRM/2ItVbcapvH7Pt6ofvGwyQ3FYxe81ABSh+qPfy7LL3IeYpmNUWlejhtNLarI/C9Z6rXLcAw9jMbz1MJXXn1Oo+rHLIvMPnGw6UuX1C2vQ+6ym3y/1Gvy9B2WGI76pk0rrvhLV7BKsJscQbZnqJws1/V4Pyz6t8rolRY+hrtlC5fUP6vVXed2afK8BoAanZxiWGYpYFWMnaqu+H/V9sA9ebVTbjwLAqsurVF5XU789ip7dU21lNQ1oNmqjcmxCquuzSqT+mufLAAAgAElEQVRpaPzfyxWJRKhXrx5evnzJLZPJZCgu/r+LwVq1anE/q6srXzJJpVLUqlVLKSn09OnTKqcjlh38J5PJUFJSgv79+2P79u1QV1dH37598c8//yAiIgJffvkltLS0AID7r3zb5euOHz8ebm5uAICXL19CXV0dOTk50NbW5v69tbU1vv32W5w4cQI7duzAyZMnsWrVKqX3RP6cZSm+bqlUWu59qMqECROQmJgIPz8/bipqdejo6GDNmjWYM2cOBg8ejPr161d7XUIIIYQQQgghhBBWanYr/xOQkJCA3NxcAEB4eHi1pwvWrVsXbdu25RJpZ8+exbhxVd/1NTY2RkREBIDSem1//vknevfuDT09PWhra+PEiRPo2bMnjI2N8euvvyqNHKvs+Q4cOID8/HwUFxdXOrXUw8MDN27cwOjRozF79mzcunWrWq8RABITE/Hq1SsUFhbi0KFD3Ci96lq4cCEiIiKQlpb2n9YzMjKCtbU1QkND/9N6hBBCCCGEEELIp0j2if7vY/NZjUgrS1dXF9OmTYOLiwuKiorQpUsXLF++vNrr+/v7Y9myZfjf//4HTU1NrlZZZaZPn45ly5bB1tYWJSUlcHd3R5cuXQCUTiM9deoU6tSpA2NjY6xZs4brclkZS0tLpKWlwcnJCSUlJejXrx8cHBzw6NEjpX/n7u6OxYsXY9OmTdDU1PxPo8P09PQwZcoU5OTkwM7OjuucWV0dOnSAvb09fH19sX37dgDA8OHDlf5NmzZtEBgYWG7duXPn4tSpU/8pHiGEEEIIIYQQQggrn00irU+fPkqjzdauXcv9PHJk+boOd+7cqfDfKj5P+/btsXv37mpvg66uLgICAip8bPr06Zg+fTqA0oL8ivFHjBjB1UEDoBTzxx9/xI8//qj0XAYGBkhISOB+79SpEyIjI8vFrCyGfN2LFy+iXbt2Sq//XRSfU04+jbSyx+XKvpc6Ojo4duzYO/8dIYQQQgghhBBCCB8+m0QaX3bs2IGoqKhyy5s0aYKtW7cKsEXv18OHD5U6aCpatWoVunXrxvMWEUIIIYQQQgghhPCDEmnv2YQJEzBhwgShN+O9KDsSDgBat279zq6bhBBCCCGEEEIIeb9kMlmFTQI/Zh/j6/nsmw0QQgghhBBCCCGEEFIdlEgjhBBCCCGEEEIIIaQaKJFGCCGEEEIIIYQQQkg1UI00QgghhBBCCCGEkA8c1Uj7MNCINEIIIYQQQgghhBBCqoESaYQQQgghhBBCCCGEVAMl0gghhBBCCCGEEEIIqQaqkUYIIYQQQgghhBDygZP9//9/Sj7G10Mj0gghhBBCCCGEEEIIqQZKpBFCCCGEEEIIIYQQUg00tZN8VnSb6am8bj2DxqrHLVF51dL1axBbS/q2RrG1WumrtF5dDe0axQWAuiq+bq1iYV4zANTTrNnrrsnnTEdao9Aqv98AoFNcsw+5jqrvuUhUo7gAULuVaq9bXaC4AKCrXrP7YDXZp2gXF9cotraKf+t6mjU/ZVH1+6VdJMxrBoD6WjX7pNWvyfe6sGY7FZW/16j5CaqOit+v93FirOrrblKrdo3iNjFoovK62pKaHbtq8hkX1fC+fk1iq9dwQpGqsUtENT2CqH4M0ZLV7DXX5Hvdpq5BjWK3aaP6+joNVN9uoGavu4anZyrHbqhVs7gNa3D8gFoN96aqrq9W8+/Wh86gZXOhN+G9+xhfk0gmq+HelBBCCCGEEEIIIYSQzwBN7SSEEEIIIYQQQgghpBookUYIIYQQQgghhBBCSDVQIo0QQgghhBBCCCGEkGqgRBohhBBCCCGEEEIIIdVAiTRCCCGEEEIIIYQQQqqBEmmEEEIIIYQQQgghhFQDJdIIIYQQQgghhBBCCKkGSqQRQgghhBBCCCGEEFINlEgjhBBCCCGEEEIIIaQaKJFGCCGEEEIIIYQQQkg1aAi9AYQQQoRTXFwMDQ0NFBcXQyKRoHbt2kJvEiGEEFLOokWLqnz8p59+YhY7Ojq6ysft7e2ZxSYkIyMDBgYGQm8GIUQBJdIIqcLZs2dhamqqtOzYsWOwsrJiFjM7OxuHDh1CXl6e0vIZM2Ywi1lWcnIy7t69C0dHR1y/fh29evViHjM4OBjTpk1TWrZ+/XrMnTuXeWyJRILffvsN//77L5YsWYIdO3Zg6tSpqFWrFvPYQoqLi8PmzZtx8OBBPH78GM7OzliyZAkGDRok9KYxk5KSguTkZIwbNw7u7u64desW/Pz80L9/f+axc3NzcevWLZiYmCA4OBipqamYP38+WrduzTRuSUkJTp48iYEDByI7OxsJCQlwdHSESCRiGlcuMzMTu3btwoIFC5Ceno6goCB4enqicePGzGJeunSpysdZ7tOEvNgHgKSkJGzcuBGhoaH4559/MGXKFPj7+6NHjx7MYv7yyy9VPs76+CV0kiMqKgoODg5Ky/bu3Ytx48Yxiff48eMqH2/RogWTuICw73Xv3r0BACdOnEB+fj7s7OygoaGBuLg41K1bl1lcALhw4QIA4OHDh3jw4AHMzc2hrq6OM2fO4IsvvmD6uoV6z1l+ht/F29sba9asAVD++zVmzBjs27ePWezMzEw0bdq0wsfOnz+Pvn37Mon777//IigoCPXr18f8+fNRp04dvH79Gr/++iv27t2L69evM4kLCLtPmTlzJoKCgpg9PyGsUCKNkArExcVBIpEgMDAQs2bN4pYXFRUhJCSEaSJtypQp+PLLL9GyZUtmMaqyc+dOxMfHIysrC9bW1liyZAm+//57TJo0iUm8gIAAvHjxAgkJCbh//z63vLi4GCkpKbwk0lasWAE9PT3cunUL6urqePjwIby9vREQEMA8dnh4ODZs2IDc3FwAgEwmg0gkwu3bt5nH3rx5M7Zv3w4AaN26NaKiojBx4kRmiTQHBwdERUWhU6dOSkkcPl/zqlWrMGvWLBw9ehTa2tqIiorCjBkzeEmkzZs3DyYmJgCAI0eOYPz48Vi8eDF2797NNK5YLIZUKsXAgQMBlF4QpqSkYMWKFUzjys2fPx9Dhw4FADRt2hRGRkbw9PTEtm3bmMUMDAys9DGRSIRdu3Yxiy2/2BeKr68vfH19AQDt2rVDSEgIPD09ERkZyTx2SkoKnj59Cmtra2hoaOD48eO8HMtOnjyJy5cvw9LSEhoaGjh16hT09fVhaGgIgF2iYceOHXj9+jVCQ0Px6NEjbnlxcTFiY2OZJSGcnZ0hEokgk8nKPSYSifDnn38yiQsIm1CSJ1N+//13hIWFQU2ttEKNjY0NnJycmMUF/i8B7uLigpiYGOjp6QEA8vLyMH36dKaxhXrPDx06hPj4eKxdu7bSxBIriucDu3btUkqkvXnzhmlsd3d3REVFASif5PHz8+Mee98WLVqE7t2749mzZ/j1119hZmYGT09PtGzZkjtXY6WifYpIJMKzZ89QVFTE9PwsIyOD2XMTwhIl0gipQH5+Pq5cuYL8/HzuBAYA1NXVMWfOHObxWY9YqEpUVBTCw8Ph5OSEhg0bIiIiAiNHjmSWSLOyssK9e/eQlJSkdAGqrq7O/ORULjU1FVFRUTh9+jR0dHTg6+sLW1tbXmJv2bIFu3btQocOHXiJp6ioqEhpVFCjRo0qvDB7X+Qnn2lpaeUek0gkzOIqkkqlMDMzw7x582BlZYXmzZujpKSEl9h5eXmYNGkSVq5cCQcHB9jb2zNN6MjdvHkTBw8eBADo6enB39+ft883UPq6R48eDQCoVasWnJycmI4mAMA8OVkVMzMz6Ovrv/MOPyuFhYX48ssvud/bt2+P4uJipjHlI85Gjx6NsLAw6OjoAADGjx8PV1dXprGB0pHcBw4cQKNGjQAAr169gru7O/Njadu2bXHz5s1yy7W0tLB27VpmcRMSEpg997sImVCSe/XqFXJzc7nYz58/R0FBAS+xs7Ky0KBBA+53HR0dPHv2jGlMod7z33//HXv37sWoUaPg4eHB6/RVxXORsuclrEdTK8ZLT0+v9LH3LScnB97e3pBIJBg2bBgOHz6MhQsXcjeiWCq7T8nPz4evry/OnDmDlStXMo2dn5+Py5cvV/re8jErhhBVUCKNkAqMHDkSI0eOZDqEuzKDBg3CH3/8AWNjY6irq3PLWQ6rVqSmpqY0pVFLS0tpO9637t27o3v37hg0aBDzqRmVEYlEkEgk3MlZTk4Ob9PeGjVqJEgSDQB69uyJuXPnwtbWFiKRCHFxcfjmm2+Yxx01ahTCwsK436VSKRwdHblkD0s6OjrYtm0bLly4gCVLlmDXrl2oU6cO87hA6eu8efMm4uPjsWfPHty+fZuXJJ5UKkVWVhaaNGkCAHjx4gU3koMP2traOHXqFMzNzQEA586d4xItrF27dg3BwcEoKCiATCaDVCrF48ePmSYixGIxgoODK73Dz3K0EFA6Cs3f3x/Dhw+HSCRCbGws2rZtyzSmXNl9Z1FRETfalqXMzEw0bNiQ+11LS6tceQQWLCwsYGFhARsbG7Rv3555vLLu37+PPXv2KH2+MzIysHfvXuaxhUgoybm7u8POzg49evSATCbDtWvX4OPjw0tsCwsLuLm5wcrKCjKZDIcPH4aNjQ0vsYV4z8eNG4fvvvsOPj4+2L9/v9IIU5aJasX9CF/nY9WJzXJb5MfFWrVqobCwEDt27OBG1fLp/PnzEIvFMDU1RUxMDHR1dZnGe/bsGQIDAysdYcvHDUdCVEGJNEKqUL9+fcyaNQt5eXlKO3iWO/WCggKsWbNG6aKAj4svud69e8PX1xdv3rxBfHw8wsLC0KdPH+Zx5dMHXr58CYDf6X6urq5wc3PDs2fPsHr1asTHxzO/sy6vedKiRQv88MMPGDhwIDQ0/m+XzMed36VLl2LXrl0ICwuDhoYGjIyMMHbsWGbxXF1dcenSJchkMnz11VfccnV1dVhaWjKLq8jf3x8REREIDAxE/fr1kZmZiXXr1vESe8GCBfDz84ObmxtatWoFJycnLFy4kHlcd3d3ODg4oGfPngCA69evY/Hixczjyi1fvhwLFiyAp6cnRCIRmjVrBj8/P15ie3t7Y9KkSYiKioKLiwuOHTuGzp07M40ZHBwMANi/f7/SRS/AzxSW1atX4+eff8a8efOgqakJIyMjrFq1inlcoPQmlKOjIzdVOiEhAePHj2ce18LCAuPHj8fgwYMBlE5Js7OzYx5X7q+//sKCBQvKJe9YH7fnzp0LCwsLJCcnw8HBAcePH+ftxoyQCSV7e3uYmJjg6tWrEIlEWLZsGTcakbVFixbh6NGjuHjxIkQiESZOnMhNm2dNiPdcJpPh2LFjuHXrFkaNGsVb2ZGioiI8efIEUqmU+1l+Hl5UVMTLNvBNMUnXsGFD3pNoBQUFWLt2LTcKrWyNaFbatGlDyTLyURLJWI5RJeQjZ2tri1GjRqFDhw5KBziWNXCGDRuGiIgIaGtrM4tRFalUivDwcJw7dw5SqRTGxsYYM2YM01FpQOlIvF9//VVpShKf/v77b1y4cAElJSXo3bs3OnXqxDSe0AXJ5V6/fo1Xr14pJYpZjX6UT+1ctWoVfHx8yo3U4SN5KK/T9rnJzMzEtWvXoKGhgW7dunGj0/iUk5MDTU1N5ne3Fdnb2yM6OhqBgYHo1asXevfuDVtbW8TFxTGLKb/gmzp1KrZu3cp9zktKSjBlyhQcOXKEWWx5HCGbS9y8eZNLMvTt25f5vlTu8OHDuHTpErS0tNCvXz+uHiEfBgwYAD8/v3L7TtZJB1tbWxw8eBDr169H//790bVrVzg6OuLQoUNM48opJpT69u3LW0JJIpFg27Zt+Oeff+Dj44OdO3fy2iBIiIZMcny+52lpaRCLxdDR0cHq1auZN8ZRZGlpKVgdQDMzM64cQWhoKPez/PczZ84wiTtgwADMnj0bMpkMgYGBmD17ttLjLM+RFEeheXl58TZSH/i/4zQhHxsakUZIFbS1teHs7MxrzJYtWyIvL0+wRNrWrVsxbdo0pRMHPrpnNmnShPckWtkDt/zEIS0tDWlpaUxPWuSJsso6w/Jhy5YtCAkJQYMGDbgTVpYnqBcvXgQAdOrUCevWrYO5uTnU1NR4KVIt17hxY1y+fBndu3fnvSvrH3/8gfXr15eb6sZ61GXZLo5jxoxh3sVR0aNHjyAWi/Ho0SPs3bsXP/74I9asWQMDAwPmsbW0tJCbmwtDQ0Ncv34dffv2ZT6dNjAwEBcuXEBWVpZSsXkNDQ1YWFgwjQ0I31zi33//RV5eHqZNm4Zjx47xlkj74osvoK+vz114X7p0ibcER+vWrdGzZ09ep0wDpVPBJBIJ2rZti9TUVBgZGfEav127dkq1Nfl6zxUbBGloaPDaIEixIZONjQ3zhkxlNW7cGF988QWXxGPJ1dUVs2fPFqRzp5B1ABXPfxV/ruj396lPnz5cXWZjY2OlGs0A20Sam5sbNDQ0cObMGZw9e5Zbzvq8EChtSETIx4hGpBFShZ9//hl6enowMzODlpYWt5xlvbKJEyciJSUFHTp0gKamJrec9bBnxe6ZitPsSkpKcP36dRw9epRp/NWrVyMzMxOmpqZK7zXLEwf5qDB5JywLCwulxE5ISAiz2JV1hi0uLkZwcDCOHz/OLLbcoEGDEB4ezhUu5ourqys2btxYrmDynj17mMc2NjbmElmKyUM+phBbWloiODiY95p4Dg4O8PX15RLV9+7d462LIwBMmjQJbm5uCAgIQFRUFP744w8cOHCAlzpOhw8fRnh4OIKCgjBy5EioqalxiVzWQkJCMHXqVOZxypKPUnrXMhYCAgLw9OlTpKam4o8//sAPP/yALl26MJ/CvHz5cpw4cQKtWrXilvFZW+fUqVPYunUrevXqpTR6W96EgZU9e/YgISEBAQEBGDVqFNq0aQOpVMq0I66ckO+5fGSxfCSLTCaDra0tYmNjmce2t7fnGjJFR0cjPz8fI0eOZDrKVU4xiRcWFoYxY8YwTeI9evRIsA7yQGkTkYKCAhgYGGD79u3Iz8+HhoYGJk6cyPuNsE+dYtfhirD8HPzyyy9VPs56P0qIqmhEGiFVOHDgAAAotZ1mfWdm8uTJSrWy+FJV98wff/yRefzXr1+jTp06uHbtmtJyPkaFCdF9TOjOsADQvHlz1K9fn5dYijIzMwUrUn369GnBTsCFaiwhRBdHRTk5OTAzM0NAQABEIhGcnJx4SaIBgI2NDaytrSESiRAZGYn79+/zNkJKIpFUeIHA+qJAyOYSZ86cQVRUFBwcHKCrq4vt27fDzs6OeSLt7NmzOHLkiGAjuTdv3gxDQ0PmJRDKcnZ2hr29PXR1dbF7927cuHGDt7pGQr7nQjYI4rshkyLFruoNGjRg3lX9XQkOliUorly5gtmzZ2PRokUwMDDAvn37YGdnh0uXLkFTU5PpCEAXF5dKP08ikQg7d+5kEtfb2xtr1qwBAG4/KjdmzBim3a4XLlwIMzMzmJqaomvXrsziVFdubi7CwsLQokULSqSRDxYl0gipghBDy/39/QWp4VRZ90yZTMZLgWz5CVleXh7vyR0hOmEJ2RlWrm3bthg7diz69OmjdGHA+qRFyCLVVlZWGDBgAEaMGIFu3brxElPoxhJCdnEESqfIP336lLswuXz5Mm/JzFevXmHTpk24ePEiNDQ00LdvXxgaGvLWNVSuqKgIiYmJ+Prrr5nHqqi5hLe3N/O4ALiEnfxvLZFIeEnitWrVqsJaSnwpKirira5l2bhRUVHc59vExIS3z7aQ77kQDYLkKmrIZGxszEtsvpN4FdUDfvDgAX777Tfm+7J169YhKCiI6yReu3ZtzJgxA8+fP8ekSZOYJtJmzpxZbllycjJ+/fVXWFtbM4t769Yt7uddu3YpJdLevHnDLC4ATJs2DZcvX4avry8ePnyIHj16wMTEBGZmZmjevDnT2GXPOf/8808sX74cY8aMYV5WhpCaoEQaIVX4559/EB4eXq4TF8sTZiFrOAGlUw7lJ4lyLVu2RHx8PNO4aWlp8PDwwNu3bxEWFgZnZ2ds3LgRXbp0YRoXEDax8+uvv2Lz5s3c7yKRCNra2mjXrh3c3d2ZJhWbNm2Kpk2bMnv+ygjZ9ezw4cM4evQo1q1bhxcvXsDe3h52dnbQ19dnFlM+4rB27dqoXbs2kpOTlR5nnUhbvXo1Nm7ciHnz5nHdWfnq4giU/r2nTZuGhw8fYvjw4cjLy8PPP//MS+zFixfDwMAAP/30E2QyGSIjI+Hj48NLLaWyFwfTp0/HxIkTmce1tbVF7969ueYSYrGYt+YS1tbW8PDwQF5eHnbs2IGYmBgMGzaMedz69etj6NCh+Pbbb5WOm3wlt0xNTbFnzx7069dPqSQDyzIQQGmtsNevX8PBwQFSqRQHDhzAnTt3IBaLmcYFhH3P7e3t0bVrV65B0ObNm3kbaerp6Ynw8HB07NgR0dHRMDc3Z1o3SxHfSTzFRA4Arsv3/Pnz4erqyiwuUDqtU55EA4COHTsCKD1HZt21UzGBKJFIsG7dOhw+fBgbNmzAoEGDmMaWK5ukZj3i0szMDGZmZgBKX/ONGzeQnJyMadOmQSKRMG+SAwAvX77EypUrkZKSgvXr1/Ne85GQ/4pqpBFShSFDhmDIkCHlagOUPbl4n4Ss4QSU1nHauXMnNm7ciDlz5uDUqVO4cuUK85pC48aNw4oVKzBv3jxER0fj7Nmz2LBhAyIiIpjGlROq+9jy5cuhoaEBR0dHAEBsbCyePn2Krl274vLly++cWkFUd/z4caxatQovX75E37594eXlhTZt2jCLV1ljCSsrK2YxKyIfZapY24ilkydPwtTUFPfv30dJSQnatWvH202C4cOHc1P05fiqF1ZWTk4OHB0dmY90zs7ORkxMDPLz8yGTySCVSpGRkQE/Pz+mceUSExOVuj4PGDCAeczKRnGzPFYrUqwrKse6DARQ/rMslUoxfPhwXj7fQrznJ06cwIABAyrt8MfypsSzZ8+gr6+Px48fV/g466QpUHFX9dGjRzMvB5Kens7VlF29ejXT46Tcd999V2mt2GHDhvFSD+/KlStYtGgRunXrBrFYrDRzgQXFruJlO4zz1XH8n3/+wZkzZ3DhwgXcu3cP7dq1g6mpKfOGEwkJCVi+fDmsra0xZ84cwabpE/Jf0Ig0QqpQr1493ufmJyUl8RqvrEaNGqFVq1bo2LEj7t69i3HjxjGtyyD35s0btG/fnvvd1NQUvr6+zOPKtWrVius+VlJSgoiICHz//ffM416/fh379+/nfu/UqRMcHR0REBDArB24/ISsU6dOSnc5+U7aCuHBgweIiYlBbGwsWrRogfnz58PKygpJSUmYMmUKk46p72oswTqRFhoaCj8/P95Hmcr5+/vDwsJCkPpwhoaGuHLlCtehNC0tjbdprZaWltz3SyaTIS8vD5MnT2Ye18PDA82bN8e1a9cwaNAgnDx5krdpzEBpB2bFxBLLTo7y5EafPn2YPH91CdVhsGnTpkhPT+eS4llZWUxH1wLCvuc3b97EgAEDynUzlGOZSBOLxQgODoazs3OFx02WSVP5e/706VP0798f/fv35x7LyspimsTbtWsXtmzZAnd39yprh71vXbp0wf79+zFixAil5dHR0ejcuTPT2BKJBOvXr0dcXByWLl3K243VoqIiPHnyBFKplPtZPt6F9Si8JUuW4Pz582jUqBFMTU0xceJEfPPNN7zU/1uwYAGOHj2KH374AUZGRrhx44bS43x1Xybkv6JEGiFVcHBwwIYNG2BsbKx0x4/lTl0ikWDbtm34999/4ePjgx07dmDq1Km8jeDQ0dFBUlISOnbsiPj4eHTr1g1v375lHrdBgwZIS0vjTtJiYmJ4q5UmFotx8eJF5OXloV27dkhLS0OPHj14SaQVFRXhr7/+4pIMf/31F6RSKd6+fcvsxEl+VzMtLa3SfyO/8/+pcXNzw4gRI7Bt2zalkabm5uZKLd/fJ6EbS4SEhODAgQPlRpnypVWrVli0aBG+/vprpbvMLC965UmswsJCHD16FO3atYO6ujru3bvHy2gKANi9ezf3s0gkQr169aCrqwsASE1NZTZtPSsrC7t27YKvry+srKwwefJkjB8/nkmssubMmYNbt24pTSVl2cmxbHJDcZIFHyPC5O7fv489e/agoKBAaRQgq6Ya8oRGTk4O7OzsuG6hycnJzBPWFb3niv9l+Z5fvHgRQOk+hY8mSIq++OILAMDSpUthbm7Oa2yhknjOzs5ISUnBxIkTUa9evXKje1nuw+fPn4+xY8ciMTERRkZGEIlESE5OxtWrV5nf3LW1tcWTJ0/g5OSE27dvl7u5yOoGe0FBgdLIL8WfWScw4+Pj0bFjR1hZWcHMzIy3EesA8PTpU3z99dc4d+4czp07p/QYn92XCfmvKJFGSBWuXr2KK1euKF10st6pr1ixAnp6ekhNTYW6ujoePHgAb29vXmr6AICPjw8iIiLg5eWFiIgI2NjY8DIqb9myZfDy8sJff/0FIyMjtGnTBv7+/szjAsC5c+dw9OhRrFy5Eq6urnjz5g3Wrl3LS2yxWIwpU6agUaNGkEqlePnyJfz8/BAUFIThw4fzsg0VCQwM/OQSaTKZDDExMVwyQ05+t59VQXahG0sINcpUrmHDhgBKR18qYnkRppjEqgzLZBaAciUBFInFYmbTdOQ3IAwNDZGWlsZLgwO5tLQ0xMXF8dbFMDg4GIBwI8Lk5s6dCwsLCyQnJ8PBwQHHjx9nmtCqqBg6UHqjQE6+X3vfqvOeh4WFYdSoUe89dkZGBjZs2IDIyEhIpdJyj7M8Vzl48CBMTU2xZs0a1KlTp1wNK5Y3WIVK4rVq1QqtWrVCZmYmMjMzyz3Och9uYGCAmJgYhIWF4cyZMwDATbGUH1NYsbW1Zfr8lRFyP3bu3DmkpqbizJkz8Pb2xosXL9CnTx+YmprC2Ni43HnT+zxIglMAACAASURBVFSd4zUhHyJKpBFShdTUVCZTvd4VMyoqCqdPn4aOjg78/Px4PajHxsZytTCCgoJ4i9u6dWvs27cPBQUFkEqlTA/aZTVp0gSamppo37497ty5g6FDh+LVq1e8xO7Tpw/i4+Nx9+5dqKmpoX379tDU1ESPHj14m0JRkU+tfGZSUhLmz58PiUSCr776Cn5+flyjhalTp/JSe0RTUxMzZ84s17yE9d1WoUaZylVVfNzHxwcrV6587zGrSmLJsUxmvQvL75exsTFmzZoFLy8vTJw4EampqbzVm/n666/x4MEDtGvXjpd48mNVZfhqNlBUVIRZs2ahuLgYnTt3hpOTE1f3koWKuimWxdd+rSKhoaFMEmmbNm3CiRMn3vvzVseMGTMQHByMrKyscs1SWN9gFSqJV9X3R57cYqlBgwaYNm2a0rKCggJmiVo5xYTsy5cvIRKJlLrZs/Kuch6sGxN16dIFXbp0wbRp05Cfn4/Dhw9j48aNuH//Pm7evMks7saNG+Hh4QGgfC3Z2bNn89aciJD/ihJphFShQ4cOSEtL460bFFB6QiaRSLgkSk5ODq8JlRMnTsDDw4P3JM7ly5exc+dO3pMMQGmdmeDgYPTt25cbBSeRSJjHBYBHjx5hz549yMvLUzo55usCsDJCJvFY8PPzw+7du9GmTRv873//g7OzM/bu3YsmTZrwljRcuHAhZsyYwUtRakVCjTKtDpYn5+8iZLKY5fdrzpw5ePjwIVq2bIn169fj0qVLmD59OgD2o/CMjY0xbNgwNGnSBOrq6synnlUnocQHHR0dSCQStG3bFqmpqR9EtzkhP9+sYnfu3BmdO3dG165deZ9e6eTkBCcnJ2zatIn7PvFFyCSeouzsbERGRiI8PByFhYU4ffo0L3GB0tGuoaGhiImJgaGhIdNEGlBaXiQoKAgZGRkASkfnzZw5k+mNbcXSDwkJCeWamLBOpN27d4+bhXP16lXUrl0b/fr1g6enJ9O4p06d4hJpAQEBSom0Bw8eMI1NSE1QIo2QKvzzzz9wcHCAvr4+NDU1eakB4urqCjc3Nzx79gyrV6/G8ePHeb3obdCgAaytrdGlSxdoaWlxy1kndoRKMgClXahOnTqF7t27w8rKCrGxsVi2bBkvsT08PGBkZMTVACFsSKVSGBoaAgBXc3DSpEnYt28fb+9706ZNmZ8IV6RDhw6VjjJlNSLsY/Apf99at24N4P9GGMixHoUXHByMnTt38rYfV+wQmZGRgb///htmZmZ48uQJrzV+7Ozs4O7ujoCAAIwaNQqJiYnciFehCPn5ZhVbvr/63//+h99++63c4ywTSvJRUDKZrMJu2izP04RM4gGlCZ7Q0FDEx8dDJBJh+fLlGDZsGPO4hYWFOHToEEJDQ3Hnzh2oqakhODiYeQL98OHD2Lx5M8RiMXr16oXi4mJcuXIFa9euhaamJqytrZnEVTzPtre35/WGap8+faCnpwdjY2NYWFjAy8uLeZdSOcXEe9kk/Kd8nCYfP0qkEVKFTZs28R7zzz//xIoVK5CUlASpVIotW7bgp59+4qXwPcC2dX1VhEoyAKVDx+Un5S4uLnBxceEtdnFxMby8vHiL97lq3Lgxfv/9d9ja2qJu3bqYMGECsrKy4ObmVm4UJCsuLi6YP39+ueYlQn3uAWFHhBH+sR6l1LBhQ0FuCsTFxWHz5s14+/YtQkNDMXr0aHh6evJWZ9LZ2Rn29vbQ1dXF7t27cePGDW5UxafauEUI8lFIP/zwg9I+lA9CjvATKom3Y8cOhIWFQVNTEzY2Npg9ezYmTpzIy3niqlWrcOTIEXTr1g3Ozs6wtLSEnZ0dL6NQt2/fjpCQEKVkvIWFBdq1a4e5c+cyS6Qp4nsfGhMTwyX/5dNZhUCJM/IxoUQaIVXQ19fHqVOnkJ+fDwAoKSlBRkYGZs+e/d5jzZgxA7dv30ZWVhZu3brFnbT99ttvaN68+XuPV5mqTpAcHByYjWYQMsnw5s0bPHnyhNf3Wa5nz55ISEiAmZkZb51Zq+NTq5H2008/wc/PD/r6+vjuu+8AAJ6entixYwd+/fVXXrYhMjIShYWFSE5OVlouZCKNCEOo7xfri5S2bdvCyckJJiYm0NTU5JazHlW9detW7Nu3D87OzmjUqBGioqLg5ubGa8MWeV3PZs2aoVmzZtzyT7Fxi1C6du0KAPD39+e9/tvo0aMBlJZj4Lv0glD7i/Xr12PgwIEYO3YslyDnK9Fx5MgRbpbAgAEDoKury1vswsLCCke0tm7dGoWFhbxsA9+aNm2KmJgY/PLLL0hPTwfAz3RWgJJn5ONFiTRCqjB37lzk5eXh4cOHMDIywoULF9CjRw8msdauXYvc3FysXr0aYrGYW66hoYFGjRoxiflfsTyZEzLJkJOTA0tLSzRq1EhpOivLKbxyR44cwZ49ewCUnkzIpw+Xbbf+PlWnoG1YWBiz+ELQ19evsAvshAkTMGHCBADspzk+f/5csOLfH6pPsY7TpUuXqny8V69evDZy4VOLFi0EmZ6vpqam1KCmSZMmUFNT4307KiLUZ1yIuBKJBLVq1WJemL1x48a4fPkyunfvzvsNqLt37yI/Px916tThLaZQSbzTp0/j4MGDWLNmDZ4/fw4bGxve6seeOnUKp06dwv79+7FixQr07dsXb9684T5jLL19+xZv3ryBjo6O0vKCggKUlJQwi6t47CgoKMDly5eVvscsO8PKp7MuXryY1+msAHD79m189dVXAEr3W4o/U5KNfMgokUZIFe7cuYNjx45h9erVcHR0hIeHB1cQ833T1dWFrq4uNm/ezOT53weWBzQhkwxTp05V+v3Ro0fQ1tbG3bt38eWXXzKNzUfnq7IUC9pWxN7eXimh+LlgPc2xe/fuOHHiBPr37w91dXWmsT4WJiYmTJ5XyGRWYGBgpY/JC4PzWb+LT1WNPJs2bRqCg4OZxO3QoQP27NmD4uJi3L59G7///juvTYKqwvK4OWnSpAprhQFQuiHHwqhRo5RuuEilUjg6OuLgwYPMi9/fuHEDzs7OAPi7ASWnpqaGAQMGwNDQUOk4yUfBf76TeA0aNODKXaSlpSEyMhLFxcUYOnQoxo4di3HjxjGLra6uDktLS1haWiI7OxsxMTHIyMhAv3794OjoyLQAvrW1NcRiMdasWcP9jV+9eoXFixczHZ2leOxo0qSJUmMJ1k0lhJzOmpaWxuy5CWGJEmmEVKFRo0YQiUQwNDTEnTt3YG9vj6KiIqE365MkZJIhISEBt2/fxqBBgyCTyXDy5Ek0adIEhw8fhq2tLTdiiQWJRIJt27bh33//hY+PD3bs2MEVw2elqjvab9++ZRb3c/fnn3+WG+nH18VfZfgYtfL06VOsWrUKFy9ehKamJvr27Qtvb2/o6ekxuxgSMpm1e/dupd9fv34NqVSKevXqMYn3Xwg5AjAzM5PZcy9ZsgSbN2+GlpYWvL29YWxs/FnUnqyqLAGrzqGurq64ePEiACglKzU0NMp1GWQlKSmJlzgVWbBggWCxhUziderUCYsXL4anpycSEhIQFRWFcePGMe8EDAB6enrc6PHU1FTs37+fabyZM2dCLBbDxMQE7du3R3FxMe7fv881FWElKCiItwL/ZQk5nVUmk+HMmTNo0KABunXrxi2/e/cufH19K71ZQIjQKJFGSBU6dOiAlStXYsyYMZg/fz6ysrI+udpRHwohkwzPnj3D/v37uQvdmTNnwt3dHWFhYRgxYgTTRNqKFSugp6eH1NRUqKur48GDB/D29kZAQACzmHIJCQnYuHEjCgoKIJPJIJVK8fbtW5w/f5557M+REKMP34XViDBF3t7eGDhwINauXQsAiIiIwKJFi5iNTgI+jGRWeno65syZg/T0dMhkMrRo0QIbN25E27ZtmcT70KeUshyZVbt2bcybNw/z5s1jFuNDlJ2drVSWgI/O4vKkzapVq5iPequMEDeg5Hr37o3k5GTcvXsXjo6OuH79OtMpd4qETOLJaWpqYvDgwRg8eDAAtp2As7OzUVBQAAMDA2zfvh35+flQV1dnniTX0NDA2rVrMWPGDNy8eRMikQjdu3dnXkfXzc1NsJkZQk1nBYClS5ciMTERb9++hY+PDywtLeHr64uIiAjBGqARUh2USCOkCsuWLcPVq1fxxRdfYObMmTh//jzWrVsHoDT5oq+vL/AW8otlElHIJENOTo7SVAktLS3k5eVBQ0ODeX2G1NRUREVF4fTp09DR0YGfnx/zwq5yP/30E1auXInt27fD3d0d8fHxePPmDS+xPyfFxcWIiIjAd999h7p16yIkJAQ3btxA165dMXXqVObTaG/duoUtW7YgLy9P6Tu8a9cuptNj5LKzs5WmAE2YMIG3iwW+k1mKlixZgsmTJ3NTYuLi4uDj41Muyfe+fI5TSjt16qS0j9bQ0IC6ujoKCwuhq6v7zuQiH1geN4UYqSGvsdm1a9cK623yUddUyBtQO3fuRHx8PLKysmBtbY0lS5bg+++/x6RJk5jHFjKJVxlWn+8rV67Aw8MDCxcuhIGBAfbt2wc7OztcvnwZWlpamDhxIpO4gPJNCXmN4oyMDGRkZABgV6tMyBv1Qk1nBUrP/2NjY5GdnY1FixYhJCSEaxrzxRdfMI1NSE1QIo2QKqirq3PTIwYOHIiBAwdyj02dOvWTLBweFRVV7g7Q3r17MW7cuHK1xN6Hilq5K2Ld7Q0ArKysMH78eNjY2EAqleLYsWMYOHAgoqOjmSdLRSIRJBIJdzGYk5PDW3HVunXrwtjYGFeuXMGrV6+wYMECDBkyhJfYHyJWJ7Hyu+eDBw+Gr68vCgoKMHbsWJw8eRLe3t5ccp4VLy8vjBo1Ch06dBCkcG/37t1x6NAhDB06FABw4sQJrvsea3wnsxTl5OQo1ZUZMmQI0xqYfLymD428ts7SpUvRo0cP2NnZQSQS4ejRo0hMTGQeX+jGLZUlClu2bMksZnVqbLIm5A2oqKgohIeHw8nJCQ0bNkRERARGjhzJSyJNyCReZVgdU9atW4fAwEB88803AEpHnc6YMQPPnz/HpEmTmCbSKropIRKJcO3aNQBASkoKk7jPnz+v8pyY5fmwUNNZgdJz0Tp16qBOnTq4d+8e3N3dMX78eKYxCXkfKJFGiIo+tSmeO3bswOvXrxEaGopHjx5xy4uLixEbG4tx48Z9skmWefPm4cSJEzh79izU1dUxefJkmJub49q1a8yTHK6urnBzc8OzZ8+wevVqxMfHY/r06Uxjymlra+Pff/9F+/btcfHiRRgbG3/WNQBNTU2ZPO/du3dx8OBBAEBycjKioqIgEolgbm7Oy3dKW1ubK8zNJ/loIZlMhvDwcIjFYohEIhQUFKB+/fpYvXo1823gO5mlqFatWkr1g27evFlu2gwL165dQ3BwsNKU7cePHyMhIYF57KqwPGampKRg+fLl3O+DBw/m5e8sdOMWxfhFRUVITk6GkZER02QWn10jKyPkDSg1NTWlKaRaWlq81XUVMonHt+zsbC6JBgAdO3YEUNqxlfV5StmbEk+fPsXixYvRoUMHrkTBp6Y601lZ1cNT/O42atSIkmjko0GJNEJU9Km1ZG7btm2FXQu1tLSYnjjwMeKsOgYMGIABAwYoLVM8iWPF3t4eXbt2xYULF1BSUoLNmzfz1m3Ow8MDGzduhL+/P0JCQhAWFgZHR0deYgvFxcWF9+9u7dq18ddff6FDhw5o164dnjx5ghYtWiAzM5OXmj5mZmbYvXs3zMzMlC7qW7RowTTuh9CJS6hkFlBaG27mzJlo0KABZDIZ8vLysGHDBl7iTpo0CVFRUXBxccGxY8fQuXNn5nEBICsrC02aNFFalpKSgu7duzNN7ujo6CAyMpIbVXzgwAHUr1+fWTw5oZNKZePn5uZizpw5vMS2tLSscF/Ksj6bnJA3oHr37g1fX1+8efMG8fHxCAsLg7GxMS+xhUzi8a24uFjpd19fX+5nNTU13rYjIiIC69evx/jx4zF58mSm77e+vr7g58QGBgYwMDCo8DFW9fAU9yOamprv/fkJYYUSaYQQAKVtri0sLGBjY4P27dvzFrdsjRs5PtvZC6HslCB5jba0tDSkpaXxMj2md+/e6N27NwAgMjISeXl5vFx8Cqlr167Iy8vDyJEjoaGhgdjYWDx9+lSphtf7tnDhQri5uaFHjx7Q0dGBk5MTvv76a6SmpiqNomHlwIEDAErb28uxLkiuqLKpKnxcMAiVzAJKu50dPXoU9+/fh1QqhaGhIS+J01q1asHR0RGPHj1CvXr1eJ32NnLkSCxcuBA2NjaQSCT4+eefcfjwYSQkJDBt2uLv74+VK1di1apVUFNTg4mJCfz8/JjFk6ssmSTH13dMrnbt2kojyllSHLVTXFyM48ePQyKR8BJbyBtQnp6eCA8PR8eOHREdHQ1zc3OMHj2al9hCJvEqw2qkaZcuXbB//36MGDFCaXl0dDQvNwYyMzPh7e2N3Nxc7Ny5Ex06dGAe80Of6cJq+27fvo2vvvqKi6H486d8HUA+fiLZh/6tJeQD5eDg8EnWSEtMTMTGjRvLFSbn+4LgU7do0aIqH+djpENlo7PkXdk+Rd9//z0iIiKUljk6OiIyMpJp3NevX+PcuXN48OABSkpK0LhxY5iZmaFZs2YA2E2Z+BAoJtKKioqQmJiIr7/+GkuXLmUeOzs7G3Xr1uU9mQUA5ubm+Oqrr2BnZ4eBAwcybyohN2rUKAQHByMxMRGPHj2Cu7s7Bg8ejKNHjzKPLS/6rq+vj3v37qFPnz7w8PCArq4u89iV8fHxwcqVK5k897uSVixrlQHK+3CZTIaMjAyYm5tj2bJlTONWZsSIEdi/fz/T5x85ciSGDRuGunXrMotTEV9fX3z//fe83mgsSyqVIjw8HOfOnYNUKoWxsTFGjx4NDQ324yLi4+ORlJQEdXV19O/fnyuFkJ6ezqSJSXp6OsaNG4eePXvCyMgIIpEIycnJuHr1Kvbt24emTZu+95hyiqPQpkyZwtsIuNzcXDRo0AAA8PLlS4hEIt4/51UR8rrnUz5HIh8vSqQRoiJ7e/t3Fhr+GA0ePBgLFy4sV5ic9QVBdnY2YmJikJ+fz9X1ycjI4GVUwYeK5QUgAFy8eJH7ubi4GH/++Sfq1auH2bNnM4spNFtbW2zcuJG7GEpLS4OPjw/++OMPQbeL5QlqXl4e/P398fDhQwQGBsLX1xeLFi1CvXr1mMR7F4lEgokTJ2LPnj3MYwmVzAJKL3qTkpIQGxuLpKQk9OnTB3Z2dujbty/TuIcPH0Z4eDiCgoIwcuRIqKmpoVOnTszrPQKlf9stW7bgjz/+gLq6OsRiMQYNGsQ8blX4uPiTyWTYt28fkpKSUFxcDGNjYzg7OzO/AFfch4tEIjRs2JC3LneKjQ5kMhn++usv/P777zh06BDTmNHR0Th58iT69u0LR0dH5t8nuV9++QUHDx5EgwYN8P3332PIkCFK3b5ZEjqJ5+vri6tXr2Lo0KGQSqU4dOgQLC0tmRehz83NRVhYGFfkv1u3bhgzZgwaNmzINK7i6EbF82A+RkjFxMQgKCgI6enpEIlEaNWqFWbOnMnbqOKqCJlI+1QHL5CPG03tJOQdXr9+jVevXimNzmrRogXEYrGAW8VOw4YNy9UK44OHhweaN2+Oa9euYdCgQTh58iS6devG+3Z8SCqqWfc+yad1ypmYmGDkyJGfdCLN09MTrq6u3N3s/Px8XhIM78LynpaPjw9MTU2RkpKC2rVro0mTJpg/fz5CQkKYxaxKfn4+Hj9+zEusEydOcMmsgIAA3pJZALgphiYmJrhw4QJ8fX0xY8YMJCcnM41rYmICa2triEQiREZG4v79+7yNarC1tUWPHj0QFxeHZ8+ewdvbG9HR0e/szvyx8/Pzw4MHD+Do6AiZTIb9+/cjPT0dixcvZhq3TZs22LVrFxYsWID09HQEBQXB09MTjRs3ZhoXUO5sKE/isS7E3qtXL/Tq1QsSiQTx8fHYsWMHli1bBjs7O4wYMUKpMPr7NmPGDMyYMQNXr17lPtMmJiZwdHTkuruzUqdOHfz444+CJPEAICEhAYcOHeJGvo0ePRr29vZME2lnz56Fqakppk2bxixGZapT35PFCKnDhw9j8+bNEIvF6NWrF4qLi3HlyhWsXbsWmpqaSo1zPjc07od8iCiRRkgVtmzZgpCQEG6oNfB/tYVYnzgJpWfPnvjpp5/Qr18/pdEbvXr1Yho3KysLu3btgq+vL6ysrDB58mTq3MOYYjJDJpPh77//Rm5uroBbxF6/fv1w4sQJ3LlzB9ra2jA0NORlWsy7sGyAkJGRgVGjRmHfvn2oVasW5syZAzs7O2bxylKsIyWvU8ZXlzmhklkAcOvWLRw8eBDHjx+HoaEh3Nzc8N133zGL9+TJE8hkMkydOhVbt27lLjzq1q2LKVOm4MiRI8xiy3l5ecHS0pKL+/vvv2Pbtm3M4wrt7NmziI6O5kagWVhY8DKCZP78+Rg6dCgAoGnTpjAyMoKnpycv73nZzoZ8qlWrFoYMGYIhQ4bgxYsX+Pnnn/Hdd98xv/kEAN9++y2+/fZbFBUV4eTJk9i9ezfEYjHT75eQSTygtAj+y5cvoaenB6B0ij7rUWEBAQHMOmm/DywK72/fvh0hISFKU2UtLCzQrl07zJ07V/BEmpDJrE+twRv5NAh/9UDIBywiIgLx8fHcycPnICUlpcKh66zrZsmL3BsaGiItLQ1ff/0103gEcHZ25n4WiUTQ09P7ZEdayj169Ah79uwpVwNQ6O57LKmrq+PVq1fciej9+/d57Xr222+/4dSpU0hKSkJBQQHc3NyUPnss8Z3MUiQWizF8+HCEhobyNkLowoULyMrKUmqeoaGhAQsLC+bxgdKk6cGDB/H333/D3d0dR48exeTJk3mJLaSSkhIUFxdz9fdKSkp46aaYl5fHFbqvVasWnJycsG/fPuZxAeDatWsIDg5GQUEBV47h8ePHSEhI4CX+/fv3ERsbi7i4ODRr1kypqyMfrl69itOnT+PWrVu8TS8VIokHAHp6etz0eA0NDSQmJkJPT4+r9fopHz8rwyKpVFhYWGG9udatW6OwsPC9x6tMZfXwgoKCeNsGQj4GlEgjpArNmzf/5LsYyinW4yp7gsDHnSBjY2PMmjULXl5emDhxIlJTU6Gtrc087ueMrwueD4mHhweMjIy44sWfg1mzZsHFxQVPnjzBjz/+iGvXrmHNmjW8xd+yZQsKCwvh5OQEqVSKAwcO4OHDh8ynvQH8J7MU7d+/HxkZGbh58yb69euHx48fMynKLSe/mA0JCcHUqVOZxalKQEAAnj59itTUVEyZMgWRkZFIS0vDwoULBdkegJ9RFLa2tnB1deVGhx06dAjDhg1jHldbWxunTp2Cubk5AODcuXPQ0dFhHhco7Yg7adIkREVFwcXFBceOHWPeTTErKwtxcXGIiYnB69evYW9vj99++43plE5F8sT84cOH0bZtW4wYMQJisZjX2osA/0m8AQMGKJX86Nq1K/OY9+/fh6ura6WPC90UicX5w9u3b/HmzZty3+GCggKUlJS893gVKVsP7+eff8aNGzfg7u7O9PhFyMeIEmmEVKFt27YYO3Ys+vTpo9TpbcaMGQJuFRujRo0CAMycOVOQ+HPmzMHDhw/RsmVLrFu3DpcvX/4k3+f/gvUF4Oc4Oqu4uBheXl5Cb0Y5LP/W/fr1Q5cuXZCSkgKpVIoVK1bwmlS6fv260ogJS0tLXpIMAP/JLEVxcXHYvHkz3rx5g7CwMIwePRqenp4YPnw407jOzs7w9/fH+fPnUVJSAmNjY8yePRu1a9dmGhcAzpw5g6ioKDg4OEBXVxfbt2+HnZ0d80TazZs3K724NzExYRobANzd3dG5c2ecP38eMpkMP/zwA5fcYmnFihWYP38+PD09IRKJ0KxZM/j7+zOPC5SOgHN0dMSjR49Qr149+Pn5MZ/Oam1tDSsrK3h5eaFPnz5MY5VlY2MDiUQCBwcH7N27l3kDprKESOI9e/YM+vr6lb7XLVq0YBZbX1//szsHtLa2hlgsxpo1a7i/66tXr7B48WLemg0IUQ+vOqhGGvkQUSKNkCo0bdqUaYvtD4n8IqRsAXq+zJw5kxs23rVrV3Tt2hXjx4/Hzp07BdkevuXm5kJdXV2pKDjrC8DPcXRWz549kZCQADMzM6XkOB9WrlwJHx8fpWVeXl7w9fVlOmXi5cuX2Lx5M5KSkqChoYH+/fvjhx9+4G3Ep4GBAR48eIA2bdoAAJ4/f87bflWoZBYAbN26Ffv27YOzszMaNWqEqKgouLm5MY+9cuVK6OjocKMOw8PDsXTpUl4SLPIpw/L9iUQi4WUa8YYNG/DgwQMYGxvDwsICpqam3KgOT09P5vEzMzNx4cIFeHl5cUX/u3Tpwjxh3alTJ8TGxiInJweamprQ1dXlHgsKCmJ6Y0xLSwu5ubkwNDTE9evX0bdvX+ajZk6fPq30Gvm0ZMkS3qZwliVUEk8sFiM4OBjOzs4QiURKyQx5vWBW6tSpI9j5qFBmzpwJsVgMExMTtG/fHsXFxbh//z7s7Ox4S2QJUQ9PEU0rJR8TkYxSvIRUm0wmQ0ZGBg1vfo9mzJiB27dvIzMzU+niuqSkBM2aNUNoaKiAW8fe7du34eXlhczMTEilUrRv3x6+vr5c0oGlz7GduJmZGZ4/f660jHU7+8WLFyM9Pb3cqJni4mK8evUKBw8eZBYbAKZNm4Z27drB3t4eMpkMkZGRyM7O5q1b6YQJE3Dt2jUYGRlBQ0MDycnJ0NfX55IMLKfoODg4YPfu3XB2dkZ0dDSysrLg5uaGQ4cOMYsp5+joiMjISNjb2yM6OhpA6RRA1n9vOzs7xMTEKC0bMmQI4uLimMYFSqeVpqam4saNG3B1dUVMTAysrKx4uQgsLCxEYv1hKgAAIABJREFUUlISEhMTceLECbRr1w5bt25lHhcAXFxcMHToUIwePRoSiQTR0dH4f+zde1zO9/8/8Mc1oWhFOcyhbY3Ix2c2hBynNKSlImaf1HdmkoRyziRUDpWxYltsGDNZWQkpojlMcj5FzRw+CVMWOUXqun5/uF3XrwuzfeL1frV63G+33W6u17Vbz5ctua7n9TykpKRIXbQg+ud7SkoKNmzYgOjoaAwZMgSvvPIKrK2tK8UW5KomIyPjL5N4ohOnSvPz86vU237L/1x/2bRV1CqVCu3atdNrXRaxLbS88ePH4+jRo0/Nw7O0tAQgtmPhybbSrVu3wt7eXno1HNGfYUUa0XNs2LABCxcuRHFxse6sefPm2LFjh8RbVS0LFizArVu3EBYWpjfo3sDAAObm5hJvpozPPvsMAQEBuvkjO3bsQGBgIH744QfhsWVWZ8myb98+xWOOGTMGV65cQVhYmF6rSo0aNdCiRQvh8a9cuYKYmBjd488++0yx1koA8PX11Xv8ySefKBb7lVde0atgadSokWKLFqysrPD999+jtLQUZ8+exQ8//ABra2vhcTUaDW7fvg0TExMAjysSlRh8DwDe3t7Yu3cvmjZtimvXrmHcuHG6n23aNjERCgsLcfDgQRw8eBCHDx+GqakpWrZsKSTWs8gc+v9nRH1OHhERgSlTpqBOnTpYuXIlVCoVNm7ciEuXLiny/V0d/Z1KuF27dglLpF2+fBmxsbG4efOmYmMgli5dinv37qFWrVqoWbOm7rykpASrVq3C6NGjhcUuT0aFVPPmzdG8efNnPidiW2h5MubhaVXWtlKiP8NEGtFzxMTEYNOmTViyZAkCAgKwe/duHD16VPa1qhRjY2MYGxvjiy++wIULF2BtbY3NmzfjzJkzGDVqVJXfmKrRaPRetLz//vtYtmyZIrFTUlLw/fff652Jrs6SZcOGDfjwww//9BNukbNYtC+Kk5KSkJ+fj0aNGuHw4cPIzs4W+smyVsuWLXH48GHY2NgAALKzsxWpeNSS2Z4jK5kFPB4Qff36ddSuXRszZsyAra2tIvP5RowYAXd3d9jb20Oj0SA9PV3R5QM9e/ZEz549nzr39vYW9gawW7duaNCgAby8vLB27VrFlwTJHPr/Z0S162/evBndu3dHWFgYwsLC9BIrR44cQadOnYTELU87h6+8devW6W2rrW5ENhiNGzcOXbt2VXQMxIYNGxASEqJL2P773/9GSkoKFi5ciLp16yqSSKuMg/dF/X+WOQ9PS3ZbKdH/iok0oucwNzeHhYUFWrdujV9//RUeHh7SP2WuqqZMmYLmzZvj4cOHiI6OhouLCwIDA/Uqaaqibt264csvv8TQoUNRo0YNJCcno0WLFrh69SoAsS9eZFRnyVIZphgEBwfj0aNH+OSTTzBp0iR0794dx44dQ2RkpNC4Fy5cgKenJ958800YGBjg4sWLMDExgb29vfA5N7LJSmYBjysB582bh0mTJikSTys9PR3Lli3DwYMHodFoEB0djXnz5sHd3V3RezxJ5J/Bbdu24cCBA8jMzISXlxdatmyJLl26YOjQocJilidz6L/S/Pz8EBMTg/z8fHzxxRd6z6lUKqGt2qtXr8bdu3cRGxuLK1eu6M5LS0uxZcsWRRJpV65cwcyZM3XLeiZPnox58+b9aQWRUkQmuDQajeJLelasWIH4+Hjk5eVhxYoVMDEx0VXdDRkyRJE7VMYKKVH/n2XOw9MyMzPDwIEDn2orDQwMBFC1F2HRPxMTaUTPYWRkhAMHDqB169ZIS0vD22+/jQcPHsi+VpWUl5eHL774AhEREXB3d4e3tzcGDx4s+1rCbdu2DQAQHx+vd659MSPyxcvt27exefNm3Lp1S+9FU1XclKVtu5L5ezt16hQ2btyIpUuXwt3dHePGjVPke3zZsmXYs2cPDhw4gNLSUri7u8POzk6xdj+ZZCWzgMdtpfb29rC0tNTbrCcq0aCdN5mfn48zZ87o/kx/8803ejN2ZBH5Rt/S0hKWlpZo37499u/fj9jYWJw6dUqxRJrMof9KGzp0KIYOHYply5Zh7NixisZ+8803cfr06afOa9eujQULFihyh1mzZmHkyJFYtGgRGjZsiA8++ADTpk3DunXrFIkvQ/v27bFjxw706dNHsdZ4IyMjWFtbw9raGjNnzkTXrl2Rmpqq6LKJ6lQhpf3QeteuXdLuILOtlKgimEgjeo6goCDExcVh+vTpiI+Ph6OjY5VMMlQGZWVlKCwsRFpaGqKjo1FQUICHDx/KvpZwMl+0TJgwAa+++iqsrKyq/NZOa2vrZ/4eNRqNYu2sZWVlUKvV2LlzJ+bMmYPi4mK9+Yui/PDDD8jNzcWgQYOg0Wjw008/IS8vD5999pnw2LIpncwqb8qUKcJjlFed500GBATg6NGjsLS0RO/evfH111/jrbfeUvwez3qTLXJ2VVFR0VNtrFeuXEGzZs2Ez18cNWoUvv76a1y8eBFBQUFYvXo1vL29hc7b7N27N3r37g1HR0e8/vrruHjxIsrKymBlZaWrGhLt5s2b6NGjByIjI6FSqTB06NAqm0TT/r2p0WgQGxurV6kk+u/N8h/0mJqaIjw8XG9WmhKqY4WUjHl4laGtlKgimEgjeg4rKytMmTIFv/32G3x9fbF48WLFXqxVNyNHjsTQoUNhb2+PVq1aoV+/fpgwYYLsawl34cIF/PjjjygqKtI7V+IF2o0bN7Bq1SrhcSqD7Oxs2VeAq6srevTogQ4dOuCdd97BgAEDFKmY+eWXX5CYmKirJOjduzecnZ2Fx60MlE5mlaf0bDjtvMmvvvpK0biVgaOjI0JDQ6HRaKBWq3WLFioDES2t165dg0ajgbe3N1asWKGLUVZWhlGjRiElJUV4y/jcuXNhZmaGrKws1KhRA7m5uZgxY4bwuMDjlu1+/fqhXr16UKvVuHHjBpYtW4Z33nlHeGxDQ0P8/vvvug9mDh8+XCmW9YhInMr8e7P8B1916tRRPIkGVM4KKdFjKmTMw6sMbaVEFaHSVIbBMUSV1KlTpzBhwgQpL9aqu7KysmrRejZgwAAMGDAAzZo10zt/cpCyCFOnTsUnn3xSrTatlZSUYOXKlYpWUZSnVqt1Ca3CwkJFlmk4OTkhISFB93t8+PAhBg8ejC1btgiPTaTl6uqKxMREIV87NzcXEydOxOXLl6HRaNC0aVMsXrwYlpaWQuL9L9zc3F76koXAwEBkZmbqlpdoGRgYoHfv3pgxY8ZLjfcs2t+X9v+rRqOBs7OzIj9Xhg0bhsDAQN1rsePHjyM0NPSpEQkinDx5EkFBQcjNzcXrr7+OoqIifPHFF4q8Ljx58iSOHDkCDw8P+Pj44MyZMwgPD0evXr2Exs3NzcXx48fh7OyM4OBgZGVlYc6cOUITS+3bt8fbb78N4PFrce2vtURWFWsrpLSzap+kRIXUn20LvXz5stBFBy4uLti0aZOwr09UlbC0hug5wsLCsHjxYr0XayEhIYq8WKtu9u7diyVLlqCoqEjv06iq/kmUiYmJtHbhc+fOYdCgQTAzM9Nre6vK/82frKL473//q1gVxfHjxxETE4P79+/rKmeuXr0qvL3X2dkZXl5ecHJyAgBs3boVH3zwgdCYVH2V30ybk5ODwYMHw9DQUK/d9GULDg7Gp59+iv79+wMAkpOTMWvWLKxdu1ZYTJm0FcvLly9XdCNreSqVCiUlJbqqlZs3bypWwXL//n29xNW7776r2CiIdu3aIT4+HpcuXUJZWRmaN2+u2Nyu0NBQjB8/HqmpqTA0NERCQgL8/PyEJ9ICAwMxZMgQ7Ny5ExcvXkRgYCBCQ0MRGxsrLKbMRVOyK6RkbguVMQ9PS0ZbKdGLYCKN6DlkvlirbkJDQzF9+vRqMa+rPDc3NyxevBi2trZ6bcOdOnUSHjsyMhJ79uzBvXv30KxZM5SVlSEzM1N4XJmysrKQkJCAPXv2wMjICOHh4Yq1Oc6YMQMjR45EQkICPD09sX37dvzrX/8SHtfHxwf/+te/kJGRAY1GAx8fH/Tu3Vt4XKp+nreZ1sbGRljcmzdv6pJowONK3+rQ4jp8+HBEREQgIyMDZWVlsLW1xYQJE1CnTh3hsb28vDBixAgUFBQgLCwMaWlpii0fMDU1RVpaGhwcHAA8rt6pV6+eIrGTk5Px1VdfYfPmzcjNzYWTkxOCgoJ0dxFJrVajR48emDRpEvr27YsmTZqgrKxMeNyHDx/C1dUVn332GZydnWFjY4OSkhKhMbWt8deuXUNWVhYAoG3btoosTpE9eF/GtlCZ8/C0ZLSVEr0IJtKInuPJF2s7duxQ7MVadVO/fn29WRTVxbFjx3D06FEcPXpUd6ZSqRQZhr548WIUFRUhNzcXNjY2yMzMRIcOHYTHlUlmFUWtWrUwePBgXLlyBSYmJoom8Xr16iW8aoFI1mbaWrVqISsrC23btgUAnD59GkZGRsLjaska+h8SEgIjIyPMmzcPAPDjjz8iODgYERERwmKWb891cnKCRqNBWVkZRowYodgM2blz52Lq1Km6hSkWFhYIDw9XJPZXX32lmy36+uuv46effsInn3yiSCLNyMgIK1euxIEDBzBr1iysWbMGdevWFR63Ro0aSE1Nxc8//4wJEyYgLS1NeLVSWVkZZs2ahS1btqBly5Z49OgRLl++jA8++ABz5sxRpFpKVoWUjG2hlWGOrEajwbRp02Rfg+hvYyKN6DlCQkIwZcoUvRdrIl+gVmcdO3bE/Pnz0bNnT702QyUqs2TKysrC9u3bpcTOycnB9u3bERYWhsGDB8Pf3x/+/v5S7qIUmVUUtWvXxq1bt2BpaYkTJ06ga9euilQTEClF1mbaGTNmYNy4cahXrx40Gg2Kiorw+eefC48re+h/VlYWkpKSdI9nzZqFAQMGCIsHQFe1nJubi9zcXLz33nt45ZVXsG/fPrRs2RKurq5C4wOApaUl4uLicP/+fajVasVaK4HHSY0GDRroHpubmwsfAK8VGRmJuLg4REdHw9TUFNevX1fk+3zu3LlYvXo1Zs2ahUaNGmHr1q0IDQ0VGjMmJga3b9/G3r17dctDCgsLERQUhJiYGIwZM0ZofEBehZTMbaEy5uFpyWwrJaoIJtKInsHT01P3l6ahoSGaN28OjUYDIyMjBAcHK1ItVN2cPHnymeXjVf2/tZWVFbKzs6UM/Dc3N4dKpYKlpSVycnLg6uqKR48eKX4PJVSGKoqPP/4YAQEBiI6OxpAhQ7B58+ZKsQWM6GV51mbaDz/8UHjcd999F6mpqbh06RLUajUsLS0VWSASFRWlG/rv4eGhO9cO/RdNo9Hg9u3bukTD7du3hS/p0b6J9/T0xKZNm3RVM0VFRcI/lNAmEv7qbiJ16NABEydOhLOzM1QqFZKTk/Huu+8Kjws8TnJ06dIFZWVlOHToEHr37o3c3Fw0btxYaNzw8HB8++23useLFy8WGg8AUlJSEBsbq9embGZmhvDwcAwdOlSRRJqsCimZ20JlzMOrDG2lRBXBRBrRM4wbN072FaqNoKAghISEAHh6rXd1mJFw4cIFDBo0CA0aNNBb767EwH8rKyuEhITgo48+wuTJk5Gfn6/YJ+tKqwxVFI6Ojujfvz9UKhU2btyIS5cuoU2bNsLjEillxIgR+L//+z9dNcH333+vyGbaK1eu4Pvvv39qWY3oxIrsof8jRoyAu7s77O3todFokJ6ertg98vPz9UZdGBkZoaCgQGhM7dys8v773//i22+/VWyb+uzZs7F27Vps2LABBgYGsLGxwX/+8x9FYkdFRel+XVpaipycHNjY2Aiv3C8uLsa1a9cUmU+mpdFonjnrr27duopVKyldIaXdFtqlS5dnPq/EtlAZ8/AqQ1spUUUwkUb0DM96sUZiaKsVqmvycunSpdi8eTN+++03+Pj44PTp04q1s86ePRvHjh1Dy5YtMW7cOGRkZGDRokWKxFZada+iIFLCyZMnsXLlyqdmComuLPb394eNjY20IdWyhv6np6dj2bJlOHjwIDQaDaKjozFv3jy4u7sLjQsAvXv3xogRI9C3b19oNBps27YNjo6OQmO6ubnpPV6zZg02bNiAyZMnw8vLS2hsrTFjxuDbb7/FyJEjFYlX3pNbaC9fvqzI3x83b96Evb09zM3NUbt2bWg0GuHbK1955RXk5eWhefPmeueXL18WXm0qq0JK9rZQQM48PC2ZbaVEFaHSVNXyAyKif4DIyEj8/vvvyMrKQlxcHMaMGYO2bdti+vTpsq9WJfXr1w/btm3TvTAsKSmBs7MzUlNThcVMSEh47vNPvjkk+qdydHTE8OHD0bJlS72ElugPp9zc3P7yz5lIgYGBMDIywtChQwE8Hvp/584dYTNV/fz8cPbsWeTn56NRo0a6N9xqtRpNmjTB+vXrhcR9UmpqKg4ePAiVSoWuXbuiT58+isS9fPmy7gOKsLAwvPHGG4rEBYD//Oc/WLRokaLVWc/Tv39/pKSkCI1x5cqVZ543a9ZMWMykpCR8++23CAoKwttvv43S0lIcP34c8+bNw4QJE9C3b19hsauznJwcrF69Gr1790a/fv0QEBCA0aNHKzJ+xMPDA0OGDIGxsTG+++47TJgwAZGRkULbSoleBCvSiIgk2rdvHxISEuDm5gZjY2OsWrUKAwcOZCJNkMpQRfFn/47MRADRy2BoaKg3K0wpHTt2xK5du9CjRw9FZqM9Semh/wsWLMCtW7cQFhaGmTNn6s4NDAxgbm4uLO6T+vXrh379+ikWD3hchfb111/Dx8dHb56tUgoLCxWvztJ6srr5/PnzaNWqlfC4zZo106ucT01NFT4OYeDAgSgtLcXUqVNx9epVqFQqvP7664om0WRVSMnaFgrImYenJaOtlOhFMJFGRCSRtjJK+2agpKSE24oECgwM1Kui+OSTTxSrongeFofTP9nVq1cBAG3atMHq1avRp08fvaH3omf7pKSk4Pvvv9f9HNUmN5QaUq300H9jY2MYGxvjq6++EhajMho+fDhOnjyJTz75BCYmJti0aZPe80rMuiyfZFBa+cpOlUqF/v37o1u3bsLjlq+cHzVqFDZu3Ijs7GzhH/gNGjQIgwYNQmFhIVQqFerXry803pNkDN4H5G0LBeTMw9OS2VZKVBFMpBERSdS/f3/4+/ujqKgIq1evRlJSEj744APZ16rSZFRR/JXqsFiDqq7yM30OHDigNxNNiWqdffv2/elz6enpehvwRJA59L86sbCwgIWFBa5fv47r168/9bwSibRDhw4981xkm6NWfn4+Ro8erXf2+eefY+LEiULjyqicX7p06XOf9/PzExZbS1aFlKxtoYCceXhac+fOxerVqzFr1iw0atQIW7duRWhoqPC4RBXFRBoRkUTe3t7Yu3cvmjZtimvXrmHcuHHC3/QREb1Mu3btAgDcunVLb5MjAOTl5cm4kk5UVJTwn6kyh/5XJ3+ntS06Olro8iLtBmgAePToEY4cOQIbGxuhSbzIyEj88ccf2LVrFy5duqQ7Lysrw4kTJ4Qn0mRUzn/99dcwNTWFg4MDGjZsKDTWn5FVIaX0ttDyvvnmG0XjlSezrZSoIphIIyKSrGfPnujZs6fsaxARVci1a9eg0Wjg7e2NFStW6FqVy8rKMGrUKOHD0J9HZNt0+aH/Z86c0cX65ptvKs0w+upm165dQhNpTybzbt26hYCAAGHxAKBv3744f/48Dhw4oNfeWaNGDfj6+gqNDcipnN+7dy9SU1ORmpqK3Nxc9O/fH3379lW0vVPpCilZ20LLkzEPT0tmWylRRXBrJxERkWSurq5ITEyUfQ2iCgkMDERmZqZui6SWgYEBevfujRkzZki7m8hFHnfv3n3u0H8DA35erTSlf5aWlJTggw8+wPbt24XHunPnDl599VXdY41Gg7y8PFhYWAiPvXfvXuzfvx9qtRq2traKVs7/8ccfSElJwfbt21GrVi04Ojpi0KBBwuOOHDlS6kw8GWRuknd0dMSlS5ektJUSVQT/hiciIpLsybk3RP8k2iqd5cuXV6vZYNV16H9lJnreZPlNodpE1nvvvSc0plZycjIWLlyI4uJi3VmzZs2QlpYmPPZrr72GPn366CqkDh06hE6dOgmPCwDm5uZwcXGBsbExfvjhByxdulSRRJqsCilZ20IBuZvkZbaVElUEE2lEREQK6NatG27evIlatWqhRo0aujdD2k9dHR0dJd+Q6MW4u7tj7dq1KCoq0jtXYjA4kRLKt41qN0m2bNlSkdgxMTHYtGkTlixZgoCAAOzevRtHjx4VHnfOnDlIT0/Xq3xTqVR6S0VEuH37NtLS0pCamopLly7Bzs4OgYGBePfdd4XG1ZI1eF/WtlBA7iZ5mW2lRBXBRBoREZECHB0d8fbbb+teGKalpWHnzp1/a4A20T/BqFGj0KpVK0U2GP5dnGBCL1Pnzp2xe/duHDhwAKWlpejSpYtiiTRzc3NYWFigdevW+PXXX+Hh4YH169cLj/vLL78gJSUFhoaGwmNpffrpp7h06RL69OmDMWPGKJY8K09WhZSsbaGA3E3y5dtKR40ahY0bNyI7O1uRajiiilB2FQgREVE1deTIEb1PVx0cHJCdnS3xRkQv3/z58+Hn56f3j2g5OTm6ge/nz5+Hh4cHLly4AADYsGGD8PhUebRo0ULo11+xYgWWLl2KJk2aoHnz5vj6668Va+01MjLCgQMH0Lp1a6Snp6OgoAAPHjwQHtfCwkLxhPS+ffuQl5eH7777DsOGDUObNm3Qpk0bWFtbo02bNorcoVmzZjh69Ch+/PFHmJmZ4dChQ4p8SFB+W2jv3r0V2xYKPN4k7+7ujn79+uk2yfv4+CgSe9++fYiIiEDt2rV1baV79uxRJDZRRbAijYiISAF16tRBXFwcBgwYAI1Gg8TERDRu3Fj2tYheGgcHB8TFxcHW1hY1atTQnTdt2lRo3KCgIIwdOxbA40SKr68vPvvsM6xfvx61a9cWGpuUd/LkSRw5cgQeHh7w8fHBmTNnEB4ejl69eiEyMlJo7KSkJMTFxemqs4YOHYpBgwZhzJgxQuMCj7/P4+LiMH36dMTHx8PR0VGRRLWpqSmcnJzQvn171KpVS3cuspr673zIlJWVhbZt2wq7g6wKKaW3hT5J1jw8mW2lRBXBRBoREZECwsPDMXfuXMybNw+Ghobo3r075s2bJ/taRC/N/fv3MW/ePNSvX193psRMoeLiYr2B7927d0dERITQmCRPaGgoxo8fj9TUVBgaGiIhIQF+fn7o1auX8NgajUavxbF27dqKbWfNz8/XbcCNjo4GAEW2hfbs2RM9e/YUHud/NXPmTGEbeQF5g/fDw8P1toUuXrxYaLzyZM3DA+S2lRJVBBNpRERECmjevDmWL18u+xpEwqSnpyMjI0PRWUoAYGZmhvXr12PgwIEAgK1bt8Lc3FzRO5By1Go1evTogUmTJqFv375o0qQJysrKFIlta2uLcePGwc3NDQCQmJiILl26CI2ZnJyMkpISREVFYfz48brz0tJSxMTEoG/fvkLjnzp1CnZ2dujSpYteRZpsottNZVVIydoWCsiZh6fl7e2NvXv3omnTprq2Ujs7O8XvQfR3MZFGRESkgL1792LJkiUoKirSewMgulqHSCnNmjVDUVGR4m/C5s+fjzlz5iA8PBy1atWCjY0NwsLCFL0DKcfIyAgrV67EgQMHMGvWLKxZswZ169ZVJLa2ZTgxMREajQa2trb48MMPhca8d+8ejh49inv37iEzM1N3XqNGDd1sQJE6duyIrVu3Ys6cOWjdujXs7Ozw3nvvoWHDhsJjP482wSWKrAopWdtCATnz8MqT1VZKVBEqDdcZERERCdevXz9Mnz4dVlZWem8AKtOGQ6IX8cknn+DkyZOwsrJCzZo1dedKtAWdOXMG//rXv3Dnzh2cPn0aXbt2FR6T5Lh+/Tri4uLQrVs3dOjQAREREfDy8lJ05mRZWRnOnj2L119/HSYmJorEzMjI0Pu+vnv3LoyNjRWJDTyugIuPj8eXX36JgoICnD17VrHYz+Lm5ia0tRN4/AHY/v37oVarYWtrq0iF1JUrV555rsRrhYkTJ+L48eOKzsPTktlWSlQRrEgjIiJSQP369dmmQFXa87a7FRQUCKtgiYyMxJkzZ7By5UoUFxfjyy+/xOHDhzFu3Dgh8Uiu3NxcdOnSBWVlZTh06BB69+6N3NxcoYm0//73vwgICMD48ePRrVs3eHh44I8//oBarcaiRYvQsWNHYbG1iouLERERAV9fX7i7u6OwsBDTpk3DoEGDhMb95ptvcOjQIZw7dw5t2rTBp59+CltbW6ExKwsZFVLNmjXD5s2b8dtvv8HHxwepqal6G79FkjkPT2ZbKVFFMJFGRESkgI4dO2L+/Pno2bOn3iZBti1QVdG5c+c/fc7b21tY9cjPP/+MTZs2AQAaNWqEVatWwc3NjYm0KioqKkr369LSUuTk5MDGxkboz9LQ0FCMHDkS7733HuLj43H//n1s374dly9fRmBgIGJjY4XF1lq2bBnCwsKQnJyMdu3aYdasWfD09BSeSEtLS8O1a9fg7OwMW1tbdOzYEUZGRkJj/h2im6pkVUjJ2hYKyJ2HJ7utlOh/xUQaERGRAk6ePAmVSvVUOwzbFqg6EPkGqbS0FA8ePNDNyXr06JGwWCTf2rVr9R5fvnxZeOvZ9evX4eTkBADYv38/+vXrBwMDA1haWuLu3btCY5dnbW2N6OhoDBw4EHXr1lXkez02Nhb379/HoUOHkJGRgXnz5sHExESR5CHwOJF34MAB1KhRA7169UL37t0B/P/NpaLIqpCStS0UkDsPz9TUFE5OTlLaSokqgok0IiIigYKCghASEgLg6WSC6GHJRJWFyO/1YcOGYdCgQbC3twcA7NmzBx4eHsLiUeViYWGBCxcuCI2txEI6AAAgAElEQVSh/dmt0WiQmZmp+/7SaDS4f/++0NhaDRo0QEhICE6fPo2IiAgsWLAATZs2FR5Xm0Tbv38/MjMzYWJigl69egmPCwALFy7EsWPH4OTkBLVajS+++AKnTp2Cj4+PXqWYCLIqpGRtCwUAJycnODk56ebhRUVFISgoSJF5eDLbSokqgok0IiIigbQb3dhmRiTGRx99hEePHqGkpAQmJiZwd3dHQUGB7GuRIIGBgXqPz58/j1atWgmN2bp1ayxfvhwlJSWoVasWOnTogJKSEqxcuRLvvvuu0NhaixYtQlpaGry8vFCnTh1YWFjAz88PAJCVlYW2bdsKievg4ICuXbuiV69eGD16NMzMzITEeZZdu3Zh69atMDB4/JZ12LBhcHV1fe48xpdFVoWUrG2hgNx5eDLbSokqgls7iYiIiEgokRv2/Pz8UFRUhNzcXNjY2CAzMxMdOnTQm6VFVUf57yOVSoX69eujW7dueptiX7Y7d+5g0aJFuHHjBsaMGYO2bdti9uzZOH/+PBYvXowGDRoIi/13iPzzVVZWht9++w2HDh1CaWkpbG1tYW1tLSTWk4YPH46oqChd8u7u3bvw9vbGDz/8IDz2n/33dHNzEx5bxrZQ4HGiUtY8vK1bt2Lv3r04fPiw4m2lRBXBRBoRERERCeXq6orExEQhX/v999/H9u3bERYWhsGDB8PY2Bj+/v7YuHGjkHgkV0xMDEaPHq139vnnn2PixImSbvRYdHS0tMpjkX++Nm3ahOjoaDg4OECtViMtLU23OVS08ePH4+jRo+jTpw8MDAywd+9emJmZwdLSEoDY6rC5c+dKq5A6d+4cioqK9FpLlVpMpG3lPXjwINLT0xWdhwdA11b65ZdfoqCgQJG2UqKKYGsnEREREb2QwsJCFBQUwMrKSm+ej7blbObMmcJim5ubQ6VSwdLSEjk5OXB1deXCgSooMjISf/zxB3bt2oVLly7pzsvKynDixAnpibRdu3ZJS6SJnEG4cuVKxMXFoX79+gAAHx8feHl5KZJIs7Oz06vG+ve//y08ppaswfuytoUCcufhyWwrJaoIJtKIiIiIqMKSk5Mxf/581KtXDyUlJYiOjtbNrJo5cyYSEhJgY2MjLL6VlRVCQkLw0UcfYfLkycjPz5cyJJzE6tu3L86fP48DBw6gc+fOuvMaNWrA19dX4s0eq6rfc2q1WpdEAwAzMzPhi3IKCgrQsGFDdOnS5ZnPK7FkQdbgfVnbQgG58/DS0tKktZUSVQQTaURERERUYV9//TU2bdoEMzMzJCcnY+TIkVi1ahVatmypSHJh9uzZOHbsGFq2bIlx48YhIyMDixYtEh6XlNWuXTu0a9cODg4OePXVV3XnGo0GeXl5Em/2WFXdwty6dWuEhYXpKtDi4+OFz0ibOXMmYmJiMHz4cKhUKr2fIyqVCjt37hQaH5BXISVrWyjweDabdh5eUlKSovPwYmNjdRVxGRkZmDdvnuJtpUT/CybSiIiIiOiFaCsXBgwYAJVKBW9vb6xfv16R5EKNGjV0FW99+vRBnz59hMckeZKTk7Fw4UIUFxfrzpo1a4a0tDSJt5JLZOIlNDQU0dHRmDFjBjQaDWxtbREcHCwsHvB4Dh7wuF1WFlkVUrK2hQLAli1b9Obh+fr6KjYPT2ZbKVFFcNkAEREREVWYv78/mjZtCi8vL7z22msAgLVr1+K7777Dw4cPsXfvXsk3pKrE3t4e3333HZYsWYKAgADs3r0bR48elV6FKHJzptafDaG/fPmy3kwtpYwePVqX9BLh8uXLiI2Nxc2bN/V+z0oklQA5g/dlbgt1cXHB6tWrda28hYWF8PLywpYtW4TH7tatm66ttGfPnoq2lRJVBCvSiIiIiKjC5s2bh+XLl+PixYu6RJqnpyeaNGmC6Ohoybejqsbc3BwWFhZo3bo1fv31V3h4eGD9+vWyr4UWLVoI/frPG0IvI4kGANevXxf69ceNG4euXbvCxsZG8dZZWRVSp06dkrYtVMY8PC2ZbaVEFcFEGhERERFVWJ06deDv7//UuYODAxwcHCTciKoyIyMjHDhwAK1bt0ZaWhrefvttPHjwQJHYJ0+exJEjR+Dh4QEfHx+cOXMG4eHh6NWrFyIjI4XGljmE/s+ITrJoNBpMmzZNaIw/I2vwvqxtoYCceXhaMttKiSrilb/+V4iIiIiIiOQLCgrCrl270LNnT9y6dQuOjo4YPny4IrFDQ0NhZWWF1NRUGBoaIiEhAV988YUisWUOoZelffv22LFjB9RqteKx9+7dC29vb9y5cwdJSUnIzs5WJK6TkxMWLFiAlJQU9OzZE1FRUYrNCgsNDUWtWrUwY8YMBAYGombNmsLn4WmtXLkScXFxmD59OmbMmIH4+HisXr1akdhEFcGKNCIiIiKqsI8//vi5b3TXrFmj4G2oqsvPz8eMGTMAQNc6vH37dkViq9Vq9OjRA5MmTULfvn3RpEkTlJWVKRJb5hB6pVlbW+u2dcbGxupt7lSpVDh79qzwO8iqkJK1LRQADA0NMWXKlGc+J3oensy2UqKKYCKNiIiIiCps1KhRmDhxIsLCwmBiYiL7OlRFJScno6SkBFFRURg/frzuvLS0FDExMejbt6/wOxgZGWHlypU4cOAAZs2ahTVr1qBu3brC4wJAz5490bNnT0Vi/V2iKuSUqv56Hm2FlDa54+PjAy8vL+GJNFnbQv+K6Hl4MttKiSqCiTQiIiIiqrDu3btj9OjR2L17N0JCQmRfh6qoe/fu4ejRo7h37x4yMzN15zVq1EBAQIAid4iMjERcXByio6NhamqK69ev4/PPP1cktpubG3799VccPHgQpaWl6NKlC9q0aaNIbAAoKiqCqamp3pmrq6vQmLm5uTh+/DicnZ0RHByMrKwszJkzB//+97+FxgXkVUjFxsbqFh1kZGRg3rx5imwL/Suif++hoaGIjo7GjBkzoNFoYGtrq1hbKVFFqDTVrdmeiIiIiF4qjUaD8+fPo2XLlrKvQlVcRkYGunbtqnt89+5dGBsbKxL70KFDzzzv1KmT8NiJiYlYunSprtVw586dGDNmjPAKqbNnzyIgIAAPHjzAhg0bMHz4cCxZsgRt27YVGhcAPDw8MGTIEBgbG+O7777DhAkTEBkZqUhSafLkyahfv75ehdStW7cQEREhNO6T20INDQ3Rq1cv+Pr6Co37V9zc3JCQkCAltui2UqKKYEUaEREREb0QlUr13CRaUFAQq9XopSguLkZERIRuXlVhYSGmTZuGQYMGCY8dFRWl+3VpaSlycnJgY2OjSCJt1apVUloNQ0NDsWzZMkyaNAmNGzfG7NmzERwcjPj4eKFxAeDhw4dwdXXFZ599BmdnZ9jY2KCkpER4XEBehZSsbaGVmei2UqKKYCKNiIiIiIQ6ffq07CtQFbFs2TKEhYUhOTkZ7dq1w6xZs+Dp6alIIm3t2rV6jy9fvqzYsH9ZrYbFxcVo0aKF7nH37t2xcOFC4XGBx227qamp+PnnnzFhwgSkpaXhlVdeUSS2rMH7e/fuxW+//YZDhw4hKSkJtra2lWJWmMwmNi4doMpImZ9EREREREREL4G1tTV+/vln2Nvbo27dunj06JGUe1hYWODChQuKxNIOY8/JyUFOTg7CwsIUSbDUq1cP2dnZumRGUlLSU7PSRJk7dy5+/vlnzJo1C40aNcLWrVsRGhqqSOznEVkhtWXLFowdOxZ5eXm4evUqfH19Fan+K6+oqOipM9Hz8Ij+aViRRkRERERE/wgNGjRASEgITp8+jYiICCxYsABNmzZVJHZgYKDe4/Pnz6NVq1aKxA4NDUVUVJSu1bBLly6KtBrOnj0b06ZNw7lz52BjY4M33nhD+JwwrfDwcHz77be6x4sXL1Yk7l8RWSEla1so8Px5eB9//LHw+ET/JEykERERERHRP8KiRYuQlpYGLy8v1KlTBxYWFvDz8wMAZGVlCR2C37lzZ92vVSoV+vfvj27dugmLV56hoSGmTp2qSKzyXn/9daxfvx7379+HWq1WbLED8Lit9Nq1a2jSpIliMWWT1cILyJ2H9zzcjUiVERNpRERERCQU3wjRy2JsbKzXZubh4aH79cyZM4VuFszPz8fo0aP1zj7//HNMnDhRWEzttkRra2u9hIpGo4FKpcLZs2eFxQaAw4cP47vvvnuq3W/NmjVC4wLAzZs3YW9vD3Nzc9SuXVv3e965c6fw2LJoW3jLbwtVakaazHl4WkVFRU+1DrOtlCojJtKIiIiI6KW7e/eurnpFqaodqt5EJWwjIyPxxx9/YNeuXbh06ZLuvKysDCdOnBCaSNMmBrOzs596TokNltOnT4efn59i7bPlffPNN4rH/DtEfjAga1soIHceHttK6Z9GpeFHhERERET0gtLT03H48GH4+vrC3d0dhYWFmDZtmiLbFImA/1+99bKdPHkS58+fR1RUFMaPH687r1GjBtq1a4c333zzpcd80ocffogNGzboHqvVari4uGDz5s1C43p4eGDdunVCYzzP5s2b8dtvv8HHxwepqamKVyc9q0Jq9erVUpI7IreFAkBubi6mTZuGU6dOwdDQUDcP76233hIWU8vDwwNz587FpEmTkJiYiF9++QWLFy+W3lZK9GdYkUZEREREL2zp0qUICwtDcnIy2rVrh1mzZsHT05OJNPrHa9euHdq1awcHBwe8+uqrunONRoO8vDyhsb28vHDw4EEA0GvxMzAwgL29vdDYAODp6YnJkyfD1tYWBgb//62jEgmtyMhI/P7778jKysKoUaOwceNGZGdnY/r06cJjV8YKKZHbQgH58/Bkt5US/S+YSCMiIiKil8La2hrR0dEYOHAg6tati0ePHsm+EtFLk5ycjIULF6K4uFh31qxZM6SlpQmLqZ1FFhoaipkzZwqL82c2btyIhw8f4siRI3rnSiTS9u3bh4SEBLi5ucHY2BirVq3CwIEDFUmkVcbB+6KXDsichyezrZSoIphIIyIiIqIX1qBBA4SEhOD06dOIiIjAggULpMxVoupL9MSamJgYbNq0CUuWLEFAQAB2796No0ePCo2pNWXKFOzYsQP37t0D8Hg+W15eHiZMmCA07o0bN4QucHieV155BcD/TyCVlJTozkSrjhVSMufhzZ49G9OmTcO5c+dgY2OjayslqqyYSCMiIiKiF7Zo0SKkpaXBy8sLderUgYWFBcaNGyf7WlQFnTt3DkVFRXqJs06dOiE6OlpoXHNzc1hYWKB169b49ddf4eHhgfXr1wuNqTVp0iQUFRUhNzcXNjY2yMzMRIcOHYTHbdeuHdLT09GrVy/UqFFDeLzy+vfvD39/fxQVFWH16tVISkrCBx98oEjs6lgh1bhxY2kbMmW2lRJVBJcNEBEREdELKy0txb59+3Dr1i29c1lvzKhqmjNnDtLT02FhYaE7U6lUirSfeXl5wdfXFw8fPkRaWhrGjx+Pjz76SGhrp9b777+P7du3IywsDIMHD4axsTH8/f2xceNGoXF79OiBGzdu6J2pVCqcPXtWaFytvXv3Yv/+/VCr1bC1tYWdnZ0icWUO3v8zrq6uSExMFPb1U1JSkJaWJmUensy2UqKKYEUaEREREb2wSZMm4erVq2jRooXeLB8m0uhl+uWXX5CSkgJDQ0PFYwcFBSEuLg7Tp09HfHw8HB0d4efnp0hsc3NzqFQqWFpaIicnB66urorMINy3b5/wGM/z2muvoU+fPrrqw0OHDqFTp07C48qukHrWtlDRP0tlzsOT2VZKVBFMpBERERHRC8vJycG2bduED8Sm6s3CwkL4LLQ/k5+fjxkzZgCAro10+/btisS2srJCSEgIPvroI0yePBn5+fmK/HcoLi7G0qVLkZGRgbKyMtja2mLChAmoU6eO8Ngyqw9lVUjJ3BYqcx6ezLZSoopgaycRERERvbCxY8ciODgYjRo1kn0VqsImTpyI48ePo3379qhVq5bufP78+cJiJicno6SkBFFRURg/frzuvLS0FDExMdixY4ew2FplZWU4duwYbGxssGvXLmRkZGDIkCFo1aqV0LiBgYEwMjLC0KFDAQA//vgj7ty5o8gg+L59+yIpKUlK9aGDg8MzK6Q6d+4sNK6Hhwfmzp2LSZMmITExEb/88gsWL16syLbQ4OBg9O7dW8o8PJltpUQVwYo0IiIiInphDx48QP/+/dGqVSu9BAdn3NDL1LNnT/Ts2VPRmPfu3cPRo0dx7949ZGZm6s5r1KiBgIAARe5w48YNpKenw8bGBlZWVti2bRvMzMyEx83KykJSUpLu8axZszBgwADhcQG51YeyKqRkbgvduXMnNmzYoHem1Dw8mW2lRBXBRBoRERERvbDRo0fLvgJVA25ubvj1119x8OBBlJaWokuXLmjTpo3QmEOGDMGQIUOQkZGBrl276s7v3r2r2OysyZMnw8nJCcDjJE+nTp0wdepUrFy5UmhcjUaD27dvw8TEBABw+/ZtxaqVTE1N4eTkpGj1oZanpycmT56seIWUzG2hMufhyWwrJaoIJtKIiIiI6IV17twZu3fvxoEDB3QJDgcHB9nXoiomMTERS5cuhYODA9RqNfz8/DBmzBi4u7sLj11cXIyIiAj4+vrC3d0dhYWFmDZtGgYNGiQ8dlFREYYNGwYAqFWrFoYOHYr169cLj/vxxx9jyJAhsLOzg0ajQXp6Ory9vYXHBeRUH2rJqpCaPXs2pk2bhnPnzsHGxka3LVQJMufhtWvXDunp6VLaSokqgjPSiIiIiOiFrVixAtu3b4ezszM0Gg02b96MPn36YMyYMbKvRlWIi4sLVq9ejfr16wMACgsL4eXlhS1btgiPPXjwYISFheHUqVM4fPgwZs2aBU9PT/z000/CYw8dOhRjx47Fe++9BwDYv38/li5dih9++EFo3JKSEsTGxuLOnTswNTWFRqPBq6++qkjL3dy5c2FnZ4cuXbroVaQpwc3NTWqFlIxtoTLn4fXo0QM3btzQO1OqrZSoIliRRkREREQvLCkpCXFxcbrB4EOHDsWgQYOYSKOXSq1W65JoAGBmZqboplhra2tER0dj4MCBqFu3Lh49eqRI3Dlz5mDKlCmYOnUqAKBJkyYIDw8XHtff3x8FBQVo0aIF8vLydOdKJNI6duyIrVu3Ys6cOWjdujXs7Ozw3nvvoWHDhsJjy6qQkrUtFJA7D09mWylRRTCRRkREREQvTKPR6G3Xq127tt5sIaKXoXXr1ggLC9O1csbHx8Pa2lqR2A0aNEBISAhOnz6NiIgILFiw4KmtjqK0adMGW7Zswc2bN1GzZk3FKpUuXLiAlJQURWI9ycnJCU5OTigtLUV8fDyioqIQFBSkSJWSrMH706dPf+a2UCXInIcns62UqCLY2klERERELyw0NBTXr1+Hm5sbgMezrBo1aoSZM2dKvhlVJQ8ePEBUVBQyMzOh0WjQpUsXjB07VpHE0t27d5GWlob27dvjjTfewLp16+Di4gJjY2NkZWWhbdu2Lz1mUFAQQkJC4Onp+czKO9GVSt7e3pg9e7aUxM4333yDQ4cO4dy5c2jTpg26dOkCW1tbtGrVSvG7KMXDwwPr1q2TEnvjxo1Yvnz5U/PwlJg/KLOtlKgimEgjIiIiohem0Wiwfv16HDhwABqNBra2tvjwww9ZlUbVgqiZWqtWrcKIESNw8ODBZz7fuXPnlx4TgC5xV1hYiGvXrsHa2lqvOkmJVsNhw4bh2rVrcHZ2hq2tLTp27AgjIyPhcQF5FVIpKSlIS0tTfFsoIHce3sCBA/XaSgFgwIABSE5OFh6bqCKYSCMiIiKiCisoKEDDhg1x9erVZz4vo5KFqh5tosra2lqvMkuj0VSKoeSurq5ITEx86V/X0dER27Ztg7u7O+Lj41/61/8zf5a40xKVwHvS/fv3cejQIRw8eBDp6ekwMTFBbGys8LiyKqRGjRqFhw8folmzZnrn8+fPFxoXAHx9fXXz8Mr/GVMitrOzM9atW6fXVurh4YHNmzcLj01UEfyIkIiIiIgqbObMmYiJicHw4cOfmeDYuXOnxNtRVaGt9srOzn7quZKSEqWv8xRRCw+aNm2KXr164ebNm+jTp4/uXPSfL6USZc+jTaLt378fmZmZMDExQa9evRSJLWvw/o0bN6RtC5U5D+/jjz/GkCFDnmorJaqsmEgjIiIiogqLiYkBAPz000+oV6+e3nPlt/wRvQwffvih3hB4tVqNwYMHV9nKlRUrVuD333+Hj48PvvrqK9nXUZSDgwO6du2KXr16YfTo0TAzM1MstqzB+7K2hQLA66+/jqtXr0qpInZ2dsa9e/d0baWenp4cC0CVGr87iYiIiKjCrl27Bo1GA29vb6xYsQLaqSFlZWUYNWqUtAoHqlq8vLx07Yblt3QaGBjA3t5e1rWE++OPP9C0aVN8/fXXsq+iuL179+K3337DoUOHkJSUBFtbW8U2tMqqkJKxLbT8PDxnZ2cp8/D8/f11baXlP4BRYj4bUUUwkUZEREREFabdoJifnw8PDw/duYGBAXr37i3vYlSlaN/Mh4aGVspNsKLGTj/ZOl0+TlVvnd6yZQuio6Ph4OAAtVoNX19f+Pr6KrJFUlaF1L59+4THeNK4ceMUj/kkmW2lRBXBZQNERERE9MKWL1+uq9jQzm8ietkePnyIPXv24N69ewAeVz7m5eVhwoQJisQ/d+4cioqK9BJanTp1wuXLl2FhYSEsbnp6Ouzs7IR9/crIxcUFq1evRv369QEAhYWF8PLywpYtW4THljV4X9a2UNm8vb0xe/ZsLqehfwxWpBERERHRC2vXrh2GDRuG2NhYXLx4EaNGjUJERAQ6dOgg+2pUhUyaNAlFRUXIzc2FjY0NMjMzFfsemzNnDtLT0/USZiqVCmvWrBGaRAOAyMjIapdIU6vVuiQaAJiZmSmWoJdVITV37lwYGRlh3rx5AB5vCw0ODha+LVSWytBWSlQRTKQRERER0QtbuHAhFi5cCAB46623sHz5ckydOhUbN26UfDOqSnJycrB9+3aEhYVh8ODB8Pf3h7+/vyKxf/nlF6SkpMDQ0FCReOVZWFggMDAQ77zzjl78qjxDqnXr1ggLC9O1csbHxys2I03W4H1Z20JlqQxtpUQVwUQaEREREb2whw8folWrVrrHLVq0QGlpqcQbUVVkbm4OlUoFS0tL5OTkwNXVFY8ePVIktoWFhbBZaH9FW5l14sQJvfOqnEgLDQ1FdHQ0ZsyYAY1GA1tbWwQHBwuNKbtCSta2UFk6d+4s+wpEFcJEGhERERG9sLfeegsRERFwcXGBSqXCli1b8Oabb8q+FlUxVlZWCAkJwUcffYTJkycjPz9fseSWqakpnJyc0L59e9SqVUt3LnpuVvkYRUVFMDU1FR6vMjA0NMSUKVOe+dzo0aMRExPz0mPKrpCStS2UiP43XDZARERERC+sqKgIX3zxBQ4dOgQDAwPY2Nhg/PjxePXVV2VfjaqQsrIyHDt2DDY2Nti1axcyMjIwZMgQvWpIURISEp557ubmJjx2dnY2/P398eDBA2zYsAHDhw/HkiVL0LZtW+GxKyNXV1ckJibKvsZLV1JSgtjYWN22UI1Gg1dffbVKVx4S/ROxIo2IiIiIXpipqSkmT56M3NxctGrVCg8ePKjym+ZIeTdu3EB6ejpsbGxgZWWFbdu2wczMTJHYbm5u+PXXX3Hw4EGUlpaiS5cuaNOmjSKxQ0JCsGzZMkyaNAmNGzfG7NmzERwcjPj4eEXiVzZVdSuwv7+/bltoXl6e7pyJNKLK5RXZFyAiIiKif76MjAy4uLjA19cXhYWFsLOzw759+2Rfi6qYyZMn6zZkNm7cGJ06dcLUqVMViZ2YmAhfX1/k5eXh6tWr8PPzUyyRVVxcjBYtWuged+/eHSUlJYrEJuVcuHABcXFxWLBgAebPn6/7h4gqFybSiIiIiOiFff755/jhhx9gYmKCBg0aYN26dQgPD5d9LapiioqKMGzYMABArVq1MHToUNy8eVOR2KtWrUJcXBymT5+OGTNmIC4uDqtXr1Ykdr169ZCdna2rxEpKSqo2s9KqE+22UCKq3NjaSUREREQvTK1Wo2HDhrrHLVu2lHgbqqoMDQ2xe/duvPfeewCA/fv3w8jISJHYarVatz0TAMzMzBRrMQwICMDcuXNx7tw52NjY4I033kBERIQisSujqjbmW/a2UCL63zCRRkREREQv7LXXXkN6ejpUKhVu376NdevWoWnTprKvRVXMnDlzMGXKFF07Z5MmTRSrfGzdujXCwsLg7u4OAIiPj4e1tbUisYODg1FSUoKxY8fC1dUVTZo0USRuZfCsTaVVbWaY7G2hRPS/4dZOIiIiInphf/zxB8LCwrB//36o1WrY2tpi5syZaNSokeyrURV08+ZN1KxZE8bGxorFfPDgAaKiopCZmQmNRoMuXbpg7Nixit3hv//9L7Zs2YKUlBTUq1cPLi4uuqReVXT27FkEBARwUykRVTpMpBERERERUaUWFBSEkJAQXQvck6pL69v9+/exc+dOrFq1Cnfv3sX27dtlX0kYDw8PzJ07F5MmTUJiYiJ++eUXLF68uNpuKiWiyoOtnURERERUYfb29s+dE7Vz504Fb0NV1VtvvQVATgucm5sbEhISYG1trfe9rtFooFKpcPbsWeF32LFjBzZv3owTJ07Azs4OM2fORIcOHYTHlelZm0oXLlwo8UZERI8xkUZEREREFbZ27VrZV6Bq4Mcff8SIESMQHh6ueEVSQkICACA7O/up50pKShS5Q1JSElxcXLBo0SLUrFlTkZiycVMpEVVWbO0kIiIiogpLTEx87vNVbSg4yTFy5EicO3cON2/e1Ju7p60KU6Ly8cMPP8SGDRt0j9VqNVxcXLB582bhsauj3NxcTJs2DadOnYKhoaFuU6m2OpGISBYm0oiIiIiowgIDA5/7/Pz58xSdQ0gAAAipSURBVBW6CVVlarUav//+O3x8fPDVV1899XyzZs2Exfby8sLBgwefOjcwMIC9vT2ioqKExabHc+HUarWiiyWIiJ6HiTQiIiIieikePXqEixcvoqysDFZWVjAw4BQRejkKCgrQsGFDXL169ZnPN23aVPgdQkNDMXPmTOFx6LHDhw/ju+++Q1FRkd55dVksQUSVFxNpRERERPTCTp8+jfHjx6NevXpQq9W4ceMGli1bhnfeeUf21agKGD16NGJiYnTLLcq/hVGqtfPhw4fYs2cP7t27BwAoKytDXl4eJkyYIDx2deTg4AA/P7+nkqSdO3eWdCMioseYSCMiIiKiFzZs2DAEBgbqEmfHjx9HaGio4oPhqWpLT0+HnZ2dlNh+fn4oKipCbm4ubGxskJmZiQ4dOrC1UxAPDw+sW7dO9jWIiJ7yiuwLEBEREdE/3/379/Wqz9599108fPhQ4o2oKoqMjJQWOycnB2vWrMH777+PTz/9FOvXr8eVK1ek3aeq8/T0xOTJkxEfH4/ExETdP0REsnFwBRERERG9MFNTU6SlpcHBwQEAkJaWhnr16km+FVU1FhYWuspHQ0ND3bkS22HNzc2hUqlgaWmJnJwcuLq64tGjR8LjVlcbN27Ew4cPceTIEb1zbgImItmYSCMiIiKiFxYSEoLRo0fjs88+053FxsZKvBFVRfXr1wcAnDhxQu9cieSKlZUVQkJC8NFHH2Hy5MnIz88Hp+SIc+PGDSQkJMi+BhHRU5hIIyIiIqIXtmfPHhgZGSEhIQG5ubkICAjAwYMHYWlpKftqVIXMnz8fAFBUVARTU1NFY8+ePRvHjh1Dy5YtMX78eGRkZGDRokWK3qE6adeuHdLT09GrVy/UqFFD9nWIiHQ4I42IiIiIXtiPP/6I9evXo06dOrC2tsZPP/2E77//Xva1qIrJzs5G//794eLiguvXr+P9999HVlaWIrFv3LiB9PR0AI+r027dugUzMzNFYldHO3fuxJgxY9C2bVtYW1vD2toabdq0kX0tIiIm0oiIiIjoxT169Ag1a9bUPS7/a6KXJSQkBMuWLUO9evXQuHFjzJ49G8HBwYrEnjx5MiwsLAAAjRs3RqdOnTB16lRFYldH+/btQ3Z2tt4/Z8+elX0tIiK2dhIRERHRi3NwcMD//d//wdHRESqVCqmpqejTp4/sa1EVU1xcjBYtWuged+/eHQsXLlQkdlFREYYNGwYAqFWrFoYOHYr169crErs6Ki4uxtKlS5GRkYGysjLY2tpiwoQJqFOnjuyrEVE1x4o0IiIiInphU6ZMgaenJy5evIjc3Fx4eXnB399f9rWoiqlXrx6ys7OhUqkAAElJSYrNSjM0NMTu3bt1j/fv3w8jIyNFYldHc+fORXFxMebNm4eFCxfi0aNHilUfEhE9j0rDVTNERERERPQPkJubi2nTpuHUqVMwNDTEG2+8gcjISEWWWpw9exZTpkxBQUEBAKBJkyYIDw9Hq1athMeujgYOHIikpCS9swEDBiA5OVnSjYiIHmNrJxERERER/SPcv38f69evx/3796FWq2FsbKxY7DZt2mDLli24efMmatasqWjs6kij0eD27dswMTEBANy+fZvbO4moUmAijYiIiIiI/hFmzJiBR48ewdnZGc7Ozooks4KCghASEgJPT09dS2l5a9asEX6H6ujjjz/GkCFDYGdnB41Gg/T0dHh7e8u+FhERE2lERERERPTP8NNPP+HSpUvYunUrvL29Ua9ePbi4uMDd3V1YzLfeegsAMG7cOGEx6GnOzs64d+8e7ty5A1NTU3h6esLAgG9fiUg+zkgjIiIiIqJ/lPv372Pnzp1YtWoV7t69i+3btwuL5ejoiG3btsHd3R3x8fHC4pA+X19fFBQUoEWLFnqVgPPnz5d4KyIiJtKIiIiIiOgfYseOHdi8eTNOnDgBOzs7DBw4EB06dBAac+TIkTh37hxu3ryJRo0a6c41Gg1UKhV27twpNH511b9/f6SkpMi+BhHRU1gbS0RERERE/whJSUlwcXHBokWLULNmTUVirlixAr///jt8fHzw1VdfKRKTgNdffx1Xr15F06ZNZV+FiEgPK9KIiIiIiIj+REFBARo2bIirV68+83kmel4u7VKHwsJCXLt2DdbW1nrbOrncgYhkYyKNiIiIiIjoT4wePRoxMTGwt7eHSqVC+bdPbO18+Q4ePPjc5zt37qzQTYiIno2JNCIiIiIior+Qnp4OOzs72dcgIiLJXpF9ASIiIiIiosouMjJS9hWIiKgSYEUaERERERHRX/Dx8UH9+vXxzjvvwNDQUHfu6uoq8VZERKQ0bu0kIiIiIiL6C/Xr1wcAnDhxQu+ciTQiouqFFWlERERERER/U1FREUxNTWVfg4iIJOGMNCIiIiIior+QnZ2N/v37w8XFBdevX8f777+PrKws2dciIiKFMZFGRERERET0F0JCQrBs2TLUq1cPjRs3xuzZsxEcHCz7WkREpDAm0oiIiIiIiP5CcXExWrRooXvcvXt3lJSUSLwRERHJ8P/au2MTBaIoDKPXBcHAYFIjMTO2AbGAcUBsw9gm7MEeFEEUixCTCcUaZBJBtoR50TyWPaeCP/6Ce4U0AACAFkVRRF3X0ev1IiLicDi4lQbwD3k2AAAA0OL1esV2u43H4xGDwSDG43HsdruYTCa5pwHQISENAACgRV3XMZ1Oo2ma+H6/MRwOc08CIAMhDQAAoMVqtYrP5xNlWUZZljEajXJPAiADIQ0AACDB8/mM0+kU5/M5iqKIqqpivV7nngVAh4Q0AACARE3TxO12i/1+H+/3Oy6XS+5JAHRISAMAAGhxvV7jeDzG/X6PxWIRy+UyZrNZ7lkAdExIAwAAaLHZbKKqqpjP59Hv93PPASATIQ0AAAAAEvzkHgAAAAAAf4GQBgAAAAAJhDQAAAAASCCkAQAAAEACIQ0AAAAAEvwC1WmohTNfDiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap on the correlations between features in the loan data\n",
    "credit_correlations = credit_data.corr()\n",
    "plt.figure(figsize=(20, 20,))\n",
    "plt.imshow(credit_correlations, cmap=None, interpolation='none', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(credit_correlations)), credit_correlations.columns, rotation='vertical')\n",
    "plt.yticks(range(len(credit_correlations)), credit_correlations.columns);\n",
    "plt.suptitle('Loan correlations Heat Map', fontsize=30, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is finished and we can now start to apply machine learning algorithms in order to predict the credit risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's usually better to visualize the data in some way. Visualization makes outliers and errors immediately stand out, whereas they might go unnoticed in a large table of numbers.\n",
    "As the dataset contains to many rows, first we need to get a subset of the original dataset in order to be possiblle to apply the supervised learning algorithms to test our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Classification\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To advance to the data analysis we need to gather the test and the train samples. To do so we will import the function train_test_split from sklearn and use it on thecredit_data data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>...</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>verification_status_Not Verified</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>home_ownership_ANY</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242263</th>\n",
       "      <td>6000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.49</td>\n",
       "      <td>212.40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25540.0</td>\n",
       "      <td>29.23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>26842.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104003</th>\n",
       "      <td>15000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13.68</td>\n",
       "      <td>346.55</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>15.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>124751.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244499</th>\n",
       "      <td>16000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.49</td>\n",
       "      <td>497.63</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>20.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>54820.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669548</th>\n",
       "      <td>15000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>343.84</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>17.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39428.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75598</th>\n",
       "      <td>9600</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.03</td>\n",
       "      <td>292.19</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>37661.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813178</th>\n",
       "      <td>8050</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.86</td>\n",
       "      <td>290.47</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>19.24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9829.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337768</th>\n",
       "      <td>11000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.99</td>\n",
       "      <td>360.08</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>24.26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>23102.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243258</th>\n",
       "      <td>22425</td>\n",
       "      <td>60.0</td>\n",
       "      <td>22.99</td>\n",
       "      <td>632.05</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>17.36</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>182226.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76237</th>\n",
       "      <td>6000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.61</td>\n",
       "      <td>209.79</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>21.14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11056.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759512</th>\n",
       "      <td>12000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.89</td>\n",
       "      <td>375.43</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>67340.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt  term  int_rate  installment  grade  emp_length  annual_inc  \\\n",
       "242263       6000  36.0     16.49       212.40      4         0.0     25540.0   \n",
       "104003      15000  60.0     13.68       346.55      3         0.0     37000.0   \n",
       "244499      16000  36.0      7.49       497.63      1        10.0     85000.0   \n",
       "669548      15000  60.0     13.33       343.84      3         1.0     43000.0   \n",
       "75598        9600  36.0      6.03       292.19      1        10.0     28000.0   \n",
       "813178       8050  36.0     17.86       290.47      4         0.0     27000.0   \n",
       "337768      11000  36.0     10.99       360.08      2         3.0     28000.0   \n",
       "243258      22425  60.0     22.99       632.05      6        10.0     95000.0   \n",
       "76237        6000  36.0     15.61       209.79      3         9.0     23000.0   \n",
       "759512      12000  36.0      7.89       375.43      1         5.0     85000.0   \n",
       "\n",
       "          dti  delinq_2yrs  inq_last_6mths  ...  tot_cur_bal  \\\n",
       "242263  29.23            0               2  ...      26842.0   \n",
       "104003  15.89            0               1  ...     124751.0   \n",
       "244499  20.32            0               0  ...      54820.0   \n",
       "669548  17.64            0               0  ...      39428.0   \n",
       "75598    6.64            0               0  ...      37661.0   \n",
       "813178  19.24            0               1  ...       9829.0   \n",
       "337768  24.26            0               1  ...      23102.0   \n",
       "243258  17.36            4               2  ...     182226.0   \n",
       "76237   21.14            0               2  ...      11056.0   \n",
       "759512  19.71            0               0  ...      67340.0   \n",
       "\n",
       "        verification_status_Not Verified  verification_status_Source Verified  \\\n",
       "242263                                 0                                    0   \n",
       "104003                                 0                                    0   \n",
       "244499                                 1                                    0   \n",
       "669548                                 0                                    1   \n",
       "75598                                  1                                    0   \n",
       "813178                                 0                                    0   \n",
       "337768                                 0                                    1   \n",
       "243258                                 0                                    1   \n",
       "76237                                  0                                    0   \n",
       "759512                                 1                                    0   \n",
       "\n",
       "        verification_status_Verified  home_ownership_ANY  \\\n",
       "242263                             1                   0   \n",
       "104003                             1                   0   \n",
       "244499                             0                   0   \n",
       "669548                             0                   0   \n",
       "75598                              0                   0   \n",
       "813178                             1                   0   \n",
       "337768                             0                   0   \n",
       "243258                             0                   0   \n",
       "76237                              1                   0   \n",
       "759512                             0                   0   \n",
       "\n",
       "        home_ownership_MORTGAGE  home_ownership_NONE  home_ownership_OTHER  \\\n",
       "242263                        0                    0                     0   \n",
       "104003                        1                    0                     0   \n",
       "244499                        1                    0                     0   \n",
       "669548                        1                    0                     0   \n",
       "75598                         0                    0                     0   \n",
       "813178                        0                    0                     0   \n",
       "337768                        0                    0                     0   \n",
       "243258                        1                    0                     0   \n",
       "76237                         0                    0                     0   \n",
       "759512                        1                    0                     0   \n",
       "\n",
       "        home_ownership_OWN  home_ownership_RENT  \n",
       "242263                   0                    1  \n",
       "104003                   0                    0  \n",
       "244499                   0                    0  \n",
       "669548                   0                    0  \n",
       "75598                    1                    0  \n",
       "813178                   0                    1  \n",
       "337768                   0                    1  \n",
       "243258                   0                    0  \n",
       "76237                    0                    1  \n",
       "759512                   0                    0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "credit_data_subset = credit_data.sample(frac=0.10)\n",
    "X = credit_data_subset[credit_data_subset.columns.drop(['default_ind', 'issue_d', 'earliest_cr_line', 'last_credit_pull_d'])] \n",
    "y = credit_data_subset['default_ind']\n",
    "\n",
    "# get a test dataset with 10% of the credit_data_subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the test sample will have results with a size corresponding to 10% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 77032 entries, 250180 to 251605\n",
      "Data columns (total 28 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   loan_amnt                            77032 non-null  int64  \n",
      " 1   term                                 77032 non-null  float64\n",
      " 2   int_rate                             77032 non-null  float64\n",
      " 3   installment                          77032 non-null  float64\n",
      " 4   grade                                77032 non-null  int64  \n",
      " 5   emp_length                           77032 non-null  float64\n",
      " 6   annual_inc                           77032 non-null  float64\n",
      " 7   dti                                  77032 non-null  float64\n",
      " 8   delinq_2yrs                          77032 non-null  int64  \n",
      " 9   inq_last_6mths                       77032 non-null  int64  \n",
      " 10  open_acc                             77032 non-null  int64  \n",
      " 11  pub_rec                              77032 non-null  int64  \n",
      " 12  revol_bal                            77032 non-null  int64  \n",
      " 13  revol_util                           77032 non-null  float64\n",
      " 14  total_acc                            77032 non-null  int64  \n",
      " 15  collections_12_mths_ex_med           77032 non-null  float64\n",
      " 16  acc_now_delinq                       77032 non-null  int64  \n",
      " 17  tot_coll_amt                         77032 non-null  float64\n",
      " 18  tot_cur_bal                          77032 non-null  float64\n",
      " 19  verification_status_Not Verified     77032 non-null  uint8  \n",
      " 20  verification_status_Source Verified  77032 non-null  uint8  \n",
      " 21  verification_status_Verified         77032 non-null  uint8  \n",
      " 22  home_ownership_ANY                   77032 non-null  uint8  \n",
      " 23  home_ownership_MORTGAGE              77032 non-null  uint8  \n",
      " 24  home_ownership_NONE                  77032 non-null  uint8  \n",
      " 25  home_ownership_OTHER                 77032 non-null  uint8  \n",
      " 26  home_ownership_OWN                   77032 non-null  uint8  \n",
      " 27  home_ownership_RENT                  77032 non-null  uint8  \n",
      "dtypes: float64(10), int64(9), uint8(9)\n",
      "memory usage: 12.4 MB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8560 entries, 242263 to 762184\n",
      "Data columns (total 28 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   loan_amnt                            8560 non-null   int64  \n",
      " 1   term                                 8560 non-null   float64\n",
      " 2   int_rate                             8560 non-null   float64\n",
      " 3   installment                          8560 non-null   float64\n",
      " 4   grade                                8560 non-null   int64  \n",
      " 5   emp_length                           8560 non-null   float64\n",
      " 6   annual_inc                           8560 non-null   float64\n",
      " 7   dti                                  8560 non-null   float64\n",
      " 8   delinq_2yrs                          8560 non-null   int64  \n",
      " 9   inq_last_6mths                       8560 non-null   int64  \n",
      " 10  open_acc                             8560 non-null   int64  \n",
      " 11  pub_rec                              8560 non-null   int64  \n",
      " 12  revol_bal                            8560 non-null   int64  \n",
      " 13  revol_util                           8560 non-null   float64\n",
      " 14  total_acc                            8560 non-null   int64  \n",
      " 15  collections_12_mths_ex_med           8560 non-null   float64\n",
      " 16  acc_now_delinq                       8560 non-null   int64  \n",
      " 17  tot_coll_amt                         8560 non-null   float64\n",
      " 18  tot_cur_bal                          8560 non-null   float64\n",
      " 19  verification_status_Not Verified     8560 non-null   uint8  \n",
      " 20  verification_status_Source Verified  8560 non-null   uint8  \n",
      " 21  verification_status_Verified         8560 non-null   uint8  \n",
      " 22  home_ownership_ANY                   8560 non-null   uint8  \n",
      " 23  home_ownership_MORTGAGE              8560 non-null   uint8  \n",
      " 24  home_ownership_NONE                  8560 non-null   uint8  \n",
      " 25  home_ownership_OTHER                 8560 non-null   uint8  \n",
      " 26  home_ownership_OWN                   8560 non-null   uint8  \n",
      " 27  home_ownership_RENT                  8560 non-null   uint8  \n",
      "dtypes: float64(10), int64(9), uint8(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()\n",
    "print()\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Decision Trees\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8898364485981308"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Validate the classifier on the testing set using classification accuracy\n",
    "decision_tree_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved an accuracy of aproximately 89%. But let's see how this accuracy varies depending on how our training and testing set was sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(100):\n",
    "    (training_inputs, testing_inputs, training_classes, testing_classes) = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing_classes)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "    \n",
    "plt.hist(model_accuracies)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f10c0a2a90>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,101,1)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(x,model_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  0.8873831775700934\n",
      "Max:  0.9049065420560748\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", min(model_accuracies))\n",
    "print(\"Max: \", max(model_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing this results, we conclude that the accuracy of the model doesn't vary much, around 3%. This is called **overfitting**, and happens because the model is learning to classify the training set so well that it doesn't generalize and perform well on data it hasn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1: Cross Validation\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to escape from *overfitting* problem, we can try to perform **k-fold cross-validation** on our model to estimate the skill of our machine learning model on unseen data. \n",
    "\n",
    "This procedure randomly splits the original dataset into *k* groups of approximately equal size, and then takes one of the subsets as training set and the other *k-1* subsets are used to fit the model. This process is then repeated *k* times such that each subset is used as the testing set exactly once.\n",
    "\n",
    "10-fold cross-validation is the most common choice, so let's use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  0.890303738317757\n",
      "Max:  0.8983526112863652\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", min(cv_scores))\n",
    "print(\"Max: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a more consistent rating of our classifier's general classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2: Parameter Tuning\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Machine Learning model comes with a variety of parameters to tune, and these parameters can be vitally important to the performance of our classifier.\n",
    "\n",
    "The most common method for model parameter tuning is **Grid Search**. The idea behind Grid Search is simple: explore a range of parameters and find the best-performing parameter combination. Focus your search on the best range of parameters, then repeat this process several times until the best parameters are discovered.\n",
    "\n",
    "Let's tune our decision tree classifier. Let's start by tuning only two parameters and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-63462652f9de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                            cv=cross_validation)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best score: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best parameters: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'max_depth': range(1,10),\n",
    "                  'max_features': range(1, 10)}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the grid search to see how the parameters interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_visualization = grid_search.cv_results_['mean_test_score']\n",
    "grid_visualization.shape = (9, 9)\n",
    "sb.heatmap(grid_visualization, cmap='Blues', annot=True)\n",
    "plt.xticks(np.arange(9) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(9) + 0.5, grid_search.param_grid['max_depth'])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a better sense of the parameter space.\n",
    "\n",
    "To allow the decision tree to make more than a one-off decision, the Decision Tree needs to have at least a `max_depth` of 2.\n",
    "\n",
    "`max_features` parameter doesn't seem to make a huge difference in the results, as they are very similar independently of the number of *max_features*.\n",
    "\n",
    "Now, let's apply grid search tunning more parameters in order to find the best options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': range(1, 10),\n",
    "                  'max_features': range(1,10)}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = grid_search.best_estimator_\n",
    "decision_tree_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the best classifier, that allows our model to achieve a score of aproximately 95%. As the other Decision Tree models that we have calculated have an accuracy around 90%, it's a huge improve.\n",
    "\n",
    "We can also visualize the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "\n",
    "tree.plot_tree(decision_tree_classifier)\n",
    "plt.figure(figsize=(15, 150))\n",
    "plt.show()\n",
    "\n",
    "with open('iris_dtc.dot', 'w') as out_file:\n",
    "    out_file = tree.export_graphviz(decision_tree_classifier, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: K-Nearest Neighbor\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The K-Nearest Neighbors algorithm (k-NN) is a non-parametric classification method. The input consists of the k closest training examples in data set. The output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "Let's start to apply this algorithmn to our model with k=5 and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "yk_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, yk_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k=5, we have an accuracy of 93.97%, which is a good result. But let's see how the accuracy varies when k goes from 1 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy for k ranging from 1 to 20\n",
    "k_range = range(1, 20)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of K and scores\n",
    "column_dict = {'K':k_range, 'accuracy score':scores}\n",
    "df = pd.DataFrame(column_dict).set_index('K')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the relationship between K and Accuracy score.\n",
    "df.plot(y='accuracy score');\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Accuracy Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graphic, we can conclude that for the first 10 values the accuracy is a little bit variable, but then, from k= 10 to 20, the accuracy is more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum accuracy score and the associated K value.\n",
    "df.sort_values('accuracy score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using KNN on this data set with the selected features, we achieve the best accuracy when k=8, when the accuracy is 94.26%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronal Networks ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data_subset = credit_data.sample(frac=0.1)\n",
    "X = credit_data_subset[credit_data_subset.columns.drop(['grade', 'issue_d', 'earliest_cr_line', 'last_credit_pull_d'])] \n",
    "y = credit_data_subset['default_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test dataset with 10% of the credit_data_subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "ANNClassifier = MLPClassifier(random_state=1, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=500, random_state=1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier on the training set\n",
    "ANNClassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16218     0]\n",
      " [    1   900]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     16218\n",
      "           1       1.00      1.00      1.00       901\n",
      "\n",
      "    accuracy                           1.00     17119\n",
      "   macro avg       1.00      1.00      1.00     17119\n",
      "weighted avg       1.00      1.00      1.00     17119\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = ANNClassifier.predict(X_test)\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test,predictions)\n",
    "print(confusion_matrix_ann)\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "sn.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "sn.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96501358\n",
      "Iteration 2, loss = 1.77978874\n",
      "Iteration 3, loss = 1.60959287\n",
      "Iteration 4, loss = 1.45961152\n",
      "Iteration 5, loss = 1.33062829\n",
      "Iteration 6, loss = 1.22370404\n",
      "Iteration 7, loss = 1.13708002\n",
      "Iteration 8, loss = 1.06561982\n",
      "Iteration 9, loss = 1.00442263\n",
      "Iteration 10, loss = 0.95273521\n",
      "Iteration 11, loss = 0.90929407\n",
      "Iteration 12, loss = 0.87149916\n",
      "Iteration 13, loss = 0.83835609\n",
      "Iteration 14, loss = 0.80860720\n",
      "Iteration 15, loss = 0.78214093\n",
      "Iteration 16, loss = 0.75802249\n",
      "Iteration 17, loss = 0.73627140\n",
      "Iteration 18, loss = 0.71663540\n",
      "Iteration 19, loss = 0.69867800\n",
      "Iteration 20, loss = 0.68278674\n",
      "Iteration 21, loss = 0.66797241\n",
      "Iteration 22, loss = 0.65485142\n",
      "Iteration 23, loss = 0.64312252\n",
      "Iteration 24, loss = 0.63225976\n",
      "Iteration 25, loss = 0.62210465\n",
      "Iteration 26, loss = 0.61354512\n",
      "Iteration 27, loss = 0.60536455\n",
      "Iteration 28, loss = 0.59807495\n",
      "Iteration 29, loss = 0.59177620\n",
      "Iteration 30, loss = 0.58549047\n",
      "Iteration 31, loss = 0.58020074\n",
      "Iteration 32, loss = 0.57496349\n",
      "Iteration 33, loss = 0.57037465\n",
      "Iteration 34, loss = 0.56577852\n",
      "Iteration 35, loss = 0.56185447\n",
      "Iteration 36, loss = 0.55799551\n",
      "Iteration 37, loss = 0.55465522\n",
      "Iteration 38, loss = 0.55120122\n",
      "Iteration 39, loss = 0.54826688\n",
      "Iteration 40, loss = 0.54555142\n",
      "Iteration 41, loss = 0.54248206\n",
      "Iteration 42, loss = 0.53982569\n",
      "Iteration 43, loss = 0.53738329\n",
      "Iteration 44, loss = 0.53488889\n",
      "Iteration 45, loss = 0.53283951\n",
      "Iteration 46, loss = 0.53064255\n",
      "Iteration 47, loss = 0.52870015\n",
      "Iteration 48, loss = 0.52685571\n",
      "Iteration 49, loss = 0.52512904\n",
      "Iteration 50, loss = 0.52319633\n",
      "Iteration 51, loss = 0.52152683\n",
      "Iteration 52, loss = 0.51965407\n",
      "Iteration 53, loss = 0.51836678\n",
      "Iteration 54, loss = 0.51678791\n",
      "Iteration 55, loss = 0.51526895\n",
      "Iteration 56, loss = 0.51382325\n",
      "Iteration 57, loss = 0.51251073\n",
      "Iteration 58, loss = 0.51119544\n",
      "Iteration 59, loss = 0.50985149\n",
      "Iteration 60, loss = 0.50865763\n",
      "Iteration 61, loss = 0.50751004\n",
      "Iteration 62, loss = 0.50616146\n",
      "Iteration 63, loss = 0.50529017\n",
      "Iteration 64, loss = 0.50381413\n",
      "Iteration 65, loss = 0.50306431\n",
      "Iteration 66, loss = 0.50184396\n",
      "Iteration 67, loss = 0.50085549\n",
      "Iteration 68, loss = 0.49991996\n",
      "Iteration 69, loss = 0.49911594\n",
      "Iteration 70, loss = 0.49800807\n",
      "Iteration 71, loss = 0.49696660\n",
      "Iteration 72, loss = 0.49608913\n",
      "Iteration 73, loss = 0.49526081\n",
      "Iteration 74, loss = 0.49454992\n",
      "Iteration 75, loss = 0.49353349\n",
      "Iteration 76, loss = 0.49281934\n",
      "Iteration 77, loss = 0.49177073\n",
      "Iteration 78, loss = 0.49095619\n",
      "Iteration 79, loss = 0.49038847\n",
      "Iteration 80, loss = 0.48946089\n",
      "Iteration 81, loss = 0.48889025\n",
      "Iteration 82, loss = 0.48786606\n",
      "Iteration 83, loss = 0.48733287\n",
      "Iteration 84, loss = 0.48660718\n",
      "Iteration 85, loss = 0.48580748\n",
      "Iteration 86, loss = 0.48523237\n",
      "Iteration 87, loss = 0.48438266\n",
      "Iteration 88, loss = 0.48370452\n",
      "Iteration 89, loss = 0.48286438\n",
      "Iteration 90, loss = 0.48257522\n",
      "Iteration 91, loss = 0.48172233\n",
      "Iteration 92, loss = 0.48116465\n",
      "Iteration 93, loss = 0.48037359\n",
      "Iteration 94, loss = 0.47986714\n",
      "Iteration 95, loss = 0.47900839\n",
      "Iteration 96, loss = 0.47854490\n",
      "Iteration 97, loss = 0.47792115\n",
      "Iteration 98, loss = 0.47711496\n",
      "Iteration 99, loss = 0.47673546\n",
      "Iteration 100, loss = 0.47620445\n",
      "Iteration 101, loss = 0.47567464\n",
      "Iteration 102, loss = 0.47486370\n",
      "Iteration 103, loss = 0.47444572\n",
      "Iteration 104, loss = 0.47379410\n",
      "Iteration 105, loss = 0.47325520\n",
      "Iteration 106, loss = 0.47261058\n",
      "Iteration 107, loss = 0.47227186\n",
      "Iteration 108, loss = 0.47170074\n",
      "Iteration 109, loss = 0.47112324\n",
      "Iteration 110, loss = 0.47076402\n",
      "Iteration 111, loss = 0.46998320\n",
      "Iteration 112, loss = 0.46942671\n",
      "Iteration 113, loss = 0.46913046\n",
      "Iteration 114, loss = 0.46835322\n",
      "Iteration 115, loss = 0.46780285\n",
      "Iteration 116, loss = 0.46754128\n",
      "Iteration 117, loss = 0.46723184\n",
      "Iteration 118, loss = 0.46657056\n",
      "Iteration 119, loss = 0.46595091\n",
      "Iteration 120, loss = 0.46536252\n",
      "Iteration 121, loss = 0.46507397\n",
      "Iteration 122, loss = 0.46469922\n",
      "Iteration 123, loss = 0.46409664\n",
      "Iteration 124, loss = 0.46366424\n",
      "Iteration 125, loss = 0.46322961\n",
      "Iteration 126, loss = 0.46258848\n",
      "Iteration 127, loss = 0.46250430\n",
      "Iteration 128, loss = 0.46187420\n",
      "Iteration 129, loss = 0.46142262\n",
      "Iteration 130, loss = 0.46083597\n",
      "Iteration 131, loss = 0.46061998\n",
      "Iteration 132, loss = 0.46008142\n",
      "Iteration 133, loss = 0.45991057\n",
      "Iteration 134, loss = 0.45903036\n",
      "Iteration 135, loss = 0.45887786\n",
      "Iteration 136, loss = 0.45833112\n",
      "Iteration 137, loss = 0.45783059\n",
      "Iteration 138, loss = 0.45739373\n",
      "Iteration 139, loss = 0.45704112\n",
      "Iteration 140, loss = 0.45658763\n",
      "Iteration 141, loss = 0.45627754\n",
      "Iteration 142, loss = 0.45601586\n",
      "Iteration 143, loss = 0.45534468\n",
      "Iteration 144, loss = 0.45499104\n",
      "Iteration 145, loss = 0.45503708\n",
      "Iteration 146, loss = 0.45428757\n",
      "Iteration 147, loss = 0.45390249\n",
      "Iteration 148, loss = 0.45345388\n",
      "Iteration 149, loss = 0.45309827\n",
      "Iteration 150, loss = 0.45273925\n",
      "Iteration 151, loss = 0.45247461\n",
      "Iteration 152, loss = 0.45219782\n",
      "Iteration 153, loss = 0.45157422\n",
      "Iteration 154, loss = 0.45139940\n",
      "Iteration 155, loss = 0.45109090\n",
      "Iteration 156, loss = 0.45073238\n",
      "Iteration 157, loss = 0.45035121\n",
      "Iteration 158, loss = 0.45016159\n",
      "Iteration 159, loss = 0.44970194\n",
      "Iteration 160, loss = 0.44953365\n",
      "Iteration 161, loss = 0.44912649\n",
      "Iteration 162, loss = 0.44866190\n",
      "Iteration 163, loss = 0.44832631\n",
      "Iteration 164, loss = 0.44830965\n",
      "Iteration 165, loss = 0.44789509\n",
      "Iteration 166, loss = 0.44759439\n",
      "Iteration 167, loss = 0.44714449\n",
      "Iteration 168, loss = 0.44691383\n",
      "Iteration 169, loss = 0.44653968\n",
      "Iteration 170, loss = 0.44634397\n",
      "Iteration 171, loss = 0.44590992\n",
      "Iteration 172, loss = 0.44586121\n",
      "Iteration 173, loss = 0.44537942\n",
      "Iteration 174, loss = 0.44511166\n",
      "Iteration 175, loss = 0.44477219\n",
      "Iteration 176, loss = 0.44474636\n",
      "Iteration 177, loss = 0.44429639\n",
      "Iteration 178, loss = 0.44404223\n",
      "Iteration 179, loss = 0.44396444\n",
      "Iteration 180, loss = 0.44347263\n",
      "Iteration 181, loss = 0.44316143\n",
      "Iteration 182, loss = 0.44325874\n",
      "Iteration 183, loss = 0.44281976\n",
      "Iteration 184, loss = 0.44269981\n",
      "Iteration 185, loss = 0.44211621\n",
      "Iteration 186, loss = 0.44186765\n",
      "Iteration 187, loss = 0.44179722\n",
      "Iteration 188, loss = 0.44146336\n",
      "Iteration 189, loss = 0.44133331\n",
      "Iteration 190, loss = 0.44093755\n",
      "Iteration 191, loss = 0.44101320\n",
      "Iteration 192, loss = 0.44066601\n",
      "Iteration 193, loss = 0.44029558\n",
      "Iteration 194, loss = 0.44014781\n",
      "Iteration 195, loss = 0.43986372\n",
      "Iteration 196, loss = 0.43977245\n",
      "Iteration 197, loss = 0.43942768\n",
      "Iteration 198, loss = 0.43924471\n",
      "Iteration 199, loss = 0.43916851\n",
      "Iteration 200, loss = 0.43906052\n",
      "Iteration 201, loss = 0.43878406\n",
      "Iteration 202, loss = 0.43828026\n",
      "Iteration 203, loss = 0.43811825\n",
      "Iteration 204, loss = 0.43793471\n",
      "Iteration 205, loss = 0.43805532\n",
      "Iteration 206, loss = 0.43766399\n",
      "Iteration 207, loss = 0.43734175\n",
      "Iteration 208, loss = 0.43718942\n",
      "Iteration 209, loss = 0.43711277\n",
      "Iteration 210, loss = 0.43693162\n",
      "Iteration 211, loss = 0.43662172\n",
      "Iteration 212, loss = 0.43648866\n",
      "Iteration 213, loss = 0.43636598\n",
      "Iteration 214, loss = 0.43655317\n",
      "Iteration 215, loss = 0.43589629\n",
      "Iteration 216, loss = 0.43561094\n",
      "Iteration 217, loss = 0.43551347\n",
      "Iteration 218, loss = 0.43535708\n",
      "Iteration 219, loss = 0.43559242\n",
      "Iteration 220, loss = 0.43496899\n",
      "Iteration 221, loss = 0.43494309\n",
      "Iteration 222, loss = 0.43478017\n",
      "Iteration 223, loss = 0.43474221\n",
      "Iteration 224, loss = 0.43448263\n",
      "Iteration 225, loss = 0.43423443\n",
      "Iteration 226, loss = 0.43411815\n",
      "Iteration 227, loss = 0.43396732\n",
      "Iteration 228, loss = 0.43378342\n",
      "Iteration 229, loss = 0.43353148\n",
      "Iteration 230, loss = 0.43355439\n",
      "Iteration 231, loss = 0.43322448\n",
      "Iteration 232, loss = 0.43297757\n",
      "Iteration 233, loss = 0.43287870\n",
      "Iteration 234, loss = 0.43281198\n",
      "Iteration 235, loss = 0.43284510\n",
      "Iteration 236, loss = 0.43249099\n",
      "Iteration 237, loss = 0.43218608\n",
      "Iteration 238, loss = 0.43251871\n",
      "Iteration 239, loss = 0.43203113\n",
      "Iteration 240, loss = 0.43210087\n",
      "Iteration 241, loss = 0.43196811\n",
      "Iteration 242, loss = 0.43173076\n",
      "Iteration 243, loss = 0.43142238\n",
      "Iteration 244, loss = 0.43140881\n",
      "Iteration 245, loss = 0.43111669\n",
      "Iteration 246, loss = 0.43136605\n",
      "Iteration 247, loss = 0.43094100\n",
      "Iteration 248, loss = 0.43081683\n",
      "Iteration 249, loss = 0.43073746\n",
      "Iteration 250, loss = 0.43062611\n",
      "Iteration 251, loss = 0.43091092\n",
      "Iteration 252, loss = 0.43051076\n",
      "Iteration 253, loss = 0.43046301\n",
      "Iteration 254, loss = 0.43019714\n",
      "Iteration 255, loss = 0.42998039\n",
      "Iteration 256, loss = 0.43013429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.42979431\n",
      "Iteration 258, loss = 0.42974121\n",
      "Iteration 259, loss = 0.42965349\n",
      "Iteration 260, loss = 0.42946302\n",
      "Iteration 261, loss = 0.42950811\n",
      "Iteration 262, loss = 0.42926681\n",
      "Iteration 263, loss = 0.42921178\n",
      "Iteration 264, loss = 0.42885570\n",
      "Iteration 265, loss = 0.42871226\n",
      "Iteration 266, loss = 0.42921496\n",
      "Iteration 267, loss = 0.42880240\n",
      "Iteration 268, loss = 0.42858387\n",
      "Iteration 269, loss = 0.42840260\n",
      "Iteration 270, loss = 0.42814478\n",
      "Iteration 271, loss = 0.42835218\n",
      "Iteration 272, loss = 0.42836002\n",
      "Iteration 273, loss = 0.42814077\n",
      "Iteration 274, loss = 0.42794222\n",
      "Iteration 275, loss = 0.42807856\n",
      "Iteration 276, loss = 0.42796838\n",
      "Iteration 277, loss = 0.42775948\n",
      "Iteration 278, loss = 0.42751025\n",
      "Iteration 279, loss = 0.42764945\n",
      "Iteration 280, loss = 0.42765701\n",
      "Iteration 281, loss = 0.42752479\n",
      "Iteration 282, loss = 0.42731434\n",
      "Iteration 283, loss = 0.42694637\n",
      "Iteration 284, loss = 0.42731503\n",
      "Iteration 285, loss = 0.42687557\n",
      "Iteration 286, loss = 0.42678657\n",
      "Iteration 287, loss = 0.42672318\n",
      "Iteration 288, loss = 0.42665041\n",
      "Iteration 289, loss = 0.42655510\n",
      "Iteration 290, loss = 0.42647319\n",
      "Iteration 291, loss = 0.42649459\n",
      "Iteration 292, loss = 0.42662261\n",
      "Iteration 293, loss = 0.42618083\n",
      "Iteration 294, loss = 0.42637474\n",
      "Iteration 295, loss = 0.42606751\n",
      "Iteration 296, loss = 0.42594469\n",
      "Iteration 297, loss = 0.42584992\n",
      "Iteration 298, loss = 0.42586451\n",
      "Iteration 299, loss = 0.42608493\n",
      "Iteration 300, loss = 0.42555468\n",
      "Iteration 301, loss = 0.42568513\n",
      "Iteration 302, loss = 0.42572520\n",
      "Iteration 303, loss = 0.42553287\n",
      "Iteration 304, loss = 0.42543751\n",
      "Iteration 305, loss = 0.42520554\n",
      "Iteration 306, loss = 0.42507068\n",
      "Iteration 307, loss = 0.42516125\n",
      "Iteration 308, loss = 0.42505809\n",
      "Iteration 309, loss = 0.42483890\n",
      "Iteration 310, loss = 0.42502952\n",
      "Iteration 311, loss = 0.42469781\n",
      "Iteration 312, loss = 0.42467709\n",
      "Iteration 313, loss = 0.42465336\n",
      "Iteration 314, loss = 0.42466274\n",
      "Iteration 315, loss = 0.42462177\n",
      "Iteration 316, loss = 0.42465434\n",
      "Iteration 317, loss = 0.42449790\n",
      "Iteration 318, loss = 0.42439976\n",
      "Iteration 319, loss = 0.42441698\n",
      "Iteration 320, loss = 0.42419019\n",
      "Iteration 321, loss = 0.42430229\n",
      "Iteration 322, loss = 0.42429024\n",
      "Iteration 323, loss = 0.42418809\n",
      "Iteration 324, loss = 0.42375651\n",
      "Iteration 325, loss = 0.42366731\n",
      "Iteration 326, loss = 0.42367039\n",
      "Iteration 327, loss = 0.42366738\n",
      "Iteration 328, loss = 0.42371840\n",
      "Iteration 329, loss = 0.42382693\n",
      "Iteration 330, loss = 0.42390389\n",
      "Iteration 331, loss = 0.42328998\n",
      "Iteration 332, loss = 0.42333893\n",
      "Iteration 333, loss = 0.42340708\n",
      "Iteration 334, loss = 0.42333813\n",
      "Iteration 335, loss = 0.42302278\n",
      "Iteration 336, loss = 0.42309845\n",
      "Iteration 337, loss = 0.42317982\n",
      "Iteration 338, loss = 0.42287219\n",
      "Iteration 339, loss = 0.42293700\n",
      "Iteration 340, loss = 0.42280083\n",
      "Iteration 341, loss = 0.42257918\n",
      "Iteration 342, loss = 0.42273876\n",
      "Iteration 343, loss = 0.42289741\n",
      "Iteration 344, loss = 0.42263592\n",
      "Iteration 345, loss = 0.42241946\n",
      "Iteration 346, loss = 0.42236241\n",
      "Iteration 347, loss = 0.42226343\n",
      "Iteration 348, loss = 0.42232188\n",
      "Iteration 349, loss = 0.42237468\n",
      "Iteration 350, loss = 0.42202153\n",
      "Iteration 351, loss = 0.42214894\n",
      "Iteration 352, loss = 0.42259142\n",
      "Iteration 353, loss = 0.42202009\n",
      "Iteration 354, loss = 0.42193411\n",
      "Iteration 355, loss = 0.42254676\n",
      "Iteration 356, loss = 0.42186623\n",
      "Iteration 357, loss = 0.42179850\n",
      "Iteration 358, loss = 0.42177459\n",
      "Iteration 359, loss = 0.42179895\n",
      "Iteration 360, loss = 0.42182489\n",
      "Iteration 361, loss = 0.42159589\n",
      "Iteration 362, loss = 0.42171451\n",
      "Iteration 363, loss = 0.42151870\n",
      "Iteration 364, loss = 0.42164624\n",
      "Iteration 365, loss = 0.42132901\n",
      "Iteration 366, loss = 0.42134020\n",
      "Iteration 367, loss = 0.42130694\n",
      "Iteration 368, loss = 0.42123982\n",
      "Iteration 369, loss = 0.42095709\n",
      "Iteration 370, loss = 0.42130740\n",
      "Iteration 371, loss = 0.42130408\n",
      "Iteration 372, loss = 0.42096175\n",
      "Iteration 373, loss = 0.42121925\n",
      "Iteration 374, loss = 0.42112193\n",
      "Iteration 375, loss = 0.42085422\n",
      "Iteration 376, loss = 0.42117874\n",
      "Iteration 377, loss = 0.42123743\n",
      "Iteration 378, loss = 0.42086352\n",
      "Iteration 379, loss = 0.42079104\n",
      "Iteration 380, loss = 0.42062782\n",
      "Iteration 381, loss = 0.42047605\n",
      "Iteration 382, loss = 0.42050533\n",
      "Iteration 383, loss = 0.42094437\n",
      "Iteration 384, loss = 0.42053718\n",
      "Iteration 385, loss = 0.42074170\n",
      "Iteration 386, loss = 0.42050631\n",
      "Iteration 387, loss = 0.42036429\n",
      "Iteration 388, loss = 0.42033347\n",
      "Iteration 389, loss = 0.42051815\n",
      "Iteration 390, loss = 0.42023413\n",
      "Iteration 391, loss = 0.42020813\n",
      "Iteration 392, loss = 0.42018727\n",
      "Iteration 393, loss = 0.42006204\n",
      "Iteration 394, loss = 0.42025172\n",
      "Iteration 395, loss = 0.42016988\n",
      "Iteration 396, loss = 0.42002453\n",
      "Iteration 397, loss = 0.41981664\n",
      "Iteration 398, loss = 0.42006975\n",
      "Iteration 399, loss = 0.41952555\n",
      "Iteration 400, loss = 0.41985586\n",
      "Iteration 401, loss = 0.41950828\n",
      "Iteration 402, loss = 0.41951718\n",
      "Iteration 403, loss = 0.41952007\n",
      "Iteration 404, loss = 0.41981918\n",
      "Iteration 405, loss = 0.41968603\n",
      "Iteration 406, loss = 0.41971057\n",
      "Iteration 407, loss = 0.41980295\n",
      "Iteration 408, loss = 0.41949028\n",
      "Iteration 409, loss = 0.41954923\n",
      "Iteration 410, loss = 0.41932693\n",
      "Iteration 411, loss = 0.41964674\n",
      "Iteration 412, loss = 0.41951171\n",
      "Iteration 413, loss = 0.41923032\n",
      "Iteration 414, loss = 0.41921880\n",
      "Iteration 415, loss = 0.41904272\n",
      "Iteration 416, loss = 0.41882418\n",
      "Iteration 417, loss = 0.41926878\n",
      "Iteration 418, loss = 0.41915164\n",
      "Iteration 419, loss = 0.41899223\n",
      "Iteration 420, loss = 0.41896204\n",
      "Iteration 421, loss = 0.41911185\n",
      "Iteration 422, loss = 0.41889125\n",
      "Iteration 423, loss = 0.41881212\n",
      "Iteration 424, loss = 0.41909999\n",
      "Iteration 425, loss = 0.41842441\n",
      "Iteration 426, loss = 0.41855794\n",
      "Iteration 427, loss = 0.41863393\n",
      "Iteration 428, loss = 0.41893973\n",
      "Iteration 429, loss = 0.41863368\n",
      "Iteration 430, loss = 0.41888205\n",
      "Iteration 431, loss = 0.41826046\n",
      "Iteration 432, loss = 0.41838607\n",
      "Iteration 433, loss = 0.41820909\n",
      "Iteration 434, loss = 0.41841293\n",
      "Iteration 435, loss = 0.41849054\n",
      "Iteration 436, loss = 0.41810262\n",
      "Iteration 437, loss = 0.41811487\n",
      "Iteration 438, loss = 0.41819648\n",
      "Iteration 439, loss = 0.41829846\n",
      "Iteration 440, loss = 0.41826002\n",
      "Iteration 441, loss = 0.41836337\n",
      "Iteration 442, loss = 0.41834158\n",
      "Iteration 443, loss = 0.41815738\n",
      "Iteration 444, loss = 0.41792631\n",
      "Iteration 445, loss = 0.41798278\n",
      "Iteration 446, loss = 0.41849362\n",
      "Iteration 447, loss = 0.41784547\n",
      "Iteration 448, loss = 0.41791089\n",
      "Iteration 449, loss = 0.41799315\n",
      "Iteration 450, loss = 0.41787533\n",
      "Iteration 451, loss = 0.41803840\n",
      "Iteration 452, loss = 0.41768938\n",
      "Iteration 453, loss = 0.41772784\n",
      "Iteration 454, loss = 0.41762402\n",
      "Iteration 455, loss = 0.41774193\n",
      "Iteration 456, loss = 0.41782485\n",
      "Iteration 457, loss = 0.41772853\n",
      "Iteration 458, loss = 0.41744748\n",
      "Iteration 459, loss = 0.41765311\n",
      "Iteration 460, loss = 0.41773845\n",
      "Iteration 461, loss = 0.41748134\n",
      "Iteration 462, loss = 0.41713153\n",
      "Iteration 463, loss = 0.41786268\n",
      "Iteration 464, loss = 0.41718088\n",
      "Iteration 465, loss = 0.41748517\n",
      "Iteration 466, loss = 0.41773191\n",
      "Iteration 467, loss = 0.41707432\n",
      "Iteration 468, loss = 0.41716098\n",
      "Iteration 469, loss = 0.41709465\n",
      "Iteration 470, loss = 0.41703687\n",
      "Iteration 471, loss = 0.41698340\n",
      "Iteration 472, loss = 0.41690155\n",
      "Iteration 473, loss = 0.41716329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96486810\n",
      "Iteration 2, loss = 1.77854241\n",
      "Iteration 3, loss = 1.60716491\n",
      "Iteration 4, loss = 1.45741975\n",
      "Iteration 5, loss = 1.32920100\n",
      "Iteration 6, loss = 1.22266528\n",
      "Iteration 7, loss = 1.13661503\n",
      "Iteration 8, loss = 1.06629007\n",
      "Iteration 9, loss = 1.00643209\n",
      "Iteration 10, loss = 0.95459663\n",
      "Iteration 11, loss = 0.91080971\n",
      "Iteration 12, loss = 0.87288900\n",
      "Iteration 13, loss = 0.83950021\n",
      "Iteration 14, loss = 0.81008172\n",
      "Iteration 15, loss = 0.78373207\n",
      "Iteration 16, loss = 0.75950668\n",
      "Iteration 17, loss = 0.73778017\n",
      "Iteration 18, loss = 0.71795744\n",
      "Iteration 19, loss = 0.70009576\n",
      "Iteration 20, loss = 0.68379292\n",
      "Iteration 21, loss = 0.66922400\n",
      "Iteration 22, loss = 0.65588577\n",
      "Iteration 23, loss = 0.64394779\n",
      "Iteration 24, loss = 0.63320847\n",
      "Iteration 25, loss = 0.62337378\n",
      "Iteration 26, loss = 0.61421625\n",
      "Iteration 27, loss = 0.60607934\n",
      "Iteration 28, loss = 0.59885578\n",
      "Iteration 29, loss = 0.59205839\n",
      "Iteration 30, loss = 0.58619068\n",
      "Iteration 31, loss = 0.58022892\n",
      "Iteration 32, loss = 0.57522279\n",
      "Iteration 33, loss = 0.57058392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.56594154\n",
      "Iteration 35, loss = 0.56197329\n",
      "Iteration 36, loss = 0.55832110\n",
      "Iteration 37, loss = 0.55461508\n",
      "Iteration 38, loss = 0.55132555\n",
      "Iteration 39, loss = 0.54816888\n",
      "Iteration 40, loss = 0.54519116\n",
      "Iteration 41, loss = 0.54253999\n",
      "Iteration 42, loss = 0.53980308\n",
      "Iteration 43, loss = 0.53728273\n",
      "Iteration 44, loss = 0.53489986\n",
      "Iteration 45, loss = 0.53258562\n",
      "Iteration 46, loss = 0.53089101\n",
      "Iteration 47, loss = 0.52880736\n",
      "Iteration 48, loss = 0.52658500\n",
      "Iteration 49, loss = 0.52474019\n",
      "Iteration 50, loss = 0.52287853\n",
      "Iteration 51, loss = 0.52143688\n",
      "Iteration 52, loss = 0.51972687\n",
      "Iteration 53, loss = 0.51803660\n",
      "Iteration 54, loss = 0.51637279\n",
      "Iteration 55, loss = 0.51507144\n",
      "Iteration 56, loss = 0.51361763\n",
      "Iteration 57, loss = 0.51234664\n",
      "Iteration 58, loss = 0.51096163\n",
      "Iteration 59, loss = 0.50960157\n",
      "Iteration 60, loss = 0.50839320\n",
      "Iteration 61, loss = 0.50719665\n",
      "Iteration 62, loss = 0.50614684\n",
      "Iteration 63, loss = 0.50472324\n",
      "Iteration 64, loss = 0.50390146\n",
      "Iteration 65, loss = 0.50250079\n",
      "Iteration 66, loss = 0.50151625\n",
      "Iteration 67, loss = 0.50053373\n",
      "Iteration 68, loss = 0.49946679\n",
      "Iteration 69, loss = 0.49853928\n",
      "Iteration 70, loss = 0.49762136\n",
      "Iteration 71, loss = 0.49673645\n",
      "Iteration 72, loss = 0.49581460\n",
      "Iteration 73, loss = 0.49472891\n",
      "Iteration 74, loss = 0.49384936\n",
      "Iteration 75, loss = 0.49307405\n",
      "Iteration 76, loss = 0.49225655\n",
      "Iteration 77, loss = 0.49152807\n",
      "Iteration 78, loss = 0.49058914\n",
      "Iteration 79, loss = 0.48980660\n",
      "Iteration 80, loss = 0.48874418\n",
      "Iteration 81, loss = 0.48831574\n",
      "Iteration 82, loss = 0.48747845\n",
      "Iteration 83, loss = 0.48673991\n",
      "Iteration 84, loss = 0.48601223\n",
      "Iteration 85, loss = 0.48533982\n",
      "Iteration 86, loss = 0.48443004\n",
      "Iteration 87, loss = 0.48389808\n",
      "Iteration 88, loss = 0.48308261\n",
      "Iteration 89, loss = 0.48253491\n",
      "Iteration 90, loss = 0.48196138\n",
      "Iteration 91, loss = 0.48093259\n",
      "Iteration 92, loss = 0.48065595\n",
      "Iteration 93, loss = 0.47972772\n",
      "Iteration 94, loss = 0.47909923\n",
      "Iteration 95, loss = 0.47838274\n",
      "Iteration 96, loss = 0.47804402\n",
      "Iteration 97, loss = 0.47714672\n",
      "Iteration 98, loss = 0.47670516\n",
      "Iteration 99, loss = 0.47621637\n",
      "Iteration 100, loss = 0.47545521\n",
      "Iteration 101, loss = 0.47482270\n",
      "Iteration 102, loss = 0.47431002\n",
      "Iteration 103, loss = 0.47383948\n",
      "Iteration 104, loss = 0.47323420\n",
      "Iteration 105, loss = 0.47273291\n",
      "Iteration 106, loss = 0.47203895\n",
      "Iteration 107, loss = 0.47147770\n",
      "Iteration 108, loss = 0.47098128\n",
      "Iteration 109, loss = 0.47030317\n",
      "Iteration 110, loss = 0.46998464\n",
      "Iteration 111, loss = 0.46943409\n",
      "Iteration 112, loss = 0.46899661\n",
      "Iteration 113, loss = 0.46828196\n",
      "Iteration 114, loss = 0.46780706\n",
      "Iteration 115, loss = 0.46713179\n",
      "Iteration 116, loss = 0.46672354\n",
      "Iteration 117, loss = 0.46633040\n",
      "Iteration 118, loss = 0.46572538\n",
      "Iteration 119, loss = 0.46527099\n",
      "Iteration 120, loss = 0.46479048\n",
      "Iteration 121, loss = 0.46426176\n",
      "Iteration 122, loss = 0.46389596\n",
      "Iteration 123, loss = 0.46350982\n",
      "Iteration 124, loss = 0.46290765\n",
      "Iteration 125, loss = 0.46251896\n",
      "Iteration 126, loss = 0.46214275\n",
      "Iteration 127, loss = 0.46150436\n",
      "Iteration 128, loss = 0.46113019\n",
      "Iteration 129, loss = 0.46070903\n",
      "Iteration 130, loss = 0.46025818\n",
      "Iteration 131, loss = 0.45979640\n",
      "Iteration 132, loss = 0.45923315\n",
      "Iteration 133, loss = 0.45882199\n",
      "Iteration 134, loss = 0.45856925\n",
      "Iteration 135, loss = 0.45820899\n",
      "Iteration 136, loss = 0.45759377\n",
      "Iteration 137, loss = 0.45715013\n",
      "Iteration 138, loss = 0.45687796\n",
      "Iteration 139, loss = 0.45633642\n",
      "Iteration 140, loss = 0.45617821\n",
      "Iteration 141, loss = 0.45579044\n",
      "Iteration 142, loss = 0.45524087\n",
      "Iteration 143, loss = 0.45463800\n",
      "Iteration 144, loss = 0.45436526\n",
      "Iteration 145, loss = 0.45413014\n",
      "Iteration 146, loss = 0.45408161\n",
      "Iteration 147, loss = 0.45341608\n",
      "Iteration 148, loss = 0.45316317\n",
      "Iteration 149, loss = 0.45275389\n",
      "Iteration 150, loss = 0.45234506\n",
      "Iteration 151, loss = 0.45175633\n",
      "Iteration 152, loss = 0.45146739\n",
      "Iteration 153, loss = 0.45094932\n",
      "Iteration 154, loss = 0.45077466\n",
      "Iteration 155, loss = 0.45045421\n",
      "Iteration 156, loss = 0.45012801\n",
      "Iteration 157, loss = 0.44975726\n",
      "Iteration 158, loss = 0.44947201\n",
      "Iteration 159, loss = 0.44912429\n",
      "Iteration 160, loss = 0.44879943\n",
      "Iteration 161, loss = 0.44840324\n",
      "Iteration 162, loss = 0.44817972\n",
      "Iteration 163, loss = 0.44769676\n",
      "Iteration 164, loss = 0.44741544\n",
      "Iteration 165, loss = 0.44719361\n",
      "Iteration 166, loss = 0.44694003\n",
      "Iteration 167, loss = 0.44666927\n",
      "Iteration 168, loss = 0.44625841\n",
      "Iteration 169, loss = 0.44566845\n",
      "Iteration 170, loss = 0.44585300\n",
      "Iteration 171, loss = 0.44550008\n",
      "Iteration 172, loss = 0.44537773\n",
      "Iteration 173, loss = 0.44485171\n",
      "Iteration 174, loss = 0.44449877\n",
      "Iteration 175, loss = 0.44435172\n",
      "Iteration 176, loss = 0.44388160\n",
      "Iteration 177, loss = 0.44369290\n",
      "Iteration 178, loss = 0.44341306\n",
      "Iteration 179, loss = 0.44319077\n",
      "Iteration 180, loss = 0.44291580\n",
      "Iteration 181, loss = 0.44264245\n",
      "Iteration 182, loss = 0.44251678\n",
      "Iteration 183, loss = 0.44245837\n",
      "Iteration 184, loss = 0.44195426\n",
      "Iteration 185, loss = 0.44181676\n",
      "Iteration 186, loss = 0.44141784\n",
      "Iteration 187, loss = 0.44132837\n",
      "Iteration 188, loss = 0.44089167\n",
      "Iteration 189, loss = 0.44062506\n",
      "Iteration 190, loss = 0.44067892\n",
      "Iteration 191, loss = 0.44018563\n",
      "Iteration 192, loss = 0.44004911\n",
      "Iteration 193, loss = 0.43975756\n",
      "Iteration 194, loss = 0.43955930\n",
      "Iteration 195, loss = 0.43923503\n",
      "Iteration 196, loss = 0.43908330\n",
      "Iteration 197, loss = 0.43888610\n",
      "Iteration 198, loss = 0.43868640\n",
      "Iteration 199, loss = 0.43830781\n",
      "Iteration 200, loss = 0.43804501\n",
      "Iteration 201, loss = 0.43800256\n",
      "Iteration 202, loss = 0.43792902\n",
      "Iteration 203, loss = 0.43762452\n",
      "Iteration 204, loss = 0.43725068\n",
      "Iteration 205, loss = 0.43703130\n",
      "Iteration 206, loss = 0.43695199\n",
      "Iteration 207, loss = 0.43671413\n",
      "Iteration 208, loss = 0.43664967\n",
      "Iteration 209, loss = 0.43637609\n",
      "Iteration 210, loss = 0.43644002\n",
      "Iteration 211, loss = 0.43614607\n",
      "Iteration 212, loss = 0.43575796\n",
      "Iteration 213, loss = 0.43571612\n",
      "Iteration 214, loss = 0.43533803\n",
      "Iteration 215, loss = 0.43538284\n",
      "Iteration 216, loss = 0.43496673\n",
      "Iteration 217, loss = 0.43512113\n",
      "Iteration 218, loss = 0.43466560\n",
      "Iteration 219, loss = 0.43467375\n",
      "Iteration 220, loss = 0.43449527\n",
      "Iteration 221, loss = 0.43410274\n",
      "Iteration 222, loss = 0.43413961\n",
      "Iteration 223, loss = 0.43392467\n",
      "Iteration 224, loss = 0.43375346\n",
      "Iteration 225, loss = 0.43352229\n",
      "Iteration 226, loss = 0.43361630\n",
      "Iteration 227, loss = 0.43327949\n",
      "Iteration 228, loss = 0.43327434\n",
      "Iteration 229, loss = 0.43316249\n",
      "Iteration 230, loss = 0.43293069\n",
      "Iteration 231, loss = 0.43262252\n",
      "Iteration 232, loss = 0.43258106\n",
      "Iteration 233, loss = 0.43242066\n",
      "Iteration 234, loss = 0.43209025\n",
      "Iteration 235, loss = 0.43195880\n",
      "Iteration 236, loss = 0.43204969\n",
      "Iteration 237, loss = 0.43167435\n",
      "Iteration 238, loss = 0.43167401\n",
      "Iteration 239, loss = 0.43169992\n",
      "Iteration 240, loss = 0.43131489\n",
      "Iteration 241, loss = 0.43135241\n",
      "Iteration 242, loss = 0.43090567\n",
      "Iteration 243, loss = 0.43105105\n",
      "Iteration 244, loss = 0.43073481\n",
      "Iteration 245, loss = 0.43068873\n",
      "Iteration 246, loss = 0.43054206\n",
      "Iteration 247, loss = 0.43022321\n",
      "Iteration 248, loss = 0.43056483\n",
      "Iteration 249, loss = 0.43019215\n",
      "Iteration 250, loss = 0.43013217\n",
      "Iteration 251, loss = 0.42983604\n",
      "Iteration 252, loss = 0.43016268\n",
      "Iteration 253, loss = 0.42980525\n",
      "Iteration 254, loss = 0.42948668\n",
      "Iteration 255, loss = 0.42935099\n",
      "Iteration 256, loss = 0.42945649\n",
      "Iteration 257, loss = 0.42934188\n",
      "Iteration 258, loss = 0.42922854\n",
      "Iteration 259, loss = 0.42896944\n",
      "Iteration 260, loss = 0.42913986\n",
      "Iteration 261, loss = 0.42878844\n",
      "Iteration 262, loss = 0.42892601\n",
      "Iteration 263, loss = 0.42858922\n",
      "Iteration 264, loss = 0.42871422\n",
      "Iteration 265, loss = 0.42846538\n",
      "Iteration 266, loss = 0.42803582\n",
      "Iteration 267, loss = 0.42817948\n",
      "Iteration 268, loss = 0.42803203\n",
      "Iteration 269, loss = 0.42812934\n",
      "Iteration 270, loss = 0.42781840\n",
      "Iteration 271, loss = 0.42797946\n",
      "Iteration 272, loss = 0.42773682\n",
      "Iteration 273, loss = 0.42759942\n",
      "Iteration 274, loss = 0.42743040\n",
      "Iteration 275, loss = 0.42742518\n",
      "Iteration 276, loss = 0.42731531\n",
      "Iteration 277, loss = 0.42709329\n",
      "Iteration 278, loss = 0.42696222\n",
      "Iteration 279, loss = 0.42681148\n",
      "Iteration 280, loss = 0.42683071\n",
      "Iteration 281, loss = 0.42683148\n",
      "Iteration 282, loss = 0.42665393\n",
      "Iteration 283, loss = 0.42659577\n",
      "Iteration 284, loss = 0.42661836\n",
      "Iteration 285, loss = 0.42651189\n",
      "Iteration 286, loss = 0.42654522\n",
      "Iteration 287, loss = 0.42608373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 288, loss = 0.42623383\n",
      "Iteration 289, loss = 0.42608943\n",
      "Iteration 290, loss = 0.42621063\n",
      "Iteration 291, loss = 0.42586359\n",
      "Iteration 292, loss = 0.42590888\n",
      "Iteration 293, loss = 0.42561735\n",
      "Iteration 294, loss = 0.42541731\n",
      "Iteration 295, loss = 0.42554061\n",
      "Iteration 296, loss = 0.42545863\n",
      "Iteration 297, loss = 0.42607823\n",
      "Iteration 298, loss = 0.42538131\n",
      "Iteration 299, loss = 0.42517991\n",
      "Iteration 300, loss = 0.42512192\n",
      "Iteration 301, loss = 0.42509756\n",
      "Iteration 302, loss = 0.42525460\n",
      "Iteration 303, loss = 0.42502754\n",
      "Iteration 304, loss = 0.42506329\n",
      "Iteration 305, loss = 0.42449605\n",
      "Iteration 306, loss = 0.42453555\n",
      "Iteration 307, loss = 0.42455020\n",
      "Iteration 308, loss = 0.42432238\n",
      "Iteration 309, loss = 0.42458346\n",
      "Iteration 310, loss = 0.42410325\n",
      "Iteration 311, loss = 0.42378359\n",
      "Iteration 312, loss = 0.42443541\n",
      "Iteration 313, loss = 0.42397879\n",
      "Iteration 314, loss = 0.42395609\n",
      "Iteration 315, loss = 0.42430622\n",
      "Iteration 316, loss = 0.42384415\n",
      "Iteration 317, loss = 0.42369942\n",
      "Iteration 318, loss = 0.42395738\n",
      "Iteration 319, loss = 0.42357723\n",
      "Iteration 320, loss = 0.42357880\n",
      "Iteration 321, loss = 0.42336912\n",
      "Iteration 322, loss = 0.42322397\n",
      "Iteration 323, loss = 0.42359289\n",
      "Iteration 324, loss = 0.42328806\n",
      "Iteration 325, loss = 0.42331236\n",
      "Iteration 326, loss = 0.42301924\n",
      "Iteration 327, loss = 0.42323623\n",
      "Iteration 328, loss = 0.42328652\n",
      "Iteration 329, loss = 0.42281980\n",
      "Iteration 330, loss = 0.42272378\n",
      "Iteration 331, loss = 0.42294813\n",
      "Iteration 332, loss = 0.42273835\n",
      "Iteration 333, loss = 0.42306900\n",
      "Iteration 334, loss = 0.42259760\n",
      "Iteration 335, loss = 0.42247806\n",
      "Iteration 336, loss = 0.42289986\n",
      "Iteration 337, loss = 0.42254973\n",
      "Iteration 338, loss = 0.42251298\n",
      "Iteration 339, loss = 0.42230269\n",
      "Iteration 340, loss = 0.42229541\n",
      "Iteration 341, loss = 0.42227942\n",
      "Iteration 342, loss = 0.42232948\n",
      "Iteration 343, loss = 0.42187004\n",
      "Iteration 344, loss = 0.42194784\n",
      "Iteration 345, loss = 0.42186060\n",
      "Iteration 346, loss = 0.42167223\n",
      "Iteration 347, loss = 0.42191512\n",
      "Iteration 348, loss = 0.42174081\n",
      "Iteration 349, loss = 0.42185572\n",
      "Iteration 350, loss = 0.42143423\n",
      "Iteration 351, loss = 0.42163454\n",
      "Iteration 352, loss = 0.42166375\n",
      "Iteration 353, loss = 0.42120340\n",
      "Iteration 354, loss = 0.42169369\n",
      "Iteration 355, loss = 0.42126978\n",
      "Iteration 356, loss = 0.42122636\n",
      "Iteration 357, loss = 0.42146684\n",
      "Iteration 358, loss = 0.42152436\n",
      "Iteration 359, loss = 0.42124143\n",
      "Iteration 360, loss = 0.42115962\n",
      "Iteration 361, loss = 0.42147531\n",
      "Iteration 362, loss = 0.42076985\n",
      "Iteration 363, loss = 0.42084319\n",
      "Iteration 364, loss = 0.42094838\n",
      "Iteration 365, loss = 0.42098006\n",
      "Iteration 366, loss = 0.42101975\n",
      "Iteration 367, loss = 0.42086553\n",
      "Iteration 368, loss = 0.42075171\n",
      "Iteration 369, loss = 0.42072002\n",
      "Iteration 370, loss = 0.42087235\n",
      "Iteration 371, loss = 0.42065821\n",
      "Iteration 372, loss = 0.42063396\n",
      "Iteration 373, loss = 0.42038229\n",
      "Iteration 374, loss = 0.42058999\n",
      "Iteration 375, loss = 0.42044882\n",
      "Iteration 376, loss = 0.42036672\n",
      "Iteration 377, loss = 0.42021643\n",
      "Iteration 378, loss = 0.42024199\n",
      "Iteration 379, loss = 0.42072269\n",
      "Iteration 380, loss = 0.42015636\n",
      "Iteration 381, loss = 0.41998278\n",
      "Iteration 382, loss = 0.42003461\n",
      "Iteration 383, loss = 0.42000891\n",
      "Iteration 384, loss = 0.42019422\n",
      "Iteration 385, loss = 0.41993712\n",
      "Iteration 386, loss = 0.41990901\n",
      "Iteration 387, loss = 0.42000689\n",
      "Iteration 388, loss = 0.41957550\n",
      "Iteration 389, loss = 0.41985342\n",
      "Iteration 390, loss = 0.41977636\n",
      "Iteration 391, loss = 0.41982880\n",
      "Iteration 392, loss = 0.41935774\n",
      "Iteration 393, loss = 0.41954628\n",
      "Iteration 394, loss = 0.41946362\n",
      "Iteration 395, loss = 0.41933095\n",
      "Iteration 396, loss = 0.41944261\n",
      "Iteration 397, loss = 0.41912253\n",
      "Iteration 398, loss = 0.41929398\n",
      "Iteration 399, loss = 0.41908050\n",
      "Iteration 400, loss = 0.41903640\n",
      "Iteration 401, loss = 0.41921969\n",
      "Iteration 402, loss = 0.41920970\n",
      "Iteration 403, loss = 0.41884433\n",
      "Iteration 404, loss = 0.41875289\n",
      "Iteration 405, loss = 0.41888804\n",
      "Iteration 406, loss = 0.41896655\n",
      "Iteration 407, loss = 0.41904724\n",
      "Iteration 408, loss = 0.41867971\n",
      "Iteration 409, loss = 0.41871501\n",
      "Iteration 410, loss = 0.41882090\n",
      "Iteration 411, loss = 0.41883382\n",
      "Iteration 412, loss = 0.41880436\n",
      "Iteration 413, loss = 0.41857023\n",
      "Iteration 414, loss = 0.41871560\n",
      "Iteration 415, loss = 0.41854691\n",
      "Iteration 416, loss = 0.41881112\n",
      "Iteration 417, loss = 0.41836996\n",
      "Iteration 418, loss = 0.41871806\n",
      "Iteration 419, loss = 0.41842430\n",
      "Iteration 420, loss = 0.41838325\n",
      "Iteration 421, loss = 0.41839262\n",
      "Iteration 422, loss = 0.41847188\n",
      "Iteration 423, loss = 0.41823448\n",
      "Iteration 424, loss = 0.41827780\n",
      "Iteration 425, loss = 0.41838691\n",
      "Iteration 426, loss = 0.41807893\n",
      "Iteration 427, loss = 0.41808737\n",
      "Iteration 428, loss = 0.41802168\n",
      "Iteration 429, loss = 0.41810330\n",
      "Iteration 430, loss = 0.41806361\n",
      "Iteration 431, loss = 0.41788270\n",
      "Iteration 432, loss = 0.41789189\n",
      "Iteration 433, loss = 0.41786002\n",
      "Iteration 434, loss = 0.41774524\n",
      "Iteration 435, loss = 0.41781269\n",
      "Iteration 436, loss = 0.41775427\n",
      "Iteration 437, loss = 0.41771205\n",
      "Iteration 438, loss = 0.41797907\n",
      "Iteration 439, loss = 0.41742982\n",
      "Iteration 440, loss = 0.41749035\n",
      "Iteration 441, loss = 0.41761552\n",
      "Iteration 442, loss = 0.41768769\n",
      "Iteration 443, loss = 0.41748721\n",
      "Iteration 444, loss = 0.41731308\n",
      "Iteration 445, loss = 0.41733152\n",
      "Iteration 446, loss = 0.41763903\n",
      "Iteration 447, loss = 0.41728840\n",
      "Iteration 448, loss = 0.41741114\n",
      "Iteration 449, loss = 0.41735785\n",
      "Iteration 450, loss = 0.41746967\n",
      "Iteration 451, loss = 0.41724998\n",
      "Iteration 452, loss = 0.41703832\n",
      "Iteration 453, loss = 0.41728058\n",
      "Iteration 454, loss = 0.41688863\n",
      "Iteration 455, loss = 0.41686880\n",
      "Iteration 456, loss = 0.41707962\n",
      "Iteration 457, loss = 0.41684720\n",
      "Iteration 458, loss = 0.41695215\n",
      "Iteration 459, loss = 0.41678624\n",
      "Iteration 460, loss = 0.41680418\n",
      "Iteration 461, loss = 0.41671170\n",
      "Iteration 462, loss = 0.41683614\n",
      "Iteration 463, loss = 0.41684933\n",
      "Iteration 464, loss = 0.41691790\n",
      "Iteration 465, loss = 0.41673149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96507459\n",
      "Iteration 2, loss = 1.77849153\n",
      "Iteration 3, loss = 1.60666027\n",
      "Iteration 4, loss = 1.45571748\n",
      "Iteration 5, loss = 1.32709595\n",
      "Iteration 6, loss = 1.22060502\n",
      "Iteration 7, loss = 1.13480440\n",
      "Iteration 8, loss = 1.06428041\n",
      "Iteration 9, loss = 1.00446566\n",
      "Iteration 10, loss = 0.95271803\n",
      "Iteration 11, loss = 0.90899780\n",
      "Iteration 12, loss = 0.87101701\n",
      "Iteration 13, loss = 0.83783220\n",
      "Iteration 14, loss = 0.80805668\n",
      "Iteration 15, loss = 0.78146152\n",
      "Iteration 16, loss = 0.75746357\n",
      "Iteration 17, loss = 0.73571815\n",
      "Iteration 18, loss = 0.71575173\n",
      "Iteration 19, loss = 0.69806407\n",
      "Iteration 20, loss = 0.68171776\n",
      "Iteration 21, loss = 0.66716383\n",
      "Iteration 22, loss = 0.65396684\n",
      "Iteration 23, loss = 0.64196871\n",
      "Iteration 24, loss = 0.63121854\n",
      "Iteration 25, loss = 0.62121597\n",
      "Iteration 26, loss = 0.61246887\n",
      "Iteration 27, loss = 0.60421483\n",
      "Iteration 28, loss = 0.59702972\n",
      "Iteration 29, loss = 0.59034339\n",
      "Iteration 30, loss = 0.58436087\n",
      "Iteration 31, loss = 0.57885715\n",
      "Iteration 32, loss = 0.57381860\n",
      "Iteration 33, loss = 0.56910815\n",
      "Iteration 34, loss = 0.56473121\n",
      "Iteration 35, loss = 0.56060624\n",
      "Iteration 36, loss = 0.55698905\n",
      "Iteration 37, loss = 0.55318758\n",
      "Iteration 38, loss = 0.55009036\n",
      "Iteration 39, loss = 0.54699966\n",
      "Iteration 40, loss = 0.54414995\n",
      "Iteration 41, loss = 0.54142056\n",
      "Iteration 42, loss = 0.53878332\n",
      "Iteration 43, loss = 0.53619556\n",
      "Iteration 44, loss = 0.53427692\n",
      "Iteration 45, loss = 0.53178968\n",
      "Iteration 46, loss = 0.52952576\n",
      "Iteration 47, loss = 0.52758780\n",
      "Iteration 48, loss = 0.52560895\n",
      "Iteration 49, loss = 0.52400762\n",
      "Iteration 50, loss = 0.52201171\n",
      "Iteration 51, loss = 0.52031374\n",
      "Iteration 52, loss = 0.51869210\n",
      "Iteration 53, loss = 0.51711657\n",
      "Iteration 54, loss = 0.51563907\n",
      "Iteration 55, loss = 0.51418604\n",
      "Iteration 56, loss = 0.51256949\n",
      "Iteration 57, loss = 0.51135769\n",
      "Iteration 58, loss = 0.51016063\n",
      "Iteration 59, loss = 0.50883870\n",
      "Iteration 60, loss = 0.50763748\n",
      "Iteration 61, loss = 0.50633633\n",
      "Iteration 62, loss = 0.50530726\n",
      "Iteration 63, loss = 0.50411498\n",
      "Iteration 64, loss = 0.50299304\n",
      "Iteration 65, loss = 0.50189724\n",
      "Iteration 66, loss = 0.50079080\n",
      "Iteration 67, loss = 0.49969152\n",
      "Iteration 68, loss = 0.49909679\n",
      "Iteration 69, loss = 0.49788782\n",
      "Iteration 70, loss = 0.49698300\n",
      "Iteration 71, loss = 0.49610101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.49506133\n",
      "Iteration 73, loss = 0.49423604\n",
      "Iteration 74, loss = 0.49337269\n",
      "Iteration 75, loss = 0.49252532\n",
      "Iteration 76, loss = 0.49159690\n",
      "Iteration 77, loss = 0.49087241\n",
      "Iteration 78, loss = 0.48987891\n",
      "Iteration 79, loss = 0.48915202\n",
      "Iteration 80, loss = 0.48844848\n",
      "Iteration 81, loss = 0.48760219\n",
      "Iteration 82, loss = 0.48681489\n",
      "Iteration 83, loss = 0.48608412\n",
      "Iteration 84, loss = 0.48541175\n",
      "Iteration 85, loss = 0.48473585\n",
      "Iteration 86, loss = 0.48402011\n",
      "Iteration 87, loss = 0.48338262\n",
      "Iteration 88, loss = 0.48262142\n",
      "Iteration 89, loss = 0.48192215\n",
      "Iteration 90, loss = 0.48119359\n",
      "Iteration 91, loss = 0.48064437\n",
      "Iteration 92, loss = 0.48007855\n",
      "Iteration 93, loss = 0.47952190\n",
      "Iteration 94, loss = 0.47855682\n",
      "Iteration 95, loss = 0.47817533\n",
      "Iteration 96, loss = 0.47744961\n",
      "Iteration 97, loss = 0.47690009\n",
      "Iteration 98, loss = 0.47642701\n",
      "Iteration 99, loss = 0.47551810\n",
      "Iteration 100, loss = 0.47496435\n",
      "Iteration 101, loss = 0.47462250\n",
      "Iteration 102, loss = 0.47402754\n",
      "Iteration 103, loss = 0.47347181\n",
      "Iteration 104, loss = 0.47296713\n",
      "Iteration 105, loss = 0.47238658\n",
      "Iteration 106, loss = 0.47171195\n",
      "Iteration 107, loss = 0.47110290\n",
      "Iteration 108, loss = 0.47063864\n",
      "Iteration 109, loss = 0.47021822\n",
      "Iteration 110, loss = 0.46959492\n",
      "Iteration 111, loss = 0.46886707\n",
      "Iteration 112, loss = 0.46831537\n",
      "Iteration 113, loss = 0.46800317\n",
      "Iteration 114, loss = 0.46738596\n",
      "Iteration 115, loss = 0.46707702\n",
      "Iteration 116, loss = 0.46643084\n",
      "Iteration 117, loss = 0.46599175\n",
      "Iteration 118, loss = 0.46540180\n",
      "Iteration 119, loss = 0.46498337\n",
      "Iteration 120, loss = 0.46446055\n",
      "Iteration 121, loss = 0.46428673\n",
      "Iteration 122, loss = 0.46347020\n",
      "Iteration 123, loss = 0.46331420\n",
      "Iteration 124, loss = 0.46257432\n",
      "Iteration 125, loss = 0.46245986\n",
      "Iteration 126, loss = 0.46184035\n",
      "Iteration 127, loss = 0.46115044\n",
      "Iteration 128, loss = 0.46091254\n",
      "Iteration 129, loss = 0.46023864\n",
      "Iteration 130, loss = 0.45981469\n",
      "Iteration 131, loss = 0.45944127\n",
      "Iteration 132, loss = 0.45900565\n",
      "Iteration 133, loss = 0.45864483\n",
      "Iteration 134, loss = 0.45819072\n",
      "Iteration 135, loss = 0.45756800\n",
      "Iteration 136, loss = 0.45725966\n",
      "Iteration 137, loss = 0.45690572\n",
      "Iteration 138, loss = 0.45649367\n",
      "Iteration 139, loss = 0.45609935\n",
      "Iteration 140, loss = 0.45559657\n",
      "Iteration 141, loss = 0.45533004\n",
      "Iteration 142, loss = 0.45483685\n",
      "Iteration 143, loss = 0.45477141\n",
      "Iteration 144, loss = 0.45416685\n",
      "Iteration 145, loss = 0.45381462\n",
      "Iteration 146, loss = 0.45357863\n",
      "Iteration 147, loss = 0.45300332\n",
      "Iteration 148, loss = 0.45255186\n",
      "Iteration 149, loss = 0.45217029\n",
      "Iteration 150, loss = 0.45188749\n",
      "Iteration 151, loss = 0.45135556\n",
      "Iteration 152, loss = 0.45116879\n",
      "Iteration 153, loss = 0.45089967\n",
      "Iteration 154, loss = 0.45062729\n",
      "Iteration 155, loss = 0.45021237\n",
      "Iteration 156, loss = 0.44991900\n",
      "Iteration 157, loss = 0.44965387\n",
      "Iteration 158, loss = 0.44907531\n",
      "Iteration 159, loss = 0.44850413\n",
      "Iteration 160, loss = 0.44833582\n",
      "Iteration 161, loss = 0.44805212\n",
      "Iteration 162, loss = 0.44774853\n",
      "Iteration 163, loss = 0.44743311\n",
      "Iteration 164, loss = 0.44752576\n",
      "Iteration 165, loss = 0.44693214\n",
      "Iteration 166, loss = 0.44649472\n",
      "Iteration 167, loss = 0.44649088\n",
      "Iteration 168, loss = 0.44594064\n",
      "Iteration 169, loss = 0.44566147\n",
      "Iteration 170, loss = 0.44585810\n",
      "Iteration 171, loss = 0.44508806\n",
      "Iteration 172, loss = 0.44474030\n",
      "Iteration 173, loss = 0.44464733\n",
      "Iteration 174, loss = 0.44412681\n",
      "Iteration 175, loss = 0.44387047\n",
      "Iteration 176, loss = 0.44374167\n",
      "Iteration 177, loss = 0.44354643\n",
      "Iteration 178, loss = 0.44319607\n",
      "Iteration 179, loss = 0.44276550\n",
      "Iteration 180, loss = 0.44274135\n",
      "Iteration 181, loss = 0.44235803\n",
      "Iteration 182, loss = 0.44206904\n",
      "Iteration 183, loss = 0.44181056\n",
      "Iteration 184, loss = 0.44147332\n",
      "Iteration 185, loss = 0.44118610\n",
      "Iteration 186, loss = 0.44104198\n",
      "Iteration 187, loss = 0.44077160\n",
      "Iteration 188, loss = 0.44075124\n",
      "Iteration 189, loss = 0.44062483\n",
      "Iteration 190, loss = 0.44043996\n",
      "Iteration 191, loss = 0.44000205\n",
      "Iteration 192, loss = 0.43955166\n",
      "Iteration 193, loss = 0.43946701\n",
      "Iteration 194, loss = 0.43938352\n",
      "Iteration 195, loss = 0.43909713\n",
      "Iteration 196, loss = 0.43868448\n",
      "Iteration 197, loss = 0.43886344\n",
      "Iteration 198, loss = 0.43843638\n",
      "Iteration 199, loss = 0.43823930\n",
      "Iteration 200, loss = 0.43805898\n",
      "Iteration 201, loss = 0.43774140\n",
      "Iteration 202, loss = 0.43765267\n",
      "Iteration 203, loss = 0.43718343\n",
      "Iteration 204, loss = 0.43744145\n",
      "Iteration 205, loss = 0.43693279\n",
      "Iteration 206, loss = 0.43686632\n",
      "Iteration 207, loss = 0.43649282\n",
      "Iteration 208, loss = 0.43660871\n",
      "Iteration 209, loss = 0.43617282\n",
      "Iteration 210, loss = 0.43608831\n",
      "Iteration 211, loss = 0.43594859\n",
      "Iteration 212, loss = 0.43569874\n",
      "Iteration 213, loss = 0.43551327\n",
      "Iteration 214, loss = 0.43521701\n",
      "Iteration 215, loss = 0.43514471\n",
      "Iteration 216, loss = 0.43504071\n",
      "Iteration 217, loss = 0.43493002\n",
      "Iteration 218, loss = 0.43458215\n",
      "Iteration 219, loss = 0.43430058\n",
      "Iteration 220, loss = 0.43431266\n",
      "Iteration 221, loss = 0.43413626\n",
      "Iteration 222, loss = 0.43390837\n",
      "Iteration 223, loss = 0.43371051\n",
      "Iteration 224, loss = 0.43374205\n",
      "Iteration 225, loss = 0.43340421\n",
      "Iteration 226, loss = 0.43326064\n",
      "Iteration 227, loss = 0.43334963\n",
      "Iteration 228, loss = 0.43305516\n",
      "Iteration 229, loss = 0.43273419\n",
      "Iteration 230, loss = 0.43270270\n",
      "Iteration 231, loss = 0.43277416\n",
      "Iteration 232, loss = 0.43225907\n",
      "Iteration 233, loss = 0.43246685\n",
      "Iteration 234, loss = 0.43188950\n",
      "Iteration 235, loss = 0.43210283\n",
      "Iteration 236, loss = 0.43171392\n",
      "Iteration 237, loss = 0.43150158\n",
      "Iteration 238, loss = 0.43152592\n",
      "Iteration 239, loss = 0.43132930\n",
      "Iteration 240, loss = 0.43130418\n",
      "Iteration 241, loss = 0.43129582\n",
      "Iteration 242, loss = 0.43100689\n",
      "Iteration 243, loss = 0.43093292\n",
      "Iteration 244, loss = 0.43075334\n",
      "Iteration 245, loss = 0.43071886\n",
      "Iteration 246, loss = 0.43055501\n",
      "Iteration 247, loss = 0.43047635\n",
      "Iteration 248, loss = 0.43027542\n",
      "Iteration 249, loss = 0.43029999\n",
      "Iteration 250, loss = 0.43009493\n",
      "Iteration 251, loss = 0.42971626\n",
      "Iteration 252, loss = 0.42988292\n",
      "Iteration 253, loss = 0.42963054\n",
      "Iteration 254, loss = 0.42966314\n",
      "Iteration 255, loss = 0.42927339\n",
      "Iteration 256, loss = 0.42936904\n",
      "Iteration 257, loss = 0.42917548\n",
      "Iteration 258, loss = 0.42900978\n",
      "Iteration 259, loss = 0.42900849\n",
      "Iteration 260, loss = 0.42897744\n",
      "Iteration 261, loss = 0.42888580\n",
      "Iteration 262, loss = 0.42843666\n",
      "Iteration 263, loss = 0.42859643\n",
      "Iteration 264, loss = 0.42862005\n",
      "Iteration 265, loss = 0.42844248\n",
      "Iteration 266, loss = 0.42847239\n",
      "Iteration 267, loss = 0.42812351\n",
      "Iteration 268, loss = 0.42811528\n",
      "Iteration 269, loss = 0.42805471\n",
      "Iteration 270, loss = 0.42797527\n",
      "Iteration 271, loss = 0.42792143\n",
      "Iteration 272, loss = 0.42751504\n",
      "Iteration 273, loss = 0.42757117\n",
      "Iteration 274, loss = 0.42724685\n",
      "Iteration 275, loss = 0.42736665\n",
      "Iteration 276, loss = 0.42715483\n",
      "Iteration 277, loss = 0.42698261\n",
      "Iteration 278, loss = 0.42704232\n",
      "Iteration 279, loss = 0.42696748\n",
      "Iteration 280, loss = 0.42693401\n",
      "Iteration 281, loss = 0.42676728\n",
      "Iteration 282, loss = 0.42632009\n",
      "Iteration 283, loss = 0.42660071\n",
      "Iteration 284, loss = 0.42672318\n",
      "Iteration 285, loss = 0.42638831\n",
      "Iteration 286, loss = 0.42612757\n",
      "Iteration 287, loss = 0.42616610\n",
      "Iteration 288, loss = 0.42618262\n",
      "Iteration 289, loss = 0.42606582\n",
      "Iteration 290, loss = 0.42599996\n",
      "Iteration 291, loss = 0.42569791\n",
      "Iteration 292, loss = 0.42583775\n",
      "Iteration 293, loss = 0.42560733\n",
      "Iteration 294, loss = 0.42556394\n",
      "Iteration 295, loss = 0.42560623\n",
      "Iteration 296, loss = 0.42526092\n",
      "Iteration 297, loss = 0.42539464\n",
      "Iteration 298, loss = 0.42523699\n",
      "Iteration 299, loss = 0.42517446\n",
      "Iteration 300, loss = 0.42522582\n",
      "Iteration 301, loss = 0.42514500\n",
      "Iteration 302, loss = 0.42501744\n",
      "Iteration 303, loss = 0.42492813\n",
      "Iteration 304, loss = 0.42444171\n",
      "Iteration 305, loss = 0.42474960\n",
      "Iteration 306, loss = 0.42474605\n",
      "Iteration 307, loss = 0.42476372\n",
      "Iteration 308, loss = 0.42459925\n",
      "Iteration 309, loss = 0.42425622\n",
      "Iteration 310, loss = 0.42431347\n",
      "Iteration 311, loss = 0.42441724\n",
      "Iteration 312, loss = 0.42408087\n",
      "Iteration 313, loss = 0.42413551\n",
      "Iteration 314, loss = 0.42413120\n",
      "Iteration 315, loss = 0.42391119\n",
      "Iteration 316, loss = 0.42400248\n",
      "Iteration 317, loss = 0.42418502\n",
      "Iteration 318, loss = 0.42412680\n",
      "Iteration 319, loss = 0.42366377\n",
      "Iteration 320, loss = 0.42369996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 321, loss = 0.42355702\n",
      "Iteration 322, loss = 0.42373860\n",
      "Iteration 323, loss = 0.42350447\n",
      "Iteration 324, loss = 0.42340593\n",
      "Iteration 325, loss = 0.42320986\n",
      "Iteration 326, loss = 0.42340245\n",
      "Iteration 327, loss = 0.42302683\n",
      "Iteration 328, loss = 0.42319153\n",
      "Iteration 329, loss = 0.42301341\n",
      "Iteration 330, loss = 0.42289191\n",
      "Iteration 331, loss = 0.42286910\n",
      "Iteration 332, loss = 0.42285354\n",
      "Iteration 333, loss = 0.42281849\n",
      "Iteration 334, loss = 0.42259041\n",
      "Iteration 335, loss = 0.42276868\n",
      "Iteration 336, loss = 0.42261880\n",
      "Iteration 337, loss = 0.42260333\n",
      "Iteration 338, loss = 0.42245350\n",
      "Iteration 339, loss = 0.42240120\n",
      "Iteration 340, loss = 0.42225159\n",
      "Iteration 341, loss = 0.42219362\n",
      "Iteration 342, loss = 0.42238144\n",
      "Iteration 343, loss = 0.42220424\n",
      "Iteration 344, loss = 0.42216563\n",
      "Iteration 345, loss = 0.42190537\n",
      "Iteration 346, loss = 0.42195058\n",
      "Iteration 347, loss = 0.42205697\n",
      "Iteration 348, loss = 0.42197628\n",
      "Iteration 349, loss = 0.42200730\n",
      "Iteration 350, loss = 0.42189320\n",
      "Iteration 351, loss = 0.42157726\n",
      "Iteration 352, loss = 0.42168057\n",
      "Iteration 353, loss = 0.42170851\n",
      "Iteration 354, loss = 0.42155679\n",
      "Iteration 355, loss = 0.42168681\n",
      "Iteration 356, loss = 0.42138012\n",
      "Iteration 357, loss = 0.42132512\n",
      "Iteration 358, loss = 0.42122944\n",
      "Iteration 359, loss = 0.42128797\n",
      "Iteration 360, loss = 0.42126720\n",
      "Iteration 361, loss = 0.42131110\n",
      "Iteration 362, loss = 0.42127653\n",
      "Iteration 363, loss = 0.42104886\n",
      "Iteration 364, loss = 0.42099696\n",
      "Iteration 365, loss = 0.42122341\n",
      "Iteration 366, loss = 0.42115453\n",
      "Iteration 367, loss = 0.42062915\n",
      "Iteration 368, loss = 0.42103588\n",
      "Iteration 369, loss = 0.42079486\n",
      "Iteration 370, loss = 0.42074882\n",
      "Iteration 371, loss = 0.42072740\n",
      "Iteration 372, loss = 0.42065263\n",
      "Iteration 373, loss = 0.42080247\n",
      "Iteration 374, loss = 0.42043499\n",
      "Iteration 375, loss = 0.42044037\n",
      "Iteration 376, loss = 0.42036076\n",
      "Iteration 377, loss = 0.42034422\n",
      "Iteration 378, loss = 0.42046216\n",
      "Iteration 379, loss = 0.42016641\n",
      "Iteration 380, loss = 0.42034290\n",
      "Iteration 381, loss = 0.42027893\n",
      "Iteration 382, loss = 0.42002502\n",
      "Iteration 383, loss = 0.42013188\n",
      "Iteration 384, loss = 0.41976842\n",
      "Iteration 385, loss = 0.41997966\n",
      "Iteration 386, loss = 0.42007643\n",
      "Iteration 387, loss = 0.41997575\n",
      "Iteration 388, loss = 0.41988307\n",
      "Iteration 389, loss = 0.41958508\n",
      "Iteration 390, loss = 0.41970811\n",
      "Iteration 391, loss = 0.41986006\n",
      "Iteration 392, loss = 0.41971787\n",
      "Iteration 393, loss = 0.41973566\n",
      "Iteration 394, loss = 0.41948571\n",
      "Iteration 395, loss = 0.41970618\n",
      "Iteration 396, loss = 0.41970919\n",
      "Iteration 397, loss = 0.41964772\n",
      "Iteration 398, loss = 0.41925997\n",
      "Iteration 399, loss = 0.41963840\n",
      "Iteration 400, loss = 0.41923992\n",
      "Iteration 401, loss = 0.41919884\n",
      "Iteration 402, loss = 0.41925779\n",
      "Iteration 403, loss = 0.41892515\n",
      "Iteration 404, loss = 0.41940241\n",
      "Iteration 405, loss = 0.41908578\n",
      "Iteration 406, loss = 0.41925377\n",
      "Iteration 407, loss = 0.41892650\n",
      "Iteration 408, loss = 0.41913229\n",
      "Iteration 409, loss = 0.41928783\n",
      "Iteration 410, loss = 0.41887826\n",
      "Iteration 411, loss = 0.41885446\n",
      "Iteration 412, loss = 0.41894785\n",
      "Iteration 413, loss = 0.41890226\n",
      "Iteration 414, loss = 0.41871312\n",
      "Iteration 415, loss = 0.41862891\n",
      "Iteration 416, loss = 0.41888212\n",
      "Iteration 417, loss = 0.41859803\n",
      "Iteration 418, loss = 0.41870728\n",
      "Iteration 419, loss = 0.41867332\n",
      "Iteration 420, loss = 0.41838468\n",
      "Iteration 421, loss = 0.41873887\n",
      "Iteration 422, loss = 0.41833981\n",
      "Iteration 423, loss = 0.41816307\n",
      "Iteration 424, loss = 0.41866245\n",
      "Iteration 425, loss = 0.41820575\n",
      "Iteration 426, loss = 0.41838198\n",
      "Iteration 427, loss = 0.41846819\n",
      "Iteration 428, loss = 0.41818528\n",
      "Iteration 429, loss = 0.41814420\n",
      "Iteration 430, loss = 0.41817957\n",
      "Iteration 431, loss = 0.41814479\n",
      "Iteration 432, loss = 0.41815991\n",
      "Iteration 433, loss = 0.41798966\n",
      "Iteration 434, loss = 0.41807352\n",
      "Iteration 435, loss = 0.41798410\n",
      "Iteration 436, loss = 0.41766511\n",
      "Iteration 437, loss = 0.41808719\n",
      "Iteration 438, loss = 0.41787316\n",
      "Iteration 439, loss = 0.41778605\n",
      "Iteration 440, loss = 0.41770691\n",
      "Iteration 441, loss = 0.41787096\n",
      "Iteration 442, loss = 0.41792805\n",
      "Iteration 443, loss = 0.41748065\n",
      "Iteration 444, loss = 0.41808260\n",
      "Iteration 445, loss = 0.41742049\n",
      "Iteration 446, loss = 0.41774013\n",
      "Iteration 447, loss = 0.41742949\n",
      "Iteration 448, loss = 0.41770871\n",
      "Iteration 449, loss = 0.41718906\n",
      "Iteration 450, loss = 0.41747868\n",
      "Iteration 451, loss = 0.41778251\n",
      "Iteration 452, loss = 0.41753407\n",
      "Iteration 453, loss = 0.41712446\n",
      "Iteration 454, loss = 0.41722055\n",
      "Iteration 455, loss = 0.41709040\n",
      "Iteration 456, loss = 0.41696861\n",
      "Iteration 457, loss = 0.41744957\n",
      "Iteration 458, loss = 0.41705444\n",
      "Iteration 459, loss = 0.41755562\n",
      "Iteration 460, loss = 0.41721459\n",
      "Iteration 461, loss = 0.41709182\n",
      "Iteration 462, loss = 0.41700052\n",
      "Iteration 463, loss = 0.41691901\n",
      "Iteration 464, loss = 0.41691648\n",
      "Iteration 465, loss = 0.41681350\n",
      "Iteration 466, loss = 0.41686457\n",
      "Iteration 467, loss = 0.41699191\n",
      "Iteration 468, loss = 0.41678609\n",
      "Iteration 469, loss = 0.41694662\n",
      "Iteration 470, loss = 0.41687809\n",
      "Iteration 471, loss = 0.41681277\n",
      "Iteration 472, loss = 0.41676093\n",
      "Iteration 473, loss = 0.41686341\n",
      "Iteration 474, loss = 0.41660543\n",
      "Iteration 475, loss = 0.41666471\n",
      "Iteration 476, loss = 0.41665937\n",
      "Iteration 477, loss = 0.41652437\n",
      "Iteration 478, loss = 0.41626618\n",
      "Iteration 479, loss = 0.41634703\n",
      "Iteration 480, loss = 0.41653126\n",
      "Iteration 481, loss = 0.41655184\n",
      "Iteration 482, loss = 0.41663644\n",
      "Iteration 483, loss = 0.41625711\n",
      "Iteration 484, loss = 0.41614263\n",
      "Iteration 485, loss = 0.41655780\n",
      "Iteration 486, loss = 0.41608108\n",
      "Iteration 487, loss = 0.41628103\n",
      "Iteration 488, loss = 0.41614979\n",
      "Iteration 489, loss = 0.41596065\n",
      "Iteration 490, loss = 0.41583403\n",
      "Iteration 491, loss = 0.41592500\n",
      "Iteration 492, loss = 0.41565301\n",
      "Iteration 493, loss = 0.41601067\n",
      "Iteration 494, loss = 0.41610426\n",
      "Iteration 495, loss = 0.41595886\n",
      "Iteration 496, loss = 0.41573999\n",
      "Iteration 497, loss = 0.41601816\n",
      "Iteration 498, loss = 0.41594507\n",
      "Iteration 499, loss = 0.41566464\n",
      "Iteration 500, loss = 0.41574621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96739071\n",
      "Iteration 2, loss = 1.78132103\n",
      "Iteration 3, loss = 1.60968976\n",
      "Iteration 4, loss = 1.45890514\n",
      "Iteration 5, loss = 1.33013014\n",
      "Iteration 6, loss = 1.22336376\n",
      "Iteration 7, loss = 1.13718794\n",
      "Iteration 8, loss = 1.06679167\n",
      "Iteration 9, loss = 1.00763870\n",
      "Iteration 10, loss = 0.95606548\n",
      "Iteration 11, loss = 0.91167978\n",
      "Iteration 12, loss = 0.87404012\n",
      "Iteration 13, loss = 0.84074028\n",
      "Iteration 14, loss = 0.81112552\n",
      "Iteration 15, loss = 0.78462990\n",
      "Iteration 16, loss = 0.76036548\n",
      "Iteration 17, loss = 0.73860287\n",
      "Iteration 18, loss = 0.71857572\n",
      "Iteration 19, loss = 0.70055204\n",
      "Iteration 20, loss = 0.68429948\n",
      "Iteration 21, loss = 0.66928247\n",
      "Iteration 22, loss = 0.65586935\n",
      "Iteration 23, loss = 0.64375352\n",
      "Iteration 24, loss = 0.63313730\n",
      "Iteration 25, loss = 0.62266679\n",
      "Iteration 26, loss = 0.61366829\n",
      "Iteration 27, loss = 0.60538703\n",
      "Iteration 28, loss = 0.59797960\n",
      "Iteration 29, loss = 0.59140068\n",
      "Iteration 30, loss = 0.58500087\n",
      "Iteration 31, loss = 0.57940464\n",
      "Iteration 32, loss = 0.57428192\n",
      "Iteration 33, loss = 0.56958001\n",
      "Iteration 34, loss = 0.56501399\n",
      "Iteration 35, loss = 0.56100322\n",
      "Iteration 36, loss = 0.55705594\n",
      "Iteration 37, loss = 0.55354357\n",
      "Iteration 38, loss = 0.55025783\n",
      "Iteration 39, loss = 0.54718958\n",
      "Iteration 40, loss = 0.54427330\n",
      "Iteration 41, loss = 0.54156458\n",
      "Iteration 42, loss = 0.53894018\n",
      "Iteration 43, loss = 0.53620927\n",
      "Iteration 44, loss = 0.53392444\n",
      "Iteration 45, loss = 0.53174464\n",
      "Iteration 46, loss = 0.52970749\n",
      "Iteration 47, loss = 0.52774768\n",
      "Iteration 48, loss = 0.52533437\n",
      "Iteration 49, loss = 0.52365464\n",
      "Iteration 50, loss = 0.52194966\n",
      "Iteration 51, loss = 0.52022559\n",
      "Iteration 52, loss = 0.51855813\n",
      "Iteration 53, loss = 0.51712672\n",
      "Iteration 54, loss = 0.51543352\n",
      "Iteration 55, loss = 0.51397960\n",
      "Iteration 56, loss = 0.51259426\n",
      "Iteration 57, loss = 0.51115620\n",
      "Iteration 58, loss = 0.50978577\n",
      "Iteration 59, loss = 0.50856246\n",
      "Iteration 60, loss = 0.50719773\n",
      "Iteration 61, loss = 0.50605815\n",
      "Iteration 62, loss = 0.50503040\n",
      "Iteration 63, loss = 0.50390946\n",
      "Iteration 64, loss = 0.50274341\n",
      "Iteration 65, loss = 0.50177856\n",
      "Iteration 66, loss = 0.50057266\n",
      "Iteration 67, loss = 0.49934498\n",
      "Iteration 68, loss = 0.49854356\n",
      "Iteration 69, loss = 0.49766710\n",
      "Iteration 70, loss = 0.49669554\n",
      "Iteration 71, loss = 0.49569590\n",
      "Iteration 72, loss = 0.49485134\n",
      "Iteration 73, loss = 0.49406693\n",
      "Iteration 74, loss = 0.49309813\n",
      "Iteration 75, loss = 0.49226940\n",
      "Iteration 76, loss = 0.49131127\n",
      "Iteration 77, loss = 0.49053781\n",
      "Iteration 78, loss = 0.48975808\n",
      "Iteration 79, loss = 0.48919084\n",
      "Iteration 80, loss = 0.48827851\n",
      "Iteration 81, loss = 0.48751019\n",
      "Iteration 82, loss = 0.48671526\n",
      "Iteration 83, loss = 0.48583755\n",
      "Iteration 84, loss = 0.48530388\n",
      "Iteration 85, loss = 0.48446534\n",
      "Iteration 86, loss = 0.48375203\n",
      "Iteration 87, loss = 0.48315145\n",
      "Iteration 88, loss = 0.48264215\n",
      "Iteration 89, loss = 0.48199290\n",
      "Iteration 90, loss = 0.48110606\n",
      "Iteration 91, loss = 0.48045450\n",
      "Iteration 92, loss = 0.47983678\n",
      "Iteration 93, loss = 0.47920231\n",
      "Iteration 94, loss = 0.47879871\n",
      "Iteration 95, loss = 0.47807039\n",
      "Iteration 96, loss = 0.47741942\n",
      "Iteration 97, loss = 0.47670821\n",
      "Iteration 98, loss = 0.47617479\n",
      "Iteration 99, loss = 0.47572885\n",
      "Iteration 100, loss = 0.47502849\n",
      "Iteration 101, loss = 0.47430175\n",
      "Iteration 102, loss = 0.47393014\n",
      "Iteration 103, loss = 0.47314445\n",
      "Iteration 104, loss = 0.47257086\n",
      "Iteration 105, loss = 0.47211417\n",
      "Iteration 106, loss = 0.47160813\n",
      "Iteration 107, loss = 0.47116338\n",
      "Iteration 108, loss = 0.47053141\n",
      "Iteration 109, loss = 0.46990494\n",
      "Iteration 110, loss = 0.46959921\n",
      "Iteration 111, loss = 0.46907671\n",
      "Iteration 112, loss = 0.46849105\n",
      "Iteration 113, loss = 0.46786956\n",
      "Iteration 114, loss = 0.46754468\n",
      "Iteration 115, loss = 0.46687319\n",
      "Iteration 116, loss = 0.46649446\n",
      "Iteration 117, loss = 0.46616747\n",
      "Iteration 118, loss = 0.46525319\n",
      "Iteration 119, loss = 0.46498593\n",
      "Iteration 120, loss = 0.46446056\n",
      "Iteration 121, loss = 0.46405356\n",
      "Iteration 122, loss = 0.46359750\n",
      "Iteration 123, loss = 0.46294744\n",
      "Iteration 124, loss = 0.46246837\n",
      "Iteration 125, loss = 0.46202151\n",
      "Iteration 126, loss = 0.46147888\n",
      "Iteration 127, loss = 0.46114327\n",
      "Iteration 128, loss = 0.46051028\n",
      "Iteration 129, loss = 0.46032800\n",
      "Iteration 130, loss = 0.46008507\n",
      "Iteration 131, loss = 0.45943586\n",
      "Iteration 132, loss = 0.45902749\n",
      "Iteration 133, loss = 0.45881854\n",
      "Iteration 134, loss = 0.45808216\n",
      "Iteration 135, loss = 0.45774347\n",
      "Iteration 136, loss = 0.45732280\n",
      "Iteration 137, loss = 0.45681306\n",
      "Iteration 138, loss = 0.45658877\n",
      "Iteration 139, loss = 0.45626009\n",
      "Iteration 140, loss = 0.45566959\n",
      "Iteration 141, loss = 0.45536110\n",
      "Iteration 142, loss = 0.45509650\n",
      "Iteration 143, loss = 0.45460494\n",
      "Iteration 144, loss = 0.45432535\n",
      "Iteration 145, loss = 0.45388005\n",
      "Iteration 146, loss = 0.45348565\n",
      "Iteration 147, loss = 0.45318549\n",
      "Iteration 148, loss = 0.45271227\n",
      "Iteration 149, loss = 0.45229863\n",
      "Iteration 150, loss = 0.45206578\n",
      "Iteration 151, loss = 0.45161496\n",
      "Iteration 152, loss = 0.45135304\n",
      "Iteration 153, loss = 0.45089115\n",
      "Iteration 154, loss = 0.45050945\n",
      "Iteration 155, loss = 0.45007934\n",
      "Iteration 156, loss = 0.44991209\n",
      "Iteration 157, loss = 0.44956794\n",
      "Iteration 158, loss = 0.44924796\n",
      "Iteration 159, loss = 0.44886892\n",
      "Iteration 160, loss = 0.44847535\n",
      "Iteration 161, loss = 0.44829656\n",
      "Iteration 162, loss = 0.44796793\n",
      "Iteration 163, loss = 0.44743109\n",
      "Iteration 164, loss = 0.44756210\n",
      "Iteration 165, loss = 0.44687562\n",
      "Iteration 166, loss = 0.44664120\n",
      "Iteration 167, loss = 0.44623909\n",
      "Iteration 168, loss = 0.44601535\n",
      "Iteration 169, loss = 0.44573446\n",
      "Iteration 170, loss = 0.44564540\n",
      "Iteration 171, loss = 0.44498875\n",
      "Iteration 172, loss = 0.44511946\n",
      "Iteration 173, loss = 0.44470613\n",
      "Iteration 174, loss = 0.44460676\n",
      "Iteration 175, loss = 0.44391613\n",
      "Iteration 176, loss = 0.44382788\n",
      "Iteration 177, loss = 0.44368602\n",
      "Iteration 178, loss = 0.44324635\n",
      "Iteration 179, loss = 0.44303458\n",
      "Iteration 180, loss = 0.44269257\n",
      "Iteration 181, loss = 0.44253751\n",
      "Iteration 182, loss = 0.44227852\n",
      "Iteration 183, loss = 0.44209620\n",
      "Iteration 184, loss = 0.44147533\n",
      "Iteration 185, loss = 0.44196958\n",
      "Iteration 186, loss = 0.44125981\n",
      "Iteration 187, loss = 0.44102710\n",
      "Iteration 188, loss = 0.44085449\n",
      "Iteration 189, loss = 0.44053442\n",
      "Iteration 190, loss = 0.44058890\n",
      "Iteration 191, loss = 0.44006325\n",
      "Iteration 192, loss = 0.43981248\n",
      "Iteration 193, loss = 0.43952241\n",
      "Iteration 194, loss = 0.43959554\n",
      "Iteration 195, loss = 0.43897306\n",
      "Iteration 196, loss = 0.43896009\n",
      "Iteration 197, loss = 0.43881742\n",
      "Iteration 198, loss = 0.43849745\n",
      "Iteration 199, loss = 0.43813157\n",
      "Iteration 200, loss = 0.43802552\n",
      "Iteration 201, loss = 0.43798053\n",
      "Iteration 202, loss = 0.43769873\n",
      "Iteration 203, loss = 0.43756033\n",
      "Iteration 204, loss = 0.43736100\n",
      "Iteration 205, loss = 0.43728154\n",
      "Iteration 206, loss = 0.43694594\n",
      "Iteration 207, loss = 0.43676396\n",
      "Iteration 208, loss = 0.43643365\n",
      "Iteration 209, loss = 0.43640728\n",
      "Iteration 210, loss = 0.43610659\n",
      "Iteration 211, loss = 0.43581740\n",
      "Iteration 212, loss = 0.43607936\n",
      "Iteration 213, loss = 0.43558599\n",
      "Iteration 214, loss = 0.43519012\n",
      "Iteration 215, loss = 0.43550464\n",
      "Iteration 216, loss = 0.43495563\n",
      "Iteration 217, loss = 0.43482602\n",
      "Iteration 218, loss = 0.43488478\n",
      "Iteration 219, loss = 0.43454028\n",
      "Iteration 220, loss = 0.43457906\n",
      "Iteration 221, loss = 0.43419685\n",
      "Iteration 222, loss = 0.43396185\n",
      "Iteration 223, loss = 0.43396705\n",
      "Iteration 224, loss = 0.43369541\n",
      "Iteration 225, loss = 0.43370765\n",
      "Iteration 226, loss = 0.43340548\n",
      "Iteration 227, loss = 0.43319929\n",
      "Iteration 228, loss = 0.43329415\n",
      "Iteration 229, loss = 0.43292215\n",
      "Iteration 230, loss = 0.43276094\n",
      "Iteration 231, loss = 0.43248293\n",
      "Iteration 232, loss = 0.43243126\n",
      "Iteration 233, loss = 0.43205721\n",
      "Iteration 234, loss = 0.43218284\n",
      "Iteration 235, loss = 0.43196744\n",
      "Iteration 236, loss = 0.43180910\n",
      "Iteration 237, loss = 0.43180505\n",
      "Iteration 238, loss = 0.43150916\n",
      "Iteration 239, loss = 0.43160309\n",
      "Iteration 240, loss = 0.43148651\n",
      "Iteration 241, loss = 0.43127331\n",
      "Iteration 242, loss = 0.43099409\n",
      "Iteration 243, loss = 0.43098160\n",
      "Iteration 244, loss = 0.43063432\n",
      "Iteration 245, loss = 0.43063191\n",
      "Iteration 246, loss = 0.43049222\n",
      "Iteration 247, loss = 0.43040272\n",
      "Iteration 248, loss = 0.43032976\n",
      "Iteration 249, loss = 0.42980495\n",
      "Iteration 250, loss = 0.43023519\n",
      "Iteration 251, loss = 0.43009797\n",
      "Iteration 252, loss = 0.42999154\n",
      "Iteration 253, loss = 0.42961430\n",
      "Iteration 254, loss = 0.42972358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.42936481\n",
      "Iteration 256, loss = 0.42942089\n",
      "Iteration 257, loss = 0.42901941\n",
      "Iteration 258, loss = 0.42917696\n",
      "Iteration 259, loss = 0.42900098\n",
      "Iteration 260, loss = 0.42881423\n",
      "Iteration 261, loss = 0.42871004\n",
      "Iteration 262, loss = 0.42866705\n",
      "Iteration 263, loss = 0.42851339\n",
      "Iteration 264, loss = 0.42836599\n",
      "Iteration 265, loss = 0.42812874\n",
      "Iteration 266, loss = 0.42817667\n",
      "Iteration 267, loss = 0.42795037\n",
      "Iteration 268, loss = 0.42803992\n",
      "Iteration 269, loss = 0.42797573\n",
      "Iteration 270, loss = 0.42766074\n",
      "Iteration 271, loss = 0.42771370\n",
      "Iteration 272, loss = 0.42770392\n",
      "Iteration 273, loss = 0.42748909\n",
      "Iteration 274, loss = 0.42753955\n",
      "Iteration 275, loss = 0.42711466\n",
      "Iteration 276, loss = 0.42717681\n",
      "Iteration 277, loss = 0.42711951\n",
      "Iteration 278, loss = 0.42701174\n",
      "Iteration 279, loss = 0.42709908\n",
      "Iteration 280, loss = 0.42686268\n",
      "Iteration 281, loss = 0.42671310\n",
      "Iteration 282, loss = 0.42651422\n",
      "Iteration 283, loss = 0.42653440\n",
      "Iteration 284, loss = 0.42667202\n",
      "Iteration 285, loss = 0.42637316\n",
      "Iteration 286, loss = 0.42640524\n",
      "Iteration 287, loss = 0.42624801\n",
      "Iteration 288, loss = 0.42649346\n",
      "Iteration 289, loss = 0.42585039\n",
      "Iteration 290, loss = 0.42592802\n",
      "Iteration 291, loss = 0.42601580\n",
      "Iteration 292, loss = 0.42584314\n",
      "Iteration 293, loss = 0.42585605\n",
      "Iteration 294, loss = 0.42548341\n",
      "Iteration 295, loss = 0.42527087\n",
      "Iteration 296, loss = 0.42541324\n",
      "Iteration 297, loss = 0.42534747\n",
      "Iteration 298, loss = 0.42523310\n",
      "Iteration 299, loss = 0.42542542\n",
      "Iteration 300, loss = 0.42528556\n",
      "Iteration 301, loss = 0.42490534\n",
      "Iteration 302, loss = 0.42500618\n",
      "Iteration 303, loss = 0.42488273\n",
      "Iteration 304, loss = 0.42497496\n",
      "Iteration 305, loss = 0.42497708\n",
      "Iteration 306, loss = 0.42435789\n",
      "Iteration 307, loss = 0.42435957\n",
      "Iteration 308, loss = 0.42450352\n",
      "Iteration 309, loss = 0.42448240\n",
      "Iteration 310, loss = 0.42417309\n",
      "Iteration 311, loss = 0.42413876\n",
      "Iteration 312, loss = 0.42425735\n",
      "Iteration 313, loss = 0.42422299\n",
      "Iteration 314, loss = 0.42404792\n",
      "Iteration 315, loss = 0.42400419\n",
      "Iteration 316, loss = 0.42399495\n",
      "Iteration 317, loss = 0.42377458\n",
      "Iteration 318, loss = 0.42388613\n",
      "Iteration 319, loss = 0.42370435\n",
      "Iteration 320, loss = 0.42382854\n",
      "Iteration 321, loss = 0.42367967\n",
      "Iteration 322, loss = 0.42345140\n",
      "Iteration 323, loss = 0.42330625\n",
      "Iteration 324, loss = 0.42335114\n",
      "Iteration 325, loss = 0.42350554\n",
      "Iteration 326, loss = 0.42306492\n",
      "Iteration 327, loss = 0.42328008\n",
      "Iteration 328, loss = 0.42302438\n",
      "Iteration 329, loss = 0.42299987\n",
      "Iteration 330, loss = 0.42293134\n",
      "Iteration 331, loss = 0.42284951\n",
      "Iteration 332, loss = 0.42270024\n",
      "Iteration 333, loss = 0.42262149\n",
      "Iteration 334, loss = 0.42294612\n",
      "Iteration 335, loss = 0.42277530\n",
      "Iteration 336, loss = 0.42253734\n",
      "Iteration 337, loss = 0.42233199\n",
      "Iteration 338, loss = 0.42224894\n",
      "Iteration 339, loss = 0.42263309\n",
      "Iteration 340, loss = 0.42246964\n",
      "Iteration 341, loss = 0.42238973\n",
      "Iteration 342, loss = 0.42208942\n",
      "Iteration 343, loss = 0.42210379\n",
      "Iteration 344, loss = 0.42204744\n",
      "Iteration 345, loss = 0.42191305\n",
      "Iteration 346, loss = 0.42201765\n",
      "Iteration 347, loss = 0.42175991\n",
      "Iteration 348, loss = 0.42182570\n",
      "Iteration 349, loss = 0.42188161\n",
      "Iteration 350, loss = 0.42155669\n",
      "Iteration 351, loss = 0.42182151\n",
      "Iteration 352, loss = 0.42157498\n",
      "Iteration 353, loss = 0.42158153\n",
      "Iteration 354, loss = 0.42154609\n",
      "Iteration 355, loss = 0.42156119\n",
      "Iteration 356, loss = 0.42150511\n",
      "Iteration 357, loss = 0.42152981\n",
      "Iteration 358, loss = 0.42140413\n",
      "Iteration 359, loss = 0.42116503\n",
      "Iteration 360, loss = 0.42121879\n",
      "Iteration 361, loss = 0.42114497\n",
      "Iteration 362, loss = 0.42095587\n",
      "Iteration 363, loss = 0.42127461\n",
      "Iteration 364, loss = 0.42113905\n",
      "Iteration 365, loss = 0.42107529\n",
      "Iteration 366, loss = 0.42079201\n",
      "Iteration 367, loss = 0.42086263\n",
      "Iteration 368, loss = 0.42095141\n",
      "Iteration 369, loss = 0.42088345\n",
      "Iteration 370, loss = 0.42050301\n",
      "Iteration 371, loss = 0.42074366\n",
      "Iteration 372, loss = 0.42059214\n",
      "Iteration 373, loss = 0.42065180\n",
      "Iteration 374, loss = 0.42031706\n",
      "Iteration 375, loss = 0.42043313\n",
      "Iteration 376, loss = 0.42038295\n",
      "Iteration 377, loss = 0.42031322\n",
      "Iteration 378, loss = 0.42018315\n",
      "Iteration 379, loss = 0.42024290\n",
      "Iteration 380, loss = 0.42024923\n",
      "Iteration 381, loss = 0.41989425\n",
      "Iteration 382, loss = 0.41998774\n",
      "Iteration 383, loss = 0.42000001\n",
      "Iteration 384, loss = 0.41984357\n",
      "Iteration 385, loss = 0.42006715\n",
      "Iteration 386, loss = 0.42004707\n",
      "Iteration 387, loss = 0.42007034\n",
      "Iteration 388, loss = 0.41994988\n",
      "Iteration 389, loss = 0.41962733\n",
      "Iteration 390, loss = 0.41958186\n",
      "Iteration 391, loss = 0.41952782\n",
      "Iteration 392, loss = 0.41962838\n",
      "Iteration 393, loss = 0.41964518\n",
      "Iteration 394, loss = 0.41960573\n",
      "Iteration 395, loss = 0.41968634\n",
      "Iteration 396, loss = 0.41956086\n",
      "Iteration 397, loss = 0.41938035\n",
      "Iteration 398, loss = 0.41917253\n",
      "Iteration 399, loss = 0.41922602\n",
      "Iteration 400, loss = 0.41948594\n",
      "Iteration 401, loss = 0.41926626\n",
      "Iteration 402, loss = 0.41909993\n",
      "Iteration 403, loss = 0.41893774\n",
      "Iteration 404, loss = 0.41901976\n",
      "Iteration 405, loss = 0.41896857\n",
      "Iteration 406, loss = 0.41879435\n",
      "Iteration 407, loss = 0.41914250\n",
      "Iteration 408, loss = 0.41881978\n",
      "Iteration 409, loss = 0.41898205\n",
      "Iteration 410, loss = 0.41883906\n",
      "Iteration 411, loss = 0.41852136\n",
      "Iteration 412, loss = 0.41885307\n",
      "Iteration 413, loss = 0.41870399\n",
      "Iteration 414, loss = 0.41866698\n",
      "Iteration 415, loss = 0.41869748\n",
      "Iteration 416, loss = 0.41882047\n",
      "Iteration 417, loss = 0.41835935\n",
      "Iteration 418, loss = 0.41837178\n",
      "Iteration 419, loss = 0.41832170\n",
      "Iteration 420, loss = 0.41835021\n",
      "Iteration 421, loss = 0.41854440\n",
      "Iteration 422, loss = 0.41836208\n",
      "Iteration 423, loss = 0.41818579\n",
      "Iteration 424, loss = 0.41840826\n",
      "Iteration 425, loss = 0.41829467\n",
      "Iteration 426, loss = 0.41810555\n",
      "Iteration 427, loss = 0.41793744\n",
      "Iteration 428, loss = 0.41834347\n",
      "Iteration 429, loss = 0.41798119\n",
      "Iteration 430, loss = 0.41817401\n",
      "Iteration 431, loss = 0.41808418\n",
      "Iteration 432, loss = 0.41799551\n",
      "Iteration 433, loss = 0.41778056\n",
      "Iteration 434, loss = 0.41783794\n",
      "Iteration 435, loss = 0.41762204\n",
      "Iteration 436, loss = 0.41764560\n",
      "Iteration 437, loss = 0.41796190\n",
      "Iteration 438, loss = 0.41801728\n",
      "Iteration 439, loss = 0.41761821\n",
      "Iteration 440, loss = 0.41754581\n",
      "Iteration 441, loss = 0.41756364\n",
      "Iteration 442, loss = 0.41744910\n",
      "Iteration 443, loss = 0.41748708\n",
      "Iteration 444, loss = 0.41774615\n",
      "Iteration 445, loss = 0.41759677\n",
      "Iteration 446, loss = 0.41745940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96623530\n",
      "Iteration 2, loss = 1.77932724\n",
      "Iteration 3, loss = 1.60656736\n",
      "Iteration 4, loss = 1.45517078\n",
      "Iteration 5, loss = 1.32637705\n",
      "Iteration 6, loss = 1.21975101\n",
      "Iteration 7, loss = 1.13308404\n",
      "Iteration 8, loss = 1.06090933\n",
      "Iteration 9, loss = 1.00074216\n",
      "Iteration 10, loss = 0.95038949\n",
      "Iteration 11, loss = 0.90727273\n",
      "Iteration 12, loss = 0.86956957\n",
      "Iteration 13, loss = 0.83648914\n",
      "Iteration 14, loss = 0.80697548\n",
      "Iteration 15, loss = 0.78052991\n",
      "Iteration 16, loss = 0.75645734\n",
      "Iteration 17, loss = 0.73467625\n",
      "Iteration 18, loss = 0.71501128\n",
      "Iteration 19, loss = 0.69726654\n",
      "Iteration 20, loss = 0.68102673\n",
      "Iteration 21, loss = 0.66648368\n",
      "Iteration 22, loss = 0.65331016\n",
      "Iteration 23, loss = 0.64140577\n",
      "Iteration 24, loss = 0.63056802\n",
      "Iteration 25, loss = 0.62065750\n",
      "Iteration 26, loss = 0.61132599\n",
      "Iteration 27, loss = 0.60358327\n",
      "Iteration 28, loss = 0.59628641\n",
      "Iteration 29, loss = 0.58963458\n",
      "Iteration 30, loss = 0.58371414\n",
      "Iteration 31, loss = 0.57808629\n",
      "Iteration 32, loss = 0.57285620\n",
      "Iteration 33, loss = 0.56806696\n",
      "Iteration 34, loss = 0.56371341\n",
      "Iteration 35, loss = 0.55973558\n",
      "Iteration 36, loss = 0.55597308\n",
      "Iteration 37, loss = 0.55252093\n",
      "Iteration 38, loss = 0.54909040\n",
      "Iteration 39, loss = 0.54585245\n",
      "Iteration 40, loss = 0.54305248\n",
      "Iteration 41, loss = 0.54024890\n",
      "Iteration 42, loss = 0.53742263\n",
      "Iteration 43, loss = 0.53492508\n",
      "Iteration 44, loss = 0.53269625\n",
      "Iteration 45, loss = 0.53047600\n",
      "Iteration 46, loss = 0.52843285\n",
      "Iteration 47, loss = 0.52649903\n",
      "Iteration 48, loss = 0.52434459\n",
      "Iteration 49, loss = 0.52263450\n",
      "Iteration 50, loss = 0.52068673\n",
      "Iteration 51, loss = 0.51889336\n",
      "Iteration 52, loss = 0.51747335\n",
      "Iteration 53, loss = 0.51582806\n",
      "Iteration 54, loss = 0.51443292\n",
      "Iteration 55, loss = 0.51283472\n",
      "Iteration 56, loss = 0.51148139\n",
      "Iteration 57, loss = 0.51020400\n",
      "Iteration 58, loss = 0.50867745\n",
      "Iteration 59, loss = 0.50766576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.50625654\n",
      "Iteration 61, loss = 0.50499495\n",
      "Iteration 62, loss = 0.50394320\n",
      "Iteration 63, loss = 0.50297342\n",
      "Iteration 64, loss = 0.50172400\n",
      "Iteration 65, loss = 0.50045006\n",
      "Iteration 66, loss = 0.49944314\n",
      "Iteration 67, loss = 0.49860379\n",
      "Iteration 68, loss = 0.49745978\n",
      "Iteration 69, loss = 0.49640644\n",
      "Iteration 70, loss = 0.49550582\n",
      "Iteration 71, loss = 0.49488360\n",
      "Iteration 72, loss = 0.49372174\n",
      "Iteration 73, loss = 0.49294450\n",
      "Iteration 74, loss = 0.49209953\n",
      "Iteration 75, loss = 0.49112829\n",
      "Iteration 76, loss = 0.49064298\n",
      "Iteration 77, loss = 0.48944375\n",
      "Iteration 78, loss = 0.48867461\n",
      "Iteration 79, loss = 0.48790615\n",
      "Iteration 80, loss = 0.48723862\n",
      "Iteration 81, loss = 0.48628240\n",
      "Iteration 82, loss = 0.48534316\n",
      "Iteration 83, loss = 0.48469727\n",
      "Iteration 84, loss = 0.48413583\n",
      "Iteration 85, loss = 0.48341673\n",
      "Iteration 86, loss = 0.48272731\n",
      "Iteration 87, loss = 0.48183553\n",
      "Iteration 88, loss = 0.48145369\n",
      "Iteration 89, loss = 0.48072858\n",
      "Iteration 90, loss = 0.47993112\n",
      "Iteration 91, loss = 0.47909724\n",
      "Iteration 92, loss = 0.47866108\n",
      "Iteration 93, loss = 0.47808053\n",
      "Iteration 94, loss = 0.47749735\n",
      "Iteration 95, loss = 0.47695265\n",
      "Iteration 96, loss = 0.47615451\n",
      "Iteration 97, loss = 0.47563868\n",
      "Iteration 98, loss = 0.47500919\n",
      "Iteration 99, loss = 0.47436257\n",
      "Iteration 100, loss = 0.47378945\n",
      "Iteration 101, loss = 0.47311616\n",
      "Iteration 102, loss = 0.47267217\n",
      "Iteration 103, loss = 0.47207235\n",
      "Iteration 104, loss = 0.47119828\n",
      "Iteration 105, loss = 0.47108543\n",
      "Iteration 106, loss = 0.47014713\n",
      "Iteration 107, loss = 0.46995849\n",
      "Iteration 108, loss = 0.46937259\n",
      "Iteration 109, loss = 0.46850797\n",
      "Iteration 110, loss = 0.46830941\n",
      "Iteration 111, loss = 0.46747929\n",
      "Iteration 112, loss = 0.46711951\n",
      "Iteration 113, loss = 0.46642513\n",
      "Iteration 114, loss = 0.46581385\n",
      "Iteration 115, loss = 0.46558799\n",
      "Iteration 116, loss = 0.46514685\n",
      "Iteration 117, loss = 0.46492091\n",
      "Iteration 118, loss = 0.46394523\n",
      "Iteration 119, loss = 0.46349966\n",
      "Iteration 120, loss = 0.46318105\n",
      "Iteration 121, loss = 0.46245527\n",
      "Iteration 122, loss = 0.46233548\n",
      "Iteration 123, loss = 0.46141700\n",
      "Iteration 124, loss = 0.46122662\n",
      "Iteration 125, loss = 0.46045813\n",
      "Iteration 126, loss = 0.46025438\n",
      "Iteration 127, loss = 0.45965070\n",
      "Iteration 128, loss = 0.45937137\n",
      "Iteration 129, loss = 0.45887539\n",
      "Iteration 130, loss = 0.45838477\n",
      "Iteration 131, loss = 0.45776904\n",
      "Iteration 132, loss = 0.45751144\n",
      "Iteration 133, loss = 0.45696288\n",
      "Iteration 134, loss = 0.45667069\n",
      "Iteration 135, loss = 0.45633538\n",
      "Iteration 136, loss = 0.45587808\n",
      "Iteration 137, loss = 0.45536125\n",
      "Iteration 138, loss = 0.45486612\n",
      "Iteration 139, loss = 0.45470447\n",
      "Iteration 140, loss = 0.45431109\n",
      "Iteration 141, loss = 0.45365876\n",
      "Iteration 142, loss = 0.45340433\n",
      "Iteration 143, loss = 0.45293171\n",
      "Iteration 144, loss = 0.45279263\n",
      "Iteration 145, loss = 0.45205414\n",
      "Iteration 146, loss = 0.45173161\n",
      "Iteration 147, loss = 0.45143568\n",
      "Iteration 148, loss = 0.45113205\n",
      "Iteration 149, loss = 0.45063960\n",
      "Iteration 150, loss = 0.45029628\n",
      "Iteration 151, loss = 0.44988916\n",
      "Iteration 152, loss = 0.44946357\n",
      "Iteration 153, loss = 0.44927475\n",
      "Iteration 154, loss = 0.44890205\n",
      "Iteration 155, loss = 0.44869954\n",
      "Iteration 156, loss = 0.44822835\n",
      "Iteration 157, loss = 0.44770214\n",
      "Iteration 158, loss = 0.44746678\n",
      "Iteration 159, loss = 0.44712606\n",
      "Iteration 160, loss = 0.44667932\n",
      "Iteration 161, loss = 0.44659455\n",
      "Iteration 162, loss = 0.44630921\n",
      "Iteration 163, loss = 0.44574436\n",
      "Iteration 164, loss = 0.44535560\n",
      "Iteration 165, loss = 0.44520813\n",
      "Iteration 166, loss = 0.44507767\n",
      "Iteration 167, loss = 0.44458182\n",
      "Iteration 168, loss = 0.44434005\n",
      "Iteration 169, loss = 0.44382818\n",
      "Iteration 170, loss = 0.44349684\n",
      "Iteration 171, loss = 0.44307935\n",
      "Iteration 172, loss = 0.44288673\n",
      "Iteration 173, loss = 0.44276196\n",
      "Iteration 174, loss = 0.44238592\n",
      "Iteration 175, loss = 0.44202987\n",
      "Iteration 176, loss = 0.44206349\n",
      "Iteration 177, loss = 0.44181830\n",
      "Iteration 178, loss = 0.44153444\n",
      "Iteration 179, loss = 0.44092395\n",
      "Iteration 180, loss = 0.44075686\n",
      "Iteration 181, loss = 0.44071183\n",
      "Iteration 182, loss = 0.43995136\n",
      "Iteration 183, loss = 0.43984685\n",
      "Iteration 184, loss = 0.43955949\n",
      "Iteration 185, loss = 0.43958031\n",
      "Iteration 186, loss = 0.43944070\n",
      "Iteration 187, loss = 0.43896294\n",
      "Iteration 188, loss = 0.43882393\n",
      "Iteration 189, loss = 0.43864139\n",
      "Iteration 190, loss = 0.43840831\n",
      "Iteration 191, loss = 0.43806414\n",
      "Iteration 192, loss = 0.43758131\n",
      "Iteration 193, loss = 0.43766831\n",
      "Iteration 194, loss = 0.43761466\n",
      "Iteration 195, loss = 0.43701991\n",
      "Iteration 196, loss = 0.43692996\n",
      "Iteration 197, loss = 0.43648332\n",
      "Iteration 198, loss = 0.43639714\n",
      "Iteration 199, loss = 0.43624503\n",
      "Iteration 200, loss = 0.43596196\n",
      "Iteration 201, loss = 0.43578030\n",
      "Iteration 202, loss = 0.43567822\n",
      "Iteration 203, loss = 0.43543745\n",
      "Iteration 204, loss = 0.43532849\n",
      "Iteration 205, loss = 0.43484855\n",
      "Iteration 206, loss = 0.43485722\n",
      "Iteration 207, loss = 0.43458766\n",
      "Iteration 208, loss = 0.43437190\n",
      "Iteration 209, loss = 0.43428017\n",
      "Iteration 210, loss = 0.43407209\n",
      "Iteration 211, loss = 0.43371208\n",
      "Iteration 212, loss = 0.43373216\n",
      "Iteration 213, loss = 0.43339652\n",
      "Iteration 214, loss = 0.43369752\n",
      "Iteration 215, loss = 0.43327484\n",
      "Iteration 216, loss = 0.43267392\n",
      "Iteration 217, loss = 0.43278305\n",
      "Iteration 218, loss = 0.43240688\n",
      "Iteration 219, loss = 0.43225722\n",
      "Iteration 220, loss = 0.43201655\n",
      "Iteration 221, loss = 0.43223579\n",
      "Iteration 222, loss = 0.43186739\n",
      "Iteration 223, loss = 0.43170311\n",
      "Iteration 224, loss = 0.43155935\n",
      "Iteration 225, loss = 0.43124882\n",
      "Iteration 226, loss = 0.43118656\n",
      "Iteration 227, loss = 0.43095758\n",
      "Iteration 228, loss = 0.43079406\n",
      "Iteration 229, loss = 0.43085488\n",
      "Iteration 230, loss = 0.43078148\n",
      "Iteration 231, loss = 0.43030647\n",
      "Iteration 232, loss = 0.43057925\n",
      "Iteration 233, loss = 0.43017139\n",
      "Iteration 234, loss = 0.43001374\n",
      "Iteration 235, loss = 0.42968591\n",
      "Iteration 236, loss = 0.42955163\n",
      "Iteration 237, loss = 0.42932635\n",
      "Iteration 238, loss = 0.42942985\n",
      "Iteration 239, loss = 0.42928992\n",
      "Iteration 240, loss = 0.42903418\n",
      "Iteration 241, loss = 0.42914131\n",
      "Iteration 242, loss = 0.42902430\n",
      "Iteration 243, loss = 0.42854222\n",
      "Iteration 244, loss = 0.42859169\n",
      "Iteration 245, loss = 0.42849224\n",
      "Iteration 246, loss = 0.42843010\n",
      "Iteration 247, loss = 0.42818964\n",
      "Iteration 248, loss = 0.42802005\n",
      "Iteration 249, loss = 0.42774332\n",
      "Iteration 250, loss = 0.42794332\n",
      "Iteration 251, loss = 0.42776785\n",
      "Iteration 252, loss = 0.42755603\n",
      "Iteration 253, loss = 0.42741441\n",
      "Iteration 254, loss = 0.42717042\n",
      "Iteration 255, loss = 0.42710879\n",
      "Iteration 256, loss = 0.42710903\n",
      "Iteration 257, loss = 0.42684497\n",
      "Iteration 258, loss = 0.42671066\n",
      "Iteration 259, loss = 0.42670324\n",
      "Iteration 260, loss = 0.42664481\n",
      "Iteration 261, loss = 0.42669191\n",
      "Iteration 262, loss = 0.42658177\n",
      "Iteration 263, loss = 0.42622577\n",
      "Iteration 264, loss = 0.42626535\n",
      "Iteration 265, loss = 0.42615574\n",
      "Iteration 266, loss = 0.42586167\n",
      "Iteration 267, loss = 0.42591970\n",
      "Iteration 268, loss = 0.42576290\n",
      "Iteration 269, loss = 0.42552971\n",
      "Iteration 270, loss = 0.42543390\n",
      "Iteration 271, loss = 0.42550997\n",
      "Iteration 272, loss = 0.42509857\n",
      "Iteration 273, loss = 0.42509644\n",
      "Iteration 274, loss = 0.42497754\n",
      "Iteration 275, loss = 0.42503632\n",
      "Iteration 276, loss = 0.42478140\n",
      "Iteration 277, loss = 0.42460570\n",
      "Iteration 278, loss = 0.42478662\n",
      "Iteration 279, loss = 0.42483189\n",
      "Iteration 280, loss = 0.42461769\n",
      "Iteration 281, loss = 0.42419301\n",
      "Iteration 282, loss = 0.42411331\n",
      "Iteration 283, loss = 0.42400350\n",
      "Iteration 284, loss = 0.42398037\n",
      "Iteration 285, loss = 0.42403927\n",
      "Iteration 286, loss = 0.42426866\n",
      "Iteration 287, loss = 0.42401452\n",
      "Iteration 288, loss = 0.42366418\n",
      "Iteration 289, loss = 0.42368869\n",
      "Iteration 290, loss = 0.42342988\n",
      "Iteration 291, loss = 0.42343671\n",
      "Iteration 292, loss = 0.42339546\n",
      "Iteration 293, loss = 0.42313890\n",
      "Iteration 294, loss = 0.42317890\n",
      "Iteration 295, loss = 0.42312730\n",
      "Iteration 296, loss = 0.42281924\n",
      "Iteration 297, loss = 0.42308758\n",
      "Iteration 298, loss = 0.42295067\n",
      "Iteration 299, loss = 0.42278628\n",
      "Iteration 300, loss = 0.42256215\n",
      "Iteration 301, loss = 0.42297470\n",
      "Iteration 302, loss = 0.42257915\n",
      "Iteration 303, loss = 0.42223246\n",
      "Iteration 304, loss = 0.42214931\n",
      "Iteration 305, loss = 0.42219998\n",
      "Iteration 306, loss = 0.42226618\n",
      "Iteration 307, loss = 0.42195838\n",
      "Iteration 308, loss = 0.42206183\n",
      "Iteration 309, loss = 0.42201091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310, loss = 0.42165193\n",
      "Iteration 311, loss = 0.42176091\n",
      "Iteration 312, loss = 0.42183273\n",
      "Iteration 313, loss = 0.42149511\n",
      "Iteration 314, loss = 0.42166669\n",
      "Iteration 315, loss = 0.42149973\n",
      "Iteration 316, loss = 0.42127805\n",
      "Iteration 317, loss = 0.42129739\n",
      "Iteration 318, loss = 0.42114949\n",
      "Iteration 319, loss = 0.42149117\n",
      "Iteration 320, loss = 0.42095263\n",
      "Iteration 321, loss = 0.42093130\n",
      "Iteration 322, loss = 0.42131334\n",
      "Iteration 323, loss = 0.42082605\n",
      "Iteration 324, loss = 0.42088684\n",
      "Iteration 325, loss = 0.42105416\n",
      "Iteration 326, loss = 0.42084924\n",
      "Iteration 327, loss = 0.42106106\n",
      "Iteration 328, loss = 0.42062146\n",
      "Iteration 329, loss = 0.42073864\n",
      "Iteration 330, loss = 0.42050573\n",
      "Iteration 331, loss = 0.42047786\n",
      "Iteration 332, loss = 0.42060974\n",
      "Iteration 333, loss = 0.42038171\n",
      "Iteration 334, loss = 0.42047328\n",
      "Iteration 335, loss = 0.42029648\n",
      "Iteration 336, loss = 0.41994207\n",
      "Iteration 337, loss = 0.41989637\n",
      "Iteration 338, loss = 0.42009252\n",
      "Iteration 339, loss = 0.42004043\n",
      "Iteration 340, loss = 0.41981556\n",
      "Iteration 341, loss = 0.42010867\n",
      "Iteration 342, loss = 0.41962309\n",
      "Iteration 343, loss = 0.41952196\n",
      "Iteration 344, loss = 0.41957311\n",
      "Iteration 345, loss = 0.41998865\n",
      "Iteration 346, loss = 0.41951581\n",
      "Iteration 347, loss = 0.41949689\n",
      "Iteration 348, loss = 0.41928949\n",
      "Iteration 349, loss = 0.41936387\n",
      "Iteration 350, loss = 0.41918554\n",
      "Iteration 351, loss = 0.41911521\n",
      "Iteration 352, loss = 0.41935606\n",
      "Iteration 353, loss = 0.41898227\n",
      "Iteration 354, loss = 0.41881772\n",
      "Iteration 355, loss = 0.41896704\n",
      "Iteration 356, loss = 0.41904872\n",
      "Iteration 357, loss = 0.41883276\n",
      "Iteration 358, loss = 0.41869762\n",
      "Iteration 359, loss = 0.41896448\n",
      "Iteration 360, loss = 0.41903982\n",
      "Iteration 361, loss = 0.41879963\n",
      "Iteration 362, loss = 0.41870503\n",
      "Iteration 363, loss = 0.41863702\n",
      "Iteration 364, loss = 0.41895344\n",
      "Iteration 365, loss = 0.41854149\n",
      "Iteration 366, loss = 0.41857138\n",
      "Iteration 367, loss = 0.41846578\n",
      "Iteration 368, loss = 0.41826238\n",
      "Iteration 369, loss = 0.41810377\n",
      "Iteration 370, loss = 0.41785925\n",
      "Iteration 371, loss = 0.41821136\n",
      "Iteration 372, loss = 0.41816488\n",
      "Iteration 373, loss = 0.41810271\n",
      "Iteration 374, loss = 0.41771224\n",
      "Iteration 375, loss = 0.41820691\n",
      "Iteration 376, loss = 0.41752892\n",
      "Iteration 377, loss = 0.41774439\n",
      "Iteration 378, loss = 0.41805762\n",
      "Iteration 379, loss = 0.41779425\n",
      "Iteration 380, loss = 0.41783483\n",
      "Iteration 381, loss = 0.41777971\n",
      "Iteration 382, loss = 0.41752370\n",
      "Iteration 383, loss = 0.41767767\n",
      "Iteration 384, loss = 0.41738647\n",
      "Iteration 385, loss = 0.41738372\n",
      "Iteration 386, loss = 0.41739053\n",
      "Iteration 387, loss = 0.41726110\n",
      "Iteration 388, loss = 0.41740114\n",
      "Iteration 389, loss = 0.41700826\n",
      "Iteration 390, loss = 0.41728595\n",
      "Iteration 391, loss = 0.41723534\n",
      "Iteration 392, loss = 0.41736140\n",
      "Iteration 393, loss = 0.41701041\n",
      "Iteration 394, loss = 0.41673040\n",
      "Iteration 395, loss = 0.41704266\n",
      "Iteration 396, loss = 0.41707968\n",
      "Iteration 397, loss = 0.41652149\n",
      "Iteration 398, loss = 0.41669437\n",
      "Iteration 399, loss = 0.41651166\n",
      "Iteration 400, loss = 0.41678229\n",
      "Iteration 401, loss = 0.41655892\n",
      "Iteration 402, loss = 0.41644278\n",
      "Iteration 403, loss = 0.41662932\n",
      "Iteration 404, loss = 0.41666123\n",
      "Iteration 405, loss = 0.41657699\n",
      "Iteration 406, loss = 0.41622115\n",
      "Iteration 407, loss = 0.41656555\n",
      "Iteration 408, loss = 0.41648418\n",
      "Iteration 409, loss = 0.41621482\n",
      "Iteration 410, loss = 0.41638386\n",
      "Iteration 411, loss = 0.41615325\n",
      "Iteration 412, loss = 0.41623838\n",
      "Iteration 413, loss = 0.41616162\n",
      "Iteration 414, loss = 0.41599352\n",
      "Iteration 415, loss = 0.41586945\n",
      "Iteration 416, loss = 0.41581767\n",
      "Iteration 417, loss = 0.41617369\n",
      "Iteration 418, loss = 0.41584049\n",
      "Iteration 419, loss = 0.41575593\n",
      "Iteration 420, loss = 0.41607029\n",
      "Iteration 421, loss = 0.41574646\n",
      "Iteration 422, loss = 0.41589944\n",
      "Iteration 423, loss = 0.41558314\n",
      "Iteration 424, loss = 0.41565270\n",
      "Iteration 425, loss = 0.41545267\n",
      "Iteration 426, loss = 0.41562186\n",
      "Iteration 427, loss = 0.41553801\n",
      "Iteration 428, loss = 0.41577544\n",
      "Iteration 429, loss = 0.41537292\n",
      "Iteration 430, loss = 0.41538531\n",
      "Iteration 431, loss = 0.41557745\n",
      "Iteration 432, loss = 0.41542636\n",
      "Iteration 433, loss = 0.41513165\n",
      "Iteration 434, loss = 0.41532453\n",
      "Iteration 435, loss = 0.41516663\n",
      "Iteration 436, loss = 0.41528438\n",
      "Iteration 437, loss = 0.41521762\n",
      "Iteration 438, loss = 0.41527302\n",
      "Iteration 439, loss = 0.41517299\n",
      "Iteration 440, loss = 0.41491872\n",
      "Iteration 441, loss = 0.41506191\n",
      "Iteration 442, loss = 0.41480687\n",
      "Iteration 443, loss = 0.41504712\n",
      "Iteration 444, loss = 0.41457708\n",
      "Iteration 445, loss = 0.41507479\n",
      "Iteration 446, loss = 0.41495545\n",
      "Iteration 447, loss = 0.41501230\n",
      "Iteration 448, loss = 0.41449067\n",
      "Iteration 449, loss = 0.41470366\n",
      "Iteration 450, loss = 0.41494578\n",
      "Iteration 451, loss = 0.41478389\n",
      "Iteration 452, loss = 0.41481971\n",
      "Iteration 453, loss = 0.41493093\n",
      "Iteration 454, loss = 0.41442304\n",
      "Iteration 455, loss = 0.41447827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96692621\n",
      "Iteration 2, loss = 1.78107029\n",
      "Iteration 3, loss = 1.60992417\n",
      "Iteration 4, loss = 1.45879776\n",
      "Iteration 5, loss = 1.32949227\n",
      "Iteration 6, loss = 1.22363994\n",
      "Iteration 7, loss = 1.13779489\n",
      "Iteration 8, loss = 1.06715718\n",
      "Iteration 9, loss = 1.00745228\n",
      "Iteration 10, loss = 0.95578928\n",
      "Iteration 11, loss = 0.91136247\n",
      "Iteration 12, loss = 0.87335988\n",
      "Iteration 13, loss = 0.84021118\n",
      "Iteration 14, loss = 0.81061760\n",
      "Iteration 15, loss = 0.78392765\n",
      "Iteration 16, loss = 0.75977388\n",
      "Iteration 17, loss = 0.73787954\n",
      "Iteration 18, loss = 0.71784409\n",
      "Iteration 19, loss = 0.69984822\n",
      "Iteration 20, loss = 0.68341603\n",
      "Iteration 21, loss = 0.66863021\n",
      "Iteration 22, loss = 0.65512147\n",
      "Iteration 23, loss = 0.64299919\n",
      "Iteration 24, loss = 0.63191085\n",
      "Iteration 25, loss = 0.62191935\n",
      "Iteration 26, loss = 0.61253015\n",
      "Iteration 27, loss = 0.60440748\n",
      "Iteration 28, loss = 0.59703178\n",
      "Iteration 29, loss = 0.59019902\n",
      "Iteration 30, loss = 0.58407257\n",
      "Iteration 31, loss = 0.57840669\n",
      "Iteration 32, loss = 0.57317562\n",
      "Iteration 33, loss = 0.56852141\n",
      "Iteration 34, loss = 0.56395908\n",
      "Iteration 35, loss = 0.55967043\n",
      "Iteration 36, loss = 0.55586831\n",
      "Iteration 37, loss = 0.55234978\n",
      "Iteration 38, loss = 0.54899648\n",
      "Iteration 39, loss = 0.54588807\n",
      "Iteration 40, loss = 0.54280505\n",
      "Iteration 41, loss = 0.53994097\n",
      "Iteration 42, loss = 0.53736966\n",
      "Iteration 43, loss = 0.53482440\n",
      "Iteration 44, loss = 0.53258203\n",
      "Iteration 45, loss = 0.53034580\n",
      "Iteration 46, loss = 0.52825307\n",
      "Iteration 47, loss = 0.52626198\n",
      "Iteration 48, loss = 0.52397495\n",
      "Iteration 49, loss = 0.52255390\n",
      "Iteration 50, loss = 0.52046444\n",
      "Iteration 51, loss = 0.51884089\n",
      "Iteration 52, loss = 0.51734999\n",
      "Iteration 53, loss = 0.51578492\n",
      "Iteration 54, loss = 0.51411199\n",
      "Iteration 55, loss = 0.51273536\n",
      "Iteration 56, loss = 0.51108433\n",
      "Iteration 57, loss = 0.50990191\n",
      "Iteration 58, loss = 0.50866924\n",
      "Iteration 59, loss = 0.50748433\n",
      "Iteration 60, loss = 0.50609326\n",
      "Iteration 61, loss = 0.50488201\n",
      "Iteration 62, loss = 0.50351544\n",
      "Iteration 63, loss = 0.50278729\n",
      "Iteration 64, loss = 0.50138485\n",
      "Iteration 65, loss = 0.50034249\n",
      "Iteration 66, loss = 0.49929464\n",
      "Iteration 67, loss = 0.49822783\n",
      "Iteration 68, loss = 0.49742823\n",
      "Iteration 69, loss = 0.49631643\n",
      "Iteration 70, loss = 0.49548787\n",
      "Iteration 71, loss = 0.49470623\n",
      "Iteration 72, loss = 0.49355094\n",
      "Iteration 73, loss = 0.49294774\n",
      "Iteration 74, loss = 0.49198840\n",
      "Iteration 75, loss = 0.49099686\n",
      "Iteration 76, loss = 0.49006540\n",
      "Iteration 77, loss = 0.48913024\n",
      "Iteration 78, loss = 0.48833410\n",
      "Iteration 79, loss = 0.48759146\n",
      "Iteration 80, loss = 0.48683289\n",
      "Iteration 81, loss = 0.48630458\n",
      "Iteration 82, loss = 0.48546705\n",
      "Iteration 83, loss = 0.48495324\n",
      "Iteration 84, loss = 0.48397517\n",
      "Iteration 85, loss = 0.48321860\n",
      "Iteration 86, loss = 0.48242414\n",
      "Iteration 87, loss = 0.48189929\n",
      "Iteration 88, loss = 0.48102386\n",
      "Iteration 89, loss = 0.48052609\n",
      "Iteration 90, loss = 0.48003725\n",
      "Iteration 91, loss = 0.47909812\n",
      "Iteration 92, loss = 0.47855112\n",
      "Iteration 93, loss = 0.47801805\n",
      "Iteration 94, loss = 0.47717402\n",
      "Iteration 95, loss = 0.47656832\n",
      "Iteration 96, loss = 0.47587183\n",
      "Iteration 97, loss = 0.47541199\n",
      "Iteration 98, loss = 0.47475939\n",
      "Iteration 99, loss = 0.47428633\n",
      "Iteration 100, loss = 0.47370122\n",
      "Iteration 101, loss = 0.47278977\n",
      "Iteration 102, loss = 0.47230429\n",
      "Iteration 103, loss = 0.47193290\n",
      "Iteration 104, loss = 0.47125109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 0.47073574\n",
      "Iteration 106, loss = 0.47027479\n",
      "Iteration 107, loss = 0.46959887\n",
      "Iteration 108, loss = 0.46920153\n",
      "Iteration 109, loss = 0.46864715\n",
      "Iteration 110, loss = 0.46814794\n",
      "Iteration 111, loss = 0.46752098\n",
      "Iteration 112, loss = 0.46705595\n",
      "Iteration 113, loss = 0.46657692\n",
      "Iteration 114, loss = 0.46618896\n",
      "Iteration 115, loss = 0.46545254\n",
      "Iteration 116, loss = 0.46493409\n",
      "Iteration 117, loss = 0.46439874\n",
      "Iteration 118, loss = 0.46385076\n",
      "Iteration 119, loss = 0.46355998\n",
      "Iteration 120, loss = 0.46323811\n",
      "Iteration 121, loss = 0.46241259\n",
      "Iteration 122, loss = 0.46210111\n",
      "Iteration 123, loss = 0.46152836\n",
      "Iteration 124, loss = 0.46133624\n",
      "Iteration 125, loss = 0.46066905\n",
      "Iteration 126, loss = 0.46028891\n",
      "Iteration 127, loss = 0.45985800\n",
      "Iteration 128, loss = 0.45926006\n",
      "Iteration 129, loss = 0.45899000\n",
      "Iteration 130, loss = 0.45861900\n",
      "Iteration 131, loss = 0.45807137\n",
      "Iteration 132, loss = 0.45759481\n",
      "Iteration 133, loss = 0.45724116\n",
      "Iteration 134, loss = 0.45673597\n",
      "Iteration 135, loss = 0.45624667\n",
      "Iteration 136, loss = 0.45605418\n",
      "Iteration 137, loss = 0.45552229\n",
      "Iteration 138, loss = 0.45529134\n",
      "Iteration 139, loss = 0.45481383\n",
      "Iteration 140, loss = 0.45449756\n",
      "Iteration 141, loss = 0.45402829\n",
      "Iteration 142, loss = 0.45336760\n",
      "Iteration 143, loss = 0.45322907\n",
      "Iteration 144, loss = 0.45264599\n",
      "Iteration 145, loss = 0.45235217\n",
      "Iteration 146, loss = 0.45204303\n",
      "Iteration 147, loss = 0.45161355\n",
      "Iteration 148, loss = 0.45108387\n",
      "Iteration 149, loss = 0.45087336\n",
      "Iteration 150, loss = 0.45056683\n",
      "Iteration 151, loss = 0.45017080\n",
      "Iteration 152, loss = 0.44985768\n",
      "Iteration 153, loss = 0.44975479\n",
      "Iteration 154, loss = 0.44908937\n",
      "Iteration 155, loss = 0.44869987\n",
      "Iteration 156, loss = 0.44860700\n",
      "Iteration 157, loss = 0.44810973\n",
      "Iteration 158, loss = 0.44780540\n",
      "Iteration 159, loss = 0.44752313\n",
      "Iteration 160, loss = 0.44720263\n",
      "Iteration 161, loss = 0.44676527\n",
      "Iteration 162, loss = 0.44651622\n",
      "Iteration 163, loss = 0.44624772\n",
      "Iteration 164, loss = 0.44591810\n",
      "Iteration 165, loss = 0.44565918\n",
      "Iteration 166, loss = 0.44537770\n",
      "Iteration 167, loss = 0.44501814\n",
      "Iteration 168, loss = 0.44479727\n",
      "Iteration 169, loss = 0.44436035\n",
      "Iteration 170, loss = 0.44390841\n",
      "Iteration 171, loss = 0.44388292\n",
      "Iteration 172, loss = 0.44366267\n",
      "Iteration 173, loss = 0.44309608\n",
      "Iteration 174, loss = 0.44325839\n",
      "Iteration 175, loss = 0.44273296\n",
      "Iteration 176, loss = 0.44259639\n",
      "Iteration 177, loss = 0.44240466\n",
      "Iteration 178, loss = 0.44185507\n",
      "Iteration 179, loss = 0.44188106\n",
      "Iteration 180, loss = 0.44167614\n",
      "Iteration 181, loss = 0.44126217\n",
      "Iteration 182, loss = 0.44097786\n",
      "Iteration 183, loss = 0.44083329\n",
      "Iteration 184, loss = 0.44081891\n",
      "Iteration 185, loss = 0.44029435\n",
      "Iteration 186, loss = 0.43990433\n",
      "Iteration 187, loss = 0.43987593\n",
      "Iteration 188, loss = 0.43958511\n",
      "Iteration 189, loss = 0.43928094\n",
      "Iteration 190, loss = 0.43924117\n",
      "Iteration 191, loss = 0.43889968\n",
      "Iteration 192, loss = 0.43843498\n",
      "Iteration 193, loss = 0.43858654\n",
      "Iteration 194, loss = 0.43839700\n",
      "Iteration 195, loss = 0.43798475\n",
      "Iteration 196, loss = 0.43828833\n",
      "Iteration 197, loss = 0.43742827\n",
      "Iteration 198, loss = 0.43729391\n",
      "Iteration 199, loss = 0.43724603\n",
      "Iteration 200, loss = 0.43711877\n",
      "Iteration 201, loss = 0.43661363\n",
      "Iteration 202, loss = 0.43658724\n",
      "Iteration 203, loss = 0.43648675\n",
      "Iteration 204, loss = 0.43598939\n",
      "Iteration 205, loss = 0.43604911\n",
      "Iteration 206, loss = 0.43576130\n",
      "Iteration 207, loss = 0.43578808\n",
      "Iteration 208, loss = 0.43546217\n",
      "Iteration 209, loss = 0.43524841\n",
      "Iteration 210, loss = 0.43513056\n",
      "Iteration 211, loss = 0.43518408\n",
      "Iteration 212, loss = 0.43486767\n",
      "Iteration 213, loss = 0.43467633\n",
      "Iteration 214, loss = 0.43433874\n",
      "Iteration 215, loss = 0.43422391\n",
      "Iteration 216, loss = 0.43400044\n",
      "Iteration 217, loss = 0.43381579\n",
      "Iteration 218, loss = 0.43356686\n",
      "Iteration 219, loss = 0.43341328\n",
      "Iteration 220, loss = 0.43325015\n",
      "Iteration 221, loss = 0.43324102\n",
      "Iteration 222, loss = 0.43309583\n",
      "Iteration 223, loss = 0.43265805\n",
      "Iteration 224, loss = 0.43275858\n",
      "Iteration 225, loss = 0.43261166\n",
      "Iteration 226, loss = 0.43258303\n",
      "Iteration 227, loss = 0.43211391\n",
      "Iteration 228, loss = 0.43229677\n",
      "Iteration 229, loss = 0.43210483\n",
      "Iteration 230, loss = 0.43171405\n",
      "Iteration 231, loss = 0.43146612\n",
      "Iteration 232, loss = 0.43157063\n",
      "Iteration 233, loss = 0.43146976\n",
      "Iteration 234, loss = 0.43122862\n",
      "Iteration 235, loss = 0.43108242\n",
      "Iteration 236, loss = 0.43102468\n",
      "Iteration 237, loss = 0.43074967\n",
      "Iteration 238, loss = 0.43052285\n",
      "Iteration 239, loss = 0.43063115\n",
      "Iteration 240, loss = 0.43028278\n",
      "Iteration 241, loss = 0.43001220\n",
      "Iteration 242, loss = 0.43003418\n",
      "Iteration 243, loss = 0.43005365\n",
      "Iteration 244, loss = 0.42984507\n",
      "Iteration 245, loss = 0.42960799\n",
      "Iteration 246, loss = 0.42952493\n",
      "Iteration 247, loss = 0.42984003\n",
      "Iteration 248, loss = 0.42950888\n",
      "Iteration 249, loss = 0.42912398\n",
      "Iteration 250, loss = 0.42897726\n",
      "Iteration 251, loss = 0.42892909\n",
      "Iteration 252, loss = 0.42881432\n",
      "Iteration 253, loss = 0.42858369\n",
      "Iteration 254, loss = 0.42861589\n",
      "Iteration 255, loss = 0.42848704\n",
      "Iteration 256, loss = 0.42840086\n",
      "Iteration 257, loss = 0.42812765\n",
      "Iteration 258, loss = 0.42803993\n",
      "Iteration 259, loss = 0.42810864\n",
      "Iteration 260, loss = 0.42799850\n",
      "Iteration 261, loss = 0.42791424\n",
      "Iteration 262, loss = 0.42756898\n",
      "Iteration 263, loss = 0.42778595\n",
      "Iteration 264, loss = 0.42736894\n",
      "Iteration 265, loss = 0.42728343\n",
      "Iteration 266, loss = 0.42738522\n",
      "Iteration 267, loss = 0.42706488\n",
      "Iteration 268, loss = 0.42728172\n",
      "Iteration 269, loss = 0.42679009\n",
      "Iteration 270, loss = 0.42682046\n",
      "Iteration 271, loss = 0.42673735\n",
      "Iteration 272, loss = 0.42687269\n",
      "Iteration 273, loss = 0.42658344\n",
      "Iteration 274, loss = 0.42649061\n",
      "Iteration 275, loss = 0.42630309\n",
      "Iteration 276, loss = 0.42621906\n",
      "Iteration 277, loss = 0.42623338\n",
      "Iteration 278, loss = 0.42619201\n",
      "Iteration 279, loss = 0.42593319\n",
      "Iteration 280, loss = 0.42554622\n",
      "Iteration 281, loss = 0.42610871\n",
      "Iteration 282, loss = 0.42570669\n",
      "Iteration 283, loss = 0.42557507\n",
      "Iteration 284, loss = 0.42579331\n",
      "Iteration 285, loss = 0.42537164\n",
      "Iteration 286, loss = 0.42528771\n",
      "Iteration 287, loss = 0.42540729\n",
      "Iteration 288, loss = 0.42485727\n",
      "Iteration 289, loss = 0.42509577\n",
      "Iteration 290, loss = 0.42515004\n",
      "Iteration 291, loss = 0.42516406\n",
      "Iteration 292, loss = 0.42492382\n",
      "Iteration 293, loss = 0.42489494\n",
      "Iteration 294, loss = 0.42468577\n",
      "Iteration 295, loss = 0.42456641\n",
      "Iteration 296, loss = 0.42449372\n",
      "Iteration 297, loss = 0.42423002\n",
      "Iteration 298, loss = 0.42447807\n",
      "Iteration 299, loss = 0.42423294\n",
      "Iteration 300, loss = 0.42436308\n",
      "Iteration 301, loss = 0.42410764\n",
      "Iteration 302, loss = 0.42417748\n",
      "Iteration 303, loss = 0.42402712\n",
      "Iteration 304, loss = 0.42394320\n",
      "Iteration 305, loss = 0.42394963\n",
      "Iteration 306, loss = 0.42389244\n",
      "Iteration 307, loss = 0.42374778\n",
      "Iteration 308, loss = 0.42366488\n",
      "Iteration 309, loss = 0.42353528\n",
      "Iteration 310, loss = 0.42346391\n",
      "Iteration 311, loss = 0.42361140\n",
      "Iteration 312, loss = 0.42345088\n",
      "Iteration 313, loss = 0.42323052\n",
      "Iteration 314, loss = 0.42301492\n",
      "Iteration 315, loss = 0.42305990\n",
      "Iteration 316, loss = 0.42321315\n",
      "Iteration 317, loss = 0.42315161\n",
      "Iteration 318, loss = 0.42299373\n",
      "Iteration 319, loss = 0.42287928\n",
      "Iteration 320, loss = 0.42286818\n",
      "Iteration 321, loss = 0.42301081\n",
      "Iteration 322, loss = 0.42280425\n",
      "Iteration 323, loss = 0.42270339\n",
      "Iteration 324, loss = 0.42251928\n",
      "Iteration 325, loss = 0.42256395\n",
      "Iteration 326, loss = 0.42246067\n",
      "Iteration 327, loss = 0.42233951\n",
      "Iteration 328, loss = 0.42242574\n",
      "Iteration 329, loss = 0.42223623\n",
      "Iteration 330, loss = 0.42196987\n",
      "Iteration 331, loss = 0.42215799\n",
      "Iteration 332, loss = 0.42209366\n",
      "Iteration 333, loss = 0.42187535\n",
      "Iteration 334, loss = 0.42193606\n",
      "Iteration 335, loss = 0.42198922\n",
      "Iteration 336, loss = 0.42184299\n",
      "Iteration 337, loss = 0.42177240\n",
      "Iteration 338, loss = 0.42171552\n",
      "Iteration 339, loss = 0.42176342\n",
      "Iteration 340, loss = 0.42146783\n",
      "Iteration 341, loss = 0.42175539\n",
      "Iteration 342, loss = 0.42128721\n",
      "Iteration 343, loss = 0.42124566\n",
      "Iteration 344, loss = 0.42121866\n",
      "Iteration 345, loss = 0.42157670\n",
      "Iteration 346, loss = 0.42120762\n",
      "Iteration 347, loss = 0.42146537\n",
      "Iteration 348, loss = 0.42136617\n",
      "Iteration 349, loss = 0.42106929\n",
      "Iteration 350, loss = 0.42113792\n",
      "Iteration 351, loss = 0.42101899\n",
      "Iteration 352, loss = 0.42088141\n",
      "Iteration 353, loss = 0.42072059\n",
      "Iteration 354, loss = 0.42075746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 355, loss = 0.42064207\n",
      "Iteration 356, loss = 0.42041749\n",
      "Iteration 357, loss = 0.42057419\n",
      "Iteration 358, loss = 0.42027866\n",
      "Iteration 359, loss = 0.42052503\n",
      "Iteration 360, loss = 0.42069507\n",
      "Iteration 361, loss = 0.42050622\n",
      "Iteration 362, loss = 0.42039701\n",
      "Iteration 363, loss = 0.42043768\n",
      "Iteration 364, loss = 0.42004724\n",
      "Iteration 365, loss = 0.42023235\n",
      "Iteration 366, loss = 0.42036793\n",
      "Iteration 367, loss = 0.42005098\n",
      "Iteration 368, loss = 0.42029768\n",
      "Iteration 369, loss = 0.41990751\n",
      "Iteration 370, loss = 0.41997198\n",
      "Iteration 371, loss = 0.41996213\n",
      "Iteration 372, loss = 0.41992623\n",
      "Iteration 373, loss = 0.41972963\n",
      "Iteration 374, loss = 0.41962527\n",
      "Iteration 375, loss = 0.42028027\n",
      "Iteration 376, loss = 0.41960427\n",
      "Iteration 377, loss = 0.41954392\n",
      "Iteration 378, loss = 0.41977952\n",
      "Iteration 379, loss = 0.41966506\n",
      "Iteration 380, loss = 0.41921373\n",
      "Iteration 381, loss = 0.41951787\n",
      "Iteration 382, loss = 0.41931760\n",
      "Iteration 383, loss = 0.41946544\n",
      "Iteration 384, loss = 0.41909635\n",
      "Iteration 385, loss = 0.41934952\n",
      "Iteration 386, loss = 0.41961831\n",
      "Iteration 387, loss = 0.41935064\n",
      "Iteration 388, loss = 0.41936622\n",
      "Iteration 389, loss = 0.41937619\n",
      "Iteration 390, loss = 0.41870477\n",
      "Iteration 391, loss = 0.41879847\n",
      "Iteration 392, loss = 0.41899616\n",
      "Iteration 393, loss = 0.41925165\n",
      "Iteration 394, loss = 0.41909518\n",
      "Iteration 395, loss = 0.41877119\n",
      "Iteration 396, loss = 0.41892321\n",
      "Iteration 397, loss = 0.41862751\n",
      "Iteration 398, loss = 0.41862792\n",
      "Iteration 399, loss = 0.41859476\n",
      "Iteration 400, loss = 0.41870948\n",
      "Iteration 401, loss = 0.41871721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96667827\n",
      "Iteration 2, loss = 1.78092170\n",
      "Iteration 3, loss = 1.60892832\n",
      "Iteration 4, loss = 1.45803512\n",
      "Iteration 5, loss = 1.32891948\n",
      "Iteration 6, loss = 1.22293026\n",
      "Iteration 7, loss = 1.13719704\n",
      "Iteration 8, loss = 1.06676008\n",
      "Iteration 9, loss = 1.00793954\n",
      "Iteration 10, loss = 0.95810809\n",
      "Iteration 11, loss = 0.91509756\n",
      "Iteration 12, loss = 0.87640045\n",
      "Iteration 13, loss = 0.84122343\n",
      "Iteration 14, loss = 0.81095999\n",
      "Iteration 15, loss = 0.78408776\n",
      "Iteration 16, loss = 0.75992602\n",
      "Iteration 17, loss = 0.73785384\n",
      "Iteration 18, loss = 0.71774035\n",
      "Iteration 19, loss = 0.69961890\n",
      "Iteration 20, loss = 0.68315659\n",
      "Iteration 21, loss = 0.66821087\n",
      "Iteration 22, loss = 0.65462940\n",
      "Iteration 23, loss = 0.64250001\n",
      "Iteration 24, loss = 0.63127454\n",
      "Iteration 25, loss = 0.62104201\n",
      "Iteration 26, loss = 0.61184059\n",
      "Iteration 27, loss = 0.60354427\n",
      "Iteration 28, loss = 0.59618306\n",
      "Iteration 29, loss = 0.58926492\n",
      "Iteration 30, loss = 0.58311468\n",
      "Iteration 31, loss = 0.57738813\n",
      "Iteration 32, loss = 0.57218906\n",
      "Iteration 33, loss = 0.56747637\n",
      "Iteration 34, loss = 0.56284822\n",
      "Iteration 35, loss = 0.55880228\n",
      "Iteration 36, loss = 0.55485186\n",
      "Iteration 37, loss = 0.55139195\n",
      "Iteration 38, loss = 0.54783423\n",
      "Iteration 39, loss = 0.54460942\n",
      "Iteration 40, loss = 0.54166296\n",
      "Iteration 41, loss = 0.53908658\n",
      "Iteration 42, loss = 0.53635812\n",
      "Iteration 43, loss = 0.53375222\n",
      "Iteration 44, loss = 0.53146176\n",
      "Iteration 45, loss = 0.52908989\n",
      "Iteration 46, loss = 0.52719663\n",
      "Iteration 47, loss = 0.52502347\n",
      "Iteration 48, loss = 0.52295114\n",
      "Iteration 49, loss = 0.52121764\n",
      "Iteration 50, loss = 0.51956016\n",
      "Iteration 51, loss = 0.51781091\n",
      "Iteration 52, loss = 0.51623428\n",
      "Iteration 53, loss = 0.51453842\n",
      "Iteration 54, loss = 0.51301292\n",
      "Iteration 55, loss = 0.51161388\n",
      "Iteration 56, loss = 0.51010295\n",
      "Iteration 57, loss = 0.50850285\n",
      "Iteration 58, loss = 0.50741332\n",
      "Iteration 59, loss = 0.50615766\n",
      "Iteration 60, loss = 0.50484273\n",
      "Iteration 61, loss = 0.50351584\n",
      "Iteration 62, loss = 0.50254877\n",
      "Iteration 63, loss = 0.50143389\n",
      "Iteration 64, loss = 0.50011564\n",
      "Iteration 65, loss = 0.49919895\n",
      "Iteration 66, loss = 0.49807474\n",
      "Iteration 67, loss = 0.49702435\n",
      "Iteration 68, loss = 0.49608636\n",
      "Iteration 69, loss = 0.49511332\n",
      "Iteration 70, loss = 0.49413857\n",
      "Iteration 71, loss = 0.49332220\n",
      "Iteration 72, loss = 0.49236577\n",
      "Iteration 73, loss = 0.49132559\n",
      "Iteration 74, loss = 0.49067754\n",
      "Iteration 75, loss = 0.48960530\n",
      "Iteration 76, loss = 0.48896370\n",
      "Iteration 77, loss = 0.48829508\n",
      "Iteration 78, loss = 0.48730759\n",
      "Iteration 79, loss = 0.48636197\n",
      "Iteration 80, loss = 0.48574943\n",
      "Iteration 81, loss = 0.48498144\n",
      "Iteration 82, loss = 0.48417538\n",
      "Iteration 83, loss = 0.48340015\n",
      "Iteration 84, loss = 0.48280776\n",
      "Iteration 85, loss = 0.48199325\n",
      "Iteration 86, loss = 0.48130354\n",
      "Iteration 87, loss = 0.48065465\n",
      "Iteration 88, loss = 0.47987647\n",
      "Iteration 89, loss = 0.47946276\n",
      "Iteration 90, loss = 0.47879609\n",
      "Iteration 91, loss = 0.47790103\n",
      "Iteration 92, loss = 0.47759851\n",
      "Iteration 93, loss = 0.47666778\n",
      "Iteration 94, loss = 0.47587988\n",
      "Iteration 95, loss = 0.47550122\n",
      "Iteration 96, loss = 0.47480265\n",
      "Iteration 97, loss = 0.47428226\n",
      "Iteration 98, loss = 0.47347331\n",
      "Iteration 99, loss = 0.47306588\n",
      "Iteration 100, loss = 0.47239799\n",
      "Iteration 101, loss = 0.47191717\n",
      "Iteration 102, loss = 0.47163482\n",
      "Iteration 103, loss = 0.47079915\n",
      "Iteration 104, loss = 0.47029462\n",
      "Iteration 105, loss = 0.46952298\n",
      "Iteration 106, loss = 0.46942720\n",
      "Iteration 107, loss = 0.46840622\n",
      "Iteration 108, loss = 0.46787922\n",
      "Iteration 109, loss = 0.46771858\n",
      "Iteration 110, loss = 0.46693370\n",
      "Iteration 111, loss = 0.46640041\n",
      "Iteration 112, loss = 0.46582149\n",
      "Iteration 113, loss = 0.46539766\n",
      "Iteration 114, loss = 0.46506787\n",
      "Iteration 115, loss = 0.46422837\n",
      "Iteration 116, loss = 0.46393433\n",
      "Iteration 117, loss = 0.46339448\n",
      "Iteration 118, loss = 0.46302818\n",
      "Iteration 119, loss = 0.46239642\n",
      "Iteration 120, loss = 0.46214962\n",
      "Iteration 121, loss = 0.46138639\n",
      "Iteration 122, loss = 0.46087051\n",
      "Iteration 123, loss = 0.46047271\n",
      "Iteration 124, loss = 0.46040334\n",
      "Iteration 125, loss = 0.45971899\n",
      "Iteration 126, loss = 0.45898146\n",
      "Iteration 127, loss = 0.45875720\n",
      "Iteration 128, loss = 0.45815478\n",
      "Iteration 129, loss = 0.45790846\n",
      "Iteration 130, loss = 0.45742114\n",
      "Iteration 131, loss = 0.45698073\n",
      "Iteration 132, loss = 0.45647403\n",
      "Iteration 133, loss = 0.45602131\n",
      "Iteration 134, loss = 0.45557302\n",
      "Iteration 135, loss = 0.45524564\n",
      "Iteration 136, loss = 0.45483827\n",
      "Iteration 137, loss = 0.45466529\n",
      "Iteration 138, loss = 0.45404855\n",
      "Iteration 139, loss = 0.45372969\n",
      "Iteration 140, loss = 0.45318383\n",
      "Iteration 141, loss = 0.45300101\n",
      "Iteration 142, loss = 0.45243996\n",
      "Iteration 143, loss = 0.45216052\n",
      "Iteration 144, loss = 0.45179346\n",
      "Iteration 145, loss = 0.45161525\n",
      "Iteration 146, loss = 0.45090648\n",
      "Iteration 147, loss = 0.45073576\n",
      "Iteration 148, loss = 0.45034424\n",
      "Iteration 149, loss = 0.44979105\n",
      "Iteration 150, loss = 0.44982840\n",
      "Iteration 151, loss = 0.44910110\n",
      "Iteration 152, loss = 0.44879135\n",
      "Iteration 153, loss = 0.44855838\n",
      "Iteration 154, loss = 0.44803028\n",
      "Iteration 155, loss = 0.44756287\n",
      "Iteration 156, loss = 0.44740267\n",
      "Iteration 157, loss = 0.44732134\n",
      "Iteration 158, loss = 0.44712634\n",
      "Iteration 159, loss = 0.44626938\n",
      "Iteration 160, loss = 0.44637237\n",
      "Iteration 161, loss = 0.44616319\n",
      "Iteration 162, loss = 0.44527703\n",
      "Iteration 163, loss = 0.44531951\n",
      "Iteration 164, loss = 0.44495721\n",
      "Iteration 165, loss = 0.44492768\n",
      "Iteration 166, loss = 0.44438502\n",
      "Iteration 167, loss = 0.44387735\n",
      "Iteration 168, loss = 0.44375029\n",
      "Iteration 169, loss = 0.44338156\n",
      "Iteration 170, loss = 0.44310561\n",
      "Iteration 171, loss = 0.44282052\n",
      "Iteration 172, loss = 0.44268336\n",
      "Iteration 173, loss = 0.44225117\n",
      "Iteration 174, loss = 0.44181877\n",
      "Iteration 175, loss = 0.44176814\n",
      "Iteration 176, loss = 0.44127304\n",
      "Iteration 177, loss = 0.44104549\n",
      "Iteration 178, loss = 0.44072694\n",
      "Iteration 179, loss = 0.44060423\n",
      "Iteration 180, loss = 0.44044828\n",
      "Iteration 181, loss = 0.44036420\n",
      "Iteration 182, loss = 0.43986715\n",
      "Iteration 183, loss = 0.43959245\n",
      "Iteration 184, loss = 0.43924023\n",
      "Iteration 185, loss = 0.43894570\n",
      "Iteration 186, loss = 0.43901630\n",
      "Iteration 187, loss = 0.43865639\n",
      "Iteration 188, loss = 0.43864957\n",
      "Iteration 189, loss = 0.43810086\n",
      "Iteration 190, loss = 0.43802998\n",
      "Iteration 191, loss = 0.43779421\n",
      "Iteration 192, loss = 0.43729382\n",
      "Iteration 193, loss = 0.43715205\n",
      "Iteration 194, loss = 0.43696684\n",
      "Iteration 195, loss = 0.43682518\n",
      "Iteration 196, loss = 0.43666262\n",
      "Iteration 197, loss = 0.43663507\n",
      "Iteration 198, loss = 0.43602395\n",
      "Iteration 199, loss = 0.43608001\n",
      "Iteration 200, loss = 0.43585521\n",
      "Iteration 201, loss = 0.43552013\n",
      "Iteration 202, loss = 0.43551607\n",
      "Iteration 203, loss = 0.43518971\n",
      "Iteration 204, loss = 0.43519150\n",
      "Iteration 205, loss = 0.43487290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 206, loss = 0.43463883\n",
      "Iteration 207, loss = 0.43428558\n",
      "Iteration 208, loss = 0.43412000\n",
      "Iteration 209, loss = 0.43402891\n",
      "Iteration 210, loss = 0.43369245\n",
      "Iteration 211, loss = 0.43368508\n",
      "Iteration 212, loss = 0.43349638\n",
      "Iteration 213, loss = 0.43324634\n",
      "Iteration 214, loss = 0.43292320\n",
      "Iteration 215, loss = 0.43284262\n",
      "Iteration 216, loss = 0.43292185\n",
      "Iteration 217, loss = 0.43280140\n",
      "Iteration 218, loss = 0.43254147\n",
      "Iteration 219, loss = 0.43209897\n",
      "Iteration 220, loss = 0.43188228\n",
      "Iteration 221, loss = 0.43205341\n",
      "Iteration 222, loss = 0.43188961\n",
      "Iteration 223, loss = 0.43167655\n",
      "Iteration 224, loss = 0.43125277\n",
      "Iteration 225, loss = 0.43126468\n",
      "Iteration 226, loss = 0.43088137\n",
      "Iteration 227, loss = 0.43120232\n",
      "Iteration 228, loss = 0.43072970\n",
      "Iteration 229, loss = 0.43075622\n",
      "Iteration 230, loss = 0.43041066\n",
      "Iteration 231, loss = 0.43029648\n",
      "Iteration 232, loss = 0.43029683\n",
      "Iteration 233, loss = 0.42984141\n",
      "Iteration 234, loss = 0.42991119\n",
      "Iteration 235, loss = 0.42987475\n",
      "Iteration 236, loss = 0.42943704\n",
      "Iteration 237, loss = 0.42942389\n",
      "Iteration 238, loss = 0.42936629\n",
      "Iteration 239, loss = 0.42913807\n",
      "Iteration 240, loss = 0.42892128\n",
      "Iteration 241, loss = 0.42895900\n",
      "Iteration 242, loss = 0.42889779\n",
      "Iteration 243, loss = 0.42879821\n",
      "Iteration 244, loss = 0.42861293\n",
      "Iteration 245, loss = 0.42868757\n",
      "Iteration 246, loss = 0.42833253\n",
      "Iteration 247, loss = 0.42816403\n",
      "Iteration 248, loss = 0.42817962\n",
      "Iteration 249, loss = 0.42801440\n",
      "Iteration 250, loss = 0.42791254\n",
      "Iteration 251, loss = 0.42769051\n",
      "Iteration 252, loss = 0.42757948\n",
      "Iteration 253, loss = 0.42728106\n",
      "Iteration 254, loss = 0.42727691\n",
      "Iteration 255, loss = 0.42726342\n",
      "Iteration 256, loss = 0.42704325\n",
      "Iteration 257, loss = 0.42671701\n",
      "Iteration 258, loss = 0.42701629\n",
      "Iteration 259, loss = 0.42678899\n",
      "Iteration 260, loss = 0.42667616\n",
      "Iteration 261, loss = 0.42640976\n",
      "Iteration 262, loss = 0.42640264\n",
      "Iteration 263, loss = 0.42622240\n",
      "Iteration 264, loss = 0.42636978\n",
      "Iteration 265, loss = 0.42610848\n",
      "Iteration 266, loss = 0.42601906\n",
      "Iteration 267, loss = 0.42635399\n",
      "Iteration 268, loss = 0.42555153\n",
      "Iteration 269, loss = 0.42588251\n",
      "Iteration 270, loss = 0.42557910\n",
      "Iteration 271, loss = 0.42549184\n",
      "Iteration 272, loss = 0.42541259\n",
      "Iteration 273, loss = 0.42555519\n",
      "Iteration 274, loss = 0.42517690\n",
      "Iteration 275, loss = 0.42502329\n",
      "Iteration 276, loss = 0.42493429\n",
      "Iteration 277, loss = 0.42460988\n",
      "Iteration 278, loss = 0.42476975\n",
      "Iteration 279, loss = 0.42492913\n",
      "Iteration 280, loss = 0.42447062\n",
      "Iteration 281, loss = 0.42446607\n",
      "Iteration 282, loss = 0.42433201\n",
      "Iteration 283, loss = 0.42437361\n",
      "Iteration 284, loss = 0.42418843\n",
      "Iteration 285, loss = 0.42441263\n",
      "Iteration 286, loss = 0.42393029\n",
      "Iteration 287, loss = 0.42385264\n",
      "Iteration 288, loss = 0.42386117\n",
      "Iteration 289, loss = 0.42381034\n",
      "Iteration 290, loss = 0.42389667\n",
      "Iteration 291, loss = 0.42367114\n",
      "Iteration 292, loss = 0.42361409\n",
      "Iteration 293, loss = 0.42349736\n",
      "Iteration 294, loss = 0.42327274\n",
      "Iteration 295, loss = 0.42328156\n",
      "Iteration 296, loss = 0.42332272\n",
      "Iteration 297, loss = 0.42317655\n",
      "Iteration 298, loss = 0.42325097\n",
      "Iteration 299, loss = 0.42329751\n",
      "Iteration 300, loss = 0.42290228\n",
      "Iteration 301, loss = 0.42282089\n",
      "Iteration 302, loss = 0.42262810\n",
      "Iteration 303, loss = 0.42239122\n",
      "Iteration 304, loss = 0.42260452\n",
      "Iteration 305, loss = 0.42240268\n",
      "Iteration 306, loss = 0.42234301\n",
      "Iteration 307, loss = 0.42229095\n",
      "Iteration 308, loss = 0.42223426\n",
      "Iteration 309, loss = 0.42211605\n",
      "Iteration 310, loss = 0.42190130\n",
      "Iteration 311, loss = 0.42209327\n",
      "Iteration 312, loss = 0.42183193\n",
      "Iteration 313, loss = 0.42193699\n",
      "Iteration 314, loss = 0.42174652\n",
      "Iteration 315, loss = 0.42150176\n",
      "Iteration 316, loss = 0.42160814\n",
      "Iteration 317, loss = 0.42156286\n",
      "Iteration 318, loss = 0.42143945\n",
      "Iteration 319, loss = 0.42148013\n",
      "Iteration 320, loss = 0.42119452\n",
      "Iteration 321, loss = 0.42100526\n",
      "Iteration 322, loss = 0.42121551\n",
      "Iteration 323, loss = 0.42106486\n",
      "Iteration 324, loss = 0.42102233\n",
      "Iteration 325, loss = 0.42112224\n",
      "Iteration 326, loss = 0.42098482\n",
      "Iteration 327, loss = 0.42089355\n",
      "Iteration 328, loss = 0.42102034\n",
      "Iteration 329, loss = 0.42077659\n",
      "Iteration 330, loss = 0.42072835\n",
      "Iteration 331, loss = 0.42037876\n",
      "Iteration 332, loss = 0.42069516\n",
      "Iteration 333, loss = 0.42055926\n",
      "Iteration 334, loss = 0.42029778\n",
      "Iteration 335, loss = 0.42027707\n",
      "Iteration 336, loss = 0.42017457\n",
      "Iteration 337, loss = 0.42035114\n",
      "Iteration 338, loss = 0.42029210\n",
      "Iteration 339, loss = 0.42002348\n",
      "Iteration 340, loss = 0.42001508\n",
      "Iteration 341, loss = 0.42014246\n",
      "Iteration 342, loss = 0.41979943\n",
      "Iteration 343, loss = 0.41991033\n",
      "Iteration 344, loss = 0.41972090\n",
      "Iteration 345, loss = 0.41971727\n",
      "Iteration 346, loss = 0.42006398\n",
      "Iteration 347, loss = 0.41981536\n",
      "Iteration 348, loss = 0.41978209\n",
      "Iteration 349, loss = 0.41966769\n",
      "Iteration 350, loss = 0.41950854\n",
      "Iteration 351, loss = 0.41956205\n",
      "Iteration 352, loss = 0.41939548\n",
      "Iteration 353, loss = 0.41932581\n",
      "Iteration 354, loss = 0.41921560\n",
      "Iteration 355, loss = 0.41938394\n",
      "Iteration 356, loss = 0.41920552\n",
      "Iteration 357, loss = 0.41912514\n",
      "Iteration 358, loss = 0.41919866\n",
      "Iteration 359, loss = 0.41933705\n",
      "Iteration 360, loss = 0.41905564\n",
      "Iteration 361, loss = 0.41902212\n",
      "Iteration 362, loss = 0.41872613\n",
      "Iteration 363, loss = 0.41887237\n",
      "Iteration 364, loss = 0.41890119\n",
      "Iteration 365, loss = 0.41875072\n",
      "Iteration 366, loss = 0.41854376\n",
      "Iteration 367, loss = 0.41880252\n",
      "Iteration 368, loss = 0.41859007\n",
      "Iteration 369, loss = 0.41854170\n",
      "Iteration 370, loss = 0.41847204\n",
      "Iteration 371, loss = 0.41862973\n",
      "Iteration 372, loss = 0.41841203\n",
      "Iteration 373, loss = 0.41876242\n",
      "Iteration 374, loss = 0.41804204\n",
      "Iteration 375, loss = 0.41822975\n",
      "Iteration 376, loss = 0.41814156\n",
      "Iteration 377, loss = 0.41836991\n",
      "Iteration 378, loss = 0.41829358\n",
      "Iteration 379, loss = 0.41815858\n",
      "Iteration 380, loss = 0.41825934\n",
      "Iteration 381, loss = 0.41813154\n",
      "Iteration 382, loss = 0.41792058\n",
      "Iteration 383, loss = 0.41780826\n",
      "Iteration 384, loss = 0.41790973\n",
      "Iteration 385, loss = 0.41762221\n",
      "Iteration 386, loss = 0.41751636\n",
      "Iteration 387, loss = 0.41753380\n",
      "Iteration 388, loss = 0.41794757\n",
      "Iteration 389, loss = 0.41756045\n",
      "Iteration 390, loss = 0.41763881\n",
      "Iteration 391, loss = 0.41766493\n",
      "Iteration 392, loss = 0.41746772\n",
      "Iteration 393, loss = 0.41743956\n",
      "Iteration 394, loss = 0.41743751\n",
      "Iteration 395, loss = 0.41728937\n",
      "Iteration 396, loss = 0.41752657\n",
      "Iteration 397, loss = 0.41755773\n",
      "Iteration 398, loss = 0.41733830\n",
      "Iteration 399, loss = 0.41741900\n",
      "Iteration 400, loss = 0.41732979\n",
      "Iteration 401, loss = 0.41707857\n",
      "Iteration 402, loss = 0.41699575\n",
      "Iteration 403, loss = 0.41708684\n",
      "Iteration 404, loss = 0.41682515\n",
      "Iteration 405, loss = 0.41688982\n",
      "Iteration 406, loss = 0.41714912\n",
      "Iteration 407, loss = 0.41683423\n",
      "Iteration 408, loss = 0.41668437\n",
      "Iteration 409, loss = 0.41705767\n",
      "Iteration 410, loss = 0.41705425\n",
      "Iteration 411, loss = 0.41675427\n",
      "Iteration 412, loss = 0.41636528\n",
      "Iteration 413, loss = 0.41648443\n",
      "Iteration 414, loss = 0.41669739\n",
      "Iteration 415, loss = 0.41641084\n",
      "Iteration 416, loss = 0.41637492\n",
      "Iteration 417, loss = 0.41644850\n",
      "Iteration 418, loss = 0.41629867\n",
      "Iteration 419, loss = 0.41650150\n",
      "Iteration 420, loss = 0.41632561\n",
      "Iteration 421, loss = 0.41664981\n",
      "Iteration 422, loss = 0.41615421\n",
      "Iteration 423, loss = 0.41602259\n",
      "Iteration 424, loss = 0.41624017\n",
      "Iteration 425, loss = 0.41645876\n",
      "Iteration 426, loss = 0.41614697\n",
      "Iteration 427, loss = 0.41627436\n",
      "Iteration 428, loss = 0.41629432\n",
      "Iteration 429, loss = 0.41591549\n",
      "Iteration 430, loss = 0.41589195\n",
      "Iteration 431, loss = 0.41629049\n",
      "Iteration 432, loss = 0.41591999\n",
      "Iteration 433, loss = 0.41563604\n",
      "Iteration 434, loss = 0.41581313\n",
      "Iteration 435, loss = 0.41576302\n",
      "Iteration 436, loss = 0.41563886\n",
      "Iteration 437, loss = 0.41591159\n",
      "Iteration 438, loss = 0.41557905\n",
      "Iteration 439, loss = 0.41554838\n",
      "Iteration 440, loss = 0.41552094\n",
      "Iteration 441, loss = 0.41544960\n",
      "Iteration 442, loss = 0.41552077\n",
      "Iteration 443, loss = 0.41567878\n",
      "Iteration 444, loss = 0.41536056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96514503\n",
      "Iteration 2, loss = 1.77945095\n",
      "Iteration 3, loss = 1.60853419\n",
      "Iteration 4, loss = 1.45851045\n",
      "Iteration 5, loss = 1.33028409\n",
      "Iteration 6, loss = 1.22336898\n",
      "Iteration 7, loss = 1.13562681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.06302765\n",
      "Iteration 9, loss = 1.00249103\n",
      "Iteration 10, loss = 0.95155118\n",
      "Iteration 11, loss = 0.90816869\n",
      "Iteration 12, loss = 0.87029278\n",
      "Iteration 13, loss = 0.83705567\n",
      "Iteration 14, loss = 0.80744215\n",
      "Iteration 15, loss = 0.78081947\n",
      "Iteration 16, loss = 0.75691998\n",
      "Iteration 17, loss = 0.73510476\n",
      "Iteration 18, loss = 0.71527041\n",
      "Iteration 19, loss = 0.69762379\n",
      "Iteration 20, loss = 0.68158025\n",
      "Iteration 21, loss = 0.66704573\n",
      "Iteration 22, loss = 0.65381863\n",
      "Iteration 23, loss = 0.64183266\n",
      "Iteration 24, loss = 0.63112384\n",
      "Iteration 25, loss = 0.62114395\n",
      "Iteration 26, loss = 0.61237618\n",
      "Iteration 27, loss = 0.60436246\n",
      "Iteration 28, loss = 0.59713067\n",
      "Iteration 29, loss = 0.59060130\n",
      "Iteration 30, loss = 0.58449550\n",
      "Iteration 31, loss = 0.57909883\n",
      "Iteration 32, loss = 0.57381721\n",
      "Iteration 33, loss = 0.56918578\n",
      "Iteration 34, loss = 0.56468443\n",
      "Iteration 35, loss = 0.56086726\n",
      "Iteration 36, loss = 0.55692601\n",
      "Iteration 37, loss = 0.55353692\n",
      "Iteration 38, loss = 0.55012914\n",
      "Iteration 39, loss = 0.54721561\n",
      "Iteration 40, loss = 0.54407679\n",
      "Iteration 41, loss = 0.54126904\n",
      "Iteration 42, loss = 0.53878269\n",
      "Iteration 43, loss = 0.53636285\n",
      "Iteration 44, loss = 0.53404327\n",
      "Iteration 45, loss = 0.53170093\n",
      "Iteration 46, loss = 0.52963623\n",
      "Iteration 47, loss = 0.52754972\n",
      "Iteration 48, loss = 0.52562900\n",
      "Iteration 49, loss = 0.52378367\n",
      "Iteration 50, loss = 0.52206383\n",
      "Iteration 51, loss = 0.52017865\n",
      "Iteration 52, loss = 0.51863683\n",
      "Iteration 53, loss = 0.51707925\n",
      "Iteration 54, loss = 0.51552518\n",
      "Iteration 55, loss = 0.51427196\n",
      "Iteration 56, loss = 0.51254953\n",
      "Iteration 57, loss = 0.51137514\n",
      "Iteration 58, loss = 0.51009727\n",
      "Iteration 59, loss = 0.50864953\n",
      "Iteration 60, loss = 0.50748451\n",
      "Iteration 61, loss = 0.50623761\n",
      "Iteration 62, loss = 0.50494246\n",
      "Iteration 63, loss = 0.50392753\n",
      "Iteration 64, loss = 0.50287949\n",
      "Iteration 65, loss = 0.50152095\n",
      "Iteration 66, loss = 0.50061106\n",
      "Iteration 67, loss = 0.49957318\n",
      "Iteration 68, loss = 0.49872580\n",
      "Iteration 69, loss = 0.49778763\n",
      "Iteration 70, loss = 0.49673384\n",
      "Iteration 71, loss = 0.49589292\n",
      "Iteration 72, loss = 0.49480101\n",
      "Iteration 73, loss = 0.49398109\n",
      "Iteration 74, loss = 0.49310408\n",
      "Iteration 75, loss = 0.49200386\n",
      "Iteration 76, loss = 0.49132458\n",
      "Iteration 77, loss = 0.49065062\n",
      "Iteration 78, loss = 0.48968684\n",
      "Iteration 79, loss = 0.48888573\n",
      "Iteration 80, loss = 0.48815600\n",
      "Iteration 81, loss = 0.48761536\n",
      "Iteration 82, loss = 0.48670310\n",
      "Iteration 83, loss = 0.48584488\n",
      "Iteration 84, loss = 0.48522364\n",
      "Iteration 85, loss = 0.48457910\n",
      "Iteration 86, loss = 0.48374772\n",
      "Iteration 87, loss = 0.48304306\n",
      "Iteration 88, loss = 0.48260120\n",
      "Iteration 89, loss = 0.48160858\n",
      "Iteration 90, loss = 0.48114052\n",
      "Iteration 91, loss = 0.48049069\n",
      "Iteration 92, loss = 0.47960227\n",
      "Iteration 93, loss = 0.47924481\n",
      "Iteration 94, loss = 0.47843724\n",
      "Iteration 95, loss = 0.47778897\n",
      "Iteration 96, loss = 0.47732991\n",
      "Iteration 97, loss = 0.47652812\n",
      "Iteration 98, loss = 0.47601061\n",
      "Iteration 99, loss = 0.47539259\n",
      "Iteration 100, loss = 0.47474107\n",
      "Iteration 101, loss = 0.47432352\n",
      "Iteration 102, loss = 0.47351146\n",
      "Iteration 103, loss = 0.47317005\n",
      "Iteration 104, loss = 0.47257787\n",
      "Iteration 105, loss = 0.47186935\n",
      "Iteration 106, loss = 0.47159797\n",
      "Iteration 107, loss = 0.47099266\n",
      "Iteration 108, loss = 0.47024976\n",
      "Iteration 109, loss = 0.46981297\n",
      "Iteration 110, loss = 0.46919872\n",
      "Iteration 111, loss = 0.46873969\n",
      "Iteration 112, loss = 0.46830118\n",
      "Iteration 113, loss = 0.46785651\n",
      "Iteration 114, loss = 0.46723461\n",
      "Iteration 115, loss = 0.46670308\n",
      "Iteration 116, loss = 0.46645530\n",
      "Iteration 117, loss = 0.46569586\n",
      "Iteration 118, loss = 0.46533063\n",
      "Iteration 119, loss = 0.46498011\n",
      "Iteration 120, loss = 0.46454930\n",
      "Iteration 121, loss = 0.46401301\n",
      "Iteration 122, loss = 0.46332062\n",
      "Iteration 123, loss = 0.46299198\n",
      "Iteration 124, loss = 0.46236625\n",
      "Iteration 125, loss = 0.46192336\n",
      "Iteration 126, loss = 0.46158972\n",
      "Iteration 127, loss = 0.46109403\n",
      "Iteration 128, loss = 0.46058200\n",
      "Iteration 129, loss = 0.46016933\n",
      "Iteration 130, loss = 0.45968938\n",
      "Iteration 131, loss = 0.45926520\n",
      "Iteration 132, loss = 0.45906480\n",
      "Iteration 133, loss = 0.45841878\n",
      "Iteration 134, loss = 0.45809003\n",
      "Iteration 135, loss = 0.45775069\n",
      "Iteration 136, loss = 0.45741430\n",
      "Iteration 137, loss = 0.45676000\n",
      "Iteration 138, loss = 0.45652404\n",
      "Iteration 139, loss = 0.45578854\n",
      "Iteration 140, loss = 0.45540591\n",
      "Iteration 141, loss = 0.45533003\n",
      "Iteration 142, loss = 0.45484998\n",
      "Iteration 143, loss = 0.45452631\n",
      "Iteration 144, loss = 0.45423091\n",
      "Iteration 145, loss = 0.45396970\n",
      "Iteration 146, loss = 0.45348243\n",
      "Iteration 147, loss = 0.45299425\n",
      "Iteration 148, loss = 0.45256033\n",
      "Iteration 149, loss = 0.45234623\n",
      "Iteration 150, loss = 0.45189135\n",
      "Iteration 151, loss = 0.45150784\n",
      "Iteration 152, loss = 0.45111645\n",
      "Iteration 153, loss = 0.45091774\n",
      "Iteration 154, loss = 0.45059781\n",
      "Iteration 155, loss = 0.45023844\n",
      "Iteration 156, loss = 0.44974575\n",
      "Iteration 157, loss = 0.44969542\n",
      "Iteration 158, loss = 0.44910156\n",
      "Iteration 159, loss = 0.44875497\n",
      "Iteration 160, loss = 0.44861650\n",
      "Iteration 161, loss = 0.44838068\n",
      "Iteration 162, loss = 0.44798791\n",
      "Iteration 163, loss = 0.44752920\n",
      "Iteration 164, loss = 0.44738039\n",
      "Iteration 165, loss = 0.44692714\n",
      "Iteration 166, loss = 0.44686339\n",
      "Iteration 167, loss = 0.44627593\n",
      "Iteration 168, loss = 0.44608523\n",
      "Iteration 169, loss = 0.44585105\n",
      "Iteration 170, loss = 0.44545499\n",
      "Iteration 171, loss = 0.44537596\n",
      "Iteration 172, loss = 0.44478770\n",
      "Iteration 173, loss = 0.44479790\n",
      "Iteration 174, loss = 0.44426774\n",
      "Iteration 175, loss = 0.44397307\n",
      "Iteration 176, loss = 0.44416551\n",
      "Iteration 177, loss = 0.44363381\n",
      "Iteration 178, loss = 0.44337786\n",
      "Iteration 179, loss = 0.44304376\n",
      "Iteration 180, loss = 0.44267310\n",
      "Iteration 181, loss = 0.44245494\n",
      "Iteration 182, loss = 0.44216982\n",
      "Iteration 183, loss = 0.44207338\n",
      "Iteration 184, loss = 0.44178871\n",
      "Iteration 185, loss = 0.44162688\n",
      "Iteration 186, loss = 0.44135031\n",
      "Iteration 187, loss = 0.44101918\n",
      "Iteration 188, loss = 0.44096824\n",
      "Iteration 189, loss = 0.44068755\n",
      "Iteration 190, loss = 0.44025423\n",
      "Iteration 191, loss = 0.44033023\n",
      "Iteration 192, loss = 0.43993112\n",
      "Iteration 193, loss = 0.43956421\n",
      "Iteration 194, loss = 0.43938890\n",
      "Iteration 195, loss = 0.43949045\n",
      "Iteration 196, loss = 0.43927004\n",
      "Iteration 197, loss = 0.43898158\n",
      "Iteration 198, loss = 0.43837085\n",
      "Iteration 199, loss = 0.43854591\n",
      "Iteration 200, loss = 0.43809568\n",
      "Iteration 201, loss = 0.43804519\n",
      "Iteration 202, loss = 0.43778780\n",
      "Iteration 203, loss = 0.43742605\n",
      "Iteration 204, loss = 0.43759383\n",
      "Iteration 205, loss = 0.43727663\n",
      "Iteration 206, loss = 0.43707787\n",
      "Iteration 207, loss = 0.43676733\n",
      "Iteration 208, loss = 0.43674745\n",
      "Iteration 209, loss = 0.43645309\n",
      "Iteration 210, loss = 0.43617039\n",
      "Iteration 211, loss = 0.43604363\n",
      "Iteration 212, loss = 0.43579012\n",
      "Iteration 213, loss = 0.43583117\n",
      "Iteration 214, loss = 0.43584300\n",
      "Iteration 215, loss = 0.43535848\n",
      "Iteration 216, loss = 0.43502844\n",
      "Iteration 217, loss = 0.43490531\n",
      "Iteration 218, loss = 0.43469328\n",
      "Iteration 219, loss = 0.43455038\n",
      "Iteration 220, loss = 0.43448535\n",
      "Iteration 221, loss = 0.43415020\n",
      "Iteration 222, loss = 0.43429152\n",
      "Iteration 223, loss = 0.43404575\n",
      "Iteration 224, loss = 0.43380694\n",
      "Iteration 225, loss = 0.43350962\n",
      "Iteration 226, loss = 0.43349413\n",
      "Iteration 227, loss = 0.43353969\n",
      "Iteration 228, loss = 0.43350961\n",
      "Iteration 229, loss = 0.43300476\n",
      "Iteration 230, loss = 0.43305027\n",
      "Iteration 231, loss = 0.43271146\n",
      "Iteration 232, loss = 0.43242428\n",
      "Iteration 233, loss = 0.43250606\n",
      "Iteration 234, loss = 0.43230742\n",
      "Iteration 235, loss = 0.43214990\n",
      "Iteration 236, loss = 0.43185409\n",
      "Iteration 237, loss = 0.43182581\n",
      "Iteration 238, loss = 0.43188939\n",
      "Iteration 239, loss = 0.43147219\n",
      "Iteration 240, loss = 0.43153609\n",
      "Iteration 241, loss = 0.43138492\n",
      "Iteration 242, loss = 0.43129697\n",
      "Iteration 243, loss = 0.43120062\n",
      "Iteration 244, loss = 0.43061377\n",
      "Iteration 245, loss = 0.43068537\n",
      "Iteration 246, loss = 0.43065630\n",
      "Iteration 247, loss = 0.43084898\n",
      "Iteration 248, loss = 0.43025878\n",
      "Iteration 249, loss = 0.43042756\n",
      "Iteration 250, loss = 0.43018862\n",
      "Iteration 251, loss = 0.43005208\n",
      "Iteration 252, loss = 0.42975638\n",
      "Iteration 253, loss = 0.42980019\n",
      "Iteration 254, loss = 0.42988425\n",
      "Iteration 255, loss = 0.42966818\n",
      "Iteration 256, loss = 0.42952909\n",
      "Iteration 257, loss = 0.42920975\n",
      "Iteration 258, loss = 0.42908518\n",
      "Iteration 259, loss = 0.42901566\n",
      "Iteration 260, loss = 0.42908895\n",
      "Iteration 261, loss = 0.42884619\n",
      "Iteration 262, loss = 0.42872301\n",
      "Iteration 263, loss = 0.42858800\n",
      "Iteration 264, loss = 0.42849876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, loss = 0.42826749\n",
      "Iteration 266, loss = 0.42828996\n",
      "Iteration 267, loss = 0.42804557\n",
      "Iteration 268, loss = 0.42805175\n",
      "Iteration 269, loss = 0.42788260\n",
      "Iteration 270, loss = 0.42805069\n",
      "Iteration 271, loss = 0.42796908\n",
      "Iteration 272, loss = 0.42774622\n",
      "Iteration 273, loss = 0.42784055\n",
      "Iteration 274, loss = 0.42772679\n",
      "Iteration 275, loss = 0.42744694\n",
      "Iteration 276, loss = 0.42741367\n",
      "Iteration 277, loss = 0.42719649\n",
      "Iteration 278, loss = 0.42719334\n",
      "Iteration 279, loss = 0.42713618\n",
      "Iteration 280, loss = 0.42705978\n",
      "Iteration 281, loss = 0.42674135\n",
      "Iteration 282, loss = 0.42677312\n",
      "Iteration 283, loss = 0.42662909\n",
      "Iteration 284, loss = 0.42643661\n",
      "Iteration 285, loss = 0.42670562\n",
      "Iteration 286, loss = 0.42634814\n",
      "Iteration 287, loss = 0.42644408\n",
      "Iteration 288, loss = 0.42610583\n",
      "Iteration 289, loss = 0.42603696\n",
      "Iteration 290, loss = 0.42606150\n",
      "Iteration 291, loss = 0.42614858\n",
      "Iteration 292, loss = 0.42621033\n",
      "Iteration 293, loss = 0.42605698\n",
      "Iteration 294, loss = 0.42580899\n",
      "Iteration 295, loss = 0.42543681\n",
      "Iteration 296, loss = 0.42555474\n",
      "Iteration 297, loss = 0.42536774\n",
      "Iteration 298, loss = 0.42553956\n",
      "Iteration 299, loss = 0.42520070\n",
      "Iteration 300, loss = 0.42551665\n",
      "Iteration 301, loss = 0.42528459\n",
      "Iteration 302, loss = 0.42486194\n",
      "Iteration 303, loss = 0.42501927\n",
      "Iteration 304, loss = 0.42483420\n",
      "Iteration 305, loss = 0.42475101\n",
      "Iteration 306, loss = 0.42469937\n",
      "Iteration 307, loss = 0.42465858\n",
      "Iteration 308, loss = 0.42449726\n",
      "Iteration 309, loss = 0.42441649\n",
      "Iteration 310, loss = 0.42455776\n",
      "Iteration 311, loss = 0.42450023\n",
      "Iteration 312, loss = 0.42440729\n",
      "Iteration 313, loss = 0.42408746\n",
      "Iteration 314, loss = 0.42408934\n",
      "Iteration 315, loss = 0.42424368\n",
      "Iteration 316, loss = 0.42416485\n",
      "Iteration 317, loss = 0.42401822\n",
      "Iteration 318, loss = 0.42373310\n",
      "Iteration 319, loss = 0.42396880\n",
      "Iteration 320, loss = 0.42374490\n",
      "Iteration 321, loss = 0.42351684\n",
      "Iteration 322, loss = 0.42345052\n",
      "Iteration 323, loss = 0.42360925\n",
      "Iteration 324, loss = 0.42340806\n",
      "Iteration 325, loss = 0.42335005\n",
      "Iteration 326, loss = 0.42315358\n",
      "Iteration 327, loss = 0.42329671\n",
      "Iteration 328, loss = 0.42328925\n",
      "Iteration 329, loss = 0.42327786\n",
      "Iteration 330, loss = 0.42312265\n",
      "Iteration 331, loss = 0.42293824\n",
      "Iteration 332, loss = 0.42286602\n",
      "Iteration 333, loss = 0.42317116\n",
      "Iteration 334, loss = 0.42270314\n",
      "Iteration 335, loss = 0.42266084\n",
      "Iteration 336, loss = 0.42274753\n",
      "Iteration 337, loss = 0.42267300\n",
      "Iteration 338, loss = 0.42247573\n",
      "Iteration 339, loss = 0.42269666\n",
      "Iteration 340, loss = 0.42229058\n",
      "Iteration 341, loss = 0.42248420\n",
      "Iteration 342, loss = 0.42237162\n",
      "Iteration 343, loss = 0.42246369\n",
      "Iteration 344, loss = 0.42242642\n",
      "Iteration 345, loss = 0.42229918\n",
      "Iteration 346, loss = 0.42228881\n",
      "Iteration 347, loss = 0.42233501\n",
      "Iteration 348, loss = 0.42189943\n",
      "Iteration 349, loss = 0.42194659\n",
      "Iteration 350, loss = 0.42206726\n",
      "Iteration 351, loss = 0.42197967\n",
      "Iteration 352, loss = 0.42170565\n",
      "Iteration 353, loss = 0.42192284\n",
      "Iteration 354, loss = 0.42168083\n",
      "Iteration 355, loss = 0.42139164\n",
      "Iteration 356, loss = 0.42130511\n",
      "Iteration 357, loss = 0.42138537\n",
      "Iteration 358, loss = 0.42150322\n",
      "Iteration 359, loss = 0.42141684\n",
      "Iteration 360, loss = 0.42149699\n",
      "Iteration 361, loss = 0.42121120\n",
      "Iteration 362, loss = 0.42112691\n",
      "Iteration 363, loss = 0.42108027\n",
      "Iteration 364, loss = 0.42098500\n",
      "Iteration 365, loss = 0.42113807\n",
      "Iteration 366, loss = 0.42121820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96304754\n",
      "Iteration 2, loss = 1.77791385\n",
      "Iteration 3, loss = 1.60698194\n",
      "Iteration 4, loss = 1.45839869\n",
      "Iteration 5, loss = 1.33266923\n",
      "Iteration 6, loss = 1.22573280\n",
      "Iteration 7, loss = 1.13739342\n",
      "Iteration 8, loss = 1.06486814\n",
      "Iteration 9, loss = 1.00380812\n",
      "Iteration 10, loss = 0.95274723\n",
      "Iteration 11, loss = 0.90912878\n",
      "Iteration 12, loss = 0.87139405\n",
      "Iteration 13, loss = 0.83840562\n",
      "Iteration 14, loss = 0.80889566\n",
      "Iteration 15, loss = 0.78242025\n",
      "Iteration 16, loss = 0.75853982\n",
      "Iteration 17, loss = 0.73706972\n",
      "Iteration 18, loss = 0.71749836\n",
      "Iteration 19, loss = 0.69989759\n",
      "Iteration 20, loss = 0.68381665\n",
      "Iteration 21, loss = 0.66932969\n",
      "Iteration 22, loss = 0.65618160\n",
      "Iteration 23, loss = 0.64436436\n",
      "Iteration 24, loss = 0.63392572\n",
      "Iteration 25, loss = 0.62351500\n",
      "Iteration 26, loss = 0.61489247\n",
      "Iteration 27, loss = 0.60709177\n",
      "Iteration 28, loss = 0.59983731\n",
      "Iteration 29, loss = 0.59329065\n",
      "Iteration 30, loss = 0.58755022\n",
      "Iteration 31, loss = 0.58186807\n",
      "Iteration 32, loss = 0.57678322\n",
      "Iteration 33, loss = 0.57218685\n",
      "Iteration 34, loss = 0.56787647\n",
      "Iteration 35, loss = 0.56387130\n",
      "Iteration 36, loss = 0.56009812\n",
      "Iteration 37, loss = 0.55662479\n",
      "Iteration 38, loss = 0.55354546\n",
      "Iteration 39, loss = 0.55055757\n",
      "Iteration 40, loss = 0.54743391\n",
      "Iteration 41, loss = 0.54463100\n",
      "Iteration 42, loss = 0.54198706\n",
      "Iteration 43, loss = 0.53982704\n",
      "Iteration 44, loss = 0.53765014\n",
      "Iteration 45, loss = 0.53531151\n",
      "Iteration 46, loss = 0.53300932\n",
      "Iteration 47, loss = 0.53122788\n",
      "Iteration 48, loss = 0.52917162\n",
      "Iteration 49, loss = 0.52720278\n",
      "Iteration 50, loss = 0.52566061\n",
      "Iteration 51, loss = 0.52364557\n",
      "Iteration 52, loss = 0.52224121\n",
      "Iteration 53, loss = 0.52066075\n",
      "Iteration 54, loss = 0.51922312\n",
      "Iteration 55, loss = 0.51761817\n",
      "Iteration 56, loss = 0.51635715\n",
      "Iteration 57, loss = 0.51495287\n",
      "Iteration 58, loss = 0.51353340\n",
      "Iteration 59, loss = 0.51247010\n",
      "Iteration 60, loss = 0.51102017\n",
      "Iteration 61, loss = 0.50981878\n",
      "Iteration 62, loss = 0.50878232\n",
      "Iteration 63, loss = 0.50749205\n",
      "Iteration 64, loss = 0.50677306\n",
      "Iteration 65, loss = 0.50535563\n",
      "Iteration 66, loss = 0.50449674\n",
      "Iteration 67, loss = 0.50339957\n",
      "Iteration 68, loss = 0.50250012\n",
      "Iteration 69, loss = 0.50141186\n",
      "Iteration 70, loss = 0.50052788\n",
      "Iteration 71, loss = 0.49942190\n",
      "Iteration 72, loss = 0.49865971\n",
      "Iteration 73, loss = 0.49772444\n",
      "Iteration 74, loss = 0.49688508\n",
      "Iteration 75, loss = 0.49613440\n",
      "Iteration 76, loss = 0.49508953\n",
      "Iteration 77, loss = 0.49443627\n",
      "Iteration 78, loss = 0.49353916\n",
      "Iteration 79, loss = 0.49257470\n",
      "Iteration 80, loss = 0.49210754\n",
      "Iteration 81, loss = 0.49110089\n",
      "Iteration 82, loss = 0.49026989\n",
      "Iteration 83, loss = 0.48947950\n",
      "Iteration 84, loss = 0.48907656\n",
      "Iteration 85, loss = 0.48821532\n",
      "Iteration 86, loss = 0.48742577\n",
      "Iteration 87, loss = 0.48678518\n",
      "Iteration 88, loss = 0.48605634\n",
      "Iteration 89, loss = 0.48531178\n",
      "Iteration 90, loss = 0.48461093\n",
      "Iteration 91, loss = 0.48417525\n",
      "Iteration 92, loss = 0.48363159\n",
      "Iteration 93, loss = 0.48262705\n",
      "Iteration 94, loss = 0.48206977\n",
      "Iteration 95, loss = 0.48148729\n",
      "Iteration 96, loss = 0.48104083\n",
      "Iteration 97, loss = 0.48019556\n",
      "Iteration 98, loss = 0.47970737\n",
      "Iteration 99, loss = 0.47920246\n",
      "Iteration 100, loss = 0.47831925\n",
      "Iteration 101, loss = 0.47764125\n",
      "Iteration 102, loss = 0.47723082\n",
      "Iteration 103, loss = 0.47657239\n",
      "Iteration 104, loss = 0.47619958\n",
      "Iteration 105, loss = 0.47566888\n",
      "Iteration 106, loss = 0.47501966\n",
      "Iteration 107, loss = 0.47435652\n",
      "Iteration 108, loss = 0.47387448\n",
      "Iteration 109, loss = 0.47316981\n",
      "Iteration 110, loss = 0.47285139\n",
      "Iteration 111, loss = 0.47248150\n",
      "Iteration 112, loss = 0.47170696\n",
      "Iteration 113, loss = 0.47105210\n",
      "Iteration 114, loss = 0.47071268\n",
      "Iteration 115, loss = 0.47006001\n",
      "Iteration 116, loss = 0.46962931\n",
      "Iteration 117, loss = 0.46917037\n",
      "Iteration 118, loss = 0.46873051\n",
      "Iteration 119, loss = 0.46805868\n",
      "Iteration 120, loss = 0.46765915\n",
      "Iteration 121, loss = 0.46712837\n",
      "Iteration 122, loss = 0.46655884\n",
      "Iteration 123, loss = 0.46629396\n",
      "Iteration 124, loss = 0.46583972\n",
      "Iteration 125, loss = 0.46541327\n",
      "Iteration 126, loss = 0.46497597\n",
      "Iteration 127, loss = 0.46423247\n",
      "Iteration 128, loss = 0.46390693\n",
      "Iteration 129, loss = 0.46371101\n",
      "Iteration 130, loss = 0.46310152\n",
      "Iteration 131, loss = 0.46270332\n",
      "Iteration 132, loss = 0.46198692\n",
      "Iteration 133, loss = 0.46169029\n",
      "Iteration 134, loss = 0.46130885\n",
      "Iteration 135, loss = 0.46079830\n",
      "Iteration 136, loss = 0.46028542\n",
      "Iteration 137, loss = 0.46003862\n",
      "Iteration 138, loss = 0.45946751\n",
      "Iteration 139, loss = 0.45930524\n",
      "Iteration 140, loss = 0.45863989\n",
      "Iteration 141, loss = 0.45819331\n",
      "Iteration 142, loss = 0.45804137\n",
      "Iteration 143, loss = 0.45769123\n",
      "Iteration 144, loss = 0.45699086\n",
      "Iteration 145, loss = 0.45640116\n",
      "Iteration 146, loss = 0.45635261\n",
      "Iteration 147, loss = 0.45587707\n",
      "Iteration 148, loss = 0.45566080\n",
      "Iteration 149, loss = 0.45533335\n",
      "Iteration 150, loss = 0.45486053\n",
      "Iteration 151, loss = 0.45443057\n",
      "Iteration 152, loss = 0.45421169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 153, loss = 0.45373682\n",
      "Iteration 154, loss = 0.45360790\n",
      "Iteration 155, loss = 0.45312133\n",
      "Iteration 156, loss = 0.45258342\n",
      "Iteration 157, loss = 0.45244742\n",
      "Iteration 158, loss = 0.45187200\n",
      "Iteration 159, loss = 0.45167896\n",
      "Iteration 160, loss = 0.45151083\n",
      "Iteration 161, loss = 0.45098333\n",
      "Iteration 162, loss = 0.45063189\n",
      "Iteration 163, loss = 0.45034004\n",
      "Iteration 164, loss = 0.45027767\n",
      "Iteration 165, loss = 0.44954904\n",
      "Iteration 166, loss = 0.44926440\n",
      "Iteration 167, loss = 0.44913409\n",
      "Iteration 168, loss = 0.44869864\n",
      "Iteration 169, loss = 0.44858880\n",
      "Iteration 170, loss = 0.44806657\n",
      "Iteration 171, loss = 0.44819389\n",
      "Iteration 172, loss = 0.44758063\n",
      "Iteration 173, loss = 0.44729905\n",
      "Iteration 174, loss = 0.44704010\n",
      "Iteration 175, loss = 0.44681529\n",
      "Iteration 176, loss = 0.44636864\n",
      "Iteration 177, loss = 0.44634981\n",
      "Iteration 178, loss = 0.44605750\n",
      "Iteration 179, loss = 0.44579117\n",
      "Iteration 180, loss = 0.44548248\n",
      "Iteration 181, loss = 0.44533664\n",
      "Iteration 182, loss = 0.44493841\n",
      "Iteration 183, loss = 0.44493966\n",
      "Iteration 184, loss = 0.44432295\n",
      "Iteration 185, loss = 0.44446750\n",
      "Iteration 186, loss = 0.44391084\n",
      "Iteration 187, loss = 0.44363579\n",
      "Iteration 188, loss = 0.44358092\n",
      "Iteration 189, loss = 0.44355005\n",
      "Iteration 190, loss = 0.44289186\n",
      "Iteration 191, loss = 0.44262437\n",
      "Iteration 192, loss = 0.44263553\n",
      "Iteration 193, loss = 0.44219602\n",
      "Iteration 194, loss = 0.44221026\n",
      "Iteration 195, loss = 0.44207221\n",
      "Iteration 196, loss = 0.44167863\n",
      "Iteration 197, loss = 0.44154136\n",
      "Iteration 198, loss = 0.44104459\n",
      "Iteration 199, loss = 0.44125774\n",
      "Iteration 200, loss = 0.44078415\n",
      "Iteration 201, loss = 0.44054395\n",
      "Iteration 202, loss = 0.44018843\n",
      "Iteration 203, loss = 0.44019853\n",
      "Iteration 204, loss = 0.43993743\n",
      "Iteration 205, loss = 0.43993719\n",
      "Iteration 206, loss = 0.43970081\n",
      "Iteration 207, loss = 0.43956403\n",
      "Iteration 208, loss = 0.43925451\n",
      "Iteration 209, loss = 0.43892944\n",
      "Iteration 210, loss = 0.43878081\n",
      "Iteration 211, loss = 0.43896328\n",
      "Iteration 212, loss = 0.43843472\n",
      "Iteration 213, loss = 0.43815365\n",
      "Iteration 214, loss = 0.43793704\n",
      "Iteration 215, loss = 0.43797670\n",
      "Iteration 216, loss = 0.43784355\n",
      "Iteration 217, loss = 0.43761886\n",
      "Iteration 218, loss = 0.43734295\n",
      "Iteration 219, loss = 0.43719846\n",
      "Iteration 220, loss = 0.43718168\n",
      "Iteration 221, loss = 0.43669747\n",
      "Iteration 222, loss = 0.43663926\n",
      "Iteration 223, loss = 0.43651000\n",
      "Iteration 224, loss = 0.43649408\n",
      "Iteration 225, loss = 0.43622366\n",
      "Iteration 226, loss = 0.43592638\n",
      "Iteration 227, loss = 0.43583917\n",
      "Iteration 228, loss = 0.43586516\n",
      "Iteration 229, loss = 0.43563751\n",
      "Iteration 230, loss = 0.43559406\n",
      "Iteration 231, loss = 0.43535914\n",
      "Iteration 232, loss = 0.43502297\n",
      "Iteration 233, loss = 0.43486487\n",
      "Iteration 234, loss = 0.43492724\n",
      "Iteration 235, loss = 0.43477951\n",
      "Iteration 236, loss = 0.43471571\n",
      "Iteration 237, loss = 0.43456971\n",
      "Iteration 238, loss = 0.43437465\n",
      "Iteration 239, loss = 0.43419880\n",
      "Iteration 240, loss = 0.43415279\n",
      "Iteration 241, loss = 0.43396953\n",
      "Iteration 242, loss = 0.43345370\n",
      "Iteration 243, loss = 0.43386661\n",
      "Iteration 244, loss = 0.43337637\n",
      "Iteration 245, loss = 0.43348997\n",
      "Iteration 246, loss = 0.43329650\n",
      "Iteration 247, loss = 0.43318723\n",
      "Iteration 248, loss = 0.43303741\n",
      "Iteration 249, loss = 0.43300697\n",
      "Iteration 250, loss = 0.43263180\n",
      "Iteration 251, loss = 0.43264444\n",
      "Iteration 252, loss = 0.43269932\n",
      "Iteration 253, loss = 0.43224351\n",
      "Iteration 254, loss = 0.43229385\n",
      "Iteration 255, loss = 0.43215552\n",
      "Iteration 256, loss = 0.43188298\n",
      "Iteration 257, loss = 0.43179231\n",
      "Iteration 258, loss = 0.43195987\n",
      "Iteration 259, loss = 0.43143559\n",
      "Iteration 260, loss = 0.43147337\n",
      "Iteration 261, loss = 0.43178134\n",
      "Iteration 262, loss = 0.43125926\n",
      "Iteration 263, loss = 0.43114033\n",
      "Iteration 264, loss = 0.43118361\n",
      "Iteration 265, loss = 0.43103154\n",
      "Iteration 266, loss = 0.43091585\n",
      "Iteration 267, loss = 0.43083711\n",
      "Iteration 268, loss = 0.43091041\n",
      "Iteration 269, loss = 0.43022196\n",
      "Iteration 270, loss = 0.43037493\n",
      "Iteration 271, loss = 0.43024791\n",
      "Iteration 272, loss = 0.43017329\n",
      "Iteration 273, loss = 0.43001662\n",
      "Iteration 274, loss = 0.42995598\n",
      "Iteration 275, loss = 0.42988017\n",
      "Iteration 276, loss = 0.42962995\n",
      "Iteration 277, loss = 0.42980082\n",
      "Iteration 278, loss = 0.42965189\n",
      "Iteration 279, loss = 0.42925163\n",
      "Iteration 280, loss = 0.42927940\n",
      "Iteration 281, loss = 0.42946966\n",
      "Iteration 282, loss = 0.42915347\n",
      "Iteration 283, loss = 0.42891099\n",
      "Iteration 284, loss = 0.42894682\n",
      "Iteration 285, loss = 0.42872181\n",
      "Iteration 286, loss = 0.42883555\n",
      "Iteration 287, loss = 0.42877163\n",
      "Iteration 288, loss = 0.42841810\n",
      "Iteration 289, loss = 0.42858147\n",
      "Iteration 290, loss = 0.42826573\n",
      "Iteration 291, loss = 0.42844147\n",
      "Iteration 292, loss = 0.42830313\n",
      "Iteration 293, loss = 0.42814034\n",
      "Iteration 294, loss = 0.42801037\n",
      "Iteration 295, loss = 0.42812200\n",
      "Iteration 296, loss = 0.42796811\n",
      "Iteration 297, loss = 0.42767250\n",
      "Iteration 298, loss = 0.42781399\n",
      "Iteration 299, loss = 0.42779136\n",
      "Iteration 300, loss = 0.42776789\n",
      "Iteration 301, loss = 0.42738912\n",
      "Iteration 302, loss = 0.42742611\n",
      "Iteration 303, loss = 0.42723027\n",
      "Iteration 304, loss = 0.42723266\n",
      "Iteration 305, loss = 0.42710392\n",
      "Iteration 306, loss = 0.42726839\n",
      "Iteration 307, loss = 0.42716192\n",
      "Iteration 308, loss = 0.42693673\n",
      "Iteration 309, loss = 0.42704010\n",
      "Iteration 310, loss = 0.42686713\n",
      "Iteration 311, loss = 0.42700845\n",
      "Iteration 312, loss = 0.42665474\n",
      "Iteration 313, loss = 0.42680784\n",
      "Iteration 314, loss = 0.42659779\n",
      "Iteration 315, loss = 0.42659008\n",
      "Iteration 316, loss = 0.42631073\n",
      "Iteration 317, loss = 0.42648747\n",
      "Iteration 318, loss = 0.42652267\n",
      "Iteration 319, loss = 0.42605498\n",
      "Iteration 320, loss = 0.42599335\n",
      "Iteration 321, loss = 0.42586028\n",
      "Iteration 322, loss = 0.42612054\n",
      "Iteration 323, loss = 0.42563700\n",
      "Iteration 324, loss = 0.42590674\n",
      "Iteration 325, loss = 0.42578775\n",
      "Iteration 326, loss = 0.42579738\n",
      "Iteration 327, loss = 0.42554359\n",
      "Iteration 328, loss = 0.42564016\n",
      "Iteration 329, loss = 0.42546147\n",
      "Iteration 330, loss = 0.42543227\n",
      "Iteration 331, loss = 0.42520236\n",
      "Iteration 332, loss = 0.42537422\n",
      "Iteration 333, loss = 0.42503442\n",
      "Iteration 334, loss = 0.42519582\n",
      "Iteration 335, loss = 0.42506731\n",
      "Iteration 336, loss = 0.42487629\n",
      "Iteration 337, loss = 0.42500240\n",
      "Iteration 338, loss = 0.42487974\n",
      "Iteration 339, loss = 0.42480956\n",
      "Iteration 340, loss = 0.42466224\n",
      "Iteration 341, loss = 0.42500835\n",
      "Iteration 342, loss = 0.42429737\n",
      "Iteration 343, loss = 0.42463283\n",
      "Iteration 344, loss = 0.42456630\n",
      "Iteration 345, loss = 0.42448244\n",
      "Iteration 346, loss = 0.42426777\n",
      "Iteration 347, loss = 0.42433955\n",
      "Iteration 348, loss = 0.42441601\n",
      "Iteration 349, loss = 0.42425921\n",
      "Iteration 350, loss = 0.42417289\n",
      "Iteration 351, loss = 0.42403355\n",
      "Iteration 352, loss = 0.42443139\n",
      "Iteration 353, loss = 0.42406610\n",
      "Iteration 354, loss = 0.42425344\n",
      "Iteration 355, loss = 0.42376367\n",
      "Iteration 356, loss = 0.42401711\n",
      "Iteration 357, loss = 0.42372688\n",
      "Iteration 358, loss = 0.42359899\n",
      "Iteration 359, loss = 0.42382505\n",
      "Iteration 360, loss = 0.42353615\n",
      "Iteration 361, loss = 0.42357958\n",
      "Iteration 362, loss = 0.42347546\n",
      "Iteration 363, loss = 0.42358241\n",
      "Iteration 364, loss = 0.42335076\n",
      "Iteration 365, loss = 0.42346540\n",
      "Iteration 366, loss = 0.42342662\n",
      "Iteration 367, loss = 0.42337942\n",
      "Iteration 368, loss = 0.42317040\n",
      "Iteration 369, loss = 0.42323837\n",
      "Iteration 370, loss = 0.42329147\n",
      "Iteration 371, loss = 0.42310063\n",
      "Iteration 372, loss = 0.42303127\n",
      "Iteration 373, loss = 0.42286161\n",
      "Iteration 374, loss = 0.42267449\n",
      "Iteration 375, loss = 0.42287120\n",
      "Iteration 376, loss = 0.42266463\n",
      "Iteration 377, loss = 0.42290553\n",
      "Iteration 378, loss = 0.42315935\n",
      "Iteration 379, loss = 0.42282999\n",
      "Iteration 380, loss = 0.42252695\n",
      "Iteration 381, loss = 0.42265758\n",
      "Iteration 382, loss = 0.42222741\n",
      "Iteration 383, loss = 0.42240812\n",
      "Iteration 384, loss = 0.42224637\n",
      "Iteration 385, loss = 0.42253511\n",
      "Iteration 386, loss = 0.42266952\n",
      "Iteration 387, loss = 0.42216909\n",
      "Iteration 388, loss = 0.42225971\n",
      "Iteration 389, loss = 0.42206512\n",
      "Iteration 390, loss = 0.42200870\n",
      "Iteration 391, loss = 0.42202693\n",
      "Iteration 392, loss = 0.42197346\n",
      "Iteration 393, loss = 0.42220297\n",
      "Iteration 394, loss = 0.42205932\n",
      "Iteration 395, loss = 0.42224886\n",
      "Iteration 396, loss = 0.42214959\n",
      "Iteration 397, loss = 0.42175022\n",
      "Iteration 398, loss = 0.42173145\n",
      "Iteration 399, loss = 0.42174188\n",
      "Iteration 400, loss = 0.42173119\n",
      "Iteration 401, loss = 0.42178580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 402, loss = 0.42169883\n",
      "Iteration 403, loss = 0.42152234\n",
      "Iteration 404, loss = 0.42191587\n",
      "Iteration 405, loss = 0.42167205\n",
      "Iteration 406, loss = 0.42156635\n",
      "Iteration 407, loss = 0.42136995\n",
      "Iteration 408, loss = 0.42154232\n",
      "Iteration 409, loss = 0.42140303\n",
      "Iteration 410, loss = 0.42157283\n",
      "Iteration 411, loss = 0.42132640\n",
      "Iteration 412, loss = 0.42114980\n",
      "Iteration 413, loss = 0.42140204\n",
      "Iteration 414, loss = 0.42097968\n",
      "Iteration 415, loss = 0.42100170\n",
      "Iteration 416, loss = 0.42108183\n",
      "Iteration 417, loss = 0.42123480\n",
      "Iteration 418, loss = 0.42110358\n",
      "Iteration 419, loss = 0.42107757\n",
      "Iteration 420, loss = 0.42070632\n",
      "Iteration 421, loss = 0.42106621\n",
      "Iteration 422, loss = 0.42077783\n",
      "Iteration 423, loss = 0.42086969\n",
      "Iteration 424, loss = 0.42101794\n",
      "Iteration 425, loss = 0.42091847\n",
      "Iteration 426, loss = 0.42067968\n",
      "Iteration 427, loss = 0.42067715\n",
      "Iteration 428, loss = 0.42053244\n",
      "Iteration 429, loss = 0.42058638\n",
      "Iteration 430, loss = 0.42039518\n",
      "Iteration 431, loss = 0.42048171\n",
      "Iteration 432, loss = 0.42047592\n",
      "Iteration 433, loss = 0.42059089\n",
      "Iteration 434, loss = 0.42033162\n",
      "Iteration 435, loss = 0.42039375\n",
      "Iteration 436, loss = 0.42017071\n",
      "Iteration 437, loss = 0.42022741\n",
      "Iteration 438, loss = 0.42031763\n",
      "Iteration 439, loss = 0.41999575\n",
      "Iteration 440, loss = 0.42002596\n",
      "Iteration 441, loss = 0.42006708\n",
      "Iteration 442, loss = 0.42038175\n",
      "Iteration 443, loss = 0.42020731\n",
      "Iteration 444, loss = 0.41987359\n",
      "Iteration 445, loss = 0.41970701\n",
      "Iteration 446, loss = 0.42008048\n",
      "Iteration 447, loss = 0.41985109\n",
      "Iteration 448, loss = 0.41967170\n",
      "Iteration 449, loss = 0.41972951\n",
      "Iteration 450, loss = 0.41981503\n",
      "Iteration 451, loss = 0.41996910\n",
      "Iteration 452, loss = 0.41961143\n",
      "Iteration 453, loss = 0.41982305\n",
      "Iteration 454, loss = 0.41973694\n",
      "Iteration 455, loss = 0.41942236\n",
      "Iteration 456, loss = 0.41959416\n",
      "Iteration 457, loss = 0.41964987\n",
      "Iteration 458, loss = 0.41938221\n",
      "Iteration 459, loss = 0.41953586\n",
      "Iteration 460, loss = 0.41980608\n",
      "Iteration 461, loss = 0.41917605\n",
      "Iteration 462, loss = 0.41920910\n",
      "Iteration 463, loss = 0.41943300\n",
      "Iteration 464, loss = 0.41943573\n",
      "Iteration 465, loss = 0.41933266\n",
      "Iteration 466, loss = 0.41926292\n",
      "Iteration 467, loss = 0.41876792\n",
      "Iteration 468, loss = 0.41939810\n",
      "Iteration 469, loss = 0.41944689\n",
      "Iteration 470, loss = 0.41888230\n",
      "Iteration 471, loss = 0.41870401\n",
      "Iteration 472, loss = 0.41916193\n",
      "Iteration 473, loss = 0.41899484\n",
      "Iteration 474, loss = 0.41899396\n",
      "Iteration 475, loss = 0.41892499\n",
      "Iteration 476, loss = 0.41850809\n",
      "Iteration 477, loss = 0.41888668\n",
      "Iteration 478, loss = 0.41892730\n",
      "Iteration 479, loss = 0.41876689\n",
      "Iteration 480, loss = 0.41905913\n",
      "Iteration 481, loss = 0.41879220\n",
      "Iteration 482, loss = 0.41885807\n",
      "Iteration 483, loss = 0.41866885\n",
      "Iteration 484, loss = 0.41841013\n",
      "Iteration 485, loss = 0.41845780\n",
      "Iteration 486, loss = 0.41858713\n",
      "Iteration 487, loss = 0.41860962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.96564211\n",
      "Iteration 2, loss = 1.77859627\n",
      "Iteration 3, loss = 1.60735533\n",
      "Iteration 4, loss = 1.45758271\n",
      "Iteration 5, loss = 1.32966651\n",
      "Iteration 6, loss = 1.22274490\n",
      "Iteration 7, loss = 1.13590223\n",
      "Iteration 8, loss = 1.06400323\n",
      "Iteration 9, loss = 1.00283713\n",
      "Iteration 10, loss = 0.95175095\n",
      "Iteration 11, loss = 0.90799865\n",
      "Iteration 12, loss = 0.87026674\n",
      "Iteration 13, loss = 0.83683790\n",
      "Iteration 14, loss = 0.80712331\n",
      "Iteration 15, loss = 0.78044284\n",
      "Iteration 16, loss = 0.75641716\n",
      "Iteration 17, loss = 0.73459413\n",
      "Iteration 18, loss = 0.71476910\n",
      "Iteration 19, loss = 0.69684079\n",
      "Iteration 20, loss = 0.68061590\n",
      "Iteration 21, loss = 0.66612793\n",
      "Iteration 22, loss = 0.65296835\n",
      "Iteration 23, loss = 0.64114806\n",
      "Iteration 24, loss = 0.63023549\n",
      "Iteration 25, loss = 0.62027061\n",
      "Iteration 26, loss = 0.61157230\n",
      "Iteration 27, loss = 0.60363512\n",
      "Iteration 28, loss = 0.59637027\n",
      "Iteration 29, loss = 0.58978973\n",
      "Iteration 30, loss = 0.58384546\n",
      "Iteration 31, loss = 0.57823321\n",
      "Iteration 32, loss = 0.57320244\n",
      "Iteration 33, loss = 0.56861682\n",
      "Iteration 34, loss = 0.56411497\n",
      "Iteration 35, loss = 0.56015479\n",
      "Iteration 36, loss = 0.55639387\n",
      "Iteration 37, loss = 0.55279374\n",
      "Iteration 38, loss = 0.54951615\n",
      "Iteration 39, loss = 0.54644481\n",
      "Iteration 40, loss = 0.54352683\n",
      "Iteration 41, loss = 0.54082346\n",
      "Iteration 42, loss = 0.53829513\n",
      "Iteration 43, loss = 0.53566550\n",
      "Iteration 44, loss = 0.53338661\n",
      "Iteration 45, loss = 0.53119334\n",
      "Iteration 46, loss = 0.52899097\n",
      "Iteration 47, loss = 0.52706887\n",
      "Iteration 48, loss = 0.52503842\n",
      "Iteration 49, loss = 0.52333050\n",
      "Iteration 50, loss = 0.52137485\n",
      "Iteration 51, loss = 0.51977374\n",
      "Iteration 52, loss = 0.51832503\n",
      "Iteration 53, loss = 0.51660308\n",
      "Iteration 54, loss = 0.51510719\n",
      "Iteration 55, loss = 0.51384031\n",
      "Iteration 56, loss = 0.51212115\n",
      "Iteration 57, loss = 0.51090200\n",
      "Iteration 58, loss = 0.50968226\n",
      "Iteration 59, loss = 0.50837855\n",
      "Iteration 60, loss = 0.50716938\n",
      "Iteration 61, loss = 0.50572944\n",
      "Iteration 62, loss = 0.50460298\n",
      "Iteration 63, loss = 0.50348361\n",
      "Iteration 64, loss = 0.50248315\n",
      "Iteration 65, loss = 0.50133677\n",
      "Iteration 66, loss = 0.50027632\n",
      "Iteration 67, loss = 0.49909034\n",
      "Iteration 68, loss = 0.49823613\n",
      "Iteration 69, loss = 0.49720826\n",
      "Iteration 70, loss = 0.49631483\n",
      "Iteration 71, loss = 0.49524810\n",
      "Iteration 72, loss = 0.49460224\n",
      "Iteration 73, loss = 0.49347098\n",
      "Iteration 74, loss = 0.49281062\n",
      "Iteration 75, loss = 0.49193969\n",
      "Iteration 76, loss = 0.49094103\n",
      "Iteration 77, loss = 0.49009086\n",
      "Iteration 78, loss = 0.48946757\n",
      "Iteration 79, loss = 0.48877658\n",
      "Iteration 80, loss = 0.48782259\n",
      "Iteration 81, loss = 0.48699137\n",
      "Iteration 82, loss = 0.48602877\n",
      "Iteration 83, loss = 0.48537430\n",
      "Iteration 84, loss = 0.48453879\n",
      "Iteration 85, loss = 0.48401772\n",
      "Iteration 86, loss = 0.48318555\n",
      "Iteration 87, loss = 0.48261780\n",
      "Iteration 88, loss = 0.48183718\n",
      "Iteration 89, loss = 0.48158864\n",
      "Iteration 90, loss = 0.48056162\n",
      "Iteration 91, loss = 0.47991739\n",
      "Iteration 92, loss = 0.47915235\n",
      "Iteration 93, loss = 0.47859871\n",
      "Iteration 94, loss = 0.47803112\n",
      "Iteration 95, loss = 0.47740480\n",
      "Iteration 96, loss = 0.47680342\n",
      "Iteration 97, loss = 0.47633435\n",
      "Iteration 98, loss = 0.47551163\n",
      "Iteration 99, loss = 0.47489464\n",
      "Iteration 100, loss = 0.47443283\n",
      "Iteration 101, loss = 0.47352428\n",
      "Iteration 102, loss = 0.47321353\n",
      "Iteration 103, loss = 0.47252479\n",
      "Iteration 104, loss = 0.47195928\n",
      "Iteration 105, loss = 0.47134359\n",
      "Iteration 106, loss = 0.47098024\n",
      "Iteration 107, loss = 0.47038348\n",
      "Iteration 108, loss = 0.46967289\n",
      "Iteration 109, loss = 0.46940637\n",
      "Iteration 110, loss = 0.46883234\n",
      "Iteration 111, loss = 0.46810916\n",
      "Iteration 112, loss = 0.46765929\n",
      "Iteration 113, loss = 0.46708887\n",
      "Iteration 114, loss = 0.46675552\n",
      "Iteration 115, loss = 0.46620308\n",
      "Iteration 116, loss = 0.46560166\n",
      "Iteration 117, loss = 0.46552295\n",
      "Iteration 118, loss = 0.46462005\n",
      "Iteration 119, loss = 0.46406856\n",
      "Iteration 120, loss = 0.46359875\n",
      "Iteration 121, loss = 0.46332534\n",
      "Iteration 122, loss = 0.46266094\n",
      "Iteration 123, loss = 0.46235421\n",
      "Iteration 124, loss = 0.46170275\n",
      "Iteration 125, loss = 0.46152283\n",
      "Iteration 126, loss = 0.46073090\n",
      "Iteration 127, loss = 0.46051389\n",
      "Iteration 128, loss = 0.45984629\n",
      "Iteration 129, loss = 0.45950946\n",
      "Iteration 130, loss = 0.45927851\n",
      "Iteration 131, loss = 0.45850856\n",
      "Iteration 132, loss = 0.45814865\n",
      "Iteration 133, loss = 0.45760872\n",
      "Iteration 134, loss = 0.45733974\n",
      "Iteration 135, loss = 0.45699172\n",
      "Iteration 136, loss = 0.45657117\n",
      "Iteration 137, loss = 0.45616603\n",
      "Iteration 138, loss = 0.45575721\n",
      "Iteration 139, loss = 0.45530284\n",
      "Iteration 140, loss = 0.45515898\n",
      "Iteration 141, loss = 0.45461627\n",
      "Iteration 142, loss = 0.45408948\n",
      "Iteration 143, loss = 0.45359492\n",
      "Iteration 144, loss = 0.45325869\n",
      "Iteration 145, loss = 0.45294725\n",
      "Iteration 146, loss = 0.45274315\n",
      "Iteration 147, loss = 0.45226828\n",
      "Iteration 148, loss = 0.45201853\n",
      "Iteration 149, loss = 0.45167152\n",
      "Iteration 150, loss = 0.45091263\n",
      "Iteration 151, loss = 0.45056492\n",
      "Iteration 152, loss = 0.45038544\n",
      "Iteration 153, loss = 0.45003599\n",
      "Iteration 154, loss = 0.44974387\n",
      "Iteration 155, loss = 0.44935528\n",
      "Iteration 156, loss = 0.44884474\n",
      "Iteration 157, loss = 0.44865000\n",
      "Iteration 158, loss = 0.44841358\n",
      "Iteration 159, loss = 0.44792538\n",
      "Iteration 160, loss = 0.44739138\n",
      "Iteration 161, loss = 0.44742991\n",
      "Iteration 162, loss = 0.44687397\n",
      "Iteration 163, loss = 0.44647864\n",
      "Iteration 164, loss = 0.44622700\n",
      "Iteration 165, loss = 0.44610129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 166, loss = 0.44583945\n",
      "Iteration 167, loss = 0.44570817\n",
      "Iteration 168, loss = 0.44521080\n",
      "Iteration 169, loss = 0.44483677\n",
      "Iteration 170, loss = 0.44458134\n",
      "Iteration 171, loss = 0.44439883\n",
      "Iteration 172, loss = 0.44415813\n",
      "Iteration 173, loss = 0.44361422\n",
      "Iteration 174, loss = 0.44340228\n",
      "Iteration 175, loss = 0.44316283\n",
      "Iteration 176, loss = 0.44282563\n",
      "Iteration 177, loss = 0.44264534\n",
      "Iteration 178, loss = 0.44224447\n",
      "Iteration 179, loss = 0.44195955\n",
      "Iteration 180, loss = 0.44201275\n",
      "Iteration 181, loss = 0.44148243\n",
      "Iteration 182, loss = 0.44136888\n",
      "Iteration 183, loss = 0.44120562\n",
      "Iteration 184, loss = 0.44080838\n",
      "Iteration 185, loss = 0.44057833\n",
      "Iteration 186, loss = 0.44036197\n",
      "Iteration 187, loss = 0.44016699\n",
      "Iteration 188, loss = 0.44000674\n",
      "Iteration 189, loss = 0.43966023\n",
      "Iteration 190, loss = 0.43948158\n",
      "Iteration 191, loss = 0.43936722\n",
      "Iteration 192, loss = 0.43890311\n",
      "Iteration 193, loss = 0.43891162\n",
      "Iteration 194, loss = 0.43849807\n",
      "Iteration 195, loss = 0.43836368\n",
      "Iteration 196, loss = 0.43803695\n",
      "Iteration 197, loss = 0.43790329\n",
      "Iteration 198, loss = 0.43777928\n",
      "Iteration 199, loss = 0.43738243\n",
      "Iteration 200, loss = 0.43713701\n",
      "Iteration 201, loss = 0.43700512\n",
      "Iteration 202, loss = 0.43680512\n",
      "Iteration 203, loss = 0.43661785\n",
      "Iteration 204, loss = 0.43645729\n",
      "Iteration 205, loss = 0.43619272\n",
      "Iteration 206, loss = 0.43595609\n",
      "Iteration 207, loss = 0.43583167\n",
      "Iteration 208, loss = 0.43546095\n",
      "Iteration 209, loss = 0.43535222\n",
      "Iteration 210, loss = 0.43528005\n",
      "Iteration 211, loss = 0.43507384\n",
      "Iteration 212, loss = 0.43477000\n",
      "Iteration 213, loss = 0.43475609\n",
      "Iteration 214, loss = 0.43434922\n",
      "Iteration 215, loss = 0.43429085\n",
      "Iteration 216, loss = 0.43434442\n",
      "Iteration 217, loss = 0.43402173\n",
      "Iteration 218, loss = 0.43390265\n",
      "Iteration 219, loss = 0.43363275\n",
      "Iteration 220, loss = 0.43353574\n",
      "Iteration 221, loss = 0.43350066\n",
      "Iteration 222, loss = 0.43311398\n",
      "Iteration 223, loss = 0.43307122\n",
      "Iteration 224, loss = 0.43269866\n",
      "Iteration 225, loss = 0.43268529\n",
      "Iteration 226, loss = 0.43254290\n",
      "Iteration 227, loss = 0.43216549\n",
      "Iteration 228, loss = 0.43211379\n",
      "Iteration 229, loss = 0.43192670\n",
      "Iteration 230, loss = 0.43181548\n",
      "Iteration 231, loss = 0.43181990\n",
      "Iteration 232, loss = 0.43150133\n",
      "Iteration 233, loss = 0.43128757\n",
      "Iteration 234, loss = 0.43138515\n",
      "Iteration 235, loss = 0.43122168\n",
      "Iteration 236, loss = 0.43096279\n",
      "Iteration 237, loss = 0.43089084\n",
      "Iteration 238, loss = 0.43062322\n",
      "Iteration 239, loss = 0.43045202\n",
      "Iteration 240, loss = 0.43020414\n",
      "Iteration 241, loss = 0.43020858\n",
      "Iteration 242, loss = 0.43013517\n",
      "Iteration 243, loss = 0.43001200\n",
      "Iteration 244, loss = 0.42971378\n",
      "Iteration 245, loss = 0.42992535\n",
      "Iteration 246, loss = 0.42979920\n",
      "Iteration 247, loss = 0.42961830\n",
      "Iteration 248, loss = 0.42951566\n",
      "Iteration 249, loss = 0.42937564\n",
      "Iteration 250, loss = 0.42915581\n",
      "Iteration 251, loss = 0.42889298\n",
      "Iteration 252, loss = 0.42885436\n",
      "Iteration 253, loss = 0.42862113\n",
      "Iteration 254, loss = 0.42872658\n",
      "Iteration 255, loss = 0.42841124\n",
      "Iteration 256, loss = 0.42844249\n",
      "Iteration 257, loss = 0.42834768\n",
      "Iteration 258, loss = 0.42806830\n",
      "Iteration 259, loss = 0.42788104\n",
      "Iteration 260, loss = 0.42781298\n",
      "Iteration 261, loss = 0.42777226\n",
      "Iteration 262, loss = 0.42793503\n",
      "Iteration 263, loss = 0.42741580\n",
      "Iteration 264, loss = 0.42779108\n",
      "Iteration 265, loss = 0.42734348\n",
      "Iteration 266, loss = 0.42728461\n",
      "Iteration 267, loss = 0.42696145\n",
      "Iteration 268, loss = 0.42748730\n",
      "Iteration 269, loss = 0.42709639\n",
      "Iteration 270, loss = 0.42698343\n",
      "Iteration 271, loss = 0.42687223\n",
      "Iteration 272, loss = 0.42651465\n",
      "Iteration 273, loss = 0.42645107\n",
      "Iteration 274, loss = 0.42654080\n",
      "Iteration 275, loss = 0.42641117\n",
      "Iteration 276, loss = 0.42629328\n",
      "Iteration 277, loss = 0.42622961\n",
      "Iteration 278, loss = 0.42601375\n",
      "Iteration 279, loss = 0.42617006\n",
      "Iteration 280, loss = 0.42600943\n",
      "Iteration 281, loss = 0.42582451\n",
      "Iteration 282, loss = 0.42580757\n",
      "Iteration 283, loss = 0.42549041\n",
      "Iteration 284, loss = 0.42568526\n",
      "Iteration 285, loss = 0.42559126\n",
      "Iteration 286, loss = 0.42530200\n",
      "Iteration 287, loss = 0.42526349\n",
      "Iteration 288, loss = 0.42536639\n",
      "Iteration 289, loss = 0.42507030\n",
      "Iteration 290, loss = 0.42503884\n",
      "Iteration 291, loss = 0.42503091\n",
      "Iteration 292, loss = 0.42494149\n",
      "Iteration 293, loss = 0.42486665\n",
      "Iteration 294, loss = 0.42485979\n",
      "Iteration 295, loss = 0.42471003\n",
      "Iteration 296, loss = 0.42445626\n",
      "Iteration 297, loss = 0.42442964\n",
      "Iteration 298, loss = 0.42454180\n",
      "Iteration 299, loss = 0.42429739\n",
      "Iteration 300, loss = 0.42416466\n",
      "Iteration 301, loss = 0.42423573\n",
      "Iteration 302, loss = 0.42402401\n",
      "Iteration 303, loss = 0.42394200\n",
      "Iteration 304, loss = 0.42402278\n",
      "Iteration 305, loss = 0.42420073\n",
      "Iteration 306, loss = 0.42343488\n",
      "Iteration 307, loss = 0.42353908\n",
      "Iteration 308, loss = 0.42381943\n",
      "Iteration 309, loss = 0.42361050\n",
      "Iteration 310, loss = 0.42345674\n",
      "Iteration 311, loss = 0.42340676\n",
      "Iteration 312, loss = 0.42332600\n",
      "Iteration 313, loss = 0.42335276\n",
      "Iteration 314, loss = 0.42329135\n",
      "Iteration 315, loss = 0.42323092\n",
      "Iteration 316, loss = 0.42299199\n",
      "Iteration 317, loss = 0.42300584\n",
      "Iteration 318, loss = 0.42328311\n",
      "Iteration 319, loss = 0.42291425\n",
      "Iteration 320, loss = 0.42315618\n",
      "Iteration 321, loss = 0.42262617\n",
      "Iteration 322, loss = 0.42255990\n",
      "Iteration 323, loss = 0.42269056\n",
      "Iteration 324, loss = 0.42268732\n",
      "Iteration 325, loss = 0.42234693\n",
      "Iteration 326, loss = 0.42254488\n",
      "Iteration 327, loss = 0.42260470\n",
      "Iteration 328, loss = 0.42222795\n",
      "Iteration 329, loss = 0.42221500\n",
      "Iteration 330, loss = 0.42237916\n",
      "Iteration 331, loss = 0.42212684\n",
      "Iteration 332, loss = 0.42204537\n",
      "Iteration 333, loss = 0.42184168\n",
      "Iteration 334, loss = 0.42183457\n",
      "Iteration 335, loss = 0.42211484\n",
      "Iteration 336, loss = 0.42190231\n",
      "Iteration 337, loss = 0.42164564\n",
      "Iteration 338, loss = 0.42161434\n",
      "Iteration 339, loss = 0.42184774\n",
      "Iteration 340, loss = 0.42146171\n",
      "Iteration 341, loss = 0.42143371\n",
      "Iteration 342, loss = 0.42141736\n",
      "Iteration 343, loss = 0.42128303\n",
      "Iteration 344, loss = 0.42134518\n",
      "Iteration 345, loss = 0.42116149\n",
      "Iteration 346, loss = 0.42101725\n",
      "Iteration 347, loss = 0.42121586\n",
      "Iteration 348, loss = 0.42080586\n",
      "Iteration 349, loss = 0.42107697\n",
      "Iteration 350, loss = 0.42086027\n",
      "Iteration 351, loss = 0.42108421\n",
      "Iteration 352, loss = 0.42060319\n",
      "Iteration 353, loss = 0.42088920\n",
      "Iteration 354, loss = 0.42070718\n",
      "Iteration 355, loss = 0.42086207\n",
      "Iteration 356, loss = 0.42054814\n",
      "Iteration 357, loss = 0.42058691\n",
      "Iteration 358, loss = 0.42020166\n",
      "Iteration 359, loss = 0.42072772\n",
      "Iteration 360, loss = 0.42032895\n",
      "Iteration 361, loss = 0.42063634\n",
      "Iteration 362, loss = 0.42023700\n",
      "Iteration 363, loss = 0.42004068\n",
      "Iteration 364, loss = 0.42024284\n",
      "Iteration 365, loss = 0.41985336\n",
      "Iteration 366, loss = 0.41995438\n",
      "Iteration 367, loss = 0.42000098\n",
      "Iteration 368, loss = 0.42020151\n",
      "Iteration 369, loss = 0.41996911\n",
      "Iteration 370, loss = 0.41988846\n",
      "Iteration 371, loss = 0.41976351\n",
      "Iteration 372, loss = 0.42001225\n",
      "Iteration 373, loss = 0.41987446\n",
      "Iteration 374, loss = 0.41967857\n",
      "Iteration 375, loss = 0.41967042\n",
      "Iteration 376, loss = 0.41974004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.99918409\n",
      "Iteration 2, loss = 1.86348633\n",
      "Iteration 3, loss = 1.74930567\n",
      "Iteration 4, loss = 1.65187364\n",
      "Iteration 5, loss = 1.56781497\n",
      "Iteration 6, loss = 1.49476970\n",
      "Iteration 7, loss = 1.43026988\n",
      "Iteration 8, loss = 1.37265822\n",
      "Iteration 9, loss = 1.32074238\n",
      "Iteration 10, loss = 1.27363808\n",
      "Iteration 11, loss = 1.23071398\n",
      "Iteration 12, loss = 1.19158556\n",
      "Iteration 13, loss = 1.15584335\n",
      "Iteration 14, loss = 1.12333350\n",
      "Iteration 15, loss = 1.09359509\n",
      "Iteration 16, loss = 1.06635536\n",
      "Iteration 17, loss = 1.04138603\n",
      "Iteration 18, loss = 1.01845877\n",
      "Iteration 19, loss = 0.99744187\n",
      "Iteration 20, loss = 0.97818473\n",
      "Iteration 21, loss = 0.96043909\n",
      "Iteration 22, loss = 0.94413075\n",
      "Iteration 23, loss = 0.92913494\n",
      "Iteration 24, loss = 0.91526678\n",
      "Iteration 25, loss = 0.90237891\n",
      "Iteration 26, loss = 0.89043669\n",
      "Iteration 27, loss = 0.87924125\n",
      "Iteration 28, loss = 0.86878201\n",
      "Iteration 29, loss = 0.85895573\n",
      "Iteration 30, loss = 0.84975945\n",
      "Iteration 31, loss = 0.84118600\n",
      "Iteration 32, loss = 0.83312544\n",
      "Iteration 33, loss = 0.82560377\n",
      "Iteration 34, loss = 0.81852055\n",
      "Iteration 35, loss = 0.81185983\n",
      "Iteration 36, loss = 0.80552044\n",
      "Iteration 37, loss = 0.79956304\n",
      "Iteration 38, loss = 0.79388042\n",
      "Iteration 39, loss = 0.78848736\n",
      "Iteration 40, loss = 0.78336332\n",
      "Iteration 41, loss = 0.77843796\n",
      "Iteration 42, loss = 0.77374580\n",
      "Iteration 43, loss = 0.76920931\n",
      "Iteration 44, loss = 0.76488823\n",
      "Iteration 45, loss = 0.76069661\n",
      "Iteration 46, loss = 0.75667156\n",
      "Iteration 47, loss = 0.75276908\n",
      "Iteration 48, loss = 0.74900686\n",
      "Iteration 49, loss = 0.74534494\n",
      "Iteration 50, loss = 0.74173036\n",
      "Iteration 51, loss = 0.73820366\n",
      "Iteration 52, loss = 0.73476414\n",
      "Iteration 53, loss = 0.73145362\n",
      "Iteration 54, loss = 0.72822856\n",
      "Iteration 55, loss = 0.72511590\n",
      "Iteration 56, loss = 0.72212470\n",
      "Iteration 57, loss = 0.71922866\n",
      "Iteration 58, loss = 0.71646705\n",
      "Iteration 59, loss = 0.71380107\n",
      "Iteration 60, loss = 0.71116916\n",
      "Iteration 61, loss = 0.70868026\n",
      "Iteration 62, loss = 0.70619796\n",
      "Iteration 63, loss = 0.70382672\n",
      "Iteration 64, loss = 0.70146992\n",
      "Iteration 65, loss = 0.69917757\n",
      "Iteration 66, loss = 0.69694737\n",
      "Iteration 67, loss = 0.69476260\n",
      "Iteration 68, loss = 0.69261114\n",
      "Iteration 69, loss = 0.69050854\n",
      "Iteration 70, loss = 0.68844483\n",
      "Iteration 71, loss = 0.68640599\n",
      "Iteration 72, loss = 0.68443364\n",
      "Iteration 73, loss = 0.68250186\n",
      "Iteration 74, loss = 0.68060906\n",
      "Iteration 75, loss = 0.67871723\n",
      "Iteration 76, loss = 0.67688135\n",
      "Iteration 77, loss = 0.67503644\n",
      "Iteration 78, loss = 0.67325828\n",
      "Iteration 79, loss = 0.67152063\n",
      "Iteration 80, loss = 0.66982196\n",
      "Iteration 81, loss = 0.66811007\n",
      "Iteration 82, loss = 0.66641622\n",
      "Iteration 83, loss = 0.66480240\n",
      "Iteration 84, loss = 0.66317855\n",
      "Iteration 85, loss = 0.66155762\n",
      "Iteration 86, loss = 0.66002649\n",
      "Iteration 87, loss = 0.65840190\n",
      "Iteration 88, loss = 0.65687748\n",
      "Iteration 89, loss = 0.65535161\n",
      "Iteration 90, loss = 0.65385430\n",
      "Iteration 91, loss = 0.65239287\n",
      "Iteration 92, loss = 0.65092042\n",
      "Iteration 93, loss = 0.64945980\n",
      "Iteration 94, loss = 0.64800936\n",
      "Iteration 95, loss = 0.64657871\n",
      "Iteration 96, loss = 0.64517798\n",
      "Iteration 97, loss = 0.64377449\n",
      "Iteration 98, loss = 0.64239046\n",
      "Iteration 99, loss = 0.64103782\n",
      "Iteration 100, loss = 0.63967808\n",
      "Iteration 101, loss = 0.63831815\n",
      "Iteration 102, loss = 0.63697399\n",
      "Iteration 103, loss = 0.63562609\n",
      "Iteration 104, loss = 0.63429539\n",
      "Iteration 105, loss = 0.63294950\n",
      "Iteration 106, loss = 0.63161983\n",
      "Iteration 107, loss = 0.63036303\n",
      "Iteration 108, loss = 0.62904595\n",
      "Iteration 109, loss = 0.62773744\n",
      "Iteration 110, loss = 0.62646224\n",
      "Iteration 111, loss = 0.62518533\n",
      "Iteration 112, loss = 0.62388676\n",
      "Iteration 113, loss = 0.62268600\n",
      "Iteration 114, loss = 0.62135323\n",
      "Iteration 115, loss = 0.62012643\n",
      "Iteration 116, loss = 0.61892230\n",
      "Iteration 117, loss = 0.61769286\n",
      "Iteration 118, loss = 0.61640959\n",
      "Iteration 119, loss = 0.61518340\n",
      "Iteration 120, loss = 0.61394242\n",
      "Iteration 121, loss = 0.61278719\n",
      "Iteration 122, loss = 0.61153948\n",
      "Iteration 123, loss = 0.61033093\n",
      "Iteration 124, loss = 0.60912352\n",
      "Iteration 125, loss = 0.60789190\n",
      "Iteration 126, loss = 0.60672049\n",
      "Iteration 127, loss = 0.60552724\n",
      "Iteration 128, loss = 0.60433460\n",
      "Iteration 129, loss = 0.60310692\n",
      "Iteration 130, loss = 0.60192414\n",
      "Iteration 131, loss = 0.60076538\n",
      "Iteration 132, loss = 0.59957612\n",
      "Iteration 133, loss = 0.59842507\n",
      "Iteration 134, loss = 0.59721107\n",
      "Iteration 135, loss = 0.59606196\n",
      "Iteration 136, loss = 0.59488908\n",
      "Iteration 137, loss = 0.59374803\n",
      "Iteration 138, loss = 0.59257775\n",
      "Iteration 139, loss = 0.59141925\n",
      "Iteration 140, loss = 0.59025220\n",
      "Iteration 141, loss = 0.58917391\n",
      "Iteration 142, loss = 0.58803356\n",
      "Iteration 143, loss = 0.58693274\n",
      "Iteration 144, loss = 0.58584188\n",
      "Iteration 145, loss = 0.58478122\n",
      "Iteration 146, loss = 0.58368010\n",
      "Iteration 147, loss = 0.58262604\n",
      "Iteration 148, loss = 0.58155134\n",
      "Iteration 149, loss = 0.58050810\n",
      "Iteration 150, loss = 0.57949263\n",
      "Iteration 151, loss = 0.57848135\n",
      "Iteration 152, loss = 0.57747887\n",
      "Iteration 153, loss = 0.57644740\n",
      "Iteration 154, loss = 0.57553334\n",
      "Iteration 155, loss = 0.57450025\n",
      "Iteration 156, loss = 0.57357873\n",
      "Iteration 157, loss = 0.57255380\n",
      "Iteration 158, loss = 0.57161468\n",
      "Iteration 159, loss = 0.57067396\n",
      "Iteration 160, loss = 0.56971369\n",
      "Iteration 161, loss = 0.56874686\n",
      "Iteration 162, loss = 0.56779866\n",
      "Iteration 163, loss = 0.56689414\n",
      "Iteration 164, loss = 0.56598528\n",
      "Iteration 165, loss = 0.56511299\n",
      "Iteration 166, loss = 0.56420839\n",
      "Iteration 167, loss = 0.56334515\n",
      "Iteration 168, loss = 0.56250798\n",
      "Iteration 169, loss = 0.56166586\n",
      "Iteration 170, loss = 0.56084107\n",
      "Iteration 171, loss = 0.56004005\n",
      "Iteration 172, loss = 0.55920909\n",
      "Iteration 173, loss = 0.55841025\n",
      "Iteration 174, loss = 0.55764255\n",
      "Iteration 175, loss = 0.55687307\n",
      "Iteration 176, loss = 0.55615170\n",
      "Iteration 177, loss = 0.55536671\n",
      "Iteration 178, loss = 0.55462939\n",
      "Iteration 179, loss = 0.55390913\n",
      "Iteration 180, loss = 0.55315473\n",
      "Iteration 181, loss = 0.55245577\n",
      "Iteration 182, loss = 0.55178830\n",
      "Iteration 183, loss = 0.55109085\n",
      "Iteration 184, loss = 0.55043260\n",
      "Iteration 185, loss = 0.54971236\n",
      "Iteration 186, loss = 0.54904061\n",
      "Iteration 187, loss = 0.54842085\n",
      "Iteration 188, loss = 0.54779184\n",
      "Iteration 189, loss = 0.54713917\n",
      "Iteration 190, loss = 0.54651443\n",
      "Iteration 191, loss = 0.54594559\n",
      "Iteration 192, loss = 0.54531697\n",
      "Iteration 193, loss = 0.54467688\n",
      "Iteration 194, loss = 0.54405586\n",
      "Iteration 195, loss = 0.54351527\n",
      "Iteration 196, loss = 0.54289830\n",
      "Iteration 197, loss = 0.54234800\n",
      "Iteration 198, loss = 0.54178596\n",
      "Iteration 199, loss = 0.54125607\n",
      "Iteration 200, loss = 0.54070575\n",
      "Iteration 201, loss = 0.54009072\n",
      "Iteration 202, loss = 0.53958583\n",
      "Iteration 203, loss = 0.53903630\n",
      "Iteration 204, loss = 0.53856119\n",
      "Iteration 205, loss = 0.53807764\n",
      "Iteration 206, loss = 0.53754619\n",
      "Iteration 207, loss = 0.53698141\n",
      "Iteration 208, loss = 0.53649665\n",
      "Iteration 209, loss = 0.53601212\n",
      "Iteration 210, loss = 0.53551248\n",
      "Iteration 211, loss = 0.53502605\n",
      "Iteration 212, loss = 0.53456902\n",
      "Iteration 213, loss = 0.53409274\n",
      "Iteration 214, loss = 0.53367982\n",
      "Iteration 215, loss = 0.53318958\n",
      "Iteration 216, loss = 0.53274292\n",
      "Iteration 217, loss = 0.53228287\n",
      "Iteration 218, loss = 0.53186242\n",
      "Iteration 219, loss = 0.53146507\n",
      "Iteration 220, loss = 0.53098962\n",
      "Iteration 221, loss = 0.53056390\n",
      "Iteration 222, loss = 0.53013794\n",
      "Iteration 223, loss = 0.52979291\n",
      "Iteration 224, loss = 0.52932652\n",
      "Iteration 225, loss = 0.52892037\n",
      "Iteration 226, loss = 0.52848444\n",
      "Iteration 227, loss = 0.52809904\n",
      "Iteration 228, loss = 0.52772741\n",
      "Iteration 229, loss = 0.52733672\n",
      "Iteration 230, loss = 0.52697531\n",
      "Iteration 231, loss = 0.52659596\n",
      "Iteration 232, loss = 0.52622580\n",
      "Iteration 233, loss = 0.52582412\n",
      "Iteration 234, loss = 0.52546193\n",
      "Iteration 235, loss = 0.52512045\n",
      "Iteration 236, loss = 0.52475112\n",
      "Iteration 237, loss = 0.52440775\n",
      "Iteration 238, loss = 0.52415148\n",
      "Iteration 239, loss = 0.52373322\n",
      "Iteration 240, loss = 0.52335901\n",
      "Iteration 241, loss = 0.52302843\n",
      "Iteration 242, loss = 0.52270254\n",
      "Iteration 243, loss = 0.52238692\n",
      "Iteration 244, loss = 0.52204799\n",
      "Iteration 245, loss = 0.52165451\n",
      "Iteration 246, loss = 0.52141832\n",
      "Iteration 247, loss = 0.52105709\n",
      "Iteration 248, loss = 0.52069385\n",
      "Iteration 249, loss = 0.52042348\n",
      "Iteration 250, loss = 0.52009915\n",
      "Iteration 251, loss = 0.51985806\n",
      "Iteration 252, loss = 0.51956199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.51923383\n",
      "Iteration 254, loss = 0.51893351\n",
      "Iteration 255, loss = 0.51861123\n",
      "Iteration 256, loss = 0.51837157\n",
      "Iteration 257, loss = 0.51802417\n",
      "Iteration 258, loss = 0.51775081\n",
      "Iteration 259, loss = 0.51749862\n",
      "Iteration 260, loss = 0.51718968\n",
      "Iteration 261, loss = 0.51690664\n",
      "Iteration 262, loss = 0.51662935\n",
      "Iteration 263, loss = 0.51637042\n",
      "Iteration 264, loss = 0.51609016\n",
      "Iteration 265, loss = 0.51585567\n",
      "Iteration 266, loss = 0.51565482\n",
      "Iteration 267, loss = 0.51535967\n",
      "Iteration 268, loss = 0.51507917\n",
      "Iteration 269, loss = 0.51478685\n",
      "Iteration 270, loss = 0.51452485\n",
      "Iteration 271, loss = 0.51430922\n",
      "Iteration 272, loss = 0.51407163\n",
      "Iteration 273, loss = 0.51377051\n",
      "Iteration 274, loss = 0.51351654\n",
      "Iteration 275, loss = 0.51335376\n",
      "Iteration 276, loss = 0.51312883\n",
      "Iteration 277, loss = 0.51282416\n",
      "Iteration 278, loss = 0.51263626\n",
      "Iteration 279, loss = 0.51239139\n",
      "Iteration 280, loss = 0.51216344\n",
      "Iteration 281, loss = 0.51194899\n",
      "Iteration 282, loss = 0.51168710\n",
      "Iteration 283, loss = 0.51143605\n",
      "Iteration 284, loss = 0.51126499\n",
      "Iteration 285, loss = 0.51096756\n",
      "Iteration 286, loss = 0.51075902\n",
      "Iteration 287, loss = 0.51053870\n",
      "Iteration 288, loss = 0.51029940\n",
      "Iteration 289, loss = 0.51008611\n",
      "Iteration 290, loss = 0.50983832\n",
      "Iteration 291, loss = 0.50965468\n",
      "Iteration 292, loss = 0.50949584\n",
      "Iteration 293, loss = 0.50927500\n",
      "Iteration 294, loss = 0.50908501\n",
      "Iteration 295, loss = 0.50884457\n",
      "Iteration 296, loss = 0.50859278\n",
      "Iteration 297, loss = 0.50837296\n",
      "Iteration 298, loss = 0.50818791\n",
      "Iteration 299, loss = 0.50803923\n",
      "Iteration 300, loss = 0.50778493\n",
      "Iteration 301, loss = 0.50763290\n",
      "Iteration 302, loss = 0.50740855\n",
      "Iteration 303, loss = 0.50721891\n",
      "Iteration 304, loss = 0.50703696\n",
      "Iteration 305, loss = 0.50679874\n",
      "Iteration 306, loss = 0.50660184\n",
      "Iteration 307, loss = 0.50641566\n",
      "Iteration 308, loss = 0.50620309\n",
      "Iteration 309, loss = 0.50601721\n",
      "Iteration 310, loss = 0.50584845\n",
      "Iteration 311, loss = 0.50564443\n",
      "Iteration 312, loss = 0.50547210\n",
      "Iteration 313, loss = 0.50529553\n",
      "Iteration 314, loss = 0.50514896\n",
      "Iteration 315, loss = 0.50494101\n",
      "Iteration 316, loss = 0.50475982\n",
      "Iteration 317, loss = 0.50462612\n",
      "Iteration 318, loss = 0.50440256\n",
      "Iteration 319, loss = 0.50421310\n",
      "Iteration 320, loss = 0.50404785\n",
      "Iteration 321, loss = 0.50389596\n",
      "Iteration 322, loss = 0.50374950\n",
      "Iteration 323, loss = 0.50355014\n",
      "Iteration 324, loss = 0.50329496\n",
      "Iteration 325, loss = 0.50313806\n",
      "Iteration 326, loss = 0.50298973\n",
      "Iteration 327, loss = 0.50279588\n",
      "Iteration 328, loss = 0.50271389\n",
      "Iteration 329, loss = 0.50253151\n",
      "Iteration 330, loss = 0.50240362\n",
      "Iteration 331, loss = 0.50218168\n",
      "Iteration 332, loss = 0.50201810\n",
      "Iteration 333, loss = 0.50181518\n",
      "Iteration 334, loss = 0.50168102\n",
      "Iteration 335, loss = 0.50146622\n",
      "Iteration 336, loss = 0.50133209\n",
      "Iteration 337, loss = 0.50122376\n",
      "Iteration 338, loss = 0.50102464\n",
      "Iteration 339, loss = 0.50090474\n",
      "Iteration 340, loss = 0.50068637\n",
      "Iteration 341, loss = 0.50053032\n",
      "Iteration 342, loss = 0.50042467\n",
      "Iteration 343, loss = 0.50025667\n",
      "Iteration 344, loss = 0.50011989\n",
      "Iteration 345, loss = 0.49985493\n",
      "Iteration 346, loss = 0.49976782\n",
      "Iteration 347, loss = 0.49960138\n",
      "Iteration 348, loss = 0.49943219\n",
      "Iteration 349, loss = 0.49932071\n",
      "Iteration 350, loss = 0.49914056\n",
      "Iteration 351, loss = 0.49898795\n",
      "Iteration 352, loss = 0.49900127\n",
      "Iteration 353, loss = 0.49876816\n",
      "Iteration 354, loss = 0.49856246\n",
      "Iteration 355, loss = 0.49851705\n",
      "Iteration 356, loss = 0.49827902\n",
      "Iteration 357, loss = 0.49810660\n",
      "Iteration 358, loss = 0.49801904\n",
      "Iteration 359, loss = 0.49792925\n",
      "Iteration 360, loss = 0.49770677\n",
      "Iteration 361, loss = 0.49758011\n",
      "Iteration 362, loss = 0.49746223\n",
      "Iteration 363, loss = 0.49729690\n",
      "Iteration 364, loss = 0.49718988\n",
      "Iteration 365, loss = 0.49704582\n",
      "Iteration 366, loss = 0.49685488\n",
      "Iteration 367, loss = 0.49677274\n",
      "Iteration 368, loss = 0.49663127\n",
      "Iteration 369, loss = 0.49645660\n",
      "Iteration 370, loss = 0.49639035\n",
      "Iteration 371, loss = 0.49622513\n",
      "Iteration 372, loss = 0.49606221\n",
      "Iteration 373, loss = 0.49598296\n",
      "Iteration 374, loss = 0.49583373\n",
      "Iteration 375, loss = 0.49568357\n",
      "Iteration 376, loss = 0.49558855\n",
      "Iteration 377, loss = 0.49541977\n",
      "Iteration 378, loss = 0.49528711\n",
      "Iteration 379, loss = 0.49510983\n",
      "Iteration 380, loss = 0.49500006\n",
      "Iteration 381, loss = 0.49486112\n",
      "Iteration 382, loss = 0.49473076\n",
      "Iteration 383, loss = 0.49468023\n",
      "Iteration 384, loss = 0.49453543\n",
      "Iteration 385, loss = 0.49439052\n",
      "Iteration 386, loss = 0.49425017\n",
      "Iteration 387, loss = 0.49411698\n",
      "Iteration 388, loss = 0.49397133\n",
      "Iteration 389, loss = 0.49391855\n",
      "Iteration 390, loss = 0.49375433\n",
      "Iteration 391, loss = 0.49365707\n",
      "Iteration 392, loss = 0.49352549\n",
      "Iteration 393, loss = 0.49331586\n",
      "Iteration 394, loss = 0.49327901\n",
      "Iteration 395, loss = 0.49317196\n",
      "Iteration 396, loss = 0.49297024\n",
      "Iteration 397, loss = 0.49287113\n",
      "Iteration 398, loss = 0.49275988\n",
      "Iteration 399, loss = 0.49254392\n",
      "Iteration 400, loss = 0.49251724\n",
      "Iteration 401, loss = 0.49232724\n",
      "Iteration 402, loss = 0.49220634\n",
      "Iteration 403, loss = 0.49213869\n",
      "Iteration 404, loss = 0.49205941\n",
      "Iteration 405, loss = 0.49193130\n",
      "Iteration 406, loss = 0.49181313\n",
      "Iteration 407, loss = 0.49175173\n",
      "Iteration 408, loss = 0.49154538\n",
      "Iteration 409, loss = 0.49146638\n",
      "Iteration 410, loss = 0.49130772\n",
      "Iteration 411, loss = 0.49125654\n",
      "Iteration 412, loss = 0.49112912\n",
      "Iteration 413, loss = 0.49099065\n",
      "Iteration 414, loss = 0.49084652\n",
      "Iteration 415, loss = 0.49067576\n",
      "Iteration 416, loss = 0.49059210\n",
      "Iteration 417, loss = 0.49055978\n",
      "Iteration 418, loss = 0.49035138\n",
      "Iteration 419, loss = 0.49030991\n",
      "Iteration 420, loss = 0.49010923\n",
      "Iteration 421, loss = 0.49008609\n",
      "Iteration 422, loss = 0.48990597\n",
      "Iteration 423, loss = 0.48983878\n",
      "Iteration 424, loss = 0.48974603\n",
      "Iteration 425, loss = 0.48952596\n",
      "Iteration 426, loss = 0.48946387\n",
      "Iteration 427, loss = 0.48934637\n",
      "Iteration 428, loss = 0.48929321\n",
      "Iteration 429, loss = 0.48917878\n",
      "Iteration 430, loss = 0.48911398\n",
      "Iteration 431, loss = 0.48889736\n",
      "Iteration 432, loss = 0.48876472\n",
      "Iteration 433, loss = 0.48863891\n",
      "Iteration 434, loss = 0.48863753\n",
      "Iteration 435, loss = 0.48860037\n",
      "Iteration 436, loss = 0.48835251\n",
      "Iteration 437, loss = 0.48821605\n",
      "Iteration 438, loss = 0.48820062\n",
      "Iteration 439, loss = 0.48812801\n",
      "Iteration 440, loss = 0.48797286\n",
      "Iteration 441, loss = 0.48791626\n",
      "Iteration 442, loss = 0.48779293\n",
      "Iteration 443, loss = 0.48761198\n",
      "Iteration 444, loss = 0.48755778\n",
      "Iteration 445, loss = 0.48748665\n",
      "Iteration 446, loss = 0.48738430\n",
      "Iteration 447, loss = 0.48721929\n",
      "Iteration 448, loss = 0.48715290\n",
      "Iteration 449, loss = 0.48706405\n",
      "Iteration 450, loss = 0.48698394\n",
      "Iteration 451, loss = 0.48686811\n",
      "Iteration 452, loss = 0.48674100\n",
      "Iteration 453, loss = 0.48657489\n",
      "Iteration 454, loss = 0.48652668\n",
      "Iteration 455, loss = 0.48641847\n",
      "Iteration 456, loss = 0.48632619\n",
      "Iteration 457, loss = 0.48623036\n",
      "Iteration 458, loss = 0.48613738\n",
      "Iteration 459, loss = 0.48598321\n",
      "Iteration 460, loss = 0.48595490\n",
      "Iteration 461, loss = 0.48582101\n",
      "Iteration 462, loss = 0.48567007\n",
      "Iteration 463, loss = 0.48568186\n",
      "Iteration 464, loss = 0.48551861\n",
      "Iteration 465, loss = 0.48550132\n",
      "Iteration 466, loss = 0.48535053\n",
      "Iteration 467, loss = 0.48516930\n",
      "Iteration 468, loss = 0.48511350\n",
      "Iteration 469, loss = 0.48505074\n",
      "Iteration 470, loss = 0.48489094\n",
      "Iteration 471, loss = 0.48482127\n",
      "Iteration 472, loss = 0.48472524\n",
      "Iteration 473, loss = 0.48466628\n",
      "Iteration 474, loss = 0.48452232\n",
      "Iteration 475, loss = 0.48440664\n",
      "Iteration 476, loss = 0.48429344\n",
      "Iteration 477, loss = 0.48428898\n",
      "Iteration 478, loss = 0.48407519\n",
      "Iteration 479, loss = 0.48403477\n",
      "Iteration 480, loss = 0.48394500\n",
      "Iteration 481, loss = 0.48386876\n",
      "Iteration 482, loss = 0.48378487\n",
      "Iteration 483, loss = 0.48367092\n",
      "Iteration 484, loss = 0.48358024\n",
      "Iteration 485, loss = 0.48347657\n",
      "Iteration 486, loss = 0.48340992\n",
      "Iteration 487, loss = 0.48329807\n",
      "Iteration 488, loss = 0.48321263\n",
      "Iteration 489, loss = 0.48310688\n",
      "Iteration 490, loss = 0.48306676\n",
      "Iteration 491, loss = 0.48289306\n",
      "Iteration 492, loss = 0.48285046\n",
      "Iteration 493, loss = 0.48268209\n",
      "Iteration 494, loss = 0.48268703\n",
      "Iteration 495, loss = 0.48256154\n",
      "Iteration 496, loss = 0.48240690\n",
      "Iteration 497, loss = 0.48240137\n",
      "Iteration 498, loss = 0.48235611\n",
      "Iteration 499, loss = 0.48215605\n",
      "Iteration 500, loss = 0.48218714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.99974748\n",
      "Iteration 2, loss = 1.86438397\n",
      "Iteration 3, loss = 1.74991645\n",
      "Iteration 4, loss = 1.65225397\n",
      "Iteration 5, loss = 1.56797060\n",
      "Iteration 6, loss = 1.49456146\n",
      "Iteration 7, loss = 1.42983309\n",
      "Iteration 8, loss = 1.37205038\n",
      "Iteration 9, loss = 1.32003744\n",
      "Iteration 10, loss = 1.27280311\n",
      "Iteration 11, loss = 1.22980464\n",
      "Iteration 12, loss = 1.19057767\n",
      "Iteration 13, loss = 1.15480990\n",
      "Iteration 14, loss = 1.12234253\n",
      "Iteration 15, loss = 1.09266329\n",
      "Iteration 16, loss = 1.06548360\n",
      "Iteration 17, loss = 1.04062043\n",
      "Iteration 18, loss = 1.01782716\n",
      "Iteration 19, loss = 0.99688738\n",
      "Iteration 20, loss = 0.97765886\n",
      "Iteration 21, loss = 0.95999686\n",
      "Iteration 22, loss = 0.94372694\n",
      "Iteration 23, loss = 0.92872627\n",
      "Iteration 24, loss = 0.91484767\n",
      "Iteration 25, loss = 0.90199677\n",
      "Iteration 26, loss = 0.89005014\n",
      "Iteration 27, loss = 0.87888250\n",
      "Iteration 28, loss = 0.86845749\n",
      "Iteration 29, loss = 0.85867488\n",
      "Iteration 30, loss = 0.84952075\n",
      "Iteration 31, loss = 0.84095738\n",
      "Iteration 32, loss = 0.83293384\n",
      "Iteration 33, loss = 0.82540707\n",
      "Iteration 34, loss = 0.81831281\n",
      "Iteration 35, loss = 0.81164450\n",
      "Iteration 36, loss = 0.80535436\n",
      "Iteration 37, loss = 0.79937779\n",
      "Iteration 38, loss = 0.79369109\n",
      "Iteration 39, loss = 0.78830383\n",
      "Iteration 40, loss = 0.78313173\n",
      "Iteration 41, loss = 0.77816224\n",
      "Iteration 42, loss = 0.77337933\n",
      "Iteration 43, loss = 0.76878593\n",
      "Iteration 44, loss = 0.76434695\n",
      "Iteration 45, loss = 0.76009014\n",
      "Iteration 46, loss = 0.75597188\n",
      "Iteration 47, loss = 0.75196005\n",
      "Iteration 48, loss = 0.74805426\n",
      "Iteration 49, loss = 0.74427395\n",
      "Iteration 50, loss = 0.74061649\n",
      "Iteration 51, loss = 0.73715125\n",
      "Iteration 52, loss = 0.73378418\n",
      "Iteration 53, loss = 0.73057462\n",
      "Iteration 54, loss = 0.72750431\n",
      "Iteration 55, loss = 0.72452934\n",
      "Iteration 56, loss = 0.72165473\n",
      "Iteration 57, loss = 0.71889111\n",
      "Iteration 58, loss = 0.71616224\n",
      "Iteration 59, loss = 0.71353590\n",
      "Iteration 60, loss = 0.71096803\n",
      "Iteration 61, loss = 0.70845888\n",
      "Iteration 62, loss = 0.70603928\n",
      "Iteration 63, loss = 0.70361419\n",
      "Iteration 64, loss = 0.70131375\n",
      "Iteration 65, loss = 0.69900080\n",
      "Iteration 66, loss = 0.69677987\n",
      "Iteration 67, loss = 0.69460413\n",
      "Iteration 68, loss = 0.69244662\n",
      "Iteration 69, loss = 0.69035691\n",
      "Iteration 70, loss = 0.68830089\n",
      "Iteration 71, loss = 0.68630018\n",
      "Iteration 72, loss = 0.68429558\n",
      "Iteration 73, loss = 0.68235549\n",
      "Iteration 74, loss = 0.68044627\n",
      "Iteration 75, loss = 0.67855925\n",
      "Iteration 76, loss = 0.67671347\n",
      "Iteration 77, loss = 0.67489857\n",
      "Iteration 78, loss = 0.67310253\n",
      "Iteration 79, loss = 0.67136286\n",
      "Iteration 80, loss = 0.66960497\n",
      "Iteration 81, loss = 0.66788867\n",
      "Iteration 82, loss = 0.66622615\n",
      "Iteration 83, loss = 0.66455675\n",
      "Iteration 84, loss = 0.66291590\n",
      "Iteration 85, loss = 0.66132288\n",
      "Iteration 86, loss = 0.65971523\n",
      "Iteration 87, loss = 0.65813797\n",
      "Iteration 88, loss = 0.65656867\n",
      "Iteration 89, loss = 0.65507105\n",
      "Iteration 90, loss = 0.65351477\n",
      "Iteration 91, loss = 0.65200830\n",
      "Iteration 92, loss = 0.65054275\n",
      "Iteration 93, loss = 0.64903936\n",
      "Iteration 94, loss = 0.64758208\n",
      "Iteration 95, loss = 0.64613116\n",
      "Iteration 96, loss = 0.64470652\n",
      "Iteration 97, loss = 0.64330739\n",
      "Iteration 98, loss = 0.64189058\n",
      "Iteration 99, loss = 0.64051001\n",
      "Iteration 100, loss = 0.63910066\n",
      "Iteration 101, loss = 0.63771661\n",
      "Iteration 102, loss = 0.63636957\n",
      "Iteration 103, loss = 0.63500915\n",
      "Iteration 104, loss = 0.63365384\n",
      "Iteration 105, loss = 0.63230273\n",
      "Iteration 106, loss = 0.63099245\n",
      "Iteration 107, loss = 0.62964602\n",
      "Iteration 108, loss = 0.62834298\n",
      "Iteration 109, loss = 0.62705095\n",
      "Iteration 110, loss = 0.62573872\n",
      "Iteration 111, loss = 0.62446612\n",
      "Iteration 112, loss = 0.62317021\n",
      "Iteration 113, loss = 0.62190068\n",
      "Iteration 114, loss = 0.62059413\n",
      "Iteration 115, loss = 0.61929040\n",
      "Iteration 116, loss = 0.61804200\n",
      "Iteration 117, loss = 0.61676159\n",
      "Iteration 118, loss = 0.61547292\n",
      "Iteration 119, loss = 0.61423399\n",
      "Iteration 120, loss = 0.61298038\n",
      "Iteration 121, loss = 0.61176296\n",
      "Iteration 122, loss = 0.61049598\n",
      "Iteration 123, loss = 0.60931057\n",
      "Iteration 124, loss = 0.60811515\n",
      "Iteration 125, loss = 0.60687996\n",
      "Iteration 126, loss = 0.60569332\n",
      "Iteration 127, loss = 0.60445548\n",
      "Iteration 128, loss = 0.60328014\n",
      "Iteration 129, loss = 0.60209715\n",
      "Iteration 130, loss = 0.60090820\n",
      "Iteration 131, loss = 0.59974911\n",
      "Iteration 132, loss = 0.59857570\n",
      "Iteration 133, loss = 0.59743133\n",
      "Iteration 134, loss = 0.59625443\n",
      "Iteration 135, loss = 0.59511558\n",
      "Iteration 136, loss = 0.59392590\n",
      "Iteration 137, loss = 0.59277546\n",
      "Iteration 138, loss = 0.59161922\n",
      "Iteration 139, loss = 0.59044298\n",
      "Iteration 140, loss = 0.58932375\n",
      "Iteration 141, loss = 0.58819559\n",
      "Iteration 142, loss = 0.58704851\n",
      "Iteration 143, loss = 0.58590462\n",
      "Iteration 144, loss = 0.58478411\n",
      "Iteration 145, loss = 0.58372828\n",
      "Iteration 146, loss = 0.58261031\n",
      "Iteration 147, loss = 0.58154617\n",
      "Iteration 148, loss = 0.58047801\n",
      "Iteration 149, loss = 0.57945410\n",
      "Iteration 150, loss = 0.57843144\n",
      "Iteration 151, loss = 0.57733170\n",
      "Iteration 152, loss = 0.57634382\n",
      "Iteration 153, loss = 0.57534359\n",
      "Iteration 154, loss = 0.57435879\n",
      "Iteration 155, loss = 0.57337486\n",
      "Iteration 156, loss = 0.57242273\n",
      "Iteration 157, loss = 0.57146376\n",
      "Iteration 158, loss = 0.57052750\n",
      "Iteration 159, loss = 0.56960420\n",
      "Iteration 160, loss = 0.56870320\n",
      "Iteration 161, loss = 0.56779486\n",
      "Iteration 162, loss = 0.56690155\n",
      "Iteration 163, loss = 0.56596912\n",
      "Iteration 164, loss = 0.56508595\n",
      "Iteration 165, loss = 0.56421583\n",
      "Iteration 166, loss = 0.56335417\n",
      "Iteration 167, loss = 0.56251505\n",
      "Iteration 168, loss = 0.56164819\n",
      "Iteration 169, loss = 0.56079220\n",
      "Iteration 170, loss = 0.55995545\n",
      "Iteration 171, loss = 0.55916330\n",
      "Iteration 172, loss = 0.55835406\n",
      "Iteration 173, loss = 0.55747121\n",
      "Iteration 174, loss = 0.55666272\n",
      "Iteration 175, loss = 0.55585705\n",
      "Iteration 176, loss = 0.55507341\n",
      "Iteration 177, loss = 0.55435231\n",
      "Iteration 178, loss = 0.55361167\n",
      "Iteration 179, loss = 0.55293846\n",
      "Iteration 180, loss = 0.55218878\n",
      "Iteration 181, loss = 0.55150098\n",
      "Iteration 182, loss = 0.55081903\n",
      "Iteration 183, loss = 0.55016076\n",
      "Iteration 184, loss = 0.54944112\n",
      "Iteration 185, loss = 0.54881216\n",
      "Iteration 186, loss = 0.54816644\n",
      "Iteration 187, loss = 0.54750149\n",
      "Iteration 188, loss = 0.54684907\n",
      "Iteration 189, loss = 0.54623844\n",
      "Iteration 190, loss = 0.54561827\n",
      "Iteration 191, loss = 0.54503318\n",
      "Iteration 192, loss = 0.54441494\n",
      "Iteration 193, loss = 0.54380377\n",
      "Iteration 194, loss = 0.54324603\n",
      "Iteration 195, loss = 0.54265848\n",
      "Iteration 196, loss = 0.54213221\n",
      "Iteration 197, loss = 0.54149004\n",
      "Iteration 198, loss = 0.54096141\n",
      "Iteration 199, loss = 0.54041255\n",
      "Iteration 200, loss = 0.53984569\n",
      "Iteration 201, loss = 0.53929263\n",
      "Iteration 202, loss = 0.53882732\n",
      "Iteration 203, loss = 0.53826473\n",
      "Iteration 204, loss = 0.53779052\n",
      "Iteration 205, loss = 0.53720895\n",
      "Iteration 206, loss = 0.53673909\n",
      "Iteration 207, loss = 0.53626999\n",
      "Iteration 208, loss = 0.53579046\n",
      "Iteration 209, loss = 0.53527694\n",
      "Iteration 210, loss = 0.53482284\n",
      "Iteration 211, loss = 0.53433505\n",
      "Iteration 212, loss = 0.53386575\n",
      "Iteration 213, loss = 0.53341063\n",
      "Iteration 214, loss = 0.53291623\n",
      "Iteration 215, loss = 0.53247765\n",
      "Iteration 216, loss = 0.53206642\n",
      "Iteration 217, loss = 0.53164459\n",
      "Iteration 218, loss = 0.53117489\n",
      "Iteration 219, loss = 0.53077674\n",
      "Iteration 220, loss = 0.53032205\n",
      "Iteration 221, loss = 0.52989547\n",
      "Iteration 222, loss = 0.52950918\n",
      "Iteration 223, loss = 0.52906752\n",
      "Iteration 224, loss = 0.52867667\n",
      "Iteration 225, loss = 0.52828239\n",
      "Iteration 226, loss = 0.52791575\n",
      "Iteration 227, loss = 0.52749304\n",
      "Iteration 228, loss = 0.52714420\n",
      "Iteration 229, loss = 0.52672719\n",
      "Iteration 230, loss = 0.52633912\n",
      "Iteration 231, loss = 0.52594704\n",
      "Iteration 232, loss = 0.52562684\n",
      "Iteration 233, loss = 0.52524401\n",
      "Iteration 234, loss = 0.52491524\n",
      "Iteration 235, loss = 0.52450473\n",
      "Iteration 236, loss = 0.52417840\n",
      "Iteration 237, loss = 0.52378411\n",
      "Iteration 238, loss = 0.52347048\n",
      "Iteration 239, loss = 0.52312567\n",
      "Iteration 240, loss = 0.52274541\n",
      "Iteration 241, loss = 0.52247348\n",
      "Iteration 242, loss = 0.52206367\n",
      "Iteration 243, loss = 0.52179734\n",
      "Iteration 244, loss = 0.52145272\n",
      "Iteration 245, loss = 0.52115066\n",
      "Iteration 246, loss = 0.52084003\n",
      "Iteration 247, loss = 0.52044540\n",
      "Iteration 248, loss = 0.52027197\n",
      "Iteration 249, loss = 0.51985398\n",
      "Iteration 250, loss = 0.51956736\n",
      "Iteration 251, loss = 0.51921858\n",
      "Iteration 252, loss = 0.51910629\n",
      "Iteration 253, loss = 0.51870871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.51838863\n",
      "Iteration 255, loss = 0.51807745\n",
      "Iteration 256, loss = 0.51779205\n",
      "Iteration 257, loss = 0.51756817\n",
      "Iteration 258, loss = 0.51721701\n",
      "Iteration 259, loss = 0.51694155\n",
      "Iteration 260, loss = 0.51668391\n",
      "Iteration 261, loss = 0.51640805\n",
      "Iteration 262, loss = 0.51613520\n",
      "Iteration 263, loss = 0.51583881\n",
      "Iteration 264, loss = 0.51562726\n",
      "Iteration 265, loss = 0.51528458\n",
      "Iteration 266, loss = 0.51502547\n",
      "Iteration 267, loss = 0.51485918\n",
      "Iteration 268, loss = 0.51455487\n",
      "Iteration 269, loss = 0.51426597\n",
      "Iteration 270, loss = 0.51404929\n",
      "Iteration 271, loss = 0.51381781\n",
      "Iteration 272, loss = 0.51354613\n",
      "Iteration 273, loss = 0.51330298\n",
      "Iteration 274, loss = 0.51301079\n",
      "Iteration 275, loss = 0.51281541\n",
      "Iteration 276, loss = 0.51251574\n",
      "Iteration 277, loss = 0.51224304\n",
      "Iteration 278, loss = 0.51204122\n",
      "Iteration 279, loss = 0.51180223\n",
      "Iteration 280, loss = 0.51156410\n",
      "Iteration 281, loss = 0.51139367\n",
      "Iteration 282, loss = 0.51113216\n",
      "Iteration 283, loss = 0.51092164\n",
      "Iteration 284, loss = 0.51068987\n",
      "Iteration 285, loss = 0.51048629\n",
      "Iteration 286, loss = 0.51030982\n",
      "Iteration 287, loss = 0.50994710\n",
      "Iteration 288, loss = 0.50978611\n",
      "Iteration 289, loss = 0.50958420\n",
      "Iteration 290, loss = 0.50937050\n",
      "Iteration 291, loss = 0.50912052\n",
      "Iteration 292, loss = 0.50892527\n",
      "Iteration 293, loss = 0.50867959\n",
      "Iteration 294, loss = 0.50844086\n",
      "Iteration 295, loss = 0.50825945\n",
      "Iteration 296, loss = 0.50809696\n",
      "Iteration 297, loss = 0.50787917\n",
      "Iteration 298, loss = 0.50770325\n",
      "Iteration 299, loss = 0.50746034\n",
      "Iteration 300, loss = 0.50727120\n",
      "Iteration 301, loss = 0.50712073\n",
      "Iteration 302, loss = 0.50685716\n",
      "Iteration 303, loss = 0.50667172\n",
      "Iteration 304, loss = 0.50650749\n",
      "Iteration 305, loss = 0.50620623\n",
      "Iteration 306, loss = 0.50603488\n",
      "Iteration 307, loss = 0.50583428\n",
      "Iteration 308, loss = 0.50564063\n",
      "Iteration 309, loss = 0.50550265\n",
      "Iteration 310, loss = 0.50527900\n",
      "Iteration 311, loss = 0.50502037\n",
      "Iteration 312, loss = 0.50494353\n",
      "Iteration 313, loss = 0.50468276\n",
      "Iteration 314, loss = 0.50450254\n",
      "Iteration 315, loss = 0.50445853\n",
      "Iteration 316, loss = 0.50420584\n",
      "Iteration 317, loss = 0.50400625\n",
      "Iteration 318, loss = 0.50383476\n",
      "Iteration 319, loss = 0.50364802\n",
      "Iteration 320, loss = 0.50348386\n",
      "Iteration 321, loss = 0.50325814\n",
      "Iteration 322, loss = 0.50304416\n",
      "Iteration 323, loss = 0.50298530\n",
      "Iteration 324, loss = 0.50275352\n",
      "Iteration 325, loss = 0.50262984\n",
      "Iteration 326, loss = 0.50238638\n",
      "Iteration 327, loss = 0.50229402\n",
      "Iteration 328, loss = 0.50206731\n",
      "Iteration 329, loss = 0.50190072\n",
      "Iteration 330, loss = 0.50166308\n",
      "Iteration 331, loss = 0.50153353\n",
      "Iteration 332, loss = 0.50140393\n",
      "Iteration 333, loss = 0.50126983\n",
      "Iteration 334, loss = 0.50110854\n",
      "Iteration 335, loss = 0.50093737\n",
      "Iteration 336, loss = 0.50082472\n",
      "Iteration 337, loss = 0.50055321\n",
      "Iteration 338, loss = 0.50042547\n",
      "Iteration 339, loss = 0.50022084\n",
      "Iteration 340, loss = 0.50012915\n",
      "Iteration 341, loss = 0.49995671\n",
      "Iteration 342, loss = 0.49983098\n",
      "Iteration 343, loss = 0.49957182\n",
      "Iteration 344, loss = 0.49942218\n",
      "Iteration 345, loss = 0.49928535\n",
      "Iteration 346, loss = 0.49916495\n",
      "Iteration 347, loss = 0.49902449\n",
      "Iteration 348, loss = 0.49885943\n",
      "Iteration 349, loss = 0.49871785\n",
      "Iteration 350, loss = 0.49852775\n",
      "Iteration 351, loss = 0.49842519\n",
      "Iteration 352, loss = 0.49826528\n",
      "Iteration 353, loss = 0.49805976\n",
      "Iteration 354, loss = 0.49800730\n",
      "Iteration 355, loss = 0.49778348\n",
      "Iteration 356, loss = 0.49767107\n",
      "Iteration 357, loss = 0.49754899\n",
      "Iteration 358, loss = 0.49747210\n",
      "Iteration 359, loss = 0.49728852\n",
      "Iteration 360, loss = 0.49711106\n",
      "Iteration 361, loss = 0.49697515\n",
      "Iteration 362, loss = 0.49672748\n",
      "Iteration 363, loss = 0.49666754\n",
      "Iteration 364, loss = 0.49658139\n",
      "Iteration 365, loss = 0.49644562\n",
      "Iteration 366, loss = 0.49626794\n",
      "Iteration 367, loss = 0.49610372\n",
      "Iteration 368, loss = 0.49604016\n",
      "Iteration 369, loss = 0.49582849\n",
      "Iteration 370, loss = 0.49573075\n",
      "Iteration 371, loss = 0.49559588\n",
      "Iteration 372, loss = 0.49542822\n",
      "Iteration 373, loss = 0.49528563\n",
      "Iteration 374, loss = 0.49516290\n",
      "Iteration 375, loss = 0.49502276\n",
      "Iteration 376, loss = 0.49487357\n",
      "Iteration 377, loss = 0.49471223\n",
      "Iteration 378, loss = 0.49464282\n",
      "Iteration 379, loss = 0.49462410\n",
      "Iteration 380, loss = 0.49433196\n",
      "Iteration 381, loss = 0.49417987\n",
      "Iteration 382, loss = 0.49403267\n",
      "Iteration 383, loss = 0.49395446\n",
      "Iteration 384, loss = 0.49384730\n",
      "Iteration 385, loss = 0.49366328\n",
      "Iteration 386, loss = 0.49355700\n",
      "Iteration 387, loss = 0.49342292\n",
      "Iteration 388, loss = 0.49324185\n",
      "Iteration 389, loss = 0.49317406\n",
      "Iteration 390, loss = 0.49304368\n",
      "Iteration 391, loss = 0.49301227\n",
      "Iteration 392, loss = 0.49276184\n",
      "Iteration 393, loss = 0.49263839\n",
      "Iteration 394, loss = 0.49254975\n",
      "Iteration 395, loss = 0.49236429\n",
      "Iteration 396, loss = 0.49226817\n",
      "Iteration 397, loss = 0.49209955\n",
      "Iteration 398, loss = 0.49203468\n",
      "Iteration 399, loss = 0.49190394\n",
      "Iteration 400, loss = 0.49176492\n",
      "Iteration 401, loss = 0.49165585\n",
      "Iteration 402, loss = 0.49157014\n",
      "Iteration 403, loss = 0.49142405\n",
      "Iteration 404, loss = 0.49127660\n",
      "Iteration 405, loss = 0.49113141\n",
      "Iteration 406, loss = 0.49106621\n",
      "Iteration 407, loss = 0.49094202\n",
      "Iteration 408, loss = 0.49079165\n",
      "Iteration 409, loss = 0.49072184\n",
      "Iteration 410, loss = 0.49059508\n",
      "Iteration 411, loss = 0.49052460\n",
      "Iteration 412, loss = 0.49037869\n",
      "Iteration 413, loss = 0.49025475\n",
      "Iteration 414, loss = 0.49009721\n",
      "Iteration 415, loss = 0.49002190\n",
      "Iteration 416, loss = 0.48993288\n",
      "Iteration 417, loss = 0.48974986\n",
      "Iteration 418, loss = 0.48967895\n",
      "Iteration 419, loss = 0.48959735\n",
      "Iteration 420, loss = 0.48940860\n",
      "Iteration 421, loss = 0.48934247\n",
      "Iteration 422, loss = 0.48919096\n",
      "Iteration 423, loss = 0.48911907\n",
      "Iteration 424, loss = 0.48895017\n",
      "Iteration 425, loss = 0.48887790\n",
      "Iteration 426, loss = 0.48875836\n",
      "Iteration 427, loss = 0.48862843\n",
      "Iteration 428, loss = 0.48854340\n",
      "Iteration 429, loss = 0.48851300\n",
      "Iteration 430, loss = 0.48830073\n",
      "Iteration 431, loss = 0.48816662\n",
      "Iteration 432, loss = 0.48814092\n",
      "Iteration 433, loss = 0.48799672\n",
      "Iteration 434, loss = 0.48779453\n",
      "Iteration 435, loss = 0.48777344\n",
      "Iteration 436, loss = 0.48764980\n",
      "Iteration 437, loss = 0.48760970\n",
      "Iteration 438, loss = 0.48755908\n",
      "Iteration 439, loss = 0.48729334\n",
      "Iteration 440, loss = 0.48720466\n",
      "Iteration 441, loss = 0.48714722\n",
      "Iteration 442, loss = 0.48704579\n",
      "Iteration 443, loss = 0.48688066\n",
      "Iteration 444, loss = 0.48678948\n",
      "Iteration 445, loss = 0.48669368\n",
      "Iteration 446, loss = 0.48657680\n",
      "Iteration 447, loss = 0.48647270\n",
      "Iteration 448, loss = 0.48639974\n",
      "Iteration 449, loss = 0.48625776\n",
      "Iteration 450, loss = 0.48625546\n",
      "Iteration 451, loss = 0.48606410\n",
      "Iteration 452, loss = 0.48594848\n",
      "Iteration 453, loss = 0.48588648\n",
      "Iteration 454, loss = 0.48571158\n",
      "Iteration 455, loss = 0.48562137\n",
      "Iteration 456, loss = 0.48555805\n",
      "Iteration 457, loss = 0.48538475\n",
      "Iteration 458, loss = 0.48531732\n",
      "Iteration 459, loss = 0.48520459\n",
      "Iteration 460, loss = 0.48514842\n",
      "Iteration 461, loss = 0.48505600\n",
      "Iteration 462, loss = 0.48489307\n",
      "Iteration 463, loss = 0.48486801\n",
      "Iteration 464, loss = 0.48472763\n",
      "Iteration 465, loss = 0.48461125\n",
      "Iteration 466, loss = 0.48447907\n",
      "Iteration 467, loss = 0.48443793\n",
      "Iteration 468, loss = 0.48439863\n",
      "Iteration 469, loss = 0.48422608\n",
      "Iteration 470, loss = 0.48412787\n",
      "Iteration 471, loss = 0.48398049\n",
      "Iteration 472, loss = 0.48387016\n",
      "Iteration 473, loss = 0.48389935\n",
      "Iteration 474, loss = 0.48365295\n",
      "Iteration 475, loss = 0.48366288\n",
      "Iteration 476, loss = 0.48355996\n",
      "Iteration 477, loss = 0.48342138\n",
      "Iteration 478, loss = 0.48333443\n",
      "Iteration 479, loss = 0.48322291\n",
      "Iteration 480, loss = 0.48313767\n",
      "Iteration 481, loss = 0.48303232\n",
      "Iteration 482, loss = 0.48300269\n",
      "Iteration 483, loss = 0.48288094\n",
      "Iteration 484, loss = 0.48285579\n",
      "Iteration 485, loss = 0.48261892\n",
      "Iteration 486, loss = 0.48261875\n",
      "Iteration 487, loss = 0.48250063\n",
      "Iteration 488, loss = 0.48232087\n",
      "Iteration 489, loss = 0.48228508\n",
      "Iteration 490, loss = 0.48218758\n",
      "Iteration 491, loss = 0.48214769\n",
      "Iteration 492, loss = 0.48204688\n",
      "Iteration 493, loss = 0.48191280\n",
      "Iteration 494, loss = 0.48183112\n",
      "Iteration 495, loss = 0.48176865\n",
      "Iteration 496, loss = 0.48168207\n",
      "Iteration 497, loss = 0.48161383\n",
      "Iteration 498, loss = 0.48149391\n",
      "Iteration 499, loss = 0.48145130\n",
      "Iteration 500, loss = 0.48126766\n",
      "Iteration 1, loss = 1.99920097\n",
      "Iteration 2, loss = 1.86314173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 3, loss = 1.74858103\n",
      "Iteration 4, loss = 1.65087397\n",
      "Iteration 5, loss = 1.56672611\n",
      "Iteration 6, loss = 1.49341248\n",
      "Iteration 7, loss = 1.42874091\n",
      "Iteration 8, loss = 1.37107213\n",
      "Iteration 9, loss = 1.31908480\n",
      "Iteration 10, loss = 1.27200011\n",
      "Iteration 11, loss = 1.22919346\n",
      "Iteration 12, loss = 1.19018873\n",
      "Iteration 13, loss = 1.15454322\n",
      "Iteration 14, loss = 1.12209038\n",
      "Iteration 15, loss = 1.09246817\n",
      "Iteration 16, loss = 1.06536729\n",
      "Iteration 17, loss = 1.04051749\n",
      "Iteration 18, loss = 1.01772565\n",
      "Iteration 19, loss = 0.99683653\n",
      "Iteration 20, loss = 0.97762117\n",
      "Iteration 21, loss = 0.95996519\n",
      "Iteration 22, loss = 0.94371284\n",
      "Iteration 23, loss = 0.92872559\n",
      "Iteration 24, loss = 0.91483871\n",
      "Iteration 25, loss = 0.90195860\n",
      "Iteration 26, loss = 0.88997043\n",
      "Iteration 27, loss = 0.87879100\n",
      "Iteration 28, loss = 0.86836674\n",
      "Iteration 29, loss = 0.85861571\n",
      "Iteration 30, loss = 0.84951058\n",
      "Iteration 31, loss = 0.84098653\n",
      "Iteration 32, loss = 0.83305797\n",
      "Iteration 33, loss = 0.82554921\n",
      "Iteration 34, loss = 0.81852090\n",
      "Iteration 35, loss = 0.81187574\n",
      "Iteration 36, loss = 0.80560412\n",
      "Iteration 37, loss = 0.79963784\n",
      "Iteration 38, loss = 0.79399621\n",
      "Iteration 39, loss = 0.78860627\n",
      "Iteration 40, loss = 0.78345462\n",
      "Iteration 41, loss = 0.77852401\n",
      "Iteration 42, loss = 0.77379621\n",
      "Iteration 43, loss = 0.76925806\n",
      "Iteration 44, loss = 0.76488973\n",
      "Iteration 45, loss = 0.76061183\n",
      "Iteration 46, loss = 0.75647777\n",
      "Iteration 47, loss = 0.75250031\n",
      "Iteration 48, loss = 0.74861676\n",
      "Iteration 49, loss = 0.74490719\n",
      "Iteration 50, loss = 0.74124061\n",
      "Iteration 51, loss = 0.73765977\n",
      "Iteration 52, loss = 0.73421555\n",
      "Iteration 53, loss = 0.73087745\n",
      "Iteration 54, loss = 0.72769164\n",
      "Iteration 55, loss = 0.72460029\n",
      "Iteration 56, loss = 0.72162569\n",
      "Iteration 57, loss = 0.71878203\n",
      "Iteration 58, loss = 0.71604687\n",
      "Iteration 59, loss = 0.71337736\n",
      "Iteration 60, loss = 0.71074291\n",
      "Iteration 61, loss = 0.70823992\n",
      "Iteration 62, loss = 0.70573784\n",
      "Iteration 63, loss = 0.70333416\n",
      "Iteration 64, loss = 0.70095885\n",
      "Iteration 65, loss = 0.69864177\n",
      "Iteration 66, loss = 0.69637779\n",
      "Iteration 67, loss = 0.69414523\n",
      "Iteration 68, loss = 0.69200307\n",
      "Iteration 69, loss = 0.68984154\n",
      "Iteration 70, loss = 0.68776408\n",
      "Iteration 71, loss = 0.68571252\n",
      "Iteration 72, loss = 0.68369108\n",
      "Iteration 73, loss = 0.68173698\n",
      "Iteration 74, loss = 0.67976322\n",
      "Iteration 75, loss = 0.67785732\n",
      "Iteration 76, loss = 0.67597739\n",
      "Iteration 77, loss = 0.67413037\n",
      "Iteration 78, loss = 0.67228347\n",
      "Iteration 79, loss = 0.67048088\n",
      "Iteration 80, loss = 0.66869776\n",
      "Iteration 81, loss = 0.66694667\n",
      "Iteration 82, loss = 0.66525105\n",
      "Iteration 83, loss = 0.66353793\n",
      "Iteration 84, loss = 0.66185591\n",
      "Iteration 85, loss = 0.66019427\n",
      "Iteration 86, loss = 0.65857985\n",
      "Iteration 87, loss = 0.65693777\n",
      "Iteration 88, loss = 0.65533231\n",
      "Iteration 89, loss = 0.65375580\n",
      "Iteration 90, loss = 0.65215562\n",
      "Iteration 91, loss = 0.65062168\n",
      "Iteration 92, loss = 0.64907411\n",
      "Iteration 93, loss = 0.64757556\n",
      "Iteration 94, loss = 0.64604822\n",
      "Iteration 95, loss = 0.64456995\n",
      "Iteration 96, loss = 0.64306798\n",
      "Iteration 97, loss = 0.64160900\n",
      "Iteration 98, loss = 0.64012075\n",
      "Iteration 99, loss = 0.63865102\n",
      "Iteration 100, loss = 0.63721987\n",
      "Iteration 101, loss = 0.63583135\n",
      "Iteration 102, loss = 0.63437917\n",
      "Iteration 103, loss = 0.63298160\n",
      "Iteration 104, loss = 0.63155108\n",
      "Iteration 105, loss = 0.63015046\n",
      "Iteration 106, loss = 0.62876591\n",
      "Iteration 107, loss = 0.62738317\n",
      "Iteration 108, loss = 0.62601240\n",
      "Iteration 109, loss = 0.62467193\n",
      "Iteration 110, loss = 0.62331597\n",
      "Iteration 111, loss = 0.62194970\n",
      "Iteration 112, loss = 0.62061949\n",
      "Iteration 113, loss = 0.61929184\n",
      "Iteration 114, loss = 0.61798702\n",
      "Iteration 115, loss = 0.61666140\n",
      "Iteration 116, loss = 0.61534043\n",
      "Iteration 117, loss = 0.61402321\n",
      "Iteration 118, loss = 0.61273354\n",
      "Iteration 119, loss = 0.61142034\n",
      "Iteration 120, loss = 0.61013288\n",
      "Iteration 121, loss = 0.60887671\n",
      "Iteration 122, loss = 0.60758869\n",
      "Iteration 123, loss = 0.60636302\n",
      "Iteration 124, loss = 0.60507567\n",
      "Iteration 125, loss = 0.60382236\n",
      "Iteration 126, loss = 0.60256418\n",
      "Iteration 127, loss = 0.60130674\n",
      "Iteration 128, loss = 0.60007195\n",
      "Iteration 129, loss = 0.59881928\n",
      "Iteration 130, loss = 0.59756500\n",
      "Iteration 131, loss = 0.59633274\n",
      "Iteration 132, loss = 0.59513271\n",
      "Iteration 133, loss = 0.59389227\n",
      "Iteration 134, loss = 0.59265335\n",
      "Iteration 135, loss = 0.59142448\n",
      "Iteration 136, loss = 0.59023790\n",
      "Iteration 137, loss = 0.58901380\n",
      "Iteration 138, loss = 0.58779967\n",
      "Iteration 139, loss = 0.58663816\n",
      "Iteration 140, loss = 0.58545433\n",
      "Iteration 141, loss = 0.58427609\n",
      "Iteration 142, loss = 0.58315437\n",
      "Iteration 143, loss = 0.58205162\n",
      "Iteration 144, loss = 0.58091749\n",
      "Iteration 145, loss = 0.57981554\n",
      "Iteration 146, loss = 0.57873018\n",
      "Iteration 147, loss = 0.57764882\n",
      "Iteration 148, loss = 0.57658889\n",
      "Iteration 149, loss = 0.57552628\n",
      "Iteration 150, loss = 0.57449880\n",
      "Iteration 151, loss = 0.57346430\n",
      "Iteration 152, loss = 0.57250749\n",
      "Iteration 153, loss = 0.57148979\n",
      "Iteration 154, loss = 0.57046062\n",
      "Iteration 155, loss = 0.56947338\n",
      "Iteration 156, loss = 0.56850490\n",
      "Iteration 157, loss = 0.56754864\n",
      "Iteration 158, loss = 0.56657198\n",
      "Iteration 159, loss = 0.56566331\n",
      "Iteration 160, loss = 0.56475101\n",
      "Iteration 161, loss = 0.56388382\n",
      "Iteration 162, loss = 0.56300270\n",
      "Iteration 163, loss = 0.56213886\n",
      "Iteration 164, loss = 0.56133302\n",
      "Iteration 165, loss = 0.56047398\n",
      "Iteration 166, loss = 0.55963531\n",
      "Iteration 167, loss = 0.55885008\n",
      "Iteration 168, loss = 0.55800454\n",
      "Iteration 169, loss = 0.55724178\n",
      "Iteration 170, loss = 0.55645829\n",
      "Iteration 171, loss = 0.55558848\n",
      "Iteration 172, loss = 0.55484271\n",
      "Iteration 173, loss = 0.55414851\n",
      "Iteration 174, loss = 0.55337930\n",
      "Iteration 175, loss = 0.55264177\n",
      "Iteration 176, loss = 0.55197069\n",
      "Iteration 177, loss = 0.55121330\n",
      "Iteration 178, loss = 0.55055914\n",
      "Iteration 179, loss = 0.54981266\n",
      "Iteration 180, loss = 0.54917182\n",
      "Iteration 181, loss = 0.54849547\n",
      "Iteration 182, loss = 0.54783875\n",
      "Iteration 183, loss = 0.54720404\n",
      "Iteration 184, loss = 0.54650022\n",
      "Iteration 185, loss = 0.54586808\n",
      "Iteration 186, loss = 0.54528235\n",
      "Iteration 187, loss = 0.54464158\n",
      "Iteration 188, loss = 0.54408951\n",
      "Iteration 189, loss = 0.54348430\n",
      "Iteration 190, loss = 0.54292540\n",
      "Iteration 191, loss = 0.54228103\n",
      "Iteration 192, loss = 0.54169415\n",
      "Iteration 193, loss = 0.54115934\n",
      "Iteration 194, loss = 0.54058508\n",
      "Iteration 195, loss = 0.54002801\n",
      "Iteration 196, loss = 0.53947195\n",
      "Iteration 197, loss = 0.53894566\n",
      "Iteration 198, loss = 0.53839273\n",
      "Iteration 199, loss = 0.53783890\n",
      "Iteration 200, loss = 0.53734402\n",
      "Iteration 201, loss = 0.53682599\n",
      "Iteration 202, loss = 0.53634779\n",
      "Iteration 203, loss = 0.53575832\n",
      "Iteration 204, loss = 0.53532985\n",
      "Iteration 205, loss = 0.53482143\n",
      "Iteration 206, loss = 0.53436209\n",
      "Iteration 207, loss = 0.53388262\n",
      "Iteration 208, loss = 0.53344905\n",
      "Iteration 209, loss = 0.53292335\n",
      "Iteration 210, loss = 0.53249640\n",
      "Iteration 211, loss = 0.53202382\n",
      "Iteration 212, loss = 0.53162195\n",
      "Iteration 213, loss = 0.53115768\n",
      "Iteration 214, loss = 0.53073549\n",
      "Iteration 215, loss = 0.53030564\n",
      "Iteration 216, loss = 0.52990021\n",
      "Iteration 217, loss = 0.52942834\n",
      "Iteration 218, loss = 0.52899286\n",
      "Iteration 219, loss = 0.52860680\n",
      "Iteration 220, loss = 0.52819872\n",
      "Iteration 221, loss = 0.52776930\n",
      "Iteration 222, loss = 0.52741902\n",
      "Iteration 223, loss = 0.52698729\n",
      "Iteration 224, loss = 0.52663244\n",
      "Iteration 225, loss = 0.52621726\n",
      "Iteration 226, loss = 0.52582724\n",
      "Iteration 227, loss = 0.52548839\n",
      "Iteration 228, loss = 0.52512643\n",
      "Iteration 229, loss = 0.52471361\n",
      "Iteration 230, loss = 0.52438661\n",
      "Iteration 231, loss = 0.52408493\n",
      "Iteration 232, loss = 0.52363930\n",
      "Iteration 233, loss = 0.52339169\n",
      "Iteration 234, loss = 0.52294338\n",
      "Iteration 235, loss = 0.52264191\n",
      "Iteration 236, loss = 0.52224837\n",
      "Iteration 237, loss = 0.52188613\n",
      "Iteration 238, loss = 0.52160552\n",
      "Iteration 239, loss = 0.52129234\n",
      "Iteration 240, loss = 0.52091733\n",
      "Iteration 241, loss = 0.52069639\n",
      "Iteration 242, loss = 0.52029545\n",
      "Iteration 243, loss = 0.52001198\n",
      "Iteration 244, loss = 0.51966420\n",
      "Iteration 245, loss = 0.51940227\n",
      "Iteration 246, loss = 0.51906796\n",
      "Iteration 247, loss = 0.51878132\n",
      "Iteration 248, loss = 0.51853106\n",
      "Iteration 249, loss = 0.51820604\n",
      "Iteration 250, loss = 0.51790870\n",
      "Iteration 251, loss = 0.51754277\n",
      "Iteration 252, loss = 0.51736004\n",
      "Iteration 253, loss = 0.51701444\n",
      "Iteration 254, loss = 0.51676556\n",
      "Iteration 255, loss = 0.51644945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.51616542\n",
      "Iteration 257, loss = 0.51591967\n",
      "Iteration 258, loss = 0.51560480\n",
      "Iteration 259, loss = 0.51536391\n",
      "Iteration 260, loss = 0.51508828\n",
      "Iteration 261, loss = 0.51488011\n",
      "Iteration 262, loss = 0.51452283\n",
      "Iteration 263, loss = 0.51432795\n",
      "Iteration 264, loss = 0.51406285\n",
      "Iteration 265, loss = 0.51381707\n",
      "Iteration 266, loss = 0.51363219\n",
      "Iteration 267, loss = 0.51325130\n",
      "Iteration 268, loss = 0.51307348\n",
      "Iteration 269, loss = 0.51281317\n",
      "Iteration 270, loss = 0.51253901\n",
      "Iteration 271, loss = 0.51235427\n",
      "Iteration 272, loss = 0.51203298\n",
      "Iteration 273, loss = 0.51185183\n",
      "Iteration 274, loss = 0.51155496\n",
      "Iteration 275, loss = 0.51134339\n",
      "Iteration 276, loss = 0.51108930\n",
      "Iteration 277, loss = 0.51086112\n",
      "Iteration 278, loss = 0.51062539\n",
      "Iteration 279, loss = 0.51041908\n",
      "Iteration 280, loss = 0.51019806\n",
      "Iteration 281, loss = 0.50997531\n",
      "Iteration 282, loss = 0.50966767\n",
      "Iteration 283, loss = 0.50954358\n",
      "Iteration 284, loss = 0.50934512\n",
      "Iteration 285, loss = 0.50907883\n",
      "Iteration 286, loss = 0.50882681\n",
      "Iteration 287, loss = 0.50863534\n",
      "Iteration 288, loss = 0.50848486\n",
      "Iteration 289, loss = 0.50821543\n",
      "Iteration 290, loss = 0.50802047\n",
      "Iteration 291, loss = 0.50774205\n",
      "Iteration 292, loss = 0.50759641\n",
      "Iteration 293, loss = 0.50738460\n",
      "Iteration 294, loss = 0.50720136\n",
      "Iteration 295, loss = 0.50698010\n",
      "Iteration 296, loss = 0.50675413\n",
      "Iteration 297, loss = 0.50663536\n",
      "Iteration 298, loss = 0.50636033\n",
      "Iteration 299, loss = 0.50616129\n",
      "Iteration 300, loss = 0.50602676\n",
      "Iteration 301, loss = 0.50581814\n",
      "Iteration 302, loss = 0.50562786\n",
      "Iteration 303, loss = 0.50542450\n",
      "Iteration 304, loss = 0.50514070\n",
      "Iteration 305, loss = 0.50503904\n",
      "Iteration 306, loss = 0.50486298\n",
      "Iteration 307, loss = 0.50466827\n",
      "Iteration 308, loss = 0.50447041\n",
      "Iteration 309, loss = 0.50431043\n",
      "Iteration 310, loss = 0.50407618\n",
      "Iteration 311, loss = 0.50392509\n",
      "Iteration 312, loss = 0.50372604\n",
      "Iteration 313, loss = 0.50356195\n",
      "Iteration 314, loss = 0.50340987\n",
      "Iteration 315, loss = 0.50321477\n",
      "Iteration 316, loss = 0.50306079\n",
      "Iteration 317, loss = 0.50291954\n",
      "Iteration 318, loss = 0.50274884\n",
      "Iteration 319, loss = 0.50248074\n",
      "Iteration 320, loss = 0.50232236\n",
      "Iteration 321, loss = 0.50216205\n",
      "Iteration 322, loss = 0.50202837\n",
      "Iteration 323, loss = 0.50183075\n",
      "Iteration 324, loss = 0.50163079\n",
      "Iteration 325, loss = 0.50143924\n",
      "Iteration 326, loss = 0.50131500\n",
      "Iteration 327, loss = 0.50112559\n",
      "Iteration 328, loss = 0.50095918\n",
      "Iteration 329, loss = 0.50076165\n",
      "Iteration 330, loss = 0.50066683\n",
      "Iteration 331, loss = 0.50048062\n",
      "Iteration 332, loss = 0.50030671\n",
      "Iteration 333, loss = 0.50013795\n",
      "Iteration 334, loss = 0.49998161\n",
      "Iteration 335, loss = 0.49991086\n",
      "Iteration 336, loss = 0.49969822\n",
      "Iteration 337, loss = 0.49953964\n",
      "Iteration 338, loss = 0.49938392\n",
      "Iteration 339, loss = 0.49921718\n",
      "Iteration 340, loss = 0.49904180\n",
      "Iteration 341, loss = 0.49884748\n",
      "Iteration 342, loss = 0.49877577\n",
      "Iteration 343, loss = 0.49862816\n",
      "Iteration 344, loss = 0.49844278\n",
      "Iteration 345, loss = 0.49829695\n",
      "Iteration 346, loss = 0.49814610\n",
      "Iteration 347, loss = 0.49798149\n",
      "Iteration 348, loss = 0.49785401\n",
      "Iteration 349, loss = 0.49778575\n",
      "Iteration 350, loss = 0.49759178\n",
      "Iteration 351, loss = 0.49741113\n",
      "Iteration 352, loss = 0.49725256\n",
      "Iteration 353, loss = 0.49715732\n",
      "Iteration 354, loss = 0.49703195\n",
      "Iteration 355, loss = 0.49686674\n",
      "Iteration 356, loss = 0.49661844\n",
      "Iteration 357, loss = 0.49651956\n",
      "Iteration 358, loss = 0.49638519\n",
      "Iteration 359, loss = 0.49623623\n",
      "Iteration 360, loss = 0.49616696\n",
      "Iteration 361, loss = 0.49603921\n",
      "Iteration 362, loss = 0.49581952\n",
      "Iteration 363, loss = 0.49575400\n",
      "Iteration 364, loss = 0.49556702\n",
      "Iteration 365, loss = 0.49554527\n",
      "Iteration 366, loss = 0.49530466\n",
      "Iteration 367, loss = 0.49508993\n",
      "Iteration 368, loss = 0.49505333\n",
      "Iteration 369, loss = 0.49487920\n",
      "Iteration 370, loss = 0.49472328\n",
      "Iteration 371, loss = 0.49468863\n",
      "Iteration 372, loss = 0.49452547\n",
      "Iteration 373, loss = 0.49440427\n",
      "Iteration 374, loss = 0.49418795\n",
      "Iteration 375, loss = 0.49414482\n",
      "Iteration 376, loss = 0.49392618\n",
      "Iteration 377, loss = 0.49385198\n",
      "Iteration 378, loss = 0.49372280\n",
      "Iteration 379, loss = 0.49352928\n",
      "Iteration 380, loss = 0.49342318\n",
      "Iteration 381, loss = 0.49336763\n",
      "Iteration 382, loss = 0.49315902\n",
      "Iteration 383, loss = 0.49301417\n",
      "Iteration 384, loss = 0.49285522\n",
      "Iteration 385, loss = 0.49280803\n",
      "Iteration 386, loss = 0.49268355\n",
      "Iteration 387, loss = 0.49253239\n",
      "Iteration 388, loss = 0.49243154\n",
      "Iteration 389, loss = 0.49224939\n",
      "Iteration 390, loss = 0.49218942\n",
      "Iteration 391, loss = 0.49210330\n",
      "Iteration 392, loss = 0.49192996\n",
      "Iteration 393, loss = 0.49190574\n",
      "Iteration 394, loss = 0.49162980\n",
      "Iteration 395, loss = 0.49159705\n",
      "Iteration 396, loss = 0.49144260\n",
      "Iteration 397, loss = 0.49134120\n",
      "Iteration 398, loss = 0.49117690\n",
      "Iteration 399, loss = 0.49110638\n",
      "Iteration 400, loss = 0.49089705\n",
      "Iteration 401, loss = 0.49084619\n",
      "Iteration 402, loss = 0.49070501\n",
      "Iteration 403, loss = 0.49056833\n",
      "Iteration 404, loss = 0.49052580\n",
      "Iteration 405, loss = 0.49035871\n",
      "Iteration 406, loss = 0.49031018\n",
      "Iteration 407, loss = 0.49013868\n",
      "Iteration 408, loss = 0.49004035\n",
      "Iteration 409, loss = 0.48991777\n",
      "Iteration 410, loss = 0.48976329\n",
      "Iteration 411, loss = 0.48964806\n",
      "Iteration 412, loss = 0.48952236\n",
      "Iteration 413, loss = 0.48944380\n",
      "Iteration 414, loss = 0.48928794\n",
      "Iteration 415, loss = 0.48916570\n",
      "Iteration 416, loss = 0.48907830\n",
      "Iteration 417, loss = 0.48896901\n",
      "Iteration 418, loss = 0.48889787\n",
      "Iteration 419, loss = 0.48874893\n",
      "Iteration 420, loss = 0.48861870\n",
      "Iteration 421, loss = 0.48852415\n",
      "Iteration 422, loss = 0.48839063\n",
      "Iteration 423, loss = 0.48826798\n",
      "Iteration 424, loss = 0.48823093\n",
      "Iteration 425, loss = 0.48803409\n",
      "Iteration 426, loss = 0.48798389\n",
      "Iteration 427, loss = 0.48788321\n",
      "Iteration 428, loss = 0.48776693\n",
      "Iteration 429, loss = 0.48763605\n",
      "Iteration 430, loss = 0.48755008\n",
      "Iteration 431, loss = 0.48742890\n",
      "Iteration 432, loss = 0.48731599\n",
      "Iteration 433, loss = 0.48720830\n",
      "Iteration 434, loss = 0.48709912\n",
      "Iteration 435, loss = 0.48697537\n",
      "Iteration 436, loss = 0.48685099\n",
      "Iteration 437, loss = 0.48675878\n",
      "Iteration 438, loss = 0.48667598\n",
      "Iteration 439, loss = 0.48653616\n",
      "Iteration 440, loss = 0.48645882\n",
      "Iteration 441, loss = 0.48634458\n",
      "Iteration 442, loss = 0.48634111\n",
      "Iteration 443, loss = 0.48607533\n",
      "Iteration 444, loss = 0.48614707\n",
      "Iteration 445, loss = 0.48591610\n",
      "Iteration 446, loss = 0.48580667\n",
      "Iteration 447, loss = 0.48572744\n",
      "Iteration 448, loss = 0.48563728\n",
      "Iteration 449, loss = 0.48548579\n",
      "Iteration 450, loss = 0.48547090\n",
      "Iteration 451, loss = 0.48543576\n",
      "Iteration 452, loss = 0.48525059\n",
      "Iteration 453, loss = 0.48507479\n",
      "Iteration 454, loss = 0.48494567\n",
      "Iteration 455, loss = 0.48487847\n",
      "Iteration 456, loss = 0.48479897\n",
      "Iteration 457, loss = 0.48473258\n",
      "Iteration 458, loss = 0.48460156\n",
      "Iteration 459, loss = 0.48460300\n",
      "Iteration 460, loss = 0.48449601\n",
      "Iteration 461, loss = 0.48431453\n",
      "Iteration 462, loss = 0.48425106\n",
      "Iteration 463, loss = 0.48413172\n",
      "Iteration 464, loss = 0.48401030\n",
      "Iteration 465, loss = 0.48389265\n",
      "Iteration 466, loss = 0.48380876\n",
      "Iteration 467, loss = 0.48374135\n",
      "Iteration 468, loss = 0.48353983\n",
      "Iteration 469, loss = 0.48353940\n",
      "Iteration 470, loss = 0.48346674\n",
      "Iteration 471, loss = 0.48338586\n",
      "Iteration 472, loss = 0.48325066\n",
      "Iteration 473, loss = 0.48318413\n",
      "Iteration 474, loss = 0.48302677\n",
      "Iteration 475, loss = 0.48297496\n",
      "Iteration 476, loss = 0.48285434\n",
      "Iteration 477, loss = 0.48274955\n",
      "Iteration 478, loss = 0.48263486\n",
      "Iteration 479, loss = 0.48260109\n",
      "Iteration 480, loss = 0.48251373\n",
      "Iteration 481, loss = 0.48239365\n",
      "Iteration 482, loss = 0.48236330\n",
      "Iteration 483, loss = 0.48214591\n",
      "Iteration 484, loss = 0.48211543\n",
      "Iteration 485, loss = 0.48200654\n",
      "Iteration 486, loss = 0.48189812\n",
      "Iteration 487, loss = 0.48187300\n",
      "Iteration 488, loss = 0.48175325\n",
      "Iteration 489, loss = 0.48156908\n",
      "Iteration 490, loss = 0.48150230\n",
      "Iteration 491, loss = 0.48139391\n",
      "Iteration 492, loss = 0.48133421\n",
      "Iteration 493, loss = 0.48125332\n",
      "Iteration 494, loss = 0.48120471\n",
      "Iteration 495, loss = 0.48104876\n",
      "Iteration 496, loss = 0.48096333\n",
      "Iteration 497, loss = 0.48094231\n",
      "Iteration 498, loss = 0.48082905\n",
      "Iteration 499, loss = 0.48068125\n",
      "Iteration 500, loss = 0.48060244\n",
      "Iteration 1, loss = 2.00120479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.86491455\n",
      "Iteration 3, loss = 1.75007644\n",
      "Iteration 4, loss = 1.65211950\n",
      "Iteration 5, loss = 1.56764505\n",
      "Iteration 6, loss = 1.49409363\n",
      "Iteration 7, loss = 1.42933969\n",
      "Iteration 8, loss = 1.37160698\n",
      "Iteration 9, loss = 1.31946489\n",
      "Iteration 10, loss = 1.27210001\n",
      "Iteration 11, loss = 1.22899540\n",
      "Iteration 12, loss = 1.18969346\n",
      "Iteration 13, loss = 1.15393258\n",
      "Iteration 14, loss = 1.12142517\n",
      "Iteration 15, loss = 1.09178009\n",
      "Iteration 16, loss = 1.06461894\n",
      "Iteration 17, loss = 1.03975444\n",
      "Iteration 18, loss = 1.01696504\n",
      "Iteration 19, loss = 0.99605695\n",
      "Iteration 20, loss = 0.97689259\n",
      "Iteration 21, loss = 0.95920692\n",
      "Iteration 22, loss = 0.94297437\n",
      "Iteration 23, loss = 0.92800756\n",
      "Iteration 24, loss = 0.91417844\n",
      "Iteration 25, loss = 0.90131088\n",
      "Iteration 26, loss = 0.88938257\n",
      "Iteration 27, loss = 0.87821363\n",
      "Iteration 28, loss = 0.86780316\n",
      "Iteration 29, loss = 0.85808496\n",
      "Iteration 30, loss = 0.84899074\n",
      "Iteration 31, loss = 0.84046784\n",
      "Iteration 32, loss = 0.83252625\n",
      "Iteration 33, loss = 0.82501853\n",
      "Iteration 34, loss = 0.81797283\n",
      "Iteration 35, loss = 0.81128929\n",
      "Iteration 36, loss = 0.80500150\n",
      "Iteration 37, loss = 0.79903353\n",
      "Iteration 38, loss = 0.79338742\n",
      "Iteration 39, loss = 0.78800018\n",
      "Iteration 40, loss = 0.78283326\n",
      "Iteration 41, loss = 0.77790979\n",
      "Iteration 42, loss = 0.77319840\n",
      "Iteration 43, loss = 0.76863278\n",
      "Iteration 44, loss = 0.76424090\n",
      "Iteration 45, loss = 0.75999019\n",
      "Iteration 46, loss = 0.75589464\n",
      "Iteration 47, loss = 0.75192161\n",
      "Iteration 48, loss = 0.74800529\n",
      "Iteration 49, loss = 0.74426503\n",
      "Iteration 50, loss = 0.74062963\n",
      "Iteration 51, loss = 0.73705433\n",
      "Iteration 52, loss = 0.73359026\n",
      "Iteration 53, loss = 0.73025052\n",
      "Iteration 54, loss = 0.72703185\n",
      "Iteration 55, loss = 0.72398266\n",
      "Iteration 56, loss = 0.72102625\n",
      "Iteration 57, loss = 0.71818353\n",
      "Iteration 58, loss = 0.71542914\n",
      "Iteration 59, loss = 0.71276269\n",
      "Iteration 60, loss = 0.71016649\n",
      "Iteration 61, loss = 0.70766776\n",
      "Iteration 62, loss = 0.70518434\n",
      "Iteration 63, loss = 0.70278190\n",
      "Iteration 64, loss = 0.70046179\n",
      "Iteration 65, loss = 0.69814592\n",
      "Iteration 66, loss = 0.69587587\n",
      "Iteration 67, loss = 0.69368760\n",
      "Iteration 68, loss = 0.69153985\n",
      "Iteration 69, loss = 0.68943518\n",
      "Iteration 70, loss = 0.68735365\n",
      "Iteration 71, loss = 0.68534769\n",
      "Iteration 72, loss = 0.68335672\n",
      "Iteration 73, loss = 0.68139348\n",
      "Iteration 74, loss = 0.67945751\n",
      "Iteration 75, loss = 0.67758447\n",
      "Iteration 76, loss = 0.67573888\n",
      "Iteration 77, loss = 0.67389288\n",
      "Iteration 78, loss = 0.67211401\n",
      "Iteration 79, loss = 0.67034268\n",
      "Iteration 80, loss = 0.66860697\n",
      "Iteration 81, loss = 0.66688864\n",
      "Iteration 82, loss = 0.66518991\n",
      "Iteration 83, loss = 0.66356424\n",
      "Iteration 84, loss = 0.66192516\n",
      "Iteration 85, loss = 0.66028164\n",
      "Iteration 86, loss = 0.65869825\n",
      "Iteration 87, loss = 0.65711975\n",
      "Iteration 88, loss = 0.65554937\n",
      "Iteration 89, loss = 0.65401412\n",
      "Iteration 90, loss = 0.65248811\n",
      "Iteration 91, loss = 0.65099605\n",
      "Iteration 92, loss = 0.64948623\n",
      "Iteration 93, loss = 0.64801389\n",
      "Iteration 94, loss = 0.64658434\n",
      "Iteration 95, loss = 0.64512049\n",
      "Iteration 96, loss = 0.64368731\n",
      "Iteration 97, loss = 0.64225079\n",
      "Iteration 98, loss = 0.64087248\n",
      "Iteration 99, loss = 0.63946215\n",
      "Iteration 100, loss = 0.63806026\n",
      "Iteration 101, loss = 0.63670280\n",
      "Iteration 102, loss = 0.63531255\n",
      "Iteration 103, loss = 0.63395735\n",
      "Iteration 104, loss = 0.63260165\n",
      "Iteration 105, loss = 0.63125279\n",
      "Iteration 106, loss = 0.62991472\n",
      "Iteration 107, loss = 0.62859218\n",
      "Iteration 108, loss = 0.62725351\n",
      "Iteration 109, loss = 0.62594319\n",
      "Iteration 110, loss = 0.62463152\n",
      "Iteration 111, loss = 0.62333920\n",
      "Iteration 112, loss = 0.62207053\n",
      "Iteration 113, loss = 0.62077057\n",
      "Iteration 114, loss = 0.61951740\n",
      "Iteration 115, loss = 0.61823462\n",
      "Iteration 116, loss = 0.61697577\n",
      "Iteration 117, loss = 0.61572611\n",
      "Iteration 118, loss = 0.61445003\n",
      "Iteration 119, loss = 0.61323459\n",
      "Iteration 120, loss = 0.61196907\n",
      "Iteration 121, loss = 0.61075413\n",
      "Iteration 122, loss = 0.60949867\n",
      "Iteration 123, loss = 0.60828860\n",
      "Iteration 124, loss = 0.60706177\n",
      "Iteration 125, loss = 0.60581811\n",
      "Iteration 126, loss = 0.60462896\n",
      "Iteration 127, loss = 0.60339663\n",
      "Iteration 128, loss = 0.60219386\n",
      "Iteration 129, loss = 0.60100415\n",
      "Iteration 130, loss = 0.59981817\n",
      "Iteration 131, loss = 0.59860018\n",
      "Iteration 132, loss = 0.59741231\n",
      "Iteration 133, loss = 0.59622592\n",
      "Iteration 134, loss = 0.59505871\n",
      "Iteration 135, loss = 0.59386655\n",
      "Iteration 136, loss = 0.59270782\n",
      "Iteration 137, loss = 0.59154295\n",
      "Iteration 138, loss = 0.59042128\n",
      "Iteration 139, loss = 0.58926614\n",
      "Iteration 140, loss = 0.58813880\n",
      "Iteration 141, loss = 0.58705201\n",
      "Iteration 142, loss = 0.58592983\n",
      "Iteration 143, loss = 0.58480833\n",
      "Iteration 144, loss = 0.58376761\n",
      "Iteration 145, loss = 0.58266259\n",
      "Iteration 146, loss = 0.58162915\n",
      "Iteration 147, loss = 0.58059801\n",
      "Iteration 148, loss = 0.57952468\n",
      "Iteration 149, loss = 0.57850196\n",
      "Iteration 150, loss = 0.57748874\n",
      "Iteration 151, loss = 0.57647483\n",
      "Iteration 152, loss = 0.57548081\n",
      "Iteration 153, loss = 0.57446722\n",
      "Iteration 154, loss = 0.57348785\n",
      "Iteration 155, loss = 0.57248678\n",
      "Iteration 156, loss = 0.57152693\n",
      "Iteration 157, loss = 0.57055107\n",
      "Iteration 158, loss = 0.56960271\n",
      "Iteration 159, loss = 0.56863638\n",
      "Iteration 160, loss = 0.56770987\n",
      "Iteration 161, loss = 0.56677950\n",
      "Iteration 162, loss = 0.56584163\n",
      "Iteration 163, loss = 0.56491170\n",
      "Iteration 164, loss = 0.56403646\n",
      "Iteration 165, loss = 0.56309594\n",
      "Iteration 166, loss = 0.56223457\n",
      "Iteration 167, loss = 0.56132834\n",
      "Iteration 168, loss = 0.56050243\n",
      "Iteration 169, loss = 0.55961257\n",
      "Iteration 170, loss = 0.55880063\n",
      "Iteration 171, loss = 0.55799622\n",
      "Iteration 172, loss = 0.55722809\n",
      "Iteration 173, loss = 0.55642559\n",
      "Iteration 174, loss = 0.55564258\n",
      "Iteration 175, loss = 0.55483563\n",
      "Iteration 176, loss = 0.55413058\n",
      "Iteration 177, loss = 0.55340738\n",
      "Iteration 178, loss = 0.55262944\n",
      "Iteration 179, loss = 0.55194174\n",
      "Iteration 180, loss = 0.55121467\n",
      "Iteration 181, loss = 0.55056097\n",
      "Iteration 182, loss = 0.54982454\n",
      "Iteration 183, loss = 0.54919025\n",
      "Iteration 184, loss = 0.54850045\n",
      "Iteration 185, loss = 0.54790611\n",
      "Iteration 186, loss = 0.54718689\n",
      "Iteration 187, loss = 0.54659195\n",
      "Iteration 188, loss = 0.54593504\n",
      "Iteration 189, loss = 0.54531513\n",
      "Iteration 190, loss = 0.54471422\n",
      "Iteration 191, loss = 0.54407176\n",
      "Iteration 192, loss = 0.54346290\n",
      "Iteration 193, loss = 0.54285345\n",
      "Iteration 194, loss = 0.54232301\n",
      "Iteration 195, loss = 0.54169646\n",
      "Iteration 196, loss = 0.54113706\n",
      "Iteration 197, loss = 0.54059970\n",
      "Iteration 198, loss = 0.54002421\n",
      "Iteration 199, loss = 0.53948464\n",
      "Iteration 200, loss = 0.53891228\n",
      "Iteration 201, loss = 0.53842631\n",
      "Iteration 202, loss = 0.53787629\n",
      "Iteration 203, loss = 0.53730390\n",
      "Iteration 204, loss = 0.53685591\n",
      "Iteration 205, loss = 0.53635044\n",
      "Iteration 206, loss = 0.53581235\n",
      "Iteration 207, loss = 0.53534480\n",
      "Iteration 208, loss = 0.53482483\n",
      "Iteration 209, loss = 0.53437342\n",
      "Iteration 210, loss = 0.53385050\n",
      "Iteration 211, loss = 0.53340058\n",
      "Iteration 212, loss = 0.53297233\n",
      "Iteration 213, loss = 0.53247677\n",
      "Iteration 214, loss = 0.53202185\n",
      "Iteration 215, loss = 0.53160125\n",
      "Iteration 216, loss = 0.53109188\n",
      "Iteration 217, loss = 0.53072051\n",
      "Iteration 218, loss = 0.53026877\n",
      "Iteration 219, loss = 0.52988342\n",
      "Iteration 220, loss = 0.52945132\n",
      "Iteration 221, loss = 0.52900880\n",
      "Iteration 222, loss = 0.52859139\n",
      "Iteration 223, loss = 0.52821906\n",
      "Iteration 224, loss = 0.52775927\n",
      "Iteration 225, loss = 0.52741218\n",
      "Iteration 226, loss = 0.52698924\n",
      "Iteration 227, loss = 0.52656857\n",
      "Iteration 228, loss = 0.52623015\n",
      "Iteration 229, loss = 0.52579444\n",
      "Iteration 230, loss = 0.52542421\n",
      "Iteration 231, loss = 0.52505271\n",
      "Iteration 232, loss = 0.52469152\n",
      "Iteration 233, loss = 0.52428942\n",
      "Iteration 234, loss = 0.52394041\n",
      "Iteration 235, loss = 0.52361503\n",
      "Iteration 236, loss = 0.52327851\n",
      "Iteration 237, loss = 0.52292686\n",
      "Iteration 238, loss = 0.52258661\n",
      "Iteration 239, loss = 0.52222221\n",
      "Iteration 240, loss = 0.52188640\n",
      "Iteration 241, loss = 0.52153539\n",
      "Iteration 242, loss = 0.52117044\n",
      "Iteration 243, loss = 0.52090546\n",
      "Iteration 244, loss = 0.52053672\n",
      "Iteration 245, loss = 0.52023958\n",
      "Iteration 246, loss = 0.51989281\n",
      "Iteration 247, loss = 0.51962097\n",
      "Iteration 248, loss = 0.51929084\n",
      "Iteration 249, loss = 0.51896946\n",
      "Iteration 250, loss = 0.51872303\n",
      "Iteration 251, loss = 0.51842074\n",
      "Iteration 252, loss = 0.51813553\n",
      "Iteration 253, loss = 0.51780168\n",
      "Iteration 254, loss = 0.51752488\n",
      "Iteration 255, loss = 0.51720489\n",
      "Iteration 256, loss = 0.51691982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.51662068\n",
      "Iteration 258, loss = 0.51637122\n",
      "Iteration 259, loss = 0.51608582\n",
      "Iteration 260, loss = 0.51579803\n",
      "Iteration 261, loss = 0.51554296\n",
      "Iteration 262, loss = 0.51528816\n",
      "Iteration 263, loss = 0.51495017\n",
      "Iteration 264, loss = 0.51471632\n",
      "Iteration 265, loss = 0.51444130\n",
      "Iteration 266, loss = 0.51415187\n",
      "Iteration 267, loss = 0.51390585\n",
      "Iteration 268, loss = 0.51370497\n",
      "Iteration 269, loss = 0.51344479\n",
      "Iteration 270, loss = 0.51312409\n",
      "Iteration 271, loss = 0.51293197\n",
      "Iteration 272, loss = 0.51268805\n",
      "Iteration 273, loss = 0.51241808\n",
      "Iteration 274, loss = 0.51219325\n",
      "Iteration 275, loss = 0.51190100\n",
      "Iteration 276, loss = 0.51166646\n",
      "Iteration 277, loss = 0.51145966\n",
      "Iteration 278, loss = 0.51124387\n",
      "Iteration 279, loss = 0.51098176\n",
      "Iteration 280, loss = 0.51077165\n",
      "Iteration 281, loss = 0.51052051\n",
      "Iteration 282, loss = 0.51025879\n",
      "Iteration 283, loss = 0.51002618\n",
      "Iteration 284, loss = 0.50986350\n",
      "Iteration 285, loss = 0.50959010\n",
      "Iteration 286, loss = 0.50940275\n",
      "Iteration 287, loss = 0.50921300\n",
      "Iteration 288, loss = 0.50899957\n",
      "Iteration 289, loss = 0.50866692\n",
      "Iteration 290, loss = 0.50852496\n",
      "Iteration 291, loss = 0.50833812\n",
      "Iteration 292, loss = 0.50809551\n",
      "Iteration 293, loss = 0.50792541\n",
      "Iteration 294, loss = 0.50764707\n",
      "Iteration 295, loss = 0.50742996\n",
      "Iteration 296, loss = 0.50727328\n",
      "Iteration 297, loss = 0.50703418\n",
      "Iteration 298, loss = 0.50687403\n",
      "Iteration 299, loss = 0.50671765\n",
      "Iteration 300, loss = 0.50650913\n",
      "Iteration 301, loss = 0.50625495\n",
      "Iteration 302, loss = 0.50606991\n",
      "Iteration 303, loss = 0.50589663\n",
      "Iteration 304, loss = 0.50569106\n",
      "Iteration 305, loss = 0.50551265\n",
      "Iteration 306, loss = 0.50524622\n",
      "Iteration 307, loss = 0.50510123\n",
      "Iteration 308, loss = 0.50490303\n",
      "Iteration 309, loss = 0.50475876\n",
      "Iteration 310, loss = 0.50452327\n",
      "Iteration 311, loss = 0.50431789\n",
      "Iteration 312, loss = 0.50420623\n",
      "Iteration 313, loss = 0.50397566\n",
      "Iteration 314, loss = 0.50382927\n",
      "Iteration 315, loss = 0.50359727\n",
      "Iteration 316, loss = 0.50348397\n",
      "Iteration 317, loss = 0.50323092\n",
      "Iteration 318, loss = 0.50308778\n",
      "Iteration 319, loss = 0.50291711\n",
      "Iteration 320, loss = 0.50276494\n",
      "Iteration 321, loss = 0.50252643\n",
      "Iteration 322, loss = 0.50236310\n",
      "Iteration 323, loss = 0.50217952\n",
      "Iteration 324, loss = 0.50201974\n",
      "Iteration 325, loss = 0.50191575\n",
      "Iteration 326, loss = 0.50165674\n",
      "Iteration 327, loss = 0.50157595\n",
      "Iteration 328, loss = 0.50133383\n",
      "Iteration 329, loss = 0.50117926\n",
      "Iteration 330, loss = 0.50098740\n",
      "Iteration 331, loss = 0.50081965\n",
      "Iteration 332, loss = 0.50071199\n",
      "Iteration 333, loss = 0.50048029\n",
      "Iteration 334, loss = 0.50039788\n",
      "Iteration 335, loss = 0.50019686\n",
      "Iteration 336, loss = 0.50004347\n",
      "Iteration 337, loss = 0.49986516\n",
      "Iteration 338, loss = 0.49966984\n",
      "Iteration 339, loss = 0.49960995\n",
      "Iteration 340, loss = 0.49941873\n",
      "Iteration 341, loss = 0.49921000\n",
      "Iteration 342, loss = 0.49908999\n",
      "Iteration 343, loss = 0.49892071\n",
      "Iteration 344, loss = 0.49877163\n",
      "Iteration 345, loss = 0.49862792\n",
      "Iteration 346, loss = 0.49847805\n",
      "Iteration 347, loss = 0.49832096\n",
      "Iteration 348, loss = 0.49819049\n",
      "Iteration 349, loss = 0.49803087\n",
      "Iteration 350, loss = 0.49784690\n",
      "Iteration 351, loss = 0.49773060\n",
      "Iteration 352, loss = 0.49759881\n",
      "Iteration 353, loss = 0.49742783\n",
      "Iteration 354, loss = 0.49728346\n",
      "Iteration 355, loss = 0.49713951\n",
      "Iteration 356, loss = 0.49703907\n",
      "Iteration 357, loss = 0.49685134\n",
      "Iteration 358, loss = 0.49672229\n",
      "Iteration 359, loss = 0.49652828\n",
      "Iteration 360, loss = 0.49641723\n",
      "Iteration 361, loss = 0.49630937\n",
      "Iteration 362, loss = 0.49611520\n",
      "Iteration 363, loss = 0.49603305\n",
      "Iteration 364, loss = 0.49591790\n",
      "Iteration 365, loss = 0.49579099\n",
      "Iteration 366, loss = 0.49559389\n",
      "Iteration 367, loss = 0.49545871\n",
      "Iteration 368, loss = 0.49529050\n",
      "Iteration 369, loss = 0.49519809\n",
      "Iteration 370, loss = 0.49504614\n",
      "Iteration 371, loss = 0.49496561\n",
      "Iteration 372, loss = 0.49478666\n",
      "Iteration 373, loss = 0.49467544\n",
      "Iteration 374, loss = 0.49447502\n",
      "Iteration 375, loss = 0.49434839\n",
      "Iteration 376, loss = 0.49422840\n",
      "Iteration 377, loss = 0.49405069\n",
      "Iteration 378, loss = 0.49396174\n",
      "Iteration 379, loss = 0.49382366\n",
      "Iteration 380, loss = 0.49370394\n",
      "Iteration 381, loss = 0.49354293\n",
      "Iteration 382, loss = 0.49342326\n",
      "Iteration 383, loss = 0.49322854\n",
      "Iteration 384, loss = 0.49316980\n",
      "Iteration 385, loss = 0.49310724\n",
      "Iteration 386, loss = 0.49298497\n",
      "Iteration 387, loss = 0.49283881\n",
      "Iteration 388, loss = 0.49272662\n",
      "Iteration 389, loss = 0.49253545\n",
      "Iteration 390, loss = 0.49242136\n",
      "Iteration 391, loss = 0.49232339\n",
      "Iteration 392, loss = 0.49217449\n",
      "Iteration 393, loss = 0.49207502\n",
      "Iteration 394, loss = 0.49193678\n",
      "Iteration 395, loss = 0.49185462\n",
      "Iteration 396, loss = 0.49174430\n",
      "Iteration 397, loss = 0.49160961\n",
      "Iteration 398, loss = 0.49146433\n",
      "Iteration 399, loss = 0.49130443\n",
      "Iteration 400, loss = 0.49129841\n",
      "Iteration 401, loss = 0.49113346\n",
      "Iteration 402, loss = 0.49094887\n",
      "Iteration 403, loss = 0.49083204\n",
      "Iteration 404, loss = 0.49070573\n",
      "Iteration 405, loss = 0.49060044\n",
      "Iteration 406, loss = 0.49045329\n",
      "Iteration 407, loss = 0.49041448\n",
      "Iteration 408, loss = 0.49024436\n",
      "Iteration 409, loss = 0.49016157\n",
      "Iteration 410, loss = 0.49000882\n",
      "Iteration 411, loss = 0.48983734\n",
      "Iteration 412, loss = 0.48977061\n",
      "Iteration 413, loss = 0.48968326\n",
      "Iteration 414, loss = 0.48954511\n",
      "Iteration 415, loss = 0.48947498\n",
      "Iteration 416, loss = 0.48937747\n",
      "Iteration 417, loss = 0.48913594\n",
      "Iteration 418, loss = 0.48908336\n",
      "Iteration 419, loss = 0.48903543\n",
      "Iteration 420, loss = 0.48889408\n",
      "Iteration 421, loss = 0.48879667\n",
      "Iteration 422, loss = 0.48862298\n",
      "Iteration 423, loss = 0.48850109\n",
      "Iteration 424, loss = 0.48846125\n",
      "Iteration 425, loss = 0.48830045\n",
      "Iteration 426, loss = 0.48825565\n",
      "Iteration 427, loss = 0.48804072\n",
      "Iteration 428, loss = 0.48809936\n",
      "Iteration 429, loss = 0.48784754\n",
      "Iteration 430, loss = 0.48775931\n",
      "Iteration 431, loss = 0.48771232\n",
      "Iteration 432, loss = 0.48752221\n",
      "Iteration 433, loss = 0.48743010\n",
      "Iteration 434, loss = 0.48731791\n",
      "Iteration 435, loss = 0.48716798\n",
      "Iteration 436, loss = 0.48708484\n",
      "Iteration 437, loss = 0.48713402\n",
      "Iteration 438, loss = 0.48697021\n",
      "Iteration 439, loss = 0.48674616\n",
      "Iteration 440, loss = 0.48672129\n",
      "Iteration 441, loss = 0.48659325\n",
      "Iteration 442, loss = 0.48649519\n",
      "Iteration 443, loss = 0.48641032\n",
      "Iteration 444, loss = 0.48634137\n",
      "Iteration 445, loss = 0.48617413\n",
      "Iteration 446, loss = 0.48611665\n",
      "Iteration 447, loss = 0.48593489\n",
      "Iteration 448, loss = 0.48588100\n",
      "Iteration 449, loss = 0.48577469\n",
      "Iteration 450, loss = 0.48569712\n",
      "Iteration 451, loss = 0.48558929\n",
      "Iteration 452, loss = 0.48543199\n",
      "Iteration 453, loss = 0.48537002\n",
      "Iteration 454, loss = 0.48532014\n",
      "Iteration 455, loss = 0.48517973\n",
      "Iteration 456, loss = 0.48511911\n",
      "Iteration 457, loss = 0.48498088\n",
      "Iteration 458, loss = 0.48485581\n",
      "Iteration 459, loss = 0.48474431\n",
      "Iteration 460, loss = 0.48477174\n",
      "Iteration 461, loss = 0.48453908\n",
      "Iteration 462, loss = 0.48444144\n",
      "Iteration 463, loss = 0.48441088\n",
      "Iteration 464, loss = 0.48428372\n",
      "Iteration 465, loss = 0.48416887\n",
      "Iteration 466, loss = 0.48404281\n",
      "Iteration 467, loss = 0.48386963\n",
      "Iteration 468, loss = 0.48379650\n",
      "Iteration 469, loss = 0.48375172\n",
      "Iteration 470, loss = 0.48365410\n",
      "Iteration 471, loss = 0.48363479\n",
      "Iteration 472, loss = 0.48350943\n",
      "Iteration 473, loss = 0.48331286\n",
      "Iteration 474, loss = 0.48327213\n",
      "Iteration 475, loss = 0.48316673\n",
      "Iteration 476, loss = 0.48307826\n",
      "Iteration 477, loss = 0.48295242\n",
      "Iteration 478, loss = 0.48296619\n",
      "Iteration 479, loss = 0.48277354\n",
      "Iteration 480, loss = 0.48262757\n",
      "Iteration 481, loss = 0.48258341\n",
      "Iteration 482, loss = 0.48256256\n",
      "Iteration 483, loss = 0.48244946\n",
      "Iteration 484, loss = 0.48237491\n",
      "Iteration 485, loss = 0.48224155\n",
      "Iteration 486, loss = 0.48213598\n",
      "Iteration 487, loss = 0.48205587\n",
      "Iteration 488, loss = 0.48199047\n",
      "Iteration 489, loss = 0.48191786\n",
      "Iteration 490, loss = 0.48173871\n",
      "Iteration 491, loss = 0.48170375\n",
      "Iteration 492, loss = 0.48160231\n",
      "Iteration 493, loss = 0.48152539\n",
      "Iteration 494, loss = 0.48143723\n",
      "Iteration 495, loss = 0.48134335\n",
      "Iteration 496, loss = 0.48121810\n",
      "Iteration 497, loss = 0.48113756\n",
      "Iteration 498, loss = 0.48113709\n",
      "Iteration 499, loss = 0.48092847\n",
      "Iteration 500, loss = 0.48085895\n",
      "Iteration 1, loss = 1.99997041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.86433611\n",
      "Iteration 3, loss = 1.74981616\n",
      "Iteration 4, loss = 1.65209549\n",
      "Iteration 5, loss = 1.56792040\n",
      "Iteration 6, loss = 1.49476034\n",
      "Iteration 7, loss = 1.43016720\n",
      "Iteration 8, loss = 1.37245787\n",
      "Iteration 9, loss = 1.32034758\n",
      "Iteration 10, loss = 1.27304684\n",
      "Iteration 11, loss = 1.22998112\n",
      "Iteration 12, loss = 1.19074196\n",
      "Iteration 13, loss = 1.15498809\n",
      "Iteration 14, loss = 1.12243198\n",
      "Iteration 15, loss = 1.09268942\n",
      "Iteration 16, loss = 1.06542324\n",
      "Iteration 17, loss = 1.04042889\n",
      "Iteration 18, loss = 1.01747541\n",
      "Iteration 19, loss = 0.99641034\n",
      "Iteration 20, loss = 0.97707539\n",
      "Iteration 21, loss = 0.95929274\n",
      "Iteration 22, loss = 0.94295753\n",
      "Iteration 23, loss = 0.92789413\n",
      "Iteration 24, loss = 0.91397600\n",
      "Iteration 25, loss = 0.90106279\n",
      "Iteration 26, loss = 0.88903627\n",
      "Iteration 27, loss = 0.87782484\n",
      "Iteration 28, loss = 0.86736195\n",
      "Iteration 29, loss = 0.85755072\n",
      "Iteration 30, loss = 0.84836783\n",
      "Iteration 31, loss = 0.83976925\n",
      "Iteration 32, loss = 0.83168937\n",
      "Iteration 33, loss = 0.82413644\n",
      "Iteration 34, loss = 0.81701270\n",
      "Iteration 35, loss = 0.81029986\n",
      "Iteration 36, loss = 0.80398103\n",
      "Iteration 37, loss = 0.79802807\n",
      "Iteration 38, loss = 0.79234053\n",
      "Iteration 39, loss = 0.78694368\n",
      "Iteration 40, loss = 0.78181869\n",
      "Iteration 41, loss = 0.77690871\n",
      "Iteration 42, loss = 0.77220467\n",
      "Iteration 43, loss = 0.76771551\n",
      "Iteration 44, loss = 0.76337299\n",
      "Iteration 45, loss = 0.75924533\n",
      "Iteration 46, loss = 0.75521655\n",
      "Iteration 47, loss = 0.75136429\n",
      "Iteration 48, loss = 0.74754745\n",
      "Iteration 49, loss = 0.74392898\n",
      "Iteration 50, loss = 0.74033004\n",
      "Iteration 51, loss = 0.73685679\n",
      "Iteration 52, loss = 0.73344507\n",
      "Iteration 53, loss = 0.73010324\n",
      "Iteration 54, loss = 0.72683944\n",
      "Iteration 55, loss = 0.72370005\n",
      "Iteration 56, loss = 0.72060891\n",
      "Iteration 57, loss = 0.71768218\n",
      "Iteration 58, loss = 0.71484448\n",
      "Iteration 59, loss = 0.71216292\n",
      "Iteration 60, loss = 0.70952041\n",
      "Iteration 61, loss = 0.70697528\n",
      "Iteration 62, loss = 0.70451609\n",
      "Iteration 63, loss = 0.70211828\n",
      "Iteration 64, loss = 0.69976473\n",
      "Iteration 65, loss = 0.69744111\n",
      "Iteration 66, loss = 0.69522057\n",
      "Iteration 67, loss = 0.69298757\n",
      "Iteration 68, loss = 0.69083668\n",
      "Iteration 69, loss = 0.68873143\n",
      "Iteration 70, loss = 0.68668620\n",
      "Iteration 71, loss = 0.68461922\n",
      "Iteration 72, loss = 0.68264824\n",
      "Iteration 73, loss = 0.68070023\n",
      "Iteration 74, loss = 0.67879644\n",
      "Iteration 75, loss = 0.67686948\n",
      "Iteration 76, loss = 0.67504147\n",
      "Iteration 77, loss = 0.67318700\n",
      "Iteration 78, loss = 0.67138947\n",
      "Iteration 79, loss = 0.66963348\n",
      "Iteration 80, loss = 0.66789389\n",
      "Iteration 81, loss = 0.66616220\n",
      "Iteration 82, loss = 0.66447014\n",
      "Iteration 83, loss = 0.66279996\n",
      "Iteration 84, loss = 0.66118668\n",
      "Iteration 85, loss = 0.65953415\n",
      "Iteration 86, loss = 0.65791564\n",
      "Iteration 87, loss = 0.65633311\n",
      "Iteration 88, loss = 0.65478927\n",
      "Iteration 89, loss = 0.65324659\n",
      "Iteration 90, loss = 0.65169613\n",
      "Iteration 91, loss = 0.65021504\n",
      "Iteration 92, loss = 0.64870133\n",
      "Iteration 93, loss = 0.64722048\n",
      "Iteration 94, loss = 0.64577001\n",
      "Iteration 95, loss = 0.64432441\n",
      "Iteration 96, loss = 0.64286085\n",
      "Iteration 97, loss = 0.64143138\n",
      "Iteration 98, loss = 0.64000843\n",
      "Iteration 99, loss = 0.63863397\n",
      "Iteration 100, loss = 0.63721042\n",
      "Iteration 101, loss = 0.63581869\n",
      "Iteration 102, loss = 0.63443294\n",
      "Iteration 103, loss = 0.63308785\n",
      "Iteration 104, loss = 0.63171311\n",
      "Iteration 105, loss = 0.63040108\n",
      "Iteration 106, loss = 0.62903215\n",
      "Iteration 107, loss = 0.62770855\n",
      "Iteration 108, loss = 0.62637833\n",
      "Iteration 109, loss = 0.62506448\n",
      "Iteration 110, loss = 0.62376075\n",
      "Iteration 111, loss = 0.62247395\n",
      "Iteration 112, loss = 0.62115028\n",
      "Iteration 113, loss = 0.61984984\n",
      "Iteration 114, loss = 0.61856405\n",
      "Iteration 115, loss = 0.61732326\n",
      "Iteration 116, loss = 0.61603858\n",
      "Iteration 117, loss = 0.61479463\n",
      "Iteration 118, loss = 0.61352628\n",
      "Iteration 119, loss = 0.61222775\n",
      "Iteration 120, loss = 0.61104551\n",
      "Iteration 121, loss = 0.60974442\n",
      "Iteration 122, loss = 0.60852768\n",
      "Iteration 123, loss = 0.60729588\n",
      "Iteration 124, loss = 0.60607274\n",
      "Iteration 125, loss = 0.60479242\n",
      "Iteration 126, loss = 0.60361194\n",
      "Iteration 127, loss = 0.60239956\n",
      "Iteration 128, loss = 0.60118771\n",
      "Iteration 129, loss = 0.59996687\n",
      "Iteration 130, loss = 0.59877102\n",
      "Iteration 131, loss = 0.59755115\n",
      "Iteration 132, loss = 0.59634898\n",
      "Iteration 133, loss = 0.59517259\n",
      "Iteration 134, loss = 0.59398735\n",
      "Iteration 135, loss = 0.59277052\n",
      "Iteration 136, loss = 0.59160236\n",
      "Iteration 137, loss = 0.59039641\n",
      "Iteration 138, loss = 0.58921438\n",
      "Iteration 139, loss = 0.58803830\n",
      "Iteration 140, loss = 0.58688005\n",
      "Iteration 141, loss = 0.58573767\n",
      "Iteration 142, loss = 0.58460699\n",
      "Iteration 143, loss = 0.58346699\n",
      "Iteration 144, loss = 0.58242845\n",
      "Iteration 145, loss = 0.58127298\n",
      "Iteration 146, loss = 0.58021990\n",
      "Iteration 147, loss = 0.57910579\n",
      "Iteration 148, loss = 0.57809522\n",
      "Iteration 149, loss = 0.57702778\n",
      "Iteration 150, loss = 0.57597581\n",
      "Iteration 151, loss = 0.57494128\n",
      "Iteration 152, loss = 0.57394348\n",
      "Iteration 153, loss = 0.57297355\n",
      "Iteration 154, loss = 0.57196872\n",
      "Iteration 155, loss = 0.57100984\n",
      "Iteration 156, loss = 0.57005187\n",
      "Iteration 157, loss = 0.56906508\n",
      "Iteration 158, loss = 0.56814387\n",
      "Iteration 159, loss = 0.56718392\n",
      "Iteration 160, loss = 0.56622845\n",
      "Iteration 161, loss = 0.56532310\n",
      "Iteration 162, loss = 0.56439928\n",
      "Iteration 163, loss = 0.56342126\n",
      "Iteration 164, loss = 0.56253004\n",
      "Iteration 165, loss = 0.56165749\n",
      "Iteration 166, loss = 0.56078275\n",
      "Iteration 167, loss = 0.55992328\n",
      "Iteration 168, loss = 0.55908679\n",
      "Iteration 169, loss = 0.55823587\n",
      "Iteration 170, loss = 0.55740106\n",
      "Iteration 171, loss = 0.55659662\n",
      "Iteration 172, loss = 0.55579897\n",
      "Iteration 173, loss = 0.55507056\n",
      "Iteration 174, loss = 0.55426849\n",
      "Iteration 175, loss = 0.55353495\n",
      "Iteration 176, loss = 0.55278439\n",
      "Iteration 177, loss = 0.55206051\n",
      "Iteration 178, loss = 0.55135120\n",
      "Iteration 179, loss = 0.55058336\n",
      "Iteration 180, loss = 0.54987950\n",
      "Iteration 181, loss = 0.54919232\n",
      "Iteration 182, loss = 0.54844371\n",
      "Iteration 183, loss = 0.54776981\n",
      "Iteration 184, loss = 0.54708448\n",
      "Iteration 185, loss = 0.54648743\n",
      "Iteration 186, loss = 0.54585083\n",
      "Iteration 187, loss = 0.54517626\n",
      "Iteration 188, loss = 0.54459024\n",
      "Iteration 189, loss = 0.54395543\n",
      "Iteration 190, loss = 0.54334880\n",
      "Iteration 191, loss = 0.54272844\n",
      "Iteration 192, loss = 0.54204858\n",
      "Iteration 193, loss = 0.54155254\n",
      "Iteration 194, loss = 0.54097688\n",
      "Iteration 195, loss = 0.54036527\n",
      "Iteration 196, loss = 0.53980022\n",
      "Iteration 197, loss = 0.53919336\n",
      "Iteration 198, loss = 0.53862685\n",
      "Iteration 199, loss = 0.53812118\n",
      "Iteration 200, loss = 0.53755776\n",
      "Iteration 201, loss = 0.53699490\n",
      "Iteration 202, loss = 0.53653710\n",
      "Iteration 203, loss = 0.53597895\n",
      "Iteration 204, loss = 0.53550055\n",
      "Iteration 205, loss = 0.53495125\n",
      "Iteration 206, loss = 0.53447847\n",
      "Iteration 207, loss = 0.53395733\n",
      "Iteration 208, loss = 0.53345529\n",
      "Iteration 209, loss = 0.53300224\n",
      "Iteration 210, loss = 0.53251987\n",
      "Iteration 211, loss = 0.53205767\n",
      "Iteration 212, loss = 0.53157861\n",
      "Iteration 213, loss = 0.53111476\n",
      "Iteration 214, loss = 0.53071178\n",
      "Iteration 215, loss = 0.53023343\n",
      "Iteration 216, loss = 0.52975301\n",
      "Iteration 217, loss = 0.52933006\n",
      "Iteration 218, loss = 0.52887904\n",
      "Iteration 219, loss = 0.52848765\n",
      "Iteration 220, loss = 0.52803054\n",
      "Iteration 221, loss = 0.52767368\n",
      "Iteration 222, loss = 0.52721094\n",
      "Iteration 223, loss = 0.52678955\n",
      "Iteration 224, loss = 0.52640985\n",
      "Iteration 225, loss = 0.52600304\n",
      "Iteration 226, loss = 0.52561958\n",
      "Iteration 227, loss = 0.52520120\n",
      "Iteration 228, loss = 0.52485642\n",
      "Iteration 229, loss = 0.52448440\n",
      "Iteration 230, loss = 0.52411179\n",
      "Iteration 231, loss = 0.52374589\n",
      "Iteration 232, loss = 0.52341962\n",
      "Iteration 233, loss = 0.52295204\n",
      "Iteration 234, loss = 0.52262405\n",
      "Iteration 235, loss = 0.52228523\n",
      "Iteration 236, loss = 0.52189958\n",
      "Iteration 237, loss = 0.52148829\n",
      "Iteration 238, loss = 0.52120209\n",
      "Iteration 239, loss = 0.52088928\n",
      "Iteration 240, loss = 0.52053422\n",
      "Iteration 241, loss = 0.52023167\n",
      "Iteration 242, loss = 0.51991709\n",
      "Iteration 243, loss = 0.51951557\n",
      "Iteration 244, loss = 0.51925795\n",
      "Iteration 245, loss = 0.51891947\n",
      "Iteration 246, loss = 0.51864033\n",
      "Iteration 247, loss = 0.51829414\n",
      "Iteration 248, loss = 0.51795086\n",
      "Iteration 249, loss = 0.51763410\n",
      "Iteration 250, loss = 0.51738559\n",
      "Iteration 251, loss = 0.51706050\n",
      "Iteration 252, loss = 0.51678361\n",
      "Iteration 253, loss = 0.51643229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.51612556\n",
      "Iteration 255, loss = 0.51581116\n",
      "Iteration 256, loss = 0.51558371\n",
      "Iteration 257, loss = 0.51527889\n",
      "Iteration 258, loss = 0.51500893\n",
      "Iteration 259, loss = 0.51469827\n",
      "Iteration 260, loss = 0.51448347\n",
      "Iteration 261, loss = 0.51420697\n",
      "Iteration 262, loss = 0.51395713\n",
      "Iteration 263, loss = 0.51366138\n",
      "Iteration 264, loss = 0.51345708\n",
      "Iteration 265, loss = 0.51313778\n",
      "Iteration 266, loss = 0.51285011\n",
      "Iteration 267, loss = 0.51259291\n",
      "Iteration 268, loss = 0.51235856\n",
      "Iteration 269, loss = 0.51206728\n",
      "Iteration 270, loss = 0.51180890\n",
      "Iteration 271, loss = 0.51161379\n",
      "Iteration 272, loss = 0.51127429\n",
      "Iteration 273, loss = 0.51105178\n",
      "Iteration 274, loss = 0.51081053\n",
      "Iteration 275, loss = 0.51054270\n",
      "Iteration 276, loss = 0.51035669\n",
      "Iteration 277, loss = 0.51010937\n",
      "Iteration 278, loss = 0.50984915\n",
      "Iteration 279, loss = 0.50971437\n",
      "Iteration 280, loss = 0.50945874\n",
      "Iteration 281, loss = 0.50917624\n",
      "Iteration 282, loss = 0.50891002\n",
      "Iteration 283, loss = 0.50871295\n",
      "Iteration 284, loss = 0.50852991\n",
      "Iteration 285, loss = 0.50828395\n",
      "Iteration 286, loss = 0.50811696\n",
      "Iteration 287, loss = 0.50783767\n",
      "Iteration 288, loss = 0.50762403\n",
      "Iteration 289, loss = 0.50740517\n",
      "Iteration 290, loss = 0.50714383\n",
      "Iteration 291, loss = 0.50695853\n",
      "Iteration 292, loss = 0.50677615\n",
      "Iteration 293, loss = 0.50652822\n",
      "Iteration 294, loss = 0.50633831\n",
      "Iteration 295, loss = 0.50610088\n",
      "Iteration 296, loss = 0.50590432\n",
      "Iteration 297, loss = 0.50568954\n",
      "Iteration 298, loss = 0.50553250\n",
      "Iteration 299, loss = 0.50534232\n",
      "Iteration 300, loss = 0.50506254\n",
      "Iteration 301, loss = 0.50497252\n",
      "Iteration 302, loss = 0.50474278\n",
      "Iteration 303, loss = 0.50450651\n",
      "Iteration 304, loss = 0.50426169\n",
      "Iteration 305, loss = 0.50414855\n",
      "Iteration 306, loss = 0.50392746\n",
      "Iteration 307, loss = 0.50373121\n",
      "Iteration 308, loss = 0.50355408\n",
      "Iteration 309, loss = 0.50337740\n",
      "Iteration 310, loss = 0.50313931\n",
      "Iteration 311, loss = 0.50299367\n",
      "Iteration 312, loss = 0.50281020\n",
      "Iteration 313, loss = 0.50260297\n",
      "Iteration 314, loss = 0.50250109\n",
      "Iteration 315, loss = 0.50223397\n",
      "Iteration 316, loss = 0.50205847\n",
      "Iteration 317, loss = 0.50190314\n",
      "Iteration 318, loss = 0.50169586\n",
      "Iteration 319, loss = 0.50161297\n",
      "Iteration 320, loss = 0.50135421\n",
      "Iteration 321, loss = 0.50120595\n",
      "Iteration 322, loss = 0.50104000\n",
      "Iteration 323, loss = 0.50082457\n",
      "Iteration 324, loss = 0.50067526\n",
      "Iteration 325, loss = 0.50061931\n",
      "Iteration 326, loss = 0.50040553\n",
      "Iteration 327, loss = 0.50026918\n",
      "Iteration 328, loss = 0.50001654\n",
      "Iteration 329, loss = 0.49984250\n",
      "Iteration 330, loss = 0.49969226\n",
      "Iteration 331, loss = 0.49945043\n",
      "Iteration 332, loss = 0.49942336\n",
      "Iteration 333, loss = 0.49928310\n",
      "Iteration 334, loss = 0.49907814\n",
      "Iteration 335, loss = 0.49884872\n",
      "Iteration 336, loss = 0.49867254\n",
      "Iteration 337, loss = 0.49853202\n",
      "Iteration 338, loss = 0.49839378\n",
      "Iteration 339, loss = 0.49820780\n",
      "Iteration 340, loss = 0.49803743\n",
      "Iteration 341, loss = 0.49792785\n",
      "Iteration 342, loss = 0.49771354\n",
      "Iteration 343, loss = 0.49755527\n",
      "Iteration 344, loss = 0.49741849\n",
      "Iteration 345, loss = 0.49734395\n",
      "Iteration 346, loss = 0.49714174\n",
      "Iteration 347, loss = 0.49695137\n",
      "Iteration 348, loss = 0.49686701\n",
      "Iteration 349, loss = 0.49675347\n",
      "Iteration 350, loss = 0.49655158\n",
      "Iteration 351, loss = 0.49636376\n",
      "Iteration 352, loss = 0.49625860\n",
      "Iteration 353, loss = 0.49608324\n",
      "Iteration 354, loss = 0.49589088\n",
      "Iteration 355, loss = 0.49582003\n",
      "Iteration 356, loss = 0.49567015\n",
      "Iteration 357, loss = 0.49555651\n",
      "Iteration 358, loss = 0.49535054\n",
      "Iteration 359, loss = 0.49529329\n",
      "Iteration 360, loss = 0.49516794\n",
      "Iteration 361, loss = 0.49499088\n",
      "Iteration 362, loss = 0.49484339\n",
      "Iteration 363, loss = 0.49468836\n",
      "Iteration 364, loss = 0.49458771\n",
      "Iteration 365, loss = 0.49437067\n",
      "Iteration 366, loss = 0.49421045\n",
      "Iteration 367, loss = 0.49416562\n",
      "Iteration 368, loss = 0.49391746\n",
      "Iteration 369, loss = 0.49382028\n",
      "Iteration 370, loss = 0.49364502\n",
      "Iteration 371, loss = 0.49361250\n",
      "Iteration 372, loss = 0.49345173\n",
      "Iteration 373, loss = 0.49330527\n",
      "Iteration 374, loss = 0.49309607\n",
      "Iteration 375, loss = 0.49305619\n",
      "Iteration 376, loss = 0.49287080\n",
      "Iteration 377, loss = 0.49274623\n",
      "Iteration 378, loss = 0.49272654\n",
      "Iteration 379, loss = 0.49245676\n",
      "Iteration 380, loss = 0.49243161\n",
      "Iteration 381, loss = 0.49224186\n",
      "Iteration 382, loss = 0.49214468\n",
      "Iteration 383, loss = 0.49203851\n",
      "Iteration 384, loss = 0.49178068\n",
      "Iteration 385, loss = 0.49171899\n",
      "Iteration 386, loss = 0.49154532\n",
      "Iteration 387, loss = 0.49147929\n",
      "Iteration 388, loss = 0.49135189\n",
      "Iteration 389, loss = 0.49124514\n",
      "Iteration 390, loss = 0.49114359\n",
      "Iteration 391, loss = 0.49096535\n",
      "Iteration 392, loss = 0.49092412\n",
      "Iteration 393, loss = 0.49075462\n",
      "Iteration 394, loss = 0.49057542\n",
      "Iteration 395, loss = 0.49046987\n",
      "Iteration 396, loss = 0.49039470\n",
      "Iteration 397, loss = 0.49016427\n",
      "Iteration 398, loss = 0.49004448\n",
      "Iteration 399, loss = 0.48994911\n",
      "Iteration 400, loss = 0.48987865\n",
      "Iteration 401, loss = 0.48968516\n",
      "Iteration 402, loss = 0.48962977\n",
      "Iteration 403, loss = 0.48954327\n",
      "Iteration 404, loss = 0.48942582\n",
      "Iteration 405, loss = 0.48925411\n",
      "Iteration 406, loss = 0.48910888\n",
      "Iteration 407, loss = 0.48907765\n",
      "Iteration 408, loss = 0.48893466\n",
      "Iteration 409, loss = 0.48876518\n",
      "Iteration 410, loss = 0.48872631\n",
      "Iteration 411, loss = 0.48855490\n",
      "Iteration 412, loss = 0.48846712\n",
      "Iteration 413, loss = 0.48830064\n",
      "Iteration 414, loss = 0.48822131\n",
      "Iteration 415, loss = 0.48809171\n",
      "Iteration 416, loss = 0.48802104\n",
      "Iteration 417, loss = 0.48788199\n",
      "Iteration 418, loss = 0.48776441\n",
      "Iteration 419, loss = 0.48763108\n",
      "Iteration 420, loss = 0.48756781\n",
      "Iteration 421, loss = 0.48737293\n",
      "Iteration 422, loss = 0.48732780\n",
      "Iteration 423, loss = 0.48719541\n",
      "Iteration 424, loss = 0.48714386\n",
      "Iteration 425, loss = 0.48691797\n",
      "Iteration 426, loss = 0.48683842\n",
      "Iteration 427, loss = 0.48673672\n",
      "Iteration 428, loss = 0.48671331\n",
      "Iteration 429, loss = 0.48649811\n",
      "Iteration 430, loss = 0.48645442\n",
      "Iteration 431, loss = 0.48637813\n",
      "Iteration 432, loss = 0.48626689\n",
      "Iteration 433, loss = 0.48602101\n",
      "Iteration 434, loss = 0.48596823\n",
      "Iteration 435, loss = 0.48585022\n",
      "Iteration 436, loss = 0.48577192\n",
      "Iteration 437, loss = 0.48564406\n",
      "Iteration 438, loss = 0.48558612\n",
      "Iteration 439, loss = 0.48545999\n",
      "Iteration 440, loss = 0.48531219\n",
      "Iteration 441, loss = 0.48525780\n",
      "Iteration 442, loss = 0.48511105\n",
      "Iteration 443, loss = 0.48507087\n",
      "Iteration 444, loss = 0.48489843\n",
      "Iteration 445, loss = 0.48487967\n",
      "Iteration 446, loss = 0.48472449\n",
      "Iteration 447, loss = 0.48464665\n",
      "Iteration 448, loss = 0.48450477\n",
      "Iteration 449, loss = 0.48441038\n",
      "Iteration 450, loss = 0.48431072\n",
      "Iteration 451, loss = 0.48426490\n",
      "Iteration 452, loss = 0.48414660\n",
      "Iteration 453, loss = 0.48407204\n",
      "Iteration 454, loss = 0.48385767\n",
      "Iteration 455, loss = 0.48381230\n",
      "Iteration 456, loss = 0.48377994\n",
      "Iteration 457, loss = 0.48359282\n",
      "Iteration 458, loss = 0.48357618\n",
      "Iteration 459, loss = 0.48343927\n",
      "Iteration 460, loss = 0.48329500\n",
      "Iteration 461, loss = 0.48318323\n",
      "Iteration 462, loss = 0.48313969\n",
      "Iteration 463, loss = 0.48312826\n",
      "Iteration 464, loss = 0.48288126\n",
      "Iteration 465, loss = 0.48276788\n",
      "Iteration 466, loss = 0.48268105\n",
      "Iteration 467, loss = 0.48269262\n",
      "Iteration 468, loss = 0.48254532\n",
      "Iteration 469, loss = 0.48236989\n",
      "Iteration 470, loss = 0.48234820\n",
      "Iteration 471, loss = 0.48219546\n",
      "Iteration 472, loss = 0.48210400\n",
      "Iteration 473, loss = 0.48193619\n",
      "Iteration 474, loss = 0.48193358\n",
      "Iteration 475, loss = 0.48177192\n",
      "Iteration 476, loss = 0.48178241\n",
      "Iteration 477, loss = 0.48165295\n",
      "Iteration 478, loss = 0.48151608\n",
      "Iteration 479, loss = 0.48154478\n",
      "Iteration 480, loss = 0.48135430\n",
      "Iteration 481, loss = 0.48116811\n",
      "Iteration 482, loss = 0.48125619\n",
      "Iteration 483, loss = 0.48102454\n",
      "Iteration 484, loss = 0.48106299\n",
      "Iteration 485, loss = 0.48086022\n",
      "Iteration 486, loss = 0.48078089\n",
      "Iteration 487, loss = 0.48071493\n",
      "Iteration 488, loss = 0.48068599\n",
      "Iteration 489, loss = 0.48063360\n",
      "Iteration 490, loss = 0.48043543\n",
      "Iteration 491, loss = 0.48031331\n",
      "Iteration 492, loss = 0.48022095\n",
      "Iteration 493, loss = 0.48012774\n",
      "Iteration 494, loss = 0.47999597\n",
      "Iteration 495, loss = 0.47987721\n",
      "Iteration 496, loss = 0.47985454\n",
      "Iteration 497, loss = 0.47967589\n",
      "Iteration 498, loss = 0.47966025\n",
      "Iteration 499, loss = 0.47957376\n",
      "Iteration 500, loss = 0.47959126\n",
      "Iteration 1, loss = 2.00103713"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2, loss = 1.86567060\n",
      "Iteration 3, loss = 1.75113852\n",
      "Iteration 4, loss = 1.65331098\n",
      "Iteration 5, loss = 1.56893030\n",
      "Iteration 6, loss = 1.49541573\n",
      "Iteration 7, loss = 1.43059495\n",
      "Iteration 8, loss = 1.37278428\n",
      "Iteration 9, loss = 1.32066563\n",
      "Iteration 10, loss = 1.27334998\n",
      "Iteration 11, loss = 1.23031506\n",
      "Iteration 12, loss = 1.19104954\n",
      "Iteration 13, loss = 1.15525976\n",
      "Iteration 14, loss = 1.12267066\n",
      "Iteration 15, loss = 1.09284136\n",
      "Iteration 16, loss = 1.06558761\n",
      "Iteration 17, loss = 1.04062283\n",
      "Iteration 18, loss = 1.01777991\n",
      "Iteration 19, loss = 0.99683072\n",
      "Iteration 20, loss = 0.97758257\n",
      "Iteration 21, loss = 0.95993467\n",
      "Iteration 22, loss = 0.94367607\n",
      "Iteration 23, loss = 0.92864413\n",
      "Iteration 24, loss = 0.91476399\n",
      "Iteration 25, loss = 0.90191747\n",
      "Iteration 26, loss = 0.88991863\n",
      "Iteration 27, loss = 0.87874797\n",
      "Iteration 28, loss = 0.86826291\n",
      "Iteration 29, loss = 0.85845032\n",
      "Iteration 30, loss = 0.84925500\n",
      "Iteration 31, loss = 0.84064703\n",
      "Iteration 32, loss = 0.83261192\n",
      "Iteration 33, loss = 0.82509668\n",
      "Iteration 34, loss = 0.81798389\n",
      "Iteration 35, loss = 0.81127997\n",
      "Iteration 36, loss = 0.80497271\n",
      "Iteration 37, loss = 0.79900430\n",
      "Iteration 38, loss = 0.79332622\n",
      "Iteration 39, loss = 0.78794675\n",
      "Iteration 40, loss = 0.78280221\n",
      "Iteration 41, loss = 0.77785614\n",
      "Iteration 42, loss = 0.77311915\n",
      "Iteration 43, loss = 0.76856151\n",
      "Iteration 44, loss = 0.76418878\n",
      "Iteration 45, loss = 0.75994584\n",
      "Iteration 46, loss = 0.75588129\n",
      "Iteration 47, loss = 0.75194164\n",
      "Iteration 48, loss = 0.74812302\n",
      "Iteration 49, loss = 0.74443363\n",
      "Iteration 50, loss = 0.74085228\n",
      "Iteration 51, loss = 0.73736286\n",
      "Iteration 52, loss = 0.73400066\n",
      "Iteration 53, loss = 0.73076337\n",
      "Iteration 54, loss = 0.72760216\n",
      "Iteration 55, loss = 0.72453030\n",
      "Iteration 56, loss = 0.72161750\n",
      "Iteration 57, loss = 0.71878070\n",
      "Iteration 58, loss = 0.71603223\n",
      "Iteration 59, loss = 0.71338060\n",
      "Iteration 60, loss = 0.71077120\n",
      "Iteration 61, loss = 0.70828016\n",
      "Iteration 62, loss = 0.70577905\n",
      "Iteration 63, loss = 0.70339897\n",
      "Iteration 64, loss = 0.70103077\n",
      "Iteration 65, loss = 0.69874000\n",
      "Iteration 66, loss = 0.69652486\n",
      "Iteration 67, loss = 0.69431820\n",
      "Iteration 68, loss = 0.69218644\n",
      "Iteration 69, loss = 0.69007513\n",
      "Iteration 70, loss = 0.68800464\n",
      "Iteration 71, loss = 0.68598789\n",
      "Iteration 72, loss = 0.68400572\n",
      "Iteration 73, loss = 0.68208017\n",
      "Iteration 74, loss = 0.68012232\n",
      "Iteration 75, loss = 0.67822524\n",
      "Iteration 76, loss = 0.67636658\n",
      "Iteration 77, loss = 0.67454151\n",
      "Iteration 78, loss = 0.67275779\n",
      "Iteration 79, loss = 0.67098537\n",
      "Iteration 80, loss = 0.66926066\n",
      "Iteration 81, loss = 0.66756789\n",
      "Iteration 82, loss = 0.66585313\n",
      "Iteration 83, loss = 0.66419014\n",
      "Iteration 84, loss = 0.66253403\n",
      "Iteration 85, loss = 0.66090467\n",
      "Iteration 86, loss = 0.65932744\n",
      "Iteration 87, loss = 0.65772997\n",
      "Iteration 88, loss = 0.65614016\n",
      "Iteration 89, loss = 0.65462686\n",
      "Iteration 90, loss = 0.65310505\n",
      "Iteration 91, loss = 0.65156613\n",
      "Iteration 92, loss = 0.65007466\n",
      "Iteration 93, loss = 0.64861889\n",
      "Iteration 94, loss = 0.64712787\n",
      "Iteration 95, loss = 0.64567074\n",
      "Iteration 96, loss = 0.64422359\n",
      "Iteration 97, loss = 0.64281258\n",
      "Iteration 98, loss = 0.64138251\n",
      "Iteration 99, loss = 0.63998813\n",
      "Iteration 100, loss = 0.63865677\n",
      "Iteration 101, loss = 0.63720272\n",
      "Iteration 102, loss = 0.63584384\n",
      "Iteration 103, loss = 0.63446597\n",
      "Iteration 104, loss = 0.63309854\n",
      "Iteration 105, loss = 0.63178408\n",
      "Iteration 106, loss = 0.63043519\n",
      "Iteration 107, loss = 0.62910472\n",
      "Iteration 108, loss = 0.62778044\n",
      "Iteration 109, loss = 0.62647014\n",
      "Iteration 110, loss = 0.62520357\n",
      "Iteration 111, loss = 0.62387902\n",
      "Iteration 112, loss = 0.62258969\n",
      "Iteration 113, loss = 0.62131541\n",
      "Iteration 114, loss = 0.62003865\n",
      "Iteration 115, loss = 0.61875200\n",
      "Iteration 116, loss = 0.61749337\n",
      "Iteration 117, loss = 0.61623262\n",
      "Iteration 118, loss = 0.61495810\n",
      "Iteration 119, loss = 0.61373121\n",
      "Iteration 120, loss = 0.61248396\n",
      "Iteration 121, loss = 0.61122859\n",
      "Iteration 122, loss = 0.60999617\n",
      "Iteration 123, loss = 0.60878394\n",
      "Iteration 124, loss = 0.60755869\n",
      "Iteration 125, loss = 0.60634411\n",
      "Iteration 126, loss = 0.60514050\n",
      "Iteration 127, loss = 0.60393599\n",
      "Iteration 128, loss = 0.60277582\n",
      "Iteration 129, loss = 0.60153617\n",
      "Iteration 130, loss = 0.60036625\n",
      "Iteration 131, loss = 0.59916816\n",
      "Iteration 132, loss = 0.59796259\n",
      "Iteration 133, loss = 0.59678910\n",
      "Iteration 134, loss = 0.59562260\n",
      "Iteration 135, loss = 0.59447094\n",
      "Iteration 136, loss = 0.59330920\n",
      "Iteration 137, loss = 0.59213086\n",
      "Iteration 138, loss = 0.59103364\n",
      "Iteration 139, loss = 0.58991318\n",
      "Iteration 140, loss = 0.58875076\n",
      "Iteration 141, loss = 0.58759605\n",
      "Iteration 142, loss = 0.58637476\n",
      "Iteration 143, loss = 0.58528368\n",
      "Iteration 144, loss = 0.58412806\n",
      "Iteration 145, loss = 0.58304299\n",
      "Iteration 146, loss = 0.58187142\n",
      "Iteration 147, loss = 0.58077090\n",
      "Iteration 148, loss = 0.57963847\n",
      "Iteration 149, loss = 0.57855523\n",
      "Iteration 150, loss = 0.57745260\n",
      "Iteration 151, loss = 0.57635335\n",
      "Iteration 152, loss = 0.57529226\n",
      "Iteration 153, loss = 0.57427308\n",
      "Iteration 154, loss = 0.57324597\n",
      "Iteration 155, loss = 0.57219895\n",
      "Iteration 156, loss = 0.57117254\n",
      "Iteration 157, loss = 0.57013753\n",
      "Iteration 158, loss = 0.56911445\n",
      "Iteration 159, loss = 0.56809421\n",
      "Iteration 160, loss = 0.56709975\n",
      "Iteration 161, loss = 0.56611737\n",
      "Iteration 162, loss = 0.56515756\n",
      "Iteration 163, loss = 0.56422032\n",
      "Iteration 164, loss = 0.56328833\n",
      "Iteration 165, loss = 0.56243351\n",
      "Iteration 166, loss = 0.56151882\n",
      "Iteration 167, loss = 0.56069105\n",
      "Iteration 168, loss = 0.55982314\n",
      "Iteration 169, loss = 0.55899017\n",
      "Iteration 170, loss = 0.55814772\n",
      "Iteration 171, loss = 0.55736220\n",
      "Iteration 172, loss = 0.55657632\n",
      "Iteration 173, loss = 0.55574034\n",
      "Iteration 174, loss = 0.55501454\n",
      "Iteration 175, loss = 0.55422898\n",
      "Iteration 176, loss = 0.55346630\n",
      "Iteration 177, loss = 0.55270354\n",
      "Iteration 178, loss = 0.55194960\n",
      "Iteration 179, loss = 0.55126329\n",
      "Iteration 180, loss = 0.55055753\n",
      "Iteration 181, loss = 0.54985986\n",
      "Iteration 182, loss = 0.54912867\n",
      "Iteration 183, loss = 0.54848024\n",
      "Iteration 184, loss = 0.54780583\n",
      "Iteration 185, loss = 0.54711726\n",
      "Iteration 186, loss = 0.54642522\n",
      "Iteration 187, loss = 0.54578106\n",
      "Iteration 188, loss = 0.54516195\n",
      "Iteration 189, loss = 0.54450124\n",
      "Iteration 190, loss = 0.54390669\n",
      "Iteration 191, loss = 0.54326173\n",
      "Iteration 192, loss = 0.54263470\n",
      "Iteration 193, loss = 0.54206613\n",
      "Iteration 194, loss = 0.54146397\n",
      "Iteration 195, loss = 0.54089234\n",
      "Iteration 196, loss = 0.54035513\n",
      "Iteration 197, loss = 0.53969203\n",
      "Iteration 198, loss = 0.53911877\n",
      "Iteration 199, loss = 0.53861391\n",
      "Iteration 200, loss = 0.53807149\n",
      "Iteration 201, loss = 0.53749264\n",
      "Iteration 202, loss = 0.53696001\n",
      "Iteration 203, loss = 0.53643832\n",
      "Iteration 204, loss = 0.53589229\n",
      "Iteration 205, loss = 0.53540074\n",
      "Iteration 206, loss = 0.53487309\n",
      "Iteration 207, loss = 0.53438360\n",
      "Iteration 208, loss = 0.53387537\n",
      "Iteration 209, loss = 0.53342426\n",
      "Iteration 210, loss = 0.53295052\n",
      "Iteration 211, loss = 0.53250259\n",
      "Iteration 212, loss = 0.53198328\n",
      "Iteration 213, loss = 0.53149427\n",
      "Iteration 214, loss = 0.53106628\n",
      "Iteration 215, loss = 0.53056400\n",
      "Iteration 216, loss = 0.53013346\n",
      "Iteration 217, loss = 0.52967414\n",
      "Iteration 218, loss = 0.52919985\n",
      "Iteration 219, loss = 0.52880912\n",
      "Iteration 220, loss = 0.52840437\n",
      "Iteration 221, loss = 0.52798569\n",
      "Iteration 222, loss = 0.52752475\n",
      "Iteration 223, loss = 0.52710790\n",
      "Iteration 224, loss = 0.52672232\n",
      "Iteration 225, loss = 0.52634922\n",
      "Iteration 226, loss = 0.52595069\n",
      "Iteration 227, loss = 0.52551398\n",
      "Iteration 228, loss = 0.52517089\n",
      "Iteration 229, loss = 0.52476696\n",
      "Iteration 230, loss = 0.52433719\n",
      "Iteration 231, loss = 0.52398239\n",
      "Iteration 232, loss = 0.52362045\n",
      "Iteration 233, loss = 0.52327286\n",
      "Iteration 234, loss = 0.52294077\n",
      "Iteration 235, loss = 0.52257903\n",
      "Iteration 236, loss = 0.52221809\n",
      "Iteration 237, loss = 0.52180801\n",
      "Iteration 238, loss = 0.52146987\n",
      "Iteration 239, loss = 0.52118077\n",
      "Iteration 240, loss = 0.52082257\n",
      "Iteration 241, loss = 0.52045122\n",
      "Iteration 242, loss = 0.52009730\n",
      "Iteration 243, loss = 0.51984527\n",
      "Iteration 244, loss = 0.51947341\n",
      "Iteration 245, loss = 0.51914985\n",
      "Iteration 246, loss = 0.51881664\n",
      "Iteration 247, loss = 0.51858701\n",
      "Iteration 248, loss = 0.51822212\n",
      "Iteration 249, loss = 0.51788899\n",
      "Iteration 250, loss = 0.51757780\n",
      "Iteration 251, loss = 0.51726641\n",
      "Iteration 252, loss = 0.51696576\n",
      "Iteration 253, loss = 0.51664291\n",
      "Iteration 254, loss = 0.51638349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.51610646\n",
      "Iteration 256, loss = 0.51578924\n",
      "Iteration 257, loss = 0.51547779\n",
      "Iteration 258, loss = 0.51523396\n",
      "Iteration 259, loss = 0.51496627\n",
      "Iteration 260, loss = 0.51465876\n",
      "Iteration 261, loss = 0.51442490\n",
      "Iteration 262, loss = 0.51409497\n",
      "Iteration 263, loss = 0.51384747\n",
      "Iteration 264, loss = 0.51356326\n",
      "Iteration 265, loss = 0.51328909\n",
      "Iteration 266, loss = 0.51308779\n",
      "Iteration 267, loss = 0.51278349\n",
      "Iteration 268, loss = 0.51255421\n",
      "Iteration 269, loss = 0.51225506\n",
      "Iteration 270, loss = 0.51202031\n",
      "Iteration 271, loss = 0.51182387\n",
      "Iteration 272, loss = 0.51152668\n",
      "Iteration 273, loss = 0.51126001\n",
      "Iteration 274, loss = 0.51107908\n",
      "Iteration 275, loss = 0.51073834\n",
      "Iteration 276, loss = 0.51053315\n",
      "Iteration 277, loss = 0.51029184\n",
      "Iteration 278, loss = 0.51007882\n",
      "Iteration 279, loss = 0.50980672\n",
      "Iteration 280, loss = 0.50954819\n",
      "Iteration 281, loss = 0.50942229\n",
      "Iteration 282, loss = 0.50911913\n",
      "Iteration 283, loss = 0.50885515\n",
      "Iteration 284, loss = 0.50869179\n",
      "Iteration 285, loss = 0.50845282\n",
      "Iteration 286, loss = 0.50815589\n",
      "Iteration 287, loss = 0.50801603\n",
      "Iteration 288, loss = 0.50772478\n",
      "Iteration 289, loss = 0.50756596\n",
      "Iteration 290, loss = 0.50739690\n",
      "Iteration 291, loss = 0.50717624\n",
      "Iteration 292, loss = 0.50691742\n",
      "Iteration 293, loss = 0.50671073\n",
      "Iteration 294, loss = 0.50654165\n",
      "Iteration 295, loss = 0.50623010\n",
      "Iteration 296, loss = 0.50609273\n",
      "Iteration 297, loss = 0.50585609\n",
      "Iteration 298, loss = 0.50570655\n",
      "Iteration 299, loss = 0.50541018\n",
      "Iteration 300, loss = 0.50526688\n",
      "Iteration 301, loss = 0.50504456\n",
      "Iteration 302, loss = 0.50490837\n",
      "Iteration 303, loss = 0.50465895\n",
      "Iteration 304, loss = 0.50450605\n",
      "Iteration 305, loss = 0.50429316\n",
      "Iteration 306, loss = 0.50411541\n",
      "Iteration 307, loss = 0.50391864\n",
      "Iteration 308, loss = 0.50378561\n",
      "Iteration 309, loss = 0.50355332\n",
      "Iteration 310, loss = 0.50330968\n",
      "Iteration 311, loss = 0.50320449\n",
      "Iteration 312, loss = 0.50296979\n",
      "Iteration 313, loss = 0.50278874\n",
      "Iteration 314, loss = 0.50255431\n",
      "Iteration 315, loss = 0.50237004\n",
      "Iteration 316, loss = 0.50228482\n",
      "Iteration 317, loss = 0.50205856\n",
      "Iteration 318, loss = 0.50191056\n",
      "Iteration 319, loss = 0.50169185\n",
      "Iteration 320, loss = 0.50160096\n",
      "Iteration 321, loss = 0.50137488\n",
      "Iteration 322, loss = 0.50123666\n",
      "Iteration 323, loss = 0.50107542\n",
      "Iteration 324, loss = 0.50084312\n",
      "Iteration 325, loss = 0.50070513\n",
      "Iteration 326, loss = 0.50050197\n",
      "Iteration 327, loss = 0.50032091\n",
      "Iteration 328, loss = 0.50022427\n",
      "Iteration 329, loss = 0.50002296\n",
      "Iteration 330, loss = 0.49986321\n",
      "Iteration 331, loss = 0.49966034\n",
      "Iteration 332, loss = 0.49956285\n",
      "Iteration 333, loss = 0.49927972\n",
      "Iteration 334, loss = 0.49922413\n",
      "Iteration 335, loss = 0.49903474\n",
      "Iteration 336, loss = 0.49887109\n",
      "Iteration 337, loss = 0.49873684\n",
      "Iteration 338, loss = 0.49858064\n",
      "Iteration 339, loss = 0.49841238\n",
      "Iteration 340, loss = 0.49821816\n",
      "Iteration 341, loss = 0.49807099\n",
      "Iteration 342, loss = 0.49783838\n",
      "Iteration 343, loss = 0.49773064\n",
      "Iteration 344, loss = 0.49760336\n",
      "Iteration 345, loss = 0.49752164\n",
      "Iteration 346, loss = 0.49727899\n",
      "Iteration 347, loss = 0.49721454\n",
      "Iteration 348, loss = 0.49706170\n",
      "Iteration 349, loss = 0.49686450\n",
      "Iteration 350, loss = 0.49675046\n",
      "Iteration 351, loss = 0.49657668\n",
      "Iteration 352, loss = 0.49637428\n",
      "Iteration 353, loss = 0.49625760\n",
      "Iteration 354, loss = 0.49611338\n",
      "Iteration 355, loss = 0.49598925\n",
      "Iteration 356, loss = 0.49581019\n",
      "Iteration 357, loss = 0.49573897\n",
      "Iteration 358, loss = 0.49548326\n",
      "Iteration 359, loss = 0.49542195\n",
      "Iteration 360, loss = 0.49525624\n",
      "Iteration 361, loss = 0.49516996\n",
      "Iteration 362, loss = 0.49495929\n",
      "Iteration 363, loss = 0.49487803\n",
      "Iteration 364, loss = 0.49466333\n",
      "Iteration 365, loss = 0.49458536\n",
      "Iteration 366, loss = 0.49445521\n",
      "Iteration 367, loss = 0.49426913\n",
      "Iteration 368, loss = 0.49417297\n",
      "Iteration 369, loss = 0.49397779\n",
      "Iteration 370, loss = 0.49393773\n",
      "Iteration 371, loss = 0.49369433\n",
      "Iteration 372, loss = 0.49363976\n",
      "Iteration 373, loss = 0.49348098\n",
      "Iteration 374, loss = 0.49328194\n",
      "Iteration 375, loss = 0.49328581\n",
      "Iteration 376, loss = 0.49305814\n",
      "Iteration 377, loss = 0.49293906\n",
      "Iteration 378, loss = 0.49291306\n",
      "Iteration 379, loss = 0.49269274\n",
      "Iteration 380, loss = 0.49250121\n",
      "Iteration 381, loss = 0.49251284\n",
      "Iteration 382, loss = 0.49230967\n",
      "Iteration 383, loss = 0.49216809\n",
      "Iteration 384, loss = 0.49197971\n",
      "Iteration 385, loss = 0.49191502\n",
      "Iteration 386, loss = 0.49190983\n",
      "Iteration 387, loss = 0.49167497\n",
      "Iteration 388, loss = 0.49154497\n",
      "Iteration 389, loss = 0.49145216\n",
      "Iteration 390, loss = 0.49125889\n",
      "Iteration 391, loss = 0.49110682\n",
      "Iteration 392, loss = 0.49106684\n",
      "Iteration 393, loss = 0.49102284\n",
      "Iteration 394, loss = 0.49082414\n",
      "Iteration 395, loss = 0.49068944\n",
      "Iteration 396, loss = 0.49065180\n",
      "Iteration 397, loss = 0.49042446\n",
      "Iteration 398, loss = 0.49032487\n",
      "Iteration 399, loss = 0.49021648\n",
      "Iteration 400, loss = 0.49012911\n",
      "Iteration 401, loss = 0.48997314\n",
      "Iteration 402, loss = 0.48987307\n",
      "Iteration 403, loss = 0.48971160\n",
      "Iteration 404, loss = 0.48959609\n",
      "Iteration 405, loss = 0.48956645\n",
      "Iteration 406, loss = 0.48936691\n",
      "Iteration 407, loss = 0.48923198\n",
      "Iteration 408, loss = 0.48914053\n",
      "Iteration 409, loss = 0.48897634\n",
      "Iteration 410, loss = 0.48888392\n",
      "Iteration 411, loss = 0.48881360\n",
      "Iteration 412, loss = 0.48865419\n",
      "Iteration 413, loss = 0.48860340\n",
      "Iteration 414, loss = 0.48847083\n",
      "Iteration 415, loss = 0.48827678\n",
      "Iteration 416, loss = 0.48822435\n",
      "Iteration 417, loss = 0.48808457\n",
      "Iteration 418, loss = 0.48797013\n",
      "Iteration 419, loss = 0.48791362\n",
      "Iteration 420, loss = 0.48779380\n",
      "Iteration 421, loss = 0.48767711\n",
      "Iteration 422, loss = 0.48761978\n",
      "Iteration 423, loss = 0.48739585\n",
      "Iteration 424, loss = 0.48738947\n",
      "Iteration 425, loss = 0.48721497\n",
      "Iteration 426, loss = 0.48713819\n",
      "Iteration 427, loss = 0.48698293\n",
      "Iteration 428, loss = 0.48690383\n",
      "Iteration 429, loss = 0.48675624\n",
      "Iteration 430, loss = 0.48664640\n",
      "Iteration 431, loss = 0.48655755\n",
      "Iteration 432, loss = 0.48646007\n",
      "Iteration 433, loss = 0.48633952\n",
      "Iteration 434, loss = 0.48629013\n",
      "Iteration 435, loss = 0.48614658\n",
      "Iteration 436, loss = 0.48607332\n",
      "Iteration 437, loss = 0.48590070\n",
      "Iteration 438, loss = 0.48582147\n",
      "Iteration 439, loss = 0.48570319\n",
      "Iteration 440, loss = 0.48555344\n",
      "Iteration 441, loss = 0.48556573\n",
      "Iteration 442, loss = 0.48535712\n",
      "Iteration 443, loss = 0.48530196\n",
      "Iteration 444, loss = 0.48515802\n",
      "Iteration 445, loss = 0.48515564\n",
      "Iteration 446, loss = 0.48497675\n",
      "Iteration 447, loss = 0.48490195\n",
      "Iteration 448, loss = 0.48475002\n",
      "Iteration 449, loss = 0.48476257\n",
      "Iteration 450, loss = 0.48457165\n",
      "Iteration 451, loss = 0.48455900\n",
      "Iteration 452, loss = 0.48444118\n",
      "Iteration 453, loss = 0.48425026\n",
      "Iteration 454, loss = 0.48418863\n",
      "Iteration 455, loss = 0.48409073\n",
      "Iteration 456, loss = 0.48399013\n",
      "Iteration 457, loss = 0.48381089\n",
      "Iteration 458, loss = 0.48384500\n",
      "Iteration 459, loss = 0.48364889\n",
      "Iteration 460, loss = 0.48353679\n",
      "Iteration 461, loss = 0.48343544\n",
      "Iteration 462, loss = 0.48335254\n",
      "Iteration 463, loss = 0.48327547\n",
      "Iteration 464, loss = 0.48317418\n",
      "Iteration 465, loss = 0.48300747\n",
      "Iteration 466, loss = 0.48303498\n",
      "Iteration 467, loss = 0.48285744\n",
      "Iteration 468, loss = 0.48280383\n",
      "Iteration 469, loss = 0.48272313\n",
      "Iteration 470, loss = 0.48263892\n",
      "Iteration 471, loss = 0.48255172\n",
      "Iteration 472, loss = 0.48245570\n",
      "Iteration 473, loss = 0.48229614\n",
      "Iteration 474, loss = 0.48227851\n",
      "Iteration 475, loss = 0.48222324\n",
      "Iteration 476, loss = 0.48201929\n",
      "Iteration 477, loss = 0.48188744\n",
      "Iteration 478, loss = 0.48186587\n",
      "Iteration 479, loss = 0.48171413\n",
      "Iteration 480, loss = 0.48161513\n",
      "Iteration 481, loss = 0.48155825\n",
      "Iteration 482, loss = 0.48144073\n",
      "Iteration 483, loss = 0.48140553\n",
      "Iteration 484, loss = 0.48128637\n",
      "Iteration 485, loss = 0.48121137\n",
      "Iteration 486, loss = 0.48113008\n",
      "Iteration 487, loss = 0.48099325\n",
      "Iteration 488, loss = 0.48089786\n",
      "Iteration 489, loss = 0.48076105\n",
      "Iteration 490, loss = 0.48069215\n",
      "Iteration 491, loss = 0.48070826\n",
      "Iteration 492, loss = 0.48062285\n",
      "Iteration 493, loss = 0.48044292\n",
      "Iteration 494, loss = 0.48032272\n",
      "Iteration 495, loss = 0.48025755\n",
      "Iteration 496, loss = 0.48018086\n",
      "Iteration 497, loss = 0.48020044\n",
      "Iteration 498, loss = 0.48005826\n",
      "Iteration 499, loss = 0.47987672\n",
      "Iteration 500, loss = 0.47987095\n",
      "Iteration 1, loss = 2.00022791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.86476258\n",
      "Iteration 3, loss = 1.75038663\n",
      "Iteration 4, loss = 1.65263112\n",
      "Iteration 5, loss = 1.56824890\n",
      "Iteration 6, loss = 1.49477582\n",
      "Iteration 7, loss = 1.42992341\n",
      "Iteration 8, loss = 1.37201317\n",
      "Iteration 9, loss = 1.31992556\n",
      "Iteration 10, loss = 1.27258260\n",
      "Iteration 11, loss = 1.22951168\n",
      "Iteration 12, loss = 1.19021997\n",
      "Iteration 13, loss = 1.15449960\n",
      "Iteration 14, loss = 1.12191636\n",
      "Iteration 15, loss = 1.09216645\n",
      "Iteration 16, loss = 1.06494542\n",
      "Iteration 17, loss = 1.04000788\n",
      "Iteration 18, loss = 1.01710806\n",
      "Iteration 19, loss = 0.99611027\n",
      "Iteration 20, loss = 0.97686448\n",
      "Iteration 21, loss = 0.95916315\n",
      "Iteration 22, loss = 0.94288095\n",
      "Iteration 23, loss = 0.92784782\n",
      "Iteration 24, loss = 0.91392561\n",
      "Iteration 25, loss = 0.90106633\n",
      "Iteration 26, loss = 0.88907800\n",
      "Iteration 27, loss = 0.87786903\n",
      "Iteration 28, loss = 0.86737812\n",
      "Iteration 29, loss = 0.85754885\n",
      "Iteration 30, loss = 0.84833909\n",
      "Iteration 31, loss = 0.83973057\n",
      "Iteration 32, loss = 0.83168207\n",
      "Iteration 33, loss = 0.82412341\n",
      "Iteration 34, loss = 0.81699710\n",
      "Iteration 35, loss = 0.81029269\n",
      "Iteration 36, loss = 0.80396937\n",
      "Iteration 37, loss = 0.79799833\n",
      "Iteration 38, loss = 0.79230434\n",
      "Iteration 39, loss = 0.78690275\n",
      "Iteration 40, loss = 0.78174543\n",
      "Iteration 41, loss = 0.77682924\n",
      "Iteration 42, loss = 0.77211035\n",
      "Iteration 43, loss = 0.76755155\n",
      "Iteration 44, loss = 0.76319112\n",
      "Iteration 45, loss = 0.75896040\n",
      "Iteration 46, loss = 0.75490846\n",
      "Iteration 47, loss = 0.75098253\n",
      "Iteration 48, loss = 0.74719310\n",
      "Iteration 49, loss = 0.74352967\n",
      "Iteration 50, loss = 0.73999856\n",
      "Iteration 51, loss = 0.73649396\n",
      "Iteration 52, loss = 0.73312626\n",
      "Iteration 53, loss = 0.72985279\n",
      "Iteration 54, loss = 0.72667642\n",
      "Iteration 55, loss = 0.72361546\n",
      "Iteration 56, loss = 0.72063614\n",
      "Iteration 57, loss = 0.71779390\n",
      "Iteration 58, loss = 0.71501962\n",
      "Iteration 59, loss = 0.71236258\n",
      "Iteration 60, loss = 0.70973106\n",
      "Iteration 61, loss = 0.70721027\n",
      "Iteration 62, loss = 0.70474159\n",
      "Iteration 63, loss = 0.70232282\n",
      "Iteration 64, loss = 0.69996169\n",
      "Iteration 65, loss = 0.69766002\n",
      "Iteration 66, loss = 0.69542277\n",
      "Iteration 67, loss = 0.69320106\n",
      "Iteration 68, loss = 0.69104036\n",
      "Iteration 69, loss = 0.68889753\n",
      "Iteration 70, loss = 0.68681815\n",
      "Iteration 71, loss = 0.68479563\n",
      "Iteration 72, loss = 0.68275990\n",
      "Iteration 73, loss = 0.68077707\n",
      "Iteration 74, loss = 0.67888371\n",
      "Iteration 75, loss = 0.67696411\n",
      "Iteration 76, loss = 0.67507917\n",
      "Iteration 77, loss = 0.67330016\n",
      "Iteration 78, loss = 0.67144124\n",
      "Iteration 79, loss = 0.66962618\n",
      "Iteration 80, loss = 0.66787722\n",
      "Iteration 81, loss = 0.66614415\n",
      "Iteration 82, loss = 0.66442734\n",
      "Iteration 83, loss = 0.66274009\n",
      "Iteration 84, loss = 0.66107067\n",
      "Iteration 85, loss = 0.65943176\n",
      "Iteration 86, loss = 0.65779807\n",
      "Iteration 87, loss = 0.65618863\n",
      "Iteration 88, loss = 0.65459238\n",
      "Iteration 89, loss = 0.65303350\n",
      "Iteration 90, loss = 0.65149273\n",
      "Iteration 91, loss = 0.64993191\n",
      "Iteration 92, loss = 0.64844159\n",
      "Iteration 93, loss = 0.64688657\n",
      "Iteration 94, loss = 0.64537602\n",
      "Iteration 95, loss = 0.64391024\n",
      "Iteration 96, loss = 0.64242362\n",
      "Iteration 97, loss = 0.64096567\n",
      "Iteration 98, loss = 0.63952228\n",
      "Iteration 99, loss = 0.63805206\n",
      "Iteration 100, loss = 0.63662593\n",
      "Iteration 101, loss = 0.63518501\n",
      "Iteration 102, loss = 0.63376441\n",
      "Iteration 103, loss = 0.63234965\n",
      "Iteration 104, loss = 0.63096326\n",
      "Iteration 105, loss = 0.62955020\n",
      "Iteration 106, loss = 0.62821261\n",
      "Iteration 107, loss = 0.62678512\n",
      "Iteration 108, loss = 0.62538753\n",
      "Iteration 109, loss = 0.62405138\n",
      "Iteration 110, loss = 0.62271455\n",
      "Iteration 111, loss = 0.62135450\n",
      "Iteration 112, loss = 0.61998856\n",
      "Iteration 113, loss = 0.61864482\n",
      "Iteration 114, loss = 0.61731769\n",
      "Iteration 115, loss = 0.61596817\n",
      "Iteration 116, loss = 0.61467140\n",
      "Iteration 117, loss = 0.61334314\n",
      "Iteration 118, loss = 0.61203093\n",
      "Iteration 119, loss = 0.61070710\n",
      "Iteration 120, loss = 0.60939876\n",
      "Iteration 121, loss = 0.60808832\n",
      "Iteration 122, loss = 0.60677733\n",
      "Iteration 123, loss = 0.60549248\n",
      "Iteration 124, loss = 0.60424688\n",
      "Iteration 125, loss = 0.60296555\n",
      "Iteration 126, loss = 0.60168642\n",
      "Iteration 127, loss = 0.60040524\n",
      "Iteration 128, loss = 0.59911455\n",
      "Iteration 129, loss = 0.59791627\n",
      "Iteration 130, loss = 0.59664753\n",
      "Iteration 131, loss = 0.59539027\n",
      "Iteration 132, loss = 0.59415064\n",
      "Iteration 133, loss = 0.59291295\n",
      "Iteration 134, loss = 0.59171805\n",
      "Iteration 135, loss = 0.59050780\n",
      "Iteration 136, loss = 0.58930344\n",
      "Iteration 137, loss = 0.58813061\n",
      "Iteration 138, loss = 0.58693510\n",
      "Iteration 139, loss = 0.58575799\n",
      "Iteration 140, loss = 0.58460809\n",
      "Iteration 141, loss = 0.58349650\n",
      "Iteration 142, loss = 0.58236344\n",
      "Iteration 143, loss = 0.58122921\n",
      "Iteration 144, loss = 0.58012480\n",
      "Iteration 145, loss = 0.57906020\n",
      "Iteration 146, loss = 0.57797302\n",
      "Iteration 147, loss = 0.57686927\n",
      "Iteration 148, loss = 0.57583443\n",
      "Iteration 149, loss = 0.57473371\n",
      "Iteration 150, loss = 0.57377075\n",
      "Iteration 151, loss = 0.57270079\n",
      "Iteration 152, loss = 0.57166488\n",
      "Iteration 153, loss = 0.57068227\n",
      "Iteration 154, loss = 0.56966830\n",
      "Iteration 155, loss = 0.56867615\n",
      "Iteration 156, loss = 0.56768860\n",
      "Iteration 157, loss = 0.56677901\n",
      "Iteration 158, loss = 0.56578451\n",
      "Iteration 159, loss = 0.56473951\n",
      "Iteration 160, loss = 0.56378852\n",
      "Iteration 161, loss = 0.56286417\n",
      "Iteration 162, loss = 0.56188165\n",
      "Iteration 163, loss = 0.56102899\n",
      "Iteration 164, loss = 0.56016244\n",
      "Iteration 165, loss = 0.55927883\n",
      "Iteration 166, loss = 0.55838393\n",
      "Iteration 167, loss = 0.55754185\n",
      "Iteration 168, loss = 0.55673892\n",
      "Iteration 169, loss = 0.55595283\n",
      "Iteration 170, loss = 0.55508000\n",
      "Iteration 171, loss = 0.55434449\n",
      "Iteration 172, loss = 0.55357631\n",
      "Iteration 173, loss = 0.55277029\n",
      "Iteration 174, loss = 0.55203420\n",
      "Iteration 175, loss = 0.55129303\n",
      "Iteration 176, loss = 0.55054600\n",
      "Iteration 177, loss = 0.54982277\n",
      "Iteration 178, loss = 0.54911666\n",
      "Iteration 179, loss = 0.54845231\n",
      "Iteration 180, loss = 0.54771075\n",
      "Iteration 181, loss = 0.54708298\n",
      "Iteration 182, loss = 0.54637114\n",
      "Iteration 183, loss = 0.54565610\n",
      "Iteration 184, loss = 0.54502280\n",
      "Iteration 185, loss = 0.54438504\n",
      "Iteration 186, loss = 0.54376496\n",
      "Iteration 187, loss = 0.54312392\n",
      "Iteration 188, loss = 0.54251921\n",
      "Iteration 189, loss = 0.54186640\n",
      "Iteration 190, loss = 0.54129346\n",
      "Iteration 191, loss = 0.54067557\n",
      "Iteration 192, loss = 0.54004637\n",
      "Iteration 193, loss = 0.53948018\n",
      "Iteration 194, loss = 0.53892420\n",
      "Iteration 195, loss = 0.53835348\n",
      "Iteration 196, loss = 0.53786192\n",
      "Iteration 197, loss = 0.53726663\n",
      "Iteration 198, loss = 0.53667926\n",
      "Iteration 199, loss = 0.53619057\n",
      "Iteration 200, loss = 0.53562983\n",
      "Iteration 201, loss = 0.53510187\n",
      "Iteration 202, loss = 0.53458356\n",
      "Iteration 203, loss = 0.53407067\n",
      "Iteration 204, loss = 0.53363249\n",
      "Iteration 205, loss = 0.53309317\n",
      "Iteration 206, loss = 0.53259103\n",
      "Iteration 207, loss = 0.53209794\n",
      "Iteration 208, loss = 0.53159720\n",
      "Iteration 209, loss = 0.53114925\n",
      "Iteration 210, loss = 0.53064063\n",
      "Iteration 211, loss = 0.53024191\n",
      "Iteration 212, loss = 0.52973195\n",
      "Iteration 213, loss = 0.52928516\n",
      "Iteration 214, loss = 0.52880187\n",
      "Iteration 215, loss = 0.52839447\n",
      "Iteration 216, loss = 0.52799405\n",
      "Iteration 217, loss = 0.52761465\n",
      "Iteration 218, loss = 0.52715678\n",
      "Iteration 219, loss = 0.52670396\n",
      "Iteration 220, loss = 0.52627677\n",
      "Iteration 221, loss = 0.52593052\n",
      "Iteration 222, loss = 0.52550620\n",
      "Iteration 223, loss = 0.52511126\n",
      "Iteration 224, loss = 0.52465378\n",
      "Iteration 225, loss = 0.52430248\n",
      "Iteration 226, loss = 0.52385948\n",
      "Iteration 227, loss = 0.52355600\n",
      "Iteration 228, loss = 0.52315789\n",
      "Iteration 229, loss = 0.52279062\n",
      "Iteration 230, loss = 0.52236679\n",
      "Iteration 231, loss = 0.52204677\n",
      "Iteration 232, loss = 0.52170412\n",
      "Iteration 233, loss = 0.52128500\n",
      "Iteration 234, loss = 0.52094850\n",
      "Iteration 235, loss = 0.52064830\n",
      "Iteration 236, loss = 0.52026914\n",
      "Iteration 237, loss = 0.51989877\n",
      "Iteration 238, loss = 0.51964993\n",
      "Iteration 239, loss = 0.51926255\n",
      "Iteration 240, loss = 0.51890187\n",
      "Iteration 241, loss = 0.51860781\n",
      "Iteration 242, loss = 0.51831129\n",
      "Iteration 243, loss = 0.51794768\n",
      "Iteration 244, loss = 0.51767918\n",
      "Iteration 245, loss = 0.51741312\n",
      "Iteration 246, loss = 0.51701791\n",
      "Iteration 247, loss = 0.51671470\n",
      "Iteration 248, loss = 0.51646013\n",
      "Iteration 249, loss = 0.51608445\n",
      "Iteration 250, loss = 0.51582729\n",
      "Iteration 251, loss = 0.51552518\n",
      "Iteration 252, loss = 0.51520747\n",
      "Iteration 253, loss = 0.51489125\n",
      "Iteration 254, loss = 0.51461735\n",
      "Iteration 255, loss = 0.51436073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.51406562\n",
      "Iteration 257, loss = 0.51375505\n",
      "Iteration 258, loss = 0.51353251\n",
      "Iteration 259, loss = 0.51328407\n",
      "Iteration 260, loss = 0.51297197\n",
      "Iteration 261, loss = 0.51267532\n",
      "Iteration 262, loss = 0.51244343\n",
      "Iteration 263, loss = 0.51214458\n",
      "Iteration 264, loss = 0.51192816\n",
      "Iteration 265, loss = 0.51169307\n",
      "Iteration 266, loss = 0.51140867\n",
      "Iteration 267, loss = 0.51112594\n",
      "Iteration 268, loss = 0.51085510\n",
      "Iteration 269, loss = 0.51065567\n",
      "Iteration 270, loss = 0.51038910\n",
      "Iteration 271, loss = 0.51015347\n",
      "Iteration 272, loss = 0.50993433\n",
      "Iteration 273, loss = 0.50970919\n",
      "Iteration 274, loss = 0.50943201\n",
      "Iteration 275, loss = 0.50916203\n",
      "Iteration 276, loss = 0.50896733\n",
      "Iteration 277, loss = 0.50864955\n",
      "Iteration 278, loss = 0.50851215\n",
      "Iteration 279, loss = 0.50827987\n",
      "Iteration 280, loss = 0.50800102\n",
      "Iteration 281, loss = 0.50775353\n",
      "Iteration 282, loss = 0.50752866\n",
      "Iteration 283, loss = 0.50736481\n",
      "Iteration 284, loss = 0.50712157\n",
      "Iteration 285, loss = 0.50694176\n",
      "Iteration 286, loss = 0.50668155\n",
      "Iteration 287, loss = 0.50648710\n",
      "Iteration 288, loss = 0.50624891\n",
      "Iteration 289, loss = 0.50600731\n",
      "Iteration 290, loss = 0.50589489\n",
      "Iteration 291, loss = 0.50563325\n",
      "Iteration 292, loss = 0.50543047\n",
      "Iteration 293, loss = 0.50518972\n",
      "Iteration 294, loss = 0.50498446\n",
      "Iteration 295, loss = 0.50485024\n",
      "Iteration 296, loss = 0.50463332\n",
      "Iteration 297, loss = 0.50446537\n",
      "Iteration 298, loss = 0.50423536\n",
      "Iteration 299, loss = 0.50404431\n",
      "Iteration 300, loss = 0.50377806\n",
      "Iteration 301, loss = 0.50362605\n",
      "Iteration 302, loss = 0.50345586\n",
      "Iteration 303, loss = 0.50318224\n",
      "Iteration 304, loss = 0.50304490\n",
      "Iteration 305, loss = 0.50288035\n",
      "Iteration 306, loss = 0.50264588\n",
      "Iteration 307, loss = 0.50248832\n",
      "Iteration 308, loss = 0.50229772\n",
      "Iteration 309, loss = 0.50214121\n",
      "Iteration 310, loss = 0.50189731\n",
      "Iteration 311, loss = 0.50174549\n",
      "Iteration 312, loss = 0.50150920\n",
      "Iteration 313, loss = 0.50145823\n",
      "Iteration 314, loss = 0.50117081\n",
      "Iteration 315, loss = 0.50106386\n",
      "Iteration 316, loss = 0.50078146\n",
      "Iteration 317, loss = 0.50069591\n",
      "Iteration 318, loss = 0.50048553\n",
      "Iteration 319, loss = 0.50029191\n",
      "Iteration 320, loss = 0.50013691\n",
      "Iteration 321, loss = 0.49995159\n",
      "Iteration 322, loss = 0.49979128\n",
      "Iteration 323, loss = 0.49957579\n",
      "Iteration 324, loss = 0.49944726\n",
      "Iteration 325, loss = 0.49928506\n",
      "Iteration 326, loss = 0.49913897\n",
      "Iteration 327, loss = 0.49895622\n",
      "Iteration 328, loss = 0.49882965\n",
      "Iteration 329, loss = 0.49864637\n",
      "Iteration 330, loss = 0.49850111\n",
      "Iteration 331, loss = 0.49824008\n",
      "Iteration 332, loss = 0.49819071\n",
      "Iteration 333, loss = 0.49798955\n",
      "Iteration 334, loss = 0.49777447\n",
      "Iteration 335, loss = 0.49764941\n",
      "Iteration 336, loss = 0.49749012\n",
      "Iteration 337, loss = 0.49737755\n",
      "Iteration 338, loss = 0.49717944\n",
      "Iteration 339, loss = 0.49700703\n",
      "Iteration 340, loss = 0.49685512\n",
      "Iteration 341, loss = 0.49671459\n",
      "Iteration 342, loss = 0.49653738\n",
      "Iteration 343, loss = 0.49641079\n",
      "Iteration 344, loss = 0.49624380\n",
      "Iteration 345, loss = 0.49610118\n",
      "Iteration 346, loss = 0.49602557\n",
      "Iteration 347, loss = 0.49586213\n",
      "Iteration 348, loss = 0.49571464\n",
      "Iteration 349, loss = 0.49552713\n",
      "Iteration 350, loss = 0.49538320\n",
      "Iteration 351, loss = 0.49523484\n",
      "Iteration 352, loss = 0.49509091\n",
      "Iteration 353, loss = 0.49490450\n",
      "Iteration 354, loss = 0.49479099\n",
      "Iteration 355, loss = 0.49468589\n",
      "Iteration 356, loss = 0.49455290\n",
      "Iteration 357, loss = 0.49439409\n",
      "Iteration 358, loss = 0.49427809\n",
      "Iteration 359, loss = 0.49410242\n",
      "Iteration 360, loss = 0.49396572\n",
      "Iteration 361, loss = 0.49382420\n",
      "Iteration 362, loss = 0.49366713\n",
      "Iteration 363, loss = 0.49355516\n",
      "Iteration 364, loss = 0.49339801\n",
      "Iteration 365, loss = 0.49327560\n",
      "Iteration 366, loss = 0.49313544\n",
      "Iteration 367, loss = 0.49298537\n",
      "Iteration 368, loss = 0.49287062\n",
      "Iteration 369, loss = 0.49270368\n",
      "Iteration 370, loss = 0.49253802\n",
      "Iteration 371, loss = 0.49244008\n",
      "Iteration 372, loss = 0.49237230\n",
      "Iteration 373, loss = 0.49228097\n",
      "Iteration 374, loss = 0.49200536\n",
      "Iteration 375, loss = 0.49199260\n",
      "Iteration 376, loss = 0.49183372\n",
      "Iteration 377, loss = 0.49170421\n",
      "Iteration 378, loss = 0.49154044\n",
      "Iteration 379, loss = 0.49142950\n",
      "Iteration 380, loss = 0.49134484\n",
      "Iteration 381, loss = 0.49119091\n",
      "Iteration 382, loss = 0.49103440\n",
      "Iteration 383, loss = 0.49090257\n",
      "Iteration 384, loss = 0.49078639\n",
      "Iteration 385, loss = 0.49055895\n",
      "Iteration 386, loss = 0.49046314\n",
      "Iteration 387, loss = 0.49036721\n",
      "Iteration 388, loss = 0.49026209\n",
      "Iteration 389, loss = 0.49011677\n",
      "Iteration 390, loss = 0.49001939\n",
      "Iteration 391, loss = 0.48992268\n",
      "Iteration 392, loss = 0.48974660\n",
      "Iteration 393, loss = 0.48963005\n",
      "Iteration 394, loss = 0.48950705\n",
      "Iteration 395, loss = 0.48943644\n",
      "Iteration 396, loss = 0.48936616\n",
      "Iteration 397, loss = 0.48919921\n",
      "Iteration 398, loss = 0.48906954\n",
      "Iteration 399, loss = 0.48899528\n",
      "Iteration 400, loss = 0.48882920\n",
      "Iteration 401, loss = 0.48870783\n",
      "Iteration 402, loss = 0.48855058\n",
      "Iteration 403, loss = 0.48848418\n",
      "Iteration 404, loss = 0.48831185\n",
      "Iteration 405, loss = 0.48823968\n",
      "Iteration 406, loss = 0.48815784\n",
      "Iteration 407, loss = 0.48802371\n",
      "Iteration 408, loss = 0.48789796\n",
      "Iteration 409, loss = 0.48787217\n",
      "Iteration 410, loss = 0.48763931\n",
      "Iteration 411, loss = 0.48754604\n",
      "Iteration 412, loss = 0.48735772\n",
      "Iteration 413, loss = 0.48730528\n",
      "Iteration 414, loss = 0.48724997\n",
      "Iteration 415, loss = 0.48701978\n",
      "Iteration 416, loss = 0.48696249\n",
      "Iteration 417, loss = 0.48686400\n",
      "Iteration 418, loss = 0.48670513\n",
      "Iteration 419, loss = 0.48666306\n",
      "Iteration 420, loss = 0.48650911\n",
      "Iteration 421, loss = 0.48644411\n",
      "Iteration 422, loss = 0.48628620\n",
      "Iteration 423, loss = 0.48617034\n",
      "Iteration 424, loss = 0.48609648\n",
      "Iteration 425, loss = 0.48602943\n",
      "Iteration 426, loss = 0.48587483\n",
      "Iteration 427, loss = 0.48579437\n",
      "Iteration 428, loss = 0.48563087\n",
      "Iteration 429, loss = 0.48553236\n",
      "Iteration 430, loss = 0.48541429\n",
      "Iteration 431, loss = 0.48538562\n",
      "Iteration 432, loss = 0.48518344\n",
      "Iteration 433, loss = 0.48511049\n",
      "Iteration 434, loss = 0.48503607\n",
      "Iteration 435, loss = 0.48488072\n",
      "Iteration 436, loss = 0.48478468\n",
      "Iteration 437, loss = 0.48470841\n",
      "Iteration 438, loss = 0.48456492\n",
      "Iteration 439, loss = 0.48449774\n",
      "Iteration 440, loss = 0.48439361\n",
      "Iteration 441, loss = 0.48422019\n",
      "Iteration 442, loss = 0.48413868\n",
      "Iteration 443, loss = 0.48405263\n",
      "Iteration 444, loss = 0.48398416\n",
      "Iteration 445, loss = 0.48386156\n",
      "Iteration 446, loss = 0.48384053\n",
      "Iteration 447, loss = 0.48369718\n",
      "Iteration 448, loss = 0.48355047\n",
      "Iteration 449, loss = 0.48347451\n",
      "Iteration 450, loss = 0.48333810\n",
      "Iteration 451, loss = 0.48328312\n",
      "Iteration 452, loss = 0.48314944\n",
      "Iteration 453, loss = 0.48308652\n",
      "Iteration 454, loss = 0.48295535\n",
      "Iteration 455, loss = 0.48282802\n",
      "Iteration 456, loss = 0.48273464\n",
      "Iteration 457, loss = 0.48261258\n",
      "Iteration 458, loss = 0.48254007\n",
      "Iteration 459, loss = 0.48247668\n",
      "Iteration 460, loss = 0.48238476\n",
      "Iteration 461, loss = 0.48222592\n",
      "Iteration 462, loss = 0.48219702\n",
      "Iteration 463, loss = 0.48209132\n",
      "Iteration 464, loss = 0.48204732\n",
      "Iteration 465, loss = 0.48184692\n",
      "Iteration 466, loss = 0.48173864\n",
      "Iteration 467, loss = 0.48174214\n",
      "Iteration 468, loss = 0.48163035\n",
      "Iteration 469, loss = 0.48151891\n",
      "Iteration 470, loss = 0.48136409\n",
      "Iteration 471, loss = 0.48129826\n",
      "Iteration 472, loss = 0.48124066\n",
      "Iteration 473, loss = 0.48117136\n",
      "Iteration 474, loss = 0.48099613\n",
      "Iteration 475, loss = 0.48091052\n",
      "Iteration 476, loss = 0.48086569\n",
      "Iteration 477, loss = 0.48076739\n",
      "Iteration 478, loss = 0.48061545\n",
      "Iteration 479, loss = 0.48054276\n",
      "Iteration 480, loss = 0.48041293\n",
      "Iteration 481, loss = 0.48034496\n",
      "Iteration 482, loss = 0.48033447\n",
      "Iteration 483, loss = 0.48013256\n",
      "Iteration 484, loss = 0.48011275\n",
      "Iteration 485, loss = 0.47998058\n",
      "Iteration 486, loss = 0.47989696\n",
      "Iteration 487, loss = 0.47982725\n",
      "Iteration 488, loss = 0.47972848\n",
      "Iteration 489, loss = 0.47952706\n",
      "Iteration 490, loss = 0.47957380\n",
      "Iteration 491, loss = 0.47952266\n",
      "Iteration 492, loss = 0.47936289\n",
      "Iteration 493, loss = 0.47928160\n",
      "Iteration 494, loss = 0.47918116\n",
      "Iteration 495, loss = 0.47904043\n",
      "Iteration 496, loss = 0.47896816\n",
      "Iteration 497, loss = 0.47893769\n",
      "Iteration 498, loss = 0.47884675\n",
      "Iteration 499, loss = 0.47876371\n",
      "Iteration 500, loss = 0.47871545\n",
      "Iteration 1, loss = 1.99914709\n",
      "Iteration 2, loss = 1.86371518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.74949475\n",
      "Iteration 4, loss = 1.65192501\n",
      "Iteration 5, loss = 1.56775082\n",
      "Iteration 6, loss = 1.49447485\n",
      "Iteration 7, loss = 1.42982178\n",
      "Iteration 8, loss = 1.37213962\n",
      "Iteration 9, loss = 1.32009832\n",
      "Iteration 10, loss = 1.27281036\n",
      "Iteration 11, loss = 1.22984525\n",
      "Iteration 12, loss = 1.19063438\n",
      "Iteration 13, loss = 1.15490437\n",
      "Iteration 14, loss = 1.12239407\n",
      "Iteration 15, loss = 1.09270407\n",
      "Iteration 16, loss = 1.06556136\n",
      "Iteration 17, loss = 1.04067973\n",
      "Iteration 18, loss = 1.01783944\n",
      "Iteration 19, loss = 0.99690505\n",
      "Iteration 20, loss = 0.97769583\n",
      "Iteration 21, loss = 0.95998907\n",
      "Iteration 22, loss = 0.94368332\n",
      "Iteration 23, loss = 0.92863829\n",
      "Iteration 24, loss = 0.91476652\n",
      "Iteration 25, loss = 0.90189447\n",
      "Iteration 26, loss = 0.88991053\n",
      "Iteration 27, loss = 0.87872537\n",
      "Iteration 28, loss = 0.86829424\n",
      "Iteration 29, loss = 0.85854242\n",
      "Iteration 30, loss = 0.84937444\n",
      "Iteration 31, loss = 0.84080824\n",
      "Iteration 32, loss = 0.83274561\n",
      "Iteration 33, loss = 0.82523764\n",
      "Iteration 34, loss = 0.81807617\n",
      "Iteration 35, loss = 0.81134548\n",
      "Iteration 36, loss = 0.80498109\n",
      "Iteration 37, loss = 0.79896361\n",
      "Iteration 38, loss = 0.79323201\n",
      "Iteration 39, loss = 0.78778653\n",
      "Iteration 40, loss = 0.78258280\n",
      "Iteration 41, loss = 0.77757496\n",
      "Iteration 42, loss = 0.77275728\n",
      "Iteration 43, loss = 0.76812829\n",
      "Iteration 44, loss = 0.76366528\n",
      "Iteration 45, loss = 0.75932760\n",
      "Iteration 46, loss = 0.75512131\n",
      "Iteration 47, loss = 0.75105438\n",
      "Iteration 48, loss = 0.74711040\n",
      "Iteration 49, loss = 0.74328658\n",
      "Iteration 50, loss = 0.73959350\n",
      "Iteration 51, loss = 0.73604353\n",
      "Iteration 52, loss = 0.73267930\n",
      "Iteration 53, loss = 0.72942626\n",
      "Iteration 54, loss = 0.72632226\n",
      "Iteration 55, loss = 0.72334486\n",
      "Iteration 56, loss = 0.72039804\n",
      "Iteration 57, loss = 0.71762434\n",
      "Iteration 58, loss = 0.71486074\n",
      "Iteration 59, loss = 0.71219601\n",
      "Iteration 60, loss = 0.70958028\n",
      "Iteration 61, loss = 0.70705912\n",
      "Iteration 62, loss = 0.70455728\n",
      "Iteration 63, loss = 0.70216797\n",
      "Iteration 64, loss = 0.69979458\n",
      "Iteration 65, loss = 0.69745937\n",
      "Iteration 66, loss = 0.69520967\n",
      "Iteration 67, loss = 0.69297808\n",
      "Iteration 68, loss = 0.69080617\n",
      "Iteration 69, loss = 0.68868708\n",
      "Iteration 70, loss = 0.68658654\n",
      "Iteration 71, loss = 0.68453506\n",
      "Iteration 72, loss = 0.68251840\n",
      "Iteration 73, loss = 0.68054231\n",
      "Iteration 74, loss = 0.67858996\n",
      "Iteration 75, loss = 0.67665092\n",
      "Iteration 76, loss = 0.67478259\n",
      "Iteration 77, loss = 0.67292509\n",
      "Iteration 78, loss = 0.67111420\n",
      "Iteration 79, loss = 0.66931098\n",
      "Iteration 80, loss = 0.66757715\n",
      "Iteration 81, loss = 0.66584863\n",
      "Iteration 82, loss = 0.66409893\n",
      "Iteration 83, loss = 0.66238616\n",
      "Iteration 84, loss = 0.66071730\n",
      "Iteration 85, loss = 0.65907847\n",
      "Iteration 86, loss = 0.65744743\n",
      "Iteration 87, loss = 0.65583363\n",
      "Iteration 88, loss = 0.65428348\n",
      "Iteration 89, loss = 0.65266506\n",
      "Iteration 90, loss = 0.65109978\n",
      "Iteration 91, loss = 0.64955802\n",
      "Iteration 92, loss = 0.64801118\n",
      "Iteration 93, loss = 0.64650593\n",
      "Iteration 94, loss = 0.64497105\n",
      "Iteration 95, loss = 0.64350032\n",
      "Iteration 96, loss = 0.64203092\n",
      "Iteration 97, loss = 0.64053978\n",
      "Iteration 98, loss = 0.63910200\n",
      "Iteration 99, loss = 0.63764998\n",
      "Iteration 100, loss = 0.63618451\n",
      "Iteration 101, loss = 0.63479543\n",
      "Iteration 102, loss = 0.63338843\n",
      "Iteration 103, loss = 0.63196433\n",
      "Iteration 104, loss = 0.63054893\n",
      "Iteration 105, loss = 0.62916740\n",
      "Iteration 106, loss = 0.62782078\n",
      "Iteration 107, loss = 0.62640347\n",
      "Iteration 108, loss = 0.62503255\n",
      "Iteration 109, loss = 0.62368979\n",
      "Iteration 110, loss = 0.62232182\n",
      "Iteration 111, loss = 0.62098352\n",
      "Iteration 112, loss = 0.61963360\n",
      "Iteration 113, loss = 0.61831836\n",
      "Iteration 114, loss = 0.61694332\n",
      "Iteration 115, loss = 0.61561020\n",
      "Iteration 116, loss = 0.61429620\n",
      "Iteration 117, loss = 0.61291322\n",
      "Iteration 118, loss = 0.61161987\n",
      "Iteration 119, loss = 0.61028881\n",
      "Iteration 120, loss = 0.60900549\n",
      "Iteration 121, loss = 0.60767045\n",
      "Iteration 122, loss = 0.60635839\n",
      "Iteration 123, loss = 0.60506831\n",
      "Iteration 124, loss = 0.60371720\n",
      "Iteration 125, loss = 0.60246636\n",
      "Iteration 126, loss = 0.60118995\n",
      "Iteration 127, loss = 0.59993321\n",
      "Iteration 128, loss = 0.59865481\n",
      "Iteration 129, loss = 0.59742002\n",
      "Iteration 130, loss = 0.59617079\n",
      "Iteration 131, loss = 0.59495550\n",
      "Iteration 132, loss = 0.59371829\n",
      "Iteration 133, loss = 0.59253909\n",
      "Iteration 134, loss = 0.59137903\n",
      "Iteration 135, loss = 0.59014343\n",
      "Iteration 136, loss = 0.58900696\n",
      "Iteration 137, loss = 0.58784198\n",
      "Iteration 138, loss = 0.58669007\n",
      "Iteration 139, loss = 0.58551122\n",
      "Iteration 140, loss = 0.58440164\n",
      "Iteration 141, loss = 0.58333908\n",
      "Iteration 142, loss = 0.58219394\n",
      "Iteration 143, loss = 0.58112233\n",
      "Iteration 144, loss = 0.58003632\n",
      "Iteration 145, loss = 0.57896819\n",
      "Iteration 146, loss = 0.57791914\n",
      "Iteration 147, loss = 0.57681780\n",
      "Iteration 148, loss = 0.57578222\n",
      "Iteration 149, loss = 0.57476114\n",
      "Iteration 150, loss = 0.57374743\n",
      "Iteration 151, loss = 0.57270224\n",
      "Iteration 152, loss = 0.57167638\n",
      "Iteration 153, loss = 0.57065379\n",
      "Iteration 154, loss = 0.56970629\n",
      "Iteration 155, loss = 0.56870039\n",
      "Iteration 156, loss = 0.56771008\n",
      "Iteration 157, loss = 0.56679203\n",
      "Iteration 158, loss = 0.56580391\n",
      "Iteration 159, loss = 0.56485930\n",
      "Iteration 160, loss = 0.56395808\n",
      "Iteration 161, loss = 0.56302794\n",
      "Iteration 162, loss = 0.56214347\n",
      "Iteration 163, loss = 0.56123873\n",
      "Iteration 164, loss = 0.56040428\n",
      "Iteration 165, loss = 0.55954435\n",
      "Iteration 166, loss = 0.55872113\n",
      "Iteration 167, loss = 0.55789764\n",
      "Iteration 168, loss = 0.55708029\n",
      "Iteration 169, loss = 0.55631701\n",
      "Iteration 170, loss = 0.55548660\n",
      "Iteration 171, loss = 0.55476129\n",
      "Iteration 172, loss = 0.55397017\n",
      "Iteration 173, loss = 0.55325912\n",
      "Iteration 174, loss = 0.55244595\n",
      "Iteration 175, loss = 0.55171080\n",
      "Iteration 176, loss = 0.55106322\n",
      "Iteration 177, loss = 0.55035122\n",
      "Iteration 178, loss = 0.54959892\n",
      "Iteration 179, loss = 0.54892021\n",
      "Iteration 180, loss = 0.54823424\n",
      "Iteration 181, loss = 0.54758204\n",
      "Iteration 182, loss = 0.54690148\n",
      "Iteration 183, loss = 0.54627315\n",
      "Iteration 184, loss = 0.54563271\n",
      "Iteration 185, loss = 0.54496751\n",
      "Iteration 186, loss = 0.54437373\n",
      "Iteration 187, loss = 0.54375350\n",
      "Iteration 188, loss = 0.54317200\n",
      "Iteration 189, loss = 0.54254352\n",
      "Iteration 190, loss = 0.54193892\n",
      "Iteration 191, loss = 0.54134806\n",
      "Iteration 192, loss = 0.54076649\n",
      "Iteration 193, loss = 0.54018734\n",
      "Iteration 194, loss = 0.53958332\n",
      "Iteration 195, loss = 0.53911144\n",
      "Iteration 196, loss = 0.53856985\n",
      "Iteration 197, loss = 0.53799986\n",
      "Iteration 198, loss = 0.53740762\n",
      "Iteration 199, loss = 0.53692020\n",
      "Iteration 200, loss = 0.53639507\n",
      "Iteration 201, loss = 0.53588924\n",
      "Iteration 202, loss = 0.53540331\n",
      "Iteration 203, loss = 0.53482160\n",
      "Iteration 204, loss = 0.53441103\n",
      "Iteration 205, loss = 0.53390523\n",
      "Iteration 206, loss = 0.53341208\n",
      "Iteration 207, loss = 0.53292151\n",
      "Iteration 208, loss = 0.53244845\n",
      "Iteration 209, loss = 0.53197444\n",
      "Iteration 210, loss = 0.53149333\n",
      "Iteration 211, loss = 0.53108872\n",
      "Iteration 212, loss = 0.53059951\n",
      "Iteration 213, loss = 0.53014923\n",
      "Iteration 214, loss = 0.52981860\n",
      "Iteration 215, loss = 0.52932041\n",
      "Iteration 216, loss = 0.52884315\n",
      "Iteration 217, loss = 0.52842412\n",
      "Iteration 218, loss = 0.52797831\n",
      "Iteration 219, loss = 0.52758500\n",
      "Iteration 220, loss = 0.52721143\n",
      "Iteration 221, loss = 0.52678778\n",
      "Iteration 222, loss = 0.52643110\n",
      "Iteration 223, loss = 0.52603850\n",
      "Iteration 224, loss = 0.52563667\n",
      "Iteration 225, loss = 0.52522202\n",
      "Iteration 226, loss = 0.52488292\n",
      "Iteration 227, loss = 0.52452664\n",
      "Iteration 228, loss = 0.52414508\n",
      "Iteration 229, loss = 0.52375392\n",
      "Iteration 230, loss = 0.52340342\n",
      "Iteration 231, loss = 0.52301118\n",
      "Iteration 232, loss = 0.52264571\n",
      "Iteration 233, loss = 0.52233469\n",
      "Iteration 234, loss = 0.52192308\n",
      "Iteration 235, loss = 0.52161137\n",
      "Iteration 236, loss = 0.52124163\n",
      "Iteration 237, loss = 0.52097892\n",
      "Iteration 238, loss = 0.52059474\n",
      "Iteration 239, loss = 0.52026633\n",
      "Iteration 240, loss = 0.51996133\n",
      "Iteration 241, loss = 0.51964139\n",
      "Iteration 242, loss = 0.51929774\n",
      "Iteration 243, loss = 0.51896031\n",
      "Iteration 244, loss = 0.51860934\n",
      "Iteration 245, loss = 0.51829077\n",
      "Iteration 246, loss = 0.51803583\n",
      "Iteration 247, loss = 0.51775853\n",
      "Iteration 248, loss = 0.51740120\n",
      "Iteration 249, loss = 0.51717346\n",
      "Iteration 250, loss = 0.51680205\n",
      "Iteration 251, loss = 0.51656504\n",
      "Iteration 252, loss = 0.51620341\n",
      "Iteration 253, loss = 0.51591616\n",
      "Iteration 254, loss = 0.51569707\n",
      "Iteration 255, loss = 0.51537578\n",
      "Iteration 256, loss = 0.51513884\n",
      "Iteration 257, loss = 0.51482226\n",
      "Iteration 258, loss = 0.51452480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259, loss = 0.51425392\n",
      "Iteration 260, loss = 0.51399606\n",
      "Iteration 261, loss = 0.51372184\n",
      "Iteration 262, loss = 0.51346535\n",
      "Iteration 263, loss = 0.51322488\n",
      "Iteration 264, loss = 0.51292789\n",
      "Iteration 265, loss = 0.51269593\n",
      "Iteration 266, loss = 0.51242538\n",
      "Iteration 267, loss = 0.51216358\n",
      "Iteration 268, loss = 0.51193219\n",
      "Iteration 269, loss = 0.51165623\n",
      "Iteration 270, loss = 0.51143352\n",
      "Iteration 271, loss = 0.51121155\n",
      "Iteration 272, loss = 0.51095888\n",
      "Iteration 273, loss = 0.51074909\n",
      "Iteration 274, loss = 0.51049320\n",
      "Iteration 275, loss = 0.51019329\n",
      "Iteration 276, loss = 0.50998098\n",
      "Iteration 277, loss = 0.50974157\n",
      "Iteration 278, loss = 0.50955647\n",
      "Iteration 279, loss = 0.50931188\n",
      "Iteration 280, loss = 0.50910114\n",
      "Iteration 281, loss = 0.50883625\n",
      "Iteration 282, loss = 0.50858484\n",
      "Iteration 283, loss = 0.50839191\n",
      "Iteration 284, loss = 0.50814705\n",
      "Iteration 285, loss = 0.50804042\n",
      "Iteration 286, loss = 0.50772704\n",
      "Iteration 287, loss = 0.50753804\n",
      "Iteration 288, loss = 0.50727405\n",
      "Iteration 289, loss = 0.50706292\n",
      "Iteration 290, loss = 0.50685574\n",
      "Iteration 291, loss = 0.50673472\n",
      "Iteration 292, loss = 0.50646175\n",
      "Iteration 293, loss = 0.50629025\n",
      "Iteration 294, loss = 0.50603923\n",
      "Iteration 295, loss = 0.50584716\n",
      "Iteration 296, loss = 0.50563279\n",
      "Iteration 297, loss = 0.50542706\n",
      "Iteration 298, loss = 0.50526137\n",
      "Iteration 299, loss = 0.50507949\n",
      "Iteration 300, loss = 0.50489393\n",
      "Iteration 301, loss = 0.50462769\n",
      "Iteration 302, loss = 0.50440745\n",
      "Iteration 303, loss = 0.50429706\n",
      "Iteration 304, loss = 0.50404813\n",
      "Iteration 305, loss = 0.50389973\n",
      "Iteration 306, loss = 0.50368203\n",
      "Iteration 307, loss = 0.50344287\n",
      "Iteration 308, loss = 0.50323867\n",
      "Iteration 309, loss = 0.50308140\n",
      "Iteration 310, loss = 0.50293605\n",
      "Iteration 311, loss = 0.50274852\n",
      "Iteration 312, loss = 0.50257018\n",
      "Iteration 313, loss = 0.50232999\n",
      "Iteration 314, loss = 0.50223740\n",
      "Iteration 315, loss = 0.50204534\n",
      "Iteration 316, loss = 0.50182813\n",
      "Iteration 317, loss = 0.50168931\n",
      "Iteration 318, loss = 0.50144017\n",
      "Iteration 319, loss = 0.50129456\n",
      "Iteration 320, loss = 0.50113200\n",
      "Iteration 321, loss = 0.50093105\n",
      "Iteration 322, loss = 0.50072291\n",
      "Iteration 323, loss = 0.50065222\n",
      "Iteration 324, loss = 0.50044313\n",
      "Iteration 325, loss = 0.50029149\n",
      "Iteration 326, loss = 0.50006326\n",
      "Iteration 327, loss = 0.49992949\n",
      "Iteration 328, loss = 0.49977452\n",
      "Iteration 329, loss = 0.49964204\n",
      "Iteration 330, loss = 0.49943665\n",
      "Iteration 331, loss = 0.49927922\n",
      "Iteration 332, loss = 0.49909311\n",
      "Iteration 333, loss = 0.49895673\n",
      "Iteration 334, loss = 0.49874538\n",
      "Iteration 335, loss = 0.49865185\n",
      "Iteration 336, loss = 0.49848981\n",
      "Iteration 337, loss = 0.49828031\n",
      "Iteration 338, loss = 0.49819669\n",
      "Iteration 339, loss = 0.49803141\n",
      "Iteration 340, loss = 0.49777798\n",
      "Iteration 341, loss = 0.49773339\n",
      "Iteration 342, loss = 0.49755109\n",
      "Iteration 343, loss = 0.49738370\n",
      "Iteration 344, loss = 0.49731106\n",
      "Iteration 345, loss = 0.49704630\n",
      "Iteration 346, loss = 0.49691583\n",
      "Iteration 347, loss = 0.49683413\n",
      "Iteration 348, loss = 0.49660802\n",
      "Iteration 349, loss = 0.49645865\n",
      "Iteration 350, loss = 0.49638398\n",
      "Iteration 351, loss = 0.49624359\n",
      "Iteration 352, loss = 0.49605036\n",
      "Iteration 353, loss = 0.49589360\n",
      "Iteration 354, loss = 0.49580294\n",
      "Iteration 355, loss = 0.49555765\n",
      "Iteration 356, loss = 0.49543259\n",
      "Iteration 357, loss = 0.49526998\n",
      "Iteration 358, loss = 0.49519298\n",
      "Iteration 359, loss = 0.49502579\n",
      "Iteration 360, loss = 0.49490280\n",
      "Iteration 361, loss = 0.49473522\n",
      "Iteration 362, loss = 0.49462082\n",
      "Iteration 363, loss = 0.49444698\n",
      "Iteration 364, loss = 0.49428387\n",
      "Iteration 365, loss = 0.49425284\n",
      "Iteration 366, loss = 0.49405319\n",
      "Iteration 367, loss = 0.49389241\n",
      "Iteration 368, loss = 0.49378648\n",
      "Iteration 369, loss = 0.49363097\n",
      "Iteration 370, loss = 0.49347685\n",
      "Iteration 371, loss = 0.49334000\n",
      "Iteration 372, loss = 0.49318821\n",
      "Iteration 373, loss = 0.49307591\n",
      "Iteration 374, loss = 0.49299052\n",
      "Iteration 375, loss = 0.49281018\n",
      "Iteration 376, loss = 0.49264651\n",
      "Iteration 377, loss = 0.49253962\n",
      "Iteration 378, loss = 0.49241913\n",
      "Iteration 379, loss = 0.49227804\n",
      "Iteration 380, loss = 0.49214297\n",
      "Iteration 381, loss = 0.49196770\n",
      "Iteration 382, loss = 0.49188099\n",
      "Iteration 383, loss = 0.49176315\n",
      "Iteration 384, loss = 0.49164870\n",
      "Iteration 385, loss = 0.49151863\n",
      "Iteration 386, loss = 0.49144268\n",
      "Iteration 387, loss = 0.49128325\n",
      "Iteration 388, loss = 0.49113318\n",
      "Iteration 389, loss = 0.49101349\n",
      "Iteration 390, loss = 0.49094394\n",
      "Iteration 391, loss = 0.49078309\n",
      "Iteration 392, loss = 0.49061365\n",
      "Iteration 393, loss = 0.49047177\n",
      "Iteration 394, loss = 0.49035937\n",
      "Iteration 395, loss = 0.49036211\n",
      "Iteration 396, loss = 0.49018170\n",
      "Iteration 397, loss = 0.49002184\n",
      "Iteration 398, loss = 0.48988101\n",
      "Iteration 399, loss = 0.48979003\n",
      "Iteration 400, loss = 0.48967352\n",
      "Iteration 401, loss = 0.48956467\n",
      "Iteration 402, loss = 0.48941680\n",
      "Iteration 403, loss = 0.48933374\n",
      "Iteration 404, loss = 0.48912619\n",
      "Iteration 405, loss = 0.48902265\n",
      "Iteration 406, loss = 0.48892877\n",
      "Iteration 407, loss = 0.48886635\n",
      "Iteration 408, loss = 0.48873057\n",
      "Iteration 409, loss = 0.48859392\n",
      "Iteration 410, loss = 0.48848480\n",
      "Iteration 411, loss = 0.48837451\n",
      "Iteration 412, loss = 0.48829045\n",
      "Iteration 413, loss = 0.48807362\n",
      "Iteration 414, loss = 0.48800749\n",
      "Iteration 415, loss = 0.48792052\n",
      "Iteration 416, loss = 0.48784278\n",
      "Iteration 417, loss = 0.48774027\n",
      "Iteration 418, loss = 0.48759888\n",
      "Iteration 419, loss = 0.48744355\n",
      "Iteration 420, loss = 0.48733997\n",
      "Iteration 421, loss = 0.48719212\n",
      "Iteration 422, loss = 0.48716530\n",
      "Iteration 423, loss = 0.48700973\n",
      "Iteration 424, loss = 0.48686660\n",
      "Iteration 425, loss = 0.48680946\n",
      "Iteration 426, loss = 0.48668415\n",
      "Iteration 427, loss = 0.48652881\n",
      "Iteration 428, loss = 0.48641957\n",
      "Iteration 429, loss = 0.48630614\n",
      "Iteration 430, loss = 0.48622186\n",
      "Iteration 431, loss = 0.48609924\n",
      "Iteration 432, loss = 0.48598579\n",
      "Iteration 433, loss = 0.48593594\n",
      "Iteration 434, loss = 0.48580077\n",
      "Iteration 435, loss = 0.48565403\n",
      "Iteration 436, loss = 0.48560326\n",
      "Iteration 437, loss = 0.48540817\n",
      "Iteration 438, loss = 0.48537401\n",
      "Iteration 439, loss = 0.48525743\n",
      "Iteration 440, loss = 0.48517096\n",
      "Iteration 441, loss = 0.48500885\n",
      "Iteration 442, loss = 0.48497812\n",
      "Iteration 443, loss = 0.48483026\n",
      "Iteration 444, loss = 0.48466983\n",
      "Iteration 445, loss = 0.48464800\n",
      "Iteration 446, loss = 0.48449094\n",
      "Iteration 447, loss = 0.48443889\n",
      "Iteration 448, loss = 0.48433673\n",
      "Iteration 449, loss = 0.48419713\n",
      "Iteration 450, loss = 0.48405622\n",
      "Iteration 451, loss = 0.48406707\n",
      "Iteration 452, loss = 0.48389790\n",
      "Iteration 453, loss = 0.48381059\n",
      "Iteration 454, loss = 0.48361727\n",
      "Iteration 455, loss = 0.48363028\n",
      "Iteration 456, loss = 0.48348723\n",
      "Iteration 457, loss = 0.48341765\n",
      "Iteration 458, loss = 0.48324237\n",
      "Iteration 459, loss = 0.48328896\n",
      "Iteration 460, loss = 0.48309072\n",
      "Iteration 461, loss = 0.48294419\n",
      "Iteration 462, loss = 0.48290125\n",
      "Iteration 463, loss = 0.48281136\n",
      "Iteration 464, loss = 0.48271548\n",
      "Iteration 465, loss = 0.48261326\n",
      "Iteration 466, loss = 0.48241655\n",
      "Iteration 467, loss = 0.48236013\n",
      "Iteration 468, loss = 0.48239477\n",
      "Iteration 469, loss = 0.48223015\n",
      "Iteration 470, loss = 0.48210204\n",
      "Iteration 471, loss = 0.48202843\n",
      "Iteration 472, loss = 0.48187866\n",
      "Iteration 473, loss = 0.48177690\n",
      "Iteration 474, loss = 0.48171603\n",
      "Iteration 475, loss = 0.48161900\n",
      "Iteration 476, loss = 0.48152260\n",
      "Iteration 477, loss = 0.48136260\n",
      "Iteration 478, loss = 0.48138058\n",
      "Iteration 479, loss = 0.48120125\n",
      "Iteration 480, loss = 0.48115772\n",
      "Iteration 481, loss = 0.48108550\n",
      "Iteration 482, loss = 0.48096174\n",
      "Iteration 483, loss = 0.48085614\n",
      "Iteration 484, loss = 0.48075498\n",
      "Iteration 485, loss = 0.48067590\n",
      "Iteration 486, loss = 0.48061152\n",
      "Iteration 487, loss = 0.48051549\n",
      "Iteration 488, loss = 0.48046005\n",
      "Iteration 489, loss = 0.48026604\n",
      "Iteration 490, loss = 0.48024800\n",
      "Iteration 491, loss = 0.48006997\n",
      "Iteration 492, loss = 0.47999112\n",
      "Iteration 493, loss = 0.47996965\n",
      "Iteration 494, loss = 0.47981910\n",
      "Iteration 495, loss = 0.47976157\n",
      "Iteration 496, loss = 0.47964148\n",
      "Iteration 497, loss = 0.47955505\n",
      "Iteration 498, loss = 0.47941870\n",
      "Iteration 499, loss = 0.47938849\n",
      "Iteration 500, loss = 0.47926331\n",
      "Iteration 1, loss = 1.99712427\n",
      "Iteration 2, loss = 1.86243433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.74853621\n",
      "Iteration 4, loss = 1.65130583\n",
      "Iteration 5, loss = 1.56736715\n",
      "Iteration 6, loss = 1.49427773\n",
      "Iteration 7, loss = 1.42968198\n",
      "Iteration 8, loss = 1.37211039\n",
      "Iteration 9, loss = 1.32015331\n",
      "Iteration 10, loss = 1.27300619\n",
      "Iteration 11, loss = 1.23014390\n",
      "Iteration 12, loss = 1.19108831\n",
      "Iteration 13, loss = 1.15550770\n",
      "Iteration 14, loss = 1.12313047\n",
      "Iteration 15, loss = 1.09351841\n",
      "Iteration 16, loss = 1.06637190\n",
      "Iteration 17, loss = 1.04151071\n",
      "Iteration 18, loss = 1.01867469\n",
      "Iteration 19, loss = 0.99771413\n",
      "Iteration 20, loss = 0.97847075\n",
      "Iteration 21, loss = 0.96075309\n",
      "Iteration 22, loss = 0.94446012\n",
      "Iteration 23, loss = 0.92942030\n",
      "Iteration 24, loss = 0.91553415\n",
      "Iteration 25, loss = 0.90259984\n",
      "Iteration 26, loss = 0.89060646\n",
      "Iteration 27, loss = 0.87941269\n",
      "Iteration 28, loss = 0.86894495\n",
      "Iteration 29, loss = 0.85914498\n",
      "Iteration 30, loss = 0.84999395\n",
      "Iteration 31, loss = 0.84139766\n",
      "Iteration 32, loss = 0.83335808\n",
      "Iteration 33, loss = 0.82581376\n",
      "Iteration 34, loss = 0.81872003\n",
      "Iteration 35, loss = 0.81203199\n",
      "Iteration 36, loss = 0.80574065\n",
      "Iteration 37, loss = 0.79973407\n",
      "Iteration 38, loss = 0.79405810\n",
      "Iteration 39, loss = 0.78865874\n",
      "Iteration 40, loss = 0.78346297\n",
      "Iteration 41, loss = 0.77849321\n",
      "Iteration 42, loss = 0.77369090\n",
      "Iteration 43, loss = 0.76910193\n",
      "Iteration 44, loss = 0.76463514\n",
      "Iteration 45, loss = 0.76034879\n",
      "Iteration 46, loss = 0.75618010\n",
      "Iteration 47, loss = 0.75214829\n",
      "Iteration 48, loss = 0.74823589\n",
      "Iteration 49, loss = 0.74446388\n",
      "Iteration 50, loss = 0.74082352\n",
      "Iteration 51, loss = 0.73732033\n",
      "Iteration 52, loss = 0.73398476\n",
      "Iteration 53, loss = 0.73079726\n",
      "Iteration 54, loss = 0.72772943\n",
      "Iteration 55, loss = 0.72473640\n",
      "Iteration 56, loss = 0.72185980\n",
      "Iteration 57, loss = 0.71906469\n",
      "Iteration 58, loss = 0.71634606\n",
      "Iteration 59, loss = 0.71375407\n",
      "Iteration 60, loss = 0.71116153\n",
      "Iteration 61, loss = 0.70866719\n",
      "Iteration 62, loss = 0.70623089\n",
      "Iteration 63, loss = 0.70384064\n",
      "Iteration 64, loss = 0.70154607\n",
      "Iteration 65, loss = 0.69925589\n",
      "Iteration 66, loss = 0.69703275\n",
      "Iteration 67, loss = 0.69486802\n",
      "Iteration 68, loss = 0.69274766\n",
      "Iteration 69, loss = 0.69065699\n",
      "Iteration 70, loss = 0.68860663\n",
      "Iteration 71, loss = 0.68661698\n",
      "Iteration 72, loss = 0.68465276\n",
      "Iteration 73, loss = 0.68271581\n",
      "Iteration 74, loss = 0.68086224\n",
      "Iteration 75, loss = 0.67900382\n",
      "Iteration 76, loss = 0.67714936\n",
      "Iteration 77, loss = 0.67537839\n",
      "Iteration 78, loss = 0.67364600\n",
      "Iteration 79, loss = 0.67188620\n",
      "Iteration 80, loss = 0.67021754\n",
      "Iteration 81, loss = 0.66853589\n",
      "Iteration 82, loss = 0.66688857\n",
      "Iteration 83, loss = 0.66526257\n",
      "Iteration 84, loss = 0.66368727\n",
      "Iteration 85, loss = 0.66209584\n",
      "Iteration 86, loss = 0.66058143\n",
      "Iteration 87, loss = 0.65901350\n",
      "Iteration 88, loss = 0.65752316\n",
      "Iteration 89, loss = 0.65603470\n",
      "Iteration 90, loss = 0.65455512\n",
      "Iteration 91, loss = 0.65313958\n",
      "Iteration 92, loss = 0.65173800\n",
      "Iteration 93, loss = 0.65029447\n",
      "Iteration 94, loss = 0.64889510\n",
      "Iteration 95, loss = 0.64752235\n",
      "Iteration 96, loss = 0.64615303\n",
      "Iteration 97, loss = 0.64480981\n",
      "Iteration 98, loss = 0.64349214\n",
      "Iteration 99, loss = 0.64216516\n",
      "Iteration 100, loss = 0.64081280\n",
      "Iteration 101, loss = 0.63951014\n",
      "Iteration 102, loss = 0.63821747\n",
      "Iteration 103, loss = 0.63690566\n",
      "Iteration 104, loss = 0.63565984\n",
      "Iteration 105, loss = 0.63438202\n",
      "Iteration 106, loss = 0.63315392\n",
      "Iteration 107, loss = 0.63189930\n",
      "Iteration 108, loss = 0.63068272\n",
      "Iteration 109, loss = 0.62938043\n",
      "Iteration 110, loss = 0.62813338\n",
      "Iteration 111, loss = 0.62694664\n",
      "Iteration 112, loss = 0.62569502\n",
      "Iteration 113, loss = 0.62449235\n",
      "Iteration 114, loss = 0.62329027\n",
      "Iteration 115, loss = 0.62211725\n",
      "Iteration 116, loss = 0.62090298\n",
      "Iteration 117, loss = 0.61970855\n",
      "Iteration 118, loss = 0.61855800\n",
      "Iteration 119, loss = 0.61734242\n",
      "Iteration 120, loss = 0.61618977\n",
      "Iteration 121, loss = 0.61501768\n",
      "Iteration 122, loss = 0.61387018\n",
      "Iteration 123, loss = 0.61269705\n",
      "Iteration 124, loss = 0.61158045\n",
      "Iteration 125, loss = 0.61041657\n",
      "Iteration 126, loss = 0.60928546\n",
      "Iteration 127, loss = 0.60811641\n",
      "Iteration 128, loss = 0.60697975\n",
      "Iteration 129, loss = 0.60590610\n",
      "Iteration 130, loss = 0.60476107\n",
      "Iteration 131, loss = 0.60363280\n",
      "Iteration 132, loss = 0.60247135\n",
      "Iteration 133, loss = 0.60132348\n",
      "Iteration 134, loss = 0.60023783\n",
      "Iteration 135, loss = 0.59911696\n",
      "Iteration 136, loss = 0.59801880\n",
      "Iteration 137, loss = 0.59691123\n",
      "Iteration 138, loss = 0.59577563\n",
      "Iteration 139, loss = 0.59471244\n",
      "Iteration 140, loss = 0.59356602\n",
      "Iteration 141, loss = 0.59242205\n",
      "Iteration 142, loss = 0.59140542\n",
      "Iteration 143, loss = 0.59026789\n",
      "Iteration 144, loss = 0.58914184\n",
      "Iteration 145, loss = 0.58803017\n",
      "Iteration 146, loss = 0.58697210\n",
      "Iteration 147, loss = 0.58590596\n",
      "Iteration 148, loss = 0.58486743\n",
      "Iteration 149, loss = 0.58376456\n",
      "Iteration 150, loss = 0.58275556\n",
      "Iteration 151, loss = 0.58169159\n",
      "Iteration 152, loss = 0.58068765\n",
      "Iteration 153, loss = 0.57966456\n",
      "Iteration 154, loss = 0.57870133\n",
      "Iteration 155, loss = 0.57768419\n",
      "Iteration 156, loss = 0.57669665\n",
      "Iteration 157, loss = 0.57575414\n",
      "Iteration 158, loss = 0.57477540\n",
      "Iteration 159, loss = 0.57383571\n",
      "Iteration 160, loss = 0.57292943\n",
      "Iteration 161, loss = 0.57197269\n",
      "Iteration 162, loss = 0.57101475\n",
      "Iteration 163, loss = 0.57011847\n",
      "Iteration 164, loss = 0.56922075\n",
      "Iteration 165, loss = 0.56825053\n",
      "Iteration 166, loss = 0.56734857\n",
      "Iteration 167, loss = 0.56649489\n",
      "Iteration 168, loss = 0.56560480\n",
      "Iteration 169, loss = 0.56475417\n",
      "Iteration 170, loss = 0.56389149\n",
      "Iteration 171, loss = 0.56308690\n",
      "Iteration 172, loss = 0.56225986\n",
      "Iteration 173, loss = 0.56144184\n",
      "Iteration 174, loss = 0.56065674\n",
      "Iteration 175, loss = 0.55988134\n",
      "Iteration 176, loss = 0.55912821\n",
      "Iteration 177, loss = 0.55835195\n",
      "Iteration 178, loss = 0.55762927\n",
      "Iteration 179, loss = 0.55686282\n",
      "Iteration 180, loss = 0.55615225\n",
      "Iteration 181, loss = 0.55546779\n",
      "Iteration 182, loss = 0.55477434\n",
      "Iteration 183, loss = 0.55405225\n",
      "Iteration 184, loss = 0.55331888\n",
      "Iteration 185, loss = 0.55273030\n",
      "Iteration 186, loss = 0.55196529\n",
      "Iteration 187, loss = 0.55133255\n",
      "Iteration 188, loss = 0.55069717\n",
      "Iteration 189, loss = 0.55006988\n",
      "Iteration 190, loss = 0.54939336\n",
      "Iteration 191, loss = 0.54875106\n",
      "Iteration 192, loss = 0.54815869\n",
      "Iteration 193, loss = 0.54753018\n",
      "Iteration 194, loss = 0.54691337\n",
      "Iteration 195, loss = 0.54640695\n",
      "Iteration 196, loss = 0.54576480\n",
      "Iteration 197, loss = 0.54517256\n",
      "Iteration 198, loss = 0.54454850\n",
      "Iteration 199, loss = 0.54405492\n",
      "Iteration 200, loss = 0.54348400\n",
      "Iteration 201, loss = 0.54290105\n",
      "Iteration 202, loss = 0.54233276\n",
      "Iteration 203, loss = 0.54183203\n",
      "Iteration 204, loss = 0.54126723\n",
      "Iteration 205, loss = 0.54076776\n",
      "Iteration 206, loss = 0.54027272\n",
      "Iteration 207, loss = 0.53973423\n",
      "Iteration 208, loss = 0.53921826\n",
      "Iteration 209, loss = 0.53869711\n",
      "Iteration 210, loss = 0.53820689\n",
      "Iteration 211, loss = 0.53775273\n",
      "Iteration 212, loss = 0.53724977\n",
      "Iteration 213, loss = 0.53672068\n",
      "Iteration 214, loss = 0.53627702\n",
      "Iteration 215, loss = 0.53584787\n",
      "Iteration 216, loss = 0.53537499\n",
      "Iteration 217, loss = 0.53492590\n",
      "Iteration 218, loss = 0.53444921\n",
      "Iteration 219, loss = 0.53399362\n",
      "Iteration 220, loss = 0.53357905\n",
      "Iteration 221, loss = 0.53310938\n",
      "Iteration 222, loss = 0.53267924\n",
      "Iteration 223, loss = 0.53224973\n",
      "Iteration 224, loss = 0.53186594\n",
      "Iteration 225, loss = 0.53144075\n",
      "Iteration 226, loss = 0.53099605\n",
      "Iteration 227, loss = 0.53063486\n",
      "Iteration 228, loss = 0.53024092\n",
      "Iteration 229, loss = 0.52983963\n",
      "Iteration 230, loss = 0.52948320\n",
      "Iteration 231, loss = 0.52909633\n",
      "Iteration 232, loss = 0.52863983\n",
      "Iteration 233, loss = 0.52823951\n",
      "Iteration 234, loss = 0.52791441\n",
      "Iteration 235, loss = 0.52756877\n",
      "Iteration 236, loss = 0.52720709\n",
      "Iteration 237, loss = 0.52685286\n",
      "Iteration 238, loss = 0.52648123\n",
      "Iteration 239, loss = 0.52608333\n",
      "Iteration 240, loss = 0.52576488\n",
      "Iteration 241, loss = 0.52540907\n",
      "Iteration 242, loss = 0.52498278\n",
      "Iteration 243, loss = 0.52476919\n",
      "Iteration 244, loss = 0.52435674\n",
      "Iteration 245, loss = 0.52404517\n",
      "Iteration 246, loss = 0.52375050\n",
      "Iteration 247, loss = 0.52343544\n",
      "Iteration 248, loss = 0.52307794\n",
      "Iteration 249, loss = 0.52277285\n",
      "Iteration 250, loss = 0.52245771\n",
      "Iteration 251, loss = 0.52212909\n",
      "Iteration 252, loss = 0.52183824\n",
      "Iteration 253, loss = 0.52148280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.52119513\n",
      "Iteration 255, loss = 0.52090391\n",
      "Iteration 256, loss = 0.52063139\n",
      "Iteration 257, loss = 0.52030586\n",
      "Iteration 258, loss = 0.52004208\n",
      "Iteration 259, loss = 0.51975622\n",
      "Iteration 260, loss = 0.51944537\n",
      "Iteration 261, loss = 0.51921094\n",
      "Iteration 262, loss = 0.51883640\n",
      "Iteration 263, loss = 0.51857346\n",
      "Iteration 264, loss = 0.51833182\n",
      "Iteration 265, loss = 0.51805499\n",
      "Iteration 266, loss = 0.51777901\n",
      "Iteration 267, loss = 0.51753031\n",
      "Iteration 268, loss = 0.51730135\n",
      "Iteration 269, loss = 0.51691414\n",
      "Iteration 270, loss = 0.51673406\n",
      "Iteration 271, loss = 0.51646199\n",
      "Iteration 272, loss = 0.51624755\n",
      "Iteration 273, loss = 0.51596133\n",
      "Iteration 274, loss = 0.51571133\n",
      "Iteration 275, loss = 0.51544804\n",
      "Iteration 276, loss = 0.51521918\n",
      "Iteration 277, loss = 0.51499111\n",
      "Iteration 278, loss = 0.51471710\n",
      "Iteration 279, loss = 0.51446929\n",
      "Iteration 280, loss = 0.51419249\n",
      "Iteration 281, loss = 0.51404147\n",
      "Iteration 282, loss = 0.51375214\n",
      "Iteration 283, loss = 0.51350390\n",
      "Iteration 284, loss = 0.51327989\n",
      "Iteration 285, loss = 0.51296320\n",
      "Iteration 286, loss = 0.51284799\n",
      "Iteration 287, loss = 0.51261233\n",
      "Iteration 288, loss = 0.51229588\n",
      "Iteration 289, loss = 0.51219408\n",
      "Iteration 290, loss = 0.51186923\n",
      "Iteration 291, loss = 0.51170641\n",
      "Iteration 292, loss = 0.51148004\n",
      "Iteration 293, loss = 0.51129047\n",
      "Iteration 294, loss = 0.51100763\n",
      "Iteration 295, loss = 0.51084465\n",
      "Iteration 296, loss = 0.51058198\n",
      "Iteration 297, loss = 0.51037554\n",
      "Iteration 298, loss = 0.51023661\n",
      "Iteration 299, loss = 0.50997742\n",
      "Iteration 300, loss = 0.50982825\n",
      "Iteration 301, loss = 0.50958623\n",
      "Iteration 302, loss = 0.50939807\n",
      "Iteration 303, loss = 0.50916480\n",
      "Iteration 304, loss = 0.50900857\n",
      "Iteration 305, loss = 0.50876910\n",
      "Iteration 306, loss = 0.50861138\n",
      "Iteration 307, loss = 0.50842325\n",
      "Iteration 308, loss = 0.50824120\n",
      "Iteration 309, loss = 0.50803749\n",
      "Iteration 310, loss = 0.50781610\n",
      "Iteration 311, loss = 0.50766235\n",
      "Iteration 312, loss = 0.50744142\n",
      "Iteration 313, loss = 0.50727134\n",
      "Iteration 314, loss = 0.50707057\n",
      "Iteration 315, loss = 0.50687898\n",
      "Iteration 316, loss = 0.50665932\n",
      "Iteration 317, loss = 0.50653496\n",
      "Iteration 318, loss = 0.50638096\n",
      "Iteration 319, loss = 0.50610247\n",
      "Iteration 320, loss = 0.50597362\n",
      "Iteration 321, loss = 0.50575845\n",
      "Iteration 322, loss = 0.50560750\n",
      "Iteration 323, loss = 0.50543123\n",
      "Iteration 324, loss = 0.50530703\n",
      "Iteration 325, loss = 0.50514278\n",
      "Iteration 326, loss = 0.50499967\n",
      "Iteration 327, loss = 0.50471510\n",
      "Iteration 328, loss = 0.50464276\n",
      "Iteration 329, loss = 0.50436566\n",
      "Iteration 330, loss = 0.50423344\n",
      "Iteration 331, loss = 0.50401932\n",
      "Iteration 332, loss = 0.50397976\n",
      "Iteration 333, loss = 0.50370086\n",
      "Iteration 334, loss = 0.50355206\n",
      "Iteration 335, loss = 0.50333767\n",
      "Iteration 336, loss = 0.50325866\n",
      "Iteration 337, loss = 0.50308600\n",
      "Iteration 338, loss = 0.50291005\n",
      "Iteration 339, loss = 0.50273855\n",
      "Iteration 340, loss = 0.50254175\n",
      "Iteration 341, loss = 0.50251721\n",
      "Iteration 342, loss = 0.50218034\n",
      "Iteration 343, loss = 0.50212460\n",
      "Iteration 344, loss = 0.50197937\n",
      "Iteration 345, loss = 0.50183674\n",
      "Iteration 346, loss = 0.50162388\n",
      "Iteration 347, loss = 0.50148936\n",
      "Iteration 348, loss = 0.50135088\n",
      "Iteration 349, loss = 0.50117139\n",
      "Iteration 350, loss = 0.50099702\n",
      "Iteration 351, loss = 0.50084955\n",
      "Iteration 352, loss = 0.50078927\n",
      "Iteration 353, loss = 0.50056980\n",
      "Iteration 354, loss = 0.50040670\n",
      "Iteration 355, loss = 0.50028719\n",
      "Iteration 356, loss = 0.50016503\n",
      "Iteration 357, loss = 0.50001130\n",
      "Iteration 358, loss = 0.49982263\n",
      "Iteration 359, loss = 0.49970593\n",
      "Iteration 360, loss = 0.49952695\n",
      "Iteration 361, loss = 0.49941961\n",
      "Iteration 362, loss = 0.49920483\n",
      "Iteration 363, loss = 0.49914872\n",
      "Iteration 364, loss = 0.49897075\n",
      "Iteration 365, loss = 0.49883988\n",
      "Iteration 366, loss = 0.49870351\n",
      "Iteration 367, loss = 0.49857706\n",
      "Iteration 368, loss = 0.49840256\n",
      "Iteration 369, loss = 0.49825503\n",
      "Iteration 370, loss = 0.49819431\n",
      "Iteration 371, loss = 0.49797069\n",
      "Iteration 372, loss = 0.49784425\n",
      "Iteration 373, loss = 0.49773800\n",
      "Iteration 374, loss = 0.49756124\n",
      "Iteration 375, loss = 0.49742430\n",
      "Iteration 376, loss = 0.49732812\n",
      "Iteration 377, loss = 0.49715276\n",
      "Iteration 378, loss = 0.49712543\n",
      "Iteration 379, loss = 0.49691912\n",
      "Iteration 380, loss = 0.49674913\n",
      "Iteration 381, loss = 0.49662019\n",
      "Iteration 382, loss = 0.49645831\n",
      "Iteration 383, loss = 0.49636169\n",
      "Iteration 384, loss = 0.49621926\n",
      "Iteration 385, loss = 0.49615486\n",
      "Iteration 386, loss = 0.49602334\n",
      "Iteration 387, loss = 0.49587994\n",
      "Iteration 388, loss = 0.49569276\n",
      "Iteration 389, loss = 0.49560201\n",
      "Iteration 390, loss = 0.49546255\n",
      "Iteration 391, loss = 0.49534767\n",
      "Iteration 392, loss = 0.49524969\n",
      "Iteration 393, loss = 0.49516272\n",
      "Iteration 394, loss = 0.49498306\n",
      "Iteration 395, loss = 0.49491268\n",
      "Iteration 396, loss = 0.49481456\n",
      "Iteration 397, loss = 0.49456466\n",
      "Iteration 398, loss = 0.49448579\n",
      "Iteration 399, loss = 0.49432602\n",
      "Iteration 400, loss = 0.49426097\n",
      "Iteration 401, loss = 0.49412603\n",
      "Iteration 402, loss = 0.49402057\n",
      "Iteration 403, loss = 0.49382238\n",
      "Iteration 404, loss = 0.49381977\n",
      "Iteration 405, loss = 0.49365017\n",
      "Iteration 406, loss = 0.49347389\n",
      "Iteration 407, loss = 0.49335552\n",
      "Iteration 408, loss = 0.49324553\n",
      "Iteration 409, loss = 0.49313757\n",
      "Iteration 410, loss = 0.49309171\n",
      "Iteration 411, loss = 0.49295585\n",
      "Iteration 412, loss = 0.49281850\n",
      "Iteration 413, loss = 0.49270416\n",
      "Iteration 414, loss = 0.49251916\n",
      "Iteration 415, loss = 0.49241874\n",
      "Iteration 416, loss = 0.49235355\n",
      "Iteration 417, loss = 0.49219911\n",
      "Iteration 418, loss = 0.49208339\n",
      "Iteration 419, loss = 0.49196248\n",
      "Iteration 420, loss = 0.49186070\n",
      "Iteration 421, loss = 0.49177342\n",
      "Iteration 422, loss = 0.49160190\n",
      "Iteration 423, loss = 0.49151528\n",
      "Iteration 424, loss = 0.49140839\n",
      "Iteration 425, loss = 0.49134815\n",
      "Iteration 426, loss = 0.49118445\n",
      "Iteration 427, loss = 0.49105759\n",
      "Iteration 428, loss = 0.49099603\n",
      "Iteration 429, loss = 0.49083918\n",
      "Iteration 430, loss = 0.49075698\n",
      "Iteration 431, loss = 0.49063261\n",
      "Iteration 432, loss = 0.49051158\n",
      "Iteration 433, loss = 0.49047009\n",
      "Iteration 434, loss = 0.49029283\n",
      "Iteration 435, loss = 0.49016114\n",
      "Iteration 436, loss = 0.49005466\n",
      "Iteration 437, loss = 0.49003465\n",
      "Iteration 438, loss = 0.48993165\n",
      "Iteration 439, loss = 0.48977281\n",
      "Iteration 440, loss = 0.48960112\n",
      "Iteration 441, loss = 0.48950001\n",
      "Iteration 442, loss = 0.48948912\n",
      "Iteration 443, loss = 0.48934035\n",
      "Iteration 444, loss = 0.48919544\n",
      "Iteration 445, loss = 0.48909825\n",
      "Iteration 446, loss = 0.48902123\n",
      "Iteration 447, loss = 0.48885674\n",
      "Iteration 448, loss = 0.48874884\n",
      "Iteration 449, loss = 0.48865584\n",
      "Iteration 450, loss = 0.48862884\n",
      "Iteration 451, loss = 0.48851144\n",
      "Iteration 452, loss = 0.48845410\n",
      "Iteration 453, loss = 0.48830289\n",
      "Iteration 454, loss = 0.48816898\n",
      "Iteration 455, loss = 0.48808167\n",
      "Iteration 456, loss = 0.48797492\n",
      "Iteration 457, loss = 0.48794463\n",
      "Iteration 458, loss = 0.48774509\n",
      "Iteration 459, loss = 0.48766483\n",
      "Iteration 460, loss = 0.48758384\n",
      "Iteration 461, loss = 0.48746503\n",
      "Iteration 462, loss = 0.48731677\n",
      "Iteration 463, loss = 0.48730710\n",
      "Iteration 464, loss = 0.48721829\n",
      "Iteration 465, loss = 0.48710163\n",
      "Iteration 466, loss = 0.48698100\n",
      "Iteration 467, loss = 0.48681280\n",
      "Iteration 468, loss = 0.48674752\n",
      "Iteration 469, loss = 0.48670366\n",
      "Iteration 470, loss = 0.48648021\n",
      "Iteration 471, loss = 0.48638307\n",
      "Iteration 472, loss = 0.48633864\n",
      "Iteration 473, loss = 0.48629169\n",
      "Iteration 474, loss = 0.48619383\n",
      "Iteration 475, loss = 0.48604669\n",
      "Iteration 476, loss = 0.48587556\n",
      "Iteration 477, loss = 0.48590362\n",
      "Iteration 478, loss = 0.48577347\n",
      "Iteration 479, loss = 0.48563076\n",
      "Iteration 480, loss = 0.48562168\n",
      "Iteration 481, loss = 0.48555708\n",
      "Iteration 482, loss = 0.48541182\n",
      "Iteration 483, loss = 0.48523610\n",
      "Iteration 484, loss = 0.48520658\n",
      "Iteration 485, loss = 0.48504505\n",
      "Iteration 486, loss = 0.48505198\n",
      "Iteration 487, loss = 0.48490311\n",
      "Iteration 488, loss = 0.48481009\n",
      "Iteration 489, loss = 0.48473270\n",
      "Iteration 490, loss = 0.48459174\n",
      "Iteration 491, loss = 0.48455805\n",
      "Iteration 492, loss = 0.48444362\n",
      "Iteration 493, loss = 0.48437045\n",
      "Iteration 494, loss = 0.48426077\n",
      "Iteration 495, loss = 0.48417742\n",
      "Iteration 496, loss = 0.48406511\n",
      "Iteration 497, loss = 0.48395405\n",
      "Iteration 498, loss = 0.48390078\n",
      "Iteration 499, loss = 0.48380192\n",
      "Iteration 500, loss = 0.48365295\n",
      "Iteration 1, loss = 2.00075729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.86463720\n",
      "Iteration 3, loss = 1.74967600\n",
      "Iteration 4, loss = 1.65157301\n",
      "Iteration 5, loss = 1.56719410\n",
      "Iteration 6, loss = 1.49376064\n",
      "Iteration 7, loss = 1.42937594\n",
      "Iteration 8, loss = 1.37199851\n",
      "Iteration 9, loss = 1.32031253\n",
      "Iteration 10, loss = 1.27339803\n",
      "Iteration 11, loss = 1.23068283\n",
      "Iteration 12, loss = 1.19164442\n",
      "Iteration 13, loss = 1.15606708\n",
      "Iteration 14, loss = 1.12361615\n",
      "Iteration 15, loss = 1.09396311\n",
      "Iteration 16, loss = 1.06679263\n",
      "Iteration 17, loss = 1.04182798\n",
      "Iteration 18, loss = 1.01890733\n",
      "Iteration 19, loss = 0.99788625\n",
      "Iteration 20, loss = 0.97859782\n",
      "Iteration 21, loss = 0.96088658\n",
      "Iteration 22, loss = 0.94459501\n",
      "Iteration 23, loss = 0.92959206\n",
      "Iteration 24, loss = 0.91568385\n",
      "Iteration 25, loss = 0.90279805\n",
      "Iteration 26, loss = 0.89085005\n",
      "Iteration 27, loss = 0.87968639\n",
      "Iteration 28, loss = 0.86924607\n",
      "Iteration 29, loss = 0.85946286\n",
      "Iteration 30, loss = 0.85029093\n",
      "Iteration 31, loss = 0.84170282\n",
      "Iteration 32, loss = 0.83364532\n",
      "Iteration 33, loss = 0.82608823\n",
      "Iteration 34, loss = 0.81896652\n",
      "Iteration 35, loss = 0.81225781\n",
      "Iteration 36, loss = 0.80592647\n",
      "Iteration 37, loss = 0.79993305\n",
      "Iteration 38, loss = 0.79421132\n",
      "Iteration 39, loss = 0.78878924\n",
      "Iteration 40, loss = 0.78360722\n",
      "Iteration 41, loss = 0.77864661\n",
      "Iteration 42, loss = 0.77388969\n",
      "Iteration 43, loss = 0.76930027\n",
      "Iteration 44, loss = 0.76488884\n",
      "Iteration 45, loss = 0.76059240\n",
      "Iteration 46, loss = 0.75645733\n",
      "Iteration 47, loss = 0.75246408\n",
      "Iteration 48, loss = 0.74858802\n",
      "Iteration 49, loss = 0.74482054\n",
      "Iteration 50, loss = 0.74115492\n",
      "Iteration 51, loss = 0.73760414\n",
      "Iteration 52, loss = 0.73419058\n",
      "Iteration 53, loss = 0.73081612\n",
      "Iteration 54, loss = 0.72763971\n",
      "Iteration 55, loss = 0.72458800\n",
      "Iteration 56, loss = 0.72158536\n",
      "Iteration 57, loss = 0.71876046\n",
      "Iteration 58, loss = 0.71601544\n",
      "Iteration 59, loss = 0.71332889\n",
      "Iteration 60, loss = 0.71072392\n",
      "Iteration 61, loss = 0.70816893\n",
      "Iteration 62, loss = 0.70571498\n",
      "Iteration 63, loss = 0.70329663\n",
      "Iteration 64, loss = 0.70093921\n",
      "Iteration 65, loss = 0.69861201\n",
      "Iteration 66, loss = 0.69635293\n",
      "Iteration 67, loss = 0.69413056\n",
      "Iteration 68, loss = 0.69196431\n",
      "Iteration 69, loss = 0.68984082\n",
      "Iteration 70, loss = 0.68776459\n",
      "Iteration 71, loss = 0.68573468\n",
      "Iteration 72, loss = 0.68373906\n",
      "Iteration 73, loss = 0.68175487\n",
      "Iteration 74, loss = 0.67980607\n",
      "Iteration 75, loss = 0.67794555\n",
      "Iteration 76, loss = 0.67604849\n",
      "Iteration 77, loss = 0.67422455\n",
      "Iteration 78, loss = 0.67242953\n",
      "Iteration 79, loss = 0.67066439\n",
      "Iteration 80, loss = 0.66886588\n",
      "Iteration 81, loss = 0.66714734\n",
      "Iteration 82, loss = 0.66543134\n",
      "Iteration 83, loss = 0.66374392\n",
      "Iteration 84, loss = 0.66209284\n",
      "Iteration 85, loss = 0.66046339\n",
      "Iteration 86, loss = 0.65886959\n",
      "Iteration 87, loss = 0.65725944\n",
      "Iteration 88, loss = 0.65568831\n",
      "Iteration 89, loss = 0.65414030\n",
      "Iteration 90, loss = 0.65258386\n",
      "Iteration 91, loss = 0.65108011\n",
      "Iteration 92, loss = 0.64955403\n",
      "Iteration 93, loss = 0.64808494\n",
      "Iteration 94, loss = 0.64659358\n",
      "Iteration 95, loss = 0.64513492\n",
      "Iteration 96, loss = 0.64370282\n",
      "Iteration 97, loss = 0.64225067\n",
      "Iteration 98, loss = 0.64085282\n",
      "Iteration 99, loss = 0.63941373\n",
      "Iteration 100, loss = 0.63801025\n",
      "Iteration 101, loss = 0.63661648\n",
      "Iteration 102, loss = 0.63522817\n",
      "Iteration 103, loss = 0.63382999\n",
      "Iteration 104, loss = 0.63245262\n",
      "Iteration 105, loss = 0.63107069\n",
      "Iteration 106, loss = 0.62975068\n",
      "Iteration 107, loss = 0.62838040\n",
      "Iteration 108, loss = 0.62703695\n",
      "Iteration 109, loss = 0.62569621\n",
      "Iteration 110, loss = 0.62435910\n",
      "Iteration 111, loss = 0.62304387\n",
      "Iteration 112, loss = 0.62175689\n",
      "Iteration 113, loss = 0.62042667\n",
      "Iteration 114, loss = 0.61917969\n",
      "Iteration 115, loss = 0.61785822\n",
      "Iteration 116, loss = 0.61657007\n",
      "Iteration 117, loss = 0.61530197\n",
      "Iteration 118, loss = 0.61402328\n",
      "Iteration 119, loss = 0.61274414\n",
      "Iteration 120, loss = 0.61148093\n",
      "Iteration 121, loss = 0.61020728\n",
      "Iteration 122, loss = 0.60894354\n",
      "Iteration 123, loss = 0.60767846\n",
      "Iteration 124, loss = 0.60642519\n",
      "Iteration 125, loss = 0.60519593\n",
      "Iteration 126, loss = 0.60391129\n",
      "Iteration 127, loss = 0.60268992\n",
      "Iteration 128, loss = 0.60143550\n",
      "Iteration 129, loss = 0.60024999\n",
      "Iteration 130, loss = 0.59899732\n",
      "Iteration 131, loss = 0.59775778\n",
      "Iteration 132, loss = 0.59650737\n",
      "Iteration 133, loss = 0.59533000\n",
      "Iteration 134, loss = 0.59407545\n",
      "Iteration 135, loss = 0.59288795\n",
      "Iteration 136, loss = 0.59169827\n",
      "Iteration 137, loss = 0.59050844\n",
      "Iteration 138, loss = 0.58929824\n",
      "Iteration 139, loss = 0.58814683\n",
      "Iteration 140, loss = 0.58698656\n",
      "Iteration 141, loss = 0.58582065\n",
      "Iteration 142, loss = 0.58463781\n",
      "Iteration 143, loss = 0.58349479\n",
      "Iteration 144, loss = 0.58240840\n",
      "Iteration 145, loss = 0.58128516\n",
      "Iteration 146, loss = 0.58023777\n",
      "Iteration 147, loss = 0.57912174\n",
      "Iteration 148, loss = 0.57806748\n",
      "Iteration 149, loss = 0.57701652\n",
      "Iteration 150, loss = 0.57589500\n",
      "Iteration 151, loss = 0.57485009\n",
      "Iteration 152, loss = 0.57381722\n",
      "Iteration 153, loss = 0.57279924\n",
      "Iteration 154, loss = 0.57179344\n",
      "Iteration 155, loss = 0.57072613\n",
      "Iteration 156, loss = 0.56971751\n",
      "Iteration 157, loss = 0.56875094\n",
      "Iteration 158, loss = 0.56781569\n",
      "Iteration 159, loss = 0.56684973\n",
      "Iteration 160, loss = 0.56589443\n",
      "Iteration 161, loss = 0.56499288\n",
      "Iteration 162, loss = 0.56406984\n",
      "Iteration 163, loss = 0.56318195\n",
      "Iteration 164, loss = 0.56230201\n",
      "Iteration 165, loss = 0.56145838\n",
      "Iteration 166, loss = 0.56056677\n",
      "Iteration 167, loss = 0.55980481\n",
      "Iteration 168, loss = 0.55892342\n",
      "Iteration 169, loss = 0.55812315\n",
      "Iteration 170, loss = 0.55730400\n",
      "Iteration 171, loss = 0.55652973\n",
      "Iteration 172, loss = 0.55575699\n",
      "Iteration 173, loss = 0.55495046\n",
      "Iteration 174, loss = 0.55419926\n",
      "Iteration 175, loss = 0.55345173\n",
      "Iteration 176, loss = 0.55271425\n",
      "Iteration 177, loss = 0.55196733\n",
      "Iteration 178, loss = 0.55122335\n",
      "Iteration 179, loss = 0.55050431\n",
      "Iteration 180, loss = 0.54985385\n",
      "Iteration 181, loss = 0.54912007\n",
      "Iteration 182, loss = 0.54843508\n",
      "Iteration 183, loss = 0.54780359\n",
      "Iteration 184, loss = 0.54706748\n",
      "Iteration 185, loss = 0.54644880\n",
      "Iteration 186, loss = 0.54582789\n",
      "Iteration 187, loss = 0.54516101\n",
      "Iteration 188, loss = 0.54453209\n",
      "Iteration 189, loss = 0.54390450\n",
      "Iteration 190, loss = 0.54329610\n",
      "Iteration 191, loss = 0.54269892\n",
      "Iteration 192, loss = 0.54203846\n",
      "Iteration 193, loss = 0.54151619\n",
      "Iteration 194, loss = 0.54087721\n",
      "Iteration 195, loss = 0.54033238\n",
      "Iteration 196, loss = 0.53975901\n",
      "Iteration 197, loss = 0.53919841\n",
      "Iteration 198, loss = 0.53864694\n",
      "Iteration 199, loss = 0.53807064\n",
      "Iteration 200, loss = 0.53755962\n",
      "Iteration 201, loss = 0.53701907\n",
      "Iteration 202, loss = 0.53649469\n",
      "Iteration 203, loss = 0.53598505\n",
      "Iteration 204, loss = 0.53545180\n",
      "Iteration 205, loss = 0.53494739\n",
      "Iteration 206, loss = 0.53441177\n",
      "Iteration 207, loss = 0.53396411\n",
      "Iteration 208, loss = 0.53343958\n",
      "Iteration 209, loss = 0.53294038\n",
      "Iteration 210, loss = 0.53256267\n",
      "Iteration 211, loss = 0.53203289\n",
      "Iteration 212, loss = 0.53152836\n",
      "Iteration 213, loss = 0.53111620\n",
      "Iteration 214, loss = 0.53066007\n",
      "Iteration 215, loss = 0.53019478\n",
      "Iteration 216, loss = 0.52979262\n",
      "Iteration 217, loss = 0.52932646\n",
      "Iteration 218, loss = 0.52891062\n",
      "Iteration 219, loss = 0.52848922\n",
      "Iteration 220, loss = 0.52801180\n",
      "Iteration 221, loss = 0.52767958\n",
      "Iteration 222, loss = 0.52724250\n",
      "Iteration 223, loss = 0.52680838\n",
      "Iteration 224, loss = 0.52639913\n",
      "Iteration 225, loss = 0.52600221\n",
      "Iteration 226, loss = 0.52561934\n",
      "Iteration 227, loss = 0.52525437\n",
      "Iteration 228, loss = 0.52484583\n",
      "Iteration 229, loss = 0.52443212\n",
      "Iteration 230, loss = 0.52412101\n",
      "Iteration 231, loss = 0.52372473\n",
      "Iteration 232, loss = 0.52334119\n",
      "Iteration 233, loss = 0.52299258\n",
      "Iteration 234, loss = 0.52267118\n",
      "Iteration 235, loss = 0.52228851\n",
      "Iteration 236, loss = 0.52194212\n",
      "Iteration 237, loss = 0.52159011\n",
      "Iteration 238, loss = 0.52120381\n",
      "Iteration 239, loss = 0.52086013\n",
      "Iteration 240, loss = 0.52047717\n",
      "Iteration 241, loss = 0.52020702\n",
      "Iteration 242, loss = 0.51994578\n",
      "Iteration 243, loss = 0.51953111\n",
      "Iteration 244, loss = 0.51919622\n",
      "Iteration 245, loss = 0.51891629\n",
      "Iteration 246, loss = 0.51865711\n",
      "Iteration 247, loss = 0.51835200\n",
      "Iteration 248, loss = 0.51805585\n",
      "Iteration 249, loss = 0.51766366\n",
      "Iteration 250, loss = 0.51737955\n",
      "Iteration 251, loss = 0.51704200\n",
      "Iteration 252, loss = 0.51679419\n",
      "Iteration 253, loss = 0.51646594\n",
      "Iteration 254, loss = 0.51617668\n",
      "Iteration 255, loss = 0.51584113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.51559147\n",
      "Iteration 257, loss = 0.51533683\n",
      "Iteration 258, loss = 0.51499438\n",
      "Iteration 259, loss = 0.51475942\n",
      "Iteration 260, loss = 0.51448080\n",
      "Iteration 261, loss = 0.51417982\n",
      "Iteration 262, loss = 0.51399769\n",
      "Iteration 263, loss = 0.51366172\n",
      "Iteration 264, loss = 0.51345585\n",
      "Iteration 265, loss = 0.51312357\n",
      "Iteration 266, loss = 0.51290065\n",
      "Iteration 267, loss = 0.51260228\n",
      "Iteration 268, loss = 0.51239006\n",
      "Iteration 269, loss = 0.51214853\n",
      "Iteration 270, loss = 0.51184769\n",
      "Iteration 271, loss = 0.51160823\n",
      "Iteration 272, loss = 0.51130874\n",
      "Iteration 273, loss = 0.51110072\n",
      "Iteration 274, loss = 0.51089044\n",
      "Iteration 275, loss = 0.51060065\n",
      "Iteration 276, loss = 0.51033334\n",
      "Iteration 277, loss = 0.51017637\n",
      "Iteration 278, loss = 0.50989067\n",
      "Iteration 279, loss = 0.50964501\n",
      "Iteration 280, loss = 0.50941052\n",
      "Iteration 281, loss = 0.50923241\n",
      "Iteration 282, loss = 0.50900964\n",
      "Iteration 283, loss = 0.50873371\n",
      "Iteration 284, loss = 0.50848952\n",
      "Iteration 285, loss = 0.50831277\n",
      "Iteration 286, loss = 0.50805393\n",
      "Iteration 287, loss = 0.50782825\n",
      "Iteration 288, loss = 0.50766918\n",
      "Iteration 289, loss = 0.50745867\n",
      "Iteration 290, loss = 0.50721180\n",
      "Iteration 291, loss = 0.50699482\n",
      "Iteration 292, loss = 0.50679968\n",
      "Iteration 293, loss = 0.50658740\n",
      "Iteration 294, loss = 0.50640047\n",
      "Iteration 295, loss = 0.50614316\n",
      "Iteration 296, loss = 0.50592995\n",
      "Iteration 297, loss = 0.50570614\n",
      "Iteration 298, loss = 0.50561615\n",
      "Iteration 299, loss = 0.50532820\n",
      "Iteration 300, loss = 0.50512575\n",
      "Iteration 301, loss = 0.50496855\n",
      "Iteration 302, loss = 0.50470925\n",
      "Iteration 303, loss = 0.50455253\n",
      "Iteration 304, loss = 0.50439153\n",
      "Iteration 305, loss = 0.50413906\n",
      "Iteration 306, loss = 0.50392398\n",
      "Iteration 307, loss = 0.50378209\n",
      "Iteration 308, loss = 0.50365035\n",
      "Iteration 309, loss = 0.50336654\n",
      "Iteration 310, loss = 0.50323200\n",
      "Iteration 311, loss = 0.50304418\n",
      "Iteration 312, loss = 0.50281123\n",
      "Iteration 313, loss = 0.50269306\n",
      "Iteration 314, loss = 0.50251965\n",
      "Iteration 315, loss = 0.50230181\n",
      "Iteration 316, loss = 0.50209860\n",
      "Iteration 317, loss = 0.50198433\n",
      "Iteration 318, loss = 0.50183652\n",
      "Iteration 319, loss = 0.50160024\n",
      "Iteration 320, loss = 0.50142083\n",
      "Iteration 321, loss = 0.50127064\n",
      "Iteration 322, loss = 0.50102703\n",
      "Iteration 323, loss = 0.50088691\n",
      "Iteration 324, loss = 0.50077713\n",
      "Iteration 325, loss = 0.50049821\n",
      "Iteration 326, loss = 0.50043622\n",
      "Iteration 327, loss = 0.50028118\n",
      "Iteration 328, loss = 0.50002442\n",
      "Iteration 329, loss = 0.49984202\n",
      "Iteration 330, loss = 0.49971799\n",
      "Iteration 331, loss = 0.49953780\n",
      "Iteration 332, loss = 0.49938806\n",
      "Iteration 333, loss = 0.49919653\n",
      "Iteration 334, loss = 0.49906966\n",
      "Iteration 335, loss = 0.49896786\n",
      "Iteration 336, loss = 0.49875412\n",
      "Iteration 337, loss = 0.49860009\n",
      "Iteration 338, loss = 0.49840961\n",
      "Iteration 339, loss = 0.49833090\n",
      "Iteration 340, loss = 0.49807412\n",
      "Iteration 341, loss = 0.49791038\n",
      "Iteration 342, loss = 0.49777597\n",
      "Iteration 343, loss = 0.49764635\n",
      "Iteration 344, loss = 0.49750954\n",
      "Iteration 345, loss = 0.49733435\n",
      "Iteration 346, loss = 0.49711727\n",
      "Iteration 347, loss = 0.49702502\n",
      "Iteration 348, loss = 0.49682869\n",
      "Iteration 349, loss = 0.49671529\n",
      "Iteration 350, loss = 0.49655176\n",
      "Iteration 351, loss = 0.49642523\n",
      "Iteration 352, loss = 0.49626812\n",
      "Iteration 353, loss = 0.49614925\n",
      "Iteration 354, loss = 0.49596075\n",
      "Iteration 355, loss = 0.49584316\n",
      "Iteration 356, loss = 0.49569939\n",
      "Iteration 357, loss = 0.49552187\n",
      "Iteration 358, loss = 0.49533853\n",
      "Iteration 359, loss = 0.49526122\n",
      "Iteration 360, loss = 0.49510240\n",
      "Iteration 361, loss = 0.49500422\n",
      "Iteration 362, loss = 0.49478087\n",
      "Iteration 363, loss = 0.49464863\n",
      "Iteration 364, loss = 0.49455543\n",
      "Iteration 365, loss = 0.49434819\n",
      "Iteration 366, loss = 0.49425193\n",
      "Iteration 367, loss = 0.49413556\n",
      "Iteration 368, loss = 0.49401286\n",
      "Iteration 369, loss = 0.49385602\n",
      "Iteration 370, loss = 0.49372055\n",
      "Iteration 371, loss = 0.49358842\n",
      "Iteration 372, loss = 0.49341299\n",
      "Iteration 373, loss = 0.49333239\n",
      "Iteration 374, loss = 0.49317497\n",
      "Iteration 375, loss = 0.49303633\n",
      "Iteration 376, loss = 0.49299094\n",
      "Iteration 377, loss = 0.49274550\n",
      "Iteration 378, loss = 0.49259124\n",
      "Iteration 379, loss = 0.49250759\n",
      "Iteration 380, loss = 0.49241904\n",
      "Iteration 381, loss = 0.49228156\n",
      "Iteration 382, loss = 0.49220130\n",
      "Iteration 383, loss = 0.49205114\n",
      "Iteration 384, loss = 0.49185244\n",
      "Iteration 385, loss = 0.49175870\n",
      "Iteration 386, loss = 0.49156922\n",
      "Iteration 387, loss = 0.49145936\n",
      "Iteration 388, loss = 0.49131172\n",
      "Iteration 389, loss = 0.49128455\n",
      "Iteration 390, loss = 0.49116921\n",
      "Iteration 391, loss = 0.49096266\n",
      "Iteration 392, loss = 0.49092825\n",
      "Iteration 393, loss = 0.49072418\n",
      "Iteration 394, loss = 0.49065377\n",
      "Iteration 395, loss = 0.49050832\n",
      "Iteration 396, loss = 0.49040663\n",
      "Iteration 397, loss = 0.49026698\n",
      "Iteration 398, loss = 0.49015598\n",
      "Iteration 399, loss = 0.49000130\n",
      "Iteration 400, loss = 0.48987782\n",
      "Iteration 401, loss = 0.48984736\n",
      "Iteration 402, loss = 0.48962118\n",
      "Iteration 403, loss = 0.48945707\n",
      "Iteration 404, loss = 0.48939824\n",
      "Iteration 405, loss = 0.48929271\n",
      "Iteration 406, loss = 0.48913518\n",
      "Iteration 407, loss = 0.48901543\n",
      "Iteration 408, loss = 0.48890732\n",
      "Iteration 409, loss = 0.48875134\n",
      "Iteration 410, loss = 0.48871945\n",
      "Iteration 411, loss = 0.48857215\n",
      "Iteration 412, loss = 0.48847926\n",
      "Iteration 413, loss = 0.48831272\n",
      "Iteration 414, loss = 0.48817100\n",
      "Iteration 415, loss = 0.48812206\n",
      "Iteration 416, loss = 0.48796976\n",
      "Iteration 417, loss = 0.48787352\n",
      "Iteration 418, loss = 0.48775638\n",
      "Iteration 419, loss = 0.48763905\n",
      "Iteration 420, loss = 0.48752126\n",
      "Iteration 421, loss = 0.48744415\n",
      "Iteration 422, loss = 0.48730293\n",
      "Iteration 423, loss = 0.48717020\n",
      "Iteration 424, loss = 0.48710459\n",
      "Iteration 425, loss = 0.48698432\n",
      "Iteration 426, loss = 0.48687110\n",
      "Iteration 427, loss = 0.48677697\n",
      "Iteration 428, loss = 0.48665719\n",
      "Iteration 429, loss = 0.48658896\n",
      "Iteration 430, loss = 0.48639277\n",
      "Iteration 431, loss = 0.48634856\n",
      "Iteration 432, loss = 0.48626574\n",
      "Iteration 433, loss = 0.48606650\n",
      "Iteration 434, loss = 0.48599758\n",
      "Iteration 435, loss = 0.48587359\n",
      "Iteration 436, loss = 0.48577244\n",
      "Iteration 437, loss = 0.48567088\n",
      "Iteration 438, loss = 0.48562381\n",
      "Iteration 439, loss = 0.48546614\n",
      "Iteration 440, loss = 0.48533135\n",
      "Iteration 441, loss = 0.48519616\n",
      "Iteration 442, loss = 0.48519719\n",
      "Iteration 443, loss = 0.48499764\n",
      "Iteration 444, loss = 0.48498214\n",
      "Iteration 445, loss = 0.48491184\n",
      "Iteration 446, loss = 0.48476759\n",
      "Iteration 447, loss = 0.48463139\n",
      "Iteration 448, loss = 0.48452322\n",
      "Iteration 449, loss = 0.48447913\n",
      "Iteration 450, loss = 0.48433295\n",
      "Iteration 451, loss = 0.48429624\n",
      "Iteration 452, loss = 0.48412578\n",
      "Iteration 453, loss = 0.48406696\n",
      "Iteration 454, loss = 0.48381571\n",
      "Iteration 455, loss = 0.48382717\n",
      "Iteration 456, loss = 0.48377613\n",
      "Iteration 457, loss = 0.48356100\n",
      "Iteration 458, loss = 0.48361228\n",
      "Iteration 459, loss = 0.48337299\n",
      "Iteration 460, loss = 0.48336342\n",
      "Iteration 461, loss = 0.48323685\n",
      "Iteration 462, loss = 0.48305690\n",
      "Iteration 463, loss = 0.48299990\n",
      "Iteration 464, loss = 0.48284385\n",
      "Iteration 465, loss = 0.48273170\n",
      "Iteration 466, loss = 0.48273441\n",
      "Iteration 467, loss = 0.48260630\n",
      "Iteration 468, loss = 0.48250480\n",
      "Iteration 469, loss = 0.48243104\n",
      "Iteration 470, loss = 0.48230299\n",
      "Iteration 471, loss = 0.48223210\n",
      "Iteration 472, loss = 0.48203265\n",
      "Iteration 473, loss = 0.48209848\n",
      "Iteration 474, loss = 0.48191881\n",
      "Iteration 475, loss = 0.48181349\n",
      "Iteration 476, loss = 0.48170820\n",
      "Iteration 477, loss = 0.48156294\n",
      "Iteration 478, loss = 0.48149474\n",
      "Iteration 479, loss = 0.48153115\n",
      "Iteration 480, loss = 0.48132170\n",
      "Iteration 481, loss = 0.48124056\n",
      "Iteration 482, loss = 0.48114143\n",
      "Iteration 483, loss = 0.48105239\n",
      "Iteration 484, loss = 0.48092368\n",
      "Iteration 485, loss = 0.48083851\n",
      "Iteration 486, loss = 0.48072413\n",
      "Iteration 487, loss = 0.48071796\n",
      "Iteration 488, loss = 0.48055902\n",
      "Iteration 489, loss = 0.48050057\n",
      "Iteration 490, loss = 0.48037323\n",
      "Iteration 491, loss = 0.48030606\n",
      "Iteration 492, loss = 0.48014697\n",
      "Iteration 493, loss = 0.48010818\n",
      "Iteration 494, loss = 0.48007123\n",
      "Iteration 495, loss = 0.47995383\n",
      "Iteration 496, loss = 0.47986379\n",
      "Iteration 497, loss = 0.47968724\n",
      "Iteration 498, loss = 0.47962812\n",
      "Iteration 499, loss = 0.47956123\n",
      "Iteration 500, loss = 0.47945946\n",
      "Iteration 1, loss = 2.06244943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.83884895\n",
      "Iteration 3, loss = 1.66588505\n",
      "Iteration 4, loss = 1.51731856\n",
      "Iteration 5, loss = 1.37204029\n",
      "Iteration 6, loss = 1.23310257\n",
      "Iteration 7, loss = 1.11639621\n",
      "Iteration 8, loss = 1.02368088\n",
      "Iteration 9, loss = 0.94976522\n",
      "Iteration 10, loss = 0.88956015\n",
      "Iteration 11, loss = 0.84046290\n",
      "Iteration 12, loss = 0.80053760\n",
      "Iteration 13, loss = 0.76775453\n",
      "Iteration 14, loss = 0.74042939\n",
      "Iteration 15, loss = 0.71678241\n",
      "Iteration 16, loss = 0.69630203\n",
      "Iteration 17, loss = 0.67816116\n",
      "Iteration 18, loss = 0.66156729\n",
      "Iteration 19, loss = 0.64667966\n",
      "Iteration 20, loss = 0.63276678\n",
      "Iteration 21, loss = 0.61997434\n",
      "Iteration 22, loss = 0.60789091\n",
      "Iteration 23, loss = 0.59658247\n",
      "Iteration 24, loss = 0.58603984\n",
      "Iteration 25, loss = 0.57601496\n",
      "Iteration 26, loss = 0.56660929\n",
      "Iteration 27, loss = 0.55780146\n",
      "Iteration 28, loss = 0.54960027\n",
      "Iteration 29, loss = 0.54165720\n",
      "Iteration 30, loss = 0.53483562\n",
      "Iteration 31, loss = 0.52771459\n",
      "Iteration 32, loss = 0.52114953\n",
      "Iteration 33, loss = 0.51558595\n",
      "Iteration 34, loss = 0.51016446\n",
      "Iteration 35, loss = 0.50506794\n",
      "Iteration 36, loss = 0.50039392\n",
      "Iteration 37, loss = 0.49605659\n",
      "Iteration 38, loss = 0.49214778\n",
      "Iteration 39, loss = 0.48868557\n",
      "Iteration 40, loss = 0.48550221\n",
      "Iteration 41, loss = 0.48233684\n",
      "Iteration 42, loss = 0.47966360\n",
      "Iteration 43, loss = 0.47707476\n",
      "Iteration 44, loss = 0.47491212\n",
      "Iteration 45, loss = 0.47264265\n",
      "Iteration 46, loss = 0.47032845\n",
      "Iteration 47, loss = 0.46858876\n",
      "Iteration 48, loss = 0.46694078\n",
      "Iteration 49, loss = 0.46508297\n",
      "Iteration 50, loss = 0.46357726\n",
      "Iteration 51, loss = 0.46216183\n",
      "Iteration 52, loss = 0.46089551\n",
      "Iteration 53, loss = 0.45969459\n",
      "Iteration 54, loss = 0.45841027\n",
      "Iteration 55, loss = 0.45747274\n",
      "Iteration 56, loss = 0.45608042\n",
      "Iteration 57, loss = 0.45512325\n",
      "Iteration 58, loss = 0.45404260\n",
      "Iteration 59, loss = 0.45308988\n",
      "Iteration 60, loss = 0.45208945\n",
      "Iteration 61, loss = 0.45125970\n",
      "Iteration 62, loss = 0.45047716\n",
      "Iteration 63, loss = 0.44974656\n",
      "Iteration 64, loss = 0.44895304\n",
      "Iteration 65, loss = 0.44834393\n",
      "Iteration 66, loss = 0.44737515\n",
      "Iteration 67, loss = 0.44692366\n",
      "Iteration 68, loss = 0.44613430\n",
      "Iteration 69, loss = 0.44550778\n",
      "Iteration 70, loss = 0.44512105\n",
      "Iteration 71, loss = 0.44447904\n",
      "Iteration 72, loss = 0.44393895\n",
      "Iteration 73, loss = 0.44306838\n",
      "Iteration 74, loss = 0.44288598\n",
      "Iteration 75, loss = 0.44203768\n",
      "Iteration 76, loss = 0.44172855\n",
      "Iteration 77, loss = 0.44127430\n",
      "Iteration 78, loss = 0.44059808\n",
      "Iteration 79, loss = 0.44044662\n",
      "Iteration 80, loss = 0.43994389\n",
      "Iteration 81, loss = 0.43920250\n",
      "Iteration 82, loss = 0.43896438\n",
      "Iteration 83, loss = 0.43880951\n",
      "Iteration 84, loss = 0.43798580\n",
      "Iteration 85, loss = 0.43777725\n",
      "Iteration 86, loss = 0.43732616\n",
      "Iteration 87, loss = 0.43694371\n",
      "Iteration 88, loss = 0.43639100\n",
      "Iteration 89, loss = 0.43623225\n",
      "Iteration 90, loss = 0.43596706\n",
      "Iteration 91, loss = 0.43527667\n",
      "Iteration 92, loss = 0.43517015\n",
      "Iteration 93, loss = 0.43455963\n",
      "Iteration 94, loss = 0.43426226\n",
      "Iteration 95, loss = 0.43406950\n",
      "Iteration 96, loss = 0.43393833\n",
      "Iteration 97, loss = 0.43355449\n",
      "Iteration 98, loss = 0.43343223\n",
      "Iteration 99, loss = 0.43306765\n",
      "Iteration 100, loss = 0.43265494\n",
      "Iteration 101, loss = 0.43229347\n",
      "Iteration 102, loss = 0.43211174\n",
      "Iteration 103, loss = 0.43179141\n",
      "Iteration 104, loss = 0.43157689\n",
      "Iteration 105, loss = 0.43116563\n",
      "Iteration 106, loss = 0.43077943\n",
      "Iteration 107, loss = 0.43078011\n",
      "Iteration 108, loss = 0.43046700\n",
      "Iteration 109, loss = 0.43020421\n",
      "Iteration 110, loss = 0.42981897\n",
      "Iteration 111, loss = 0.42968967\n",
      "Iteration 112, loss = 0.42957395\n",
      "Iteration 113, loss = 0.42918298\n",
      "Iteration 114, loss = 0.42899245\n",
      "Iteration 115, loss = 0.42869591\n",
      "Iteration 116, loss = 0.42849763\n",
      "Iteration 117, loss = 0.42840125\n",
      "Iteration 118, loss = 0.42781965\n",
      "Iteration 119, loss = 0.42795738\n",
      "Iteration 120, loss = 0.42755784\n",
      "Iteration 121, loss = 0.42742250\n",
      "Iteration 122, loss = 0.42720574\n",
      "Iteration 123, loss = 0.42691736\n",
      "Iteration 124, loss = 0.42641575\n",
      "Iteration 125, loss = 0.42651576\n",
      "Iteration 126, loss = 0.42611647\n",
      "Iteration 127, loss = 0.42626635\n",
      "Iteration 128, loss = 0.42580851\n",
      "Iteration 129, loss = 0.42577384\n",
      "Iteration 130, loss = 0.42550477\n",
      "Iteration 131, loss = 0.42520922\n",
      "Iteration 132, loss = 0.42508700\n",
      "Iteration 133, loss = 0.42497405\n",
      "Iteration 134, loss = 0.42467482\n",
      "Iteration 135, loss = 0.42456660\n",
      "Iteration 136, loss = 0.42445460\n",
      "Iteration 137, loss = 0.42424475\n",
      "Iteration 138, loss = 0.42414862\n",
      "Iteration 139, loss = 0.42388229\n",
      "Iteration 140, loss = 0.42344989\n",
      "Iteration 141, loss = 0.42360283\n",
      "Iteration 142, loss = 0.42321773\n",
      "Iteration 143, loss = 0.42309746\n",
      "Iteration 144, loss = 0.42305296\n",
      "Iteration 145, loss = 0.42293573\n",
      "Iteration 146, loss = 0.42250739\n",
      "Iteration 147, loss = 0.42236810\n",
      "Iteration 148, loss = 0.42232952\n",
      "Iteration 149, loss = 0.42216491\n",
      "Iteration 150, loss = 0.42218377\n",
      "Iteration 151, loss = 0.42167913\n",
      "Iteration 152, loss = 0.42161890\n",
      "Iteration 153, loss = 0.42151519\n",
      "Iteration 154, loss = 0.42123718\n",
      "Iteration 155, loss = 0.42113135\n",
      "Iteration 156, loss = 0.42089839\n",
      "Iteration 157, loss = 0.42088302\n",
      "Iteration 158, loss = 0.42056624\n",
      "Iteration 159, loss = 0.42039916\n",
      "Iteration 160, loss = 0.42059021\n",
      "Iteration 161, loss = 0.42016476\n",
      "Iteration 162, loss = 0.41969199\n",
      "Iteration 163, loss = 0.41992092\n",
      "Iteration 164, loss = 0.41955086\n",
      "Iteration 165, loss = 0.41931991\n",
      "Iteration 166, loss = 0.41916025\n",
      "Iteration 167, loss = 0.41918065\n",
      "Iteration 168, loss = 0.41921922\n",
      "Iteration 169, loss = 0.41880910\n",
      "Iteration 170, loss = 0.41866185\n",
      "Iteration 171, loss = 0.41859119\n",
      "Iteration 172, loss = 0.41824377\n",
      "Iteration 173, loss = 0.41845166\n",
      "Iteration 174, loss = 0.41806747\n",
      "Iteration 175, loss = 0.41806159\n",
      "Iteration 176, loss = 0.41801166\n",
      "Iteration 177, loss = 0.41772520\n",
      "Iteration 178, loss = 0.41742108\n",
      "Iteration 179, loss = 0.41744103\n",
      "Iteration 180, loss = 0.41740723\n",
      "Iteration 181, loss = 0.41716553\n",
      "Iteration 182, loss = 0.41711079\n",
      "Iteration 183, loss = 0.41747676\n",
      "Iteration 184, loss = 0.41671509\n",
      "Iteration 185, loss = 0.41661664\n",
      "Iteration 186, loss = 0.41660248\n",
      "Iteration 187, loss = 0.41624040\n",
      "Iteration 188, loss = 0.41625635\n",
      "Iteration 189, loss = 0.41625544\n",
      "Iteration 190, loss = 0.41590382\n",
      "Iteration 191, loss = 0.41584557\n",
      "Iteration 192, loss = 0.41585241\n",
      "Iteration 193, loss = 0.41557276\n",
      "Iteration 194, loss = 0.41557631\n",
      "Iteration 195, loss = 0.41535837\n",
      "Iteration 196, loss = 0.41534701\n",
      "Iteration 197, loss = 0.41518007\n",
      "Iteration 198, loss = 0.41491999\n",
      "Iteration 199, loss = 0.41518505\n",
      "Iteration 200, loss = 0.41478635\n",
      "Iteration 201, loss = 0.41495225\n",
      "Iteration 202, loss = 0.41462761\n",
      "Iteration 203, loss = 0.41443631\n",
      "Iteration 204, loss = 0.41436846\n",
      "Iteration 205, loss = 0.41420582\n",
      "Iteration 206, loss = 0.41431308\n",
      "Iteration 207, loss = 0.41407724\n",
      "Iteration 208, loss = 0.41379945\n",
      "Iteration 209, loss = 0.41367537\n",
      "Iteration 210, loss = 0.41365301\n",
      "Iteration 211, loss = 0.41318111\n",
      "Iteration 212, loss = 0.41329535\n",
      "Iteration 213, loss = 0.41326373\n",
      "Iteration 214, loss = 0.41317558\n",
      "Iteration 215, loss = 0.41321360\n",
      "Iteration 216, loss = 0.41298652\n",
      "Iteration 217, loss = 0.41304023\n",
      "Iteration 218, loss = 0.41281343\n",
      "Iteration 219, loss = 0.41291187\n",
      "Iteration 220, loss = 0.41273797\n",
      "Iteration 221, loss = 0.41243909\n",
      "Iteration 222, loss = 0.41215335\n",
      "Iteration 223, loss = 0.41217142\n",
      "Iteration 224, loss = 0.41196934\n",
      "Iteration 225, loss = 0.41214965\n",
      "Iteration 226, loss = 0.41189835\n",
      "Iteration 227, loss = 0.41204644\n",
      "Iteration 228, loss = 0.41172871\n",
      "Iteration 229, loss = 0.41193544\n",
      "Iteration 230, loss = 0.41162102\n",
      "Iteration 231, loss = 0.41125302\n",
      "Iteration 232, loss = 0.41141914\n",
      "Iteration 233, loss = 0.41129613\n",
      "Iteration 234, loss = 0.41102725\n",
      "Iteration 235, loss = 0.41092071\n",
      "Iteration 236, loss = 0.41079594\n",
      "Iteration 237, loss = 0.41063218\n",
      "Iteration 238, loss = 0.41086109\n",
      "Iteration 239, loss = 0.41064403\n",
      "Iteration 240, loss = 0.41070658\n",
      "Iteration 241, loss = 0.41046046\n",
      "Iteration 242, loss = 0.41026465\n",
      "Iteration 243, loss = 0.41010214\n",
      "Iteration 244, loss = 0.41013804\n",
      "Iteration 245, loss = 0.41028686\n",
      "Iteration 246, loss = 0.41010052\n",
      "Iteration 247, loss = 0.41013861\n",
      "Iteration 248, loss = 0.40996102\n",
      "Iteration 249, loss = 0.40966685\n",
      "Iteration 250, loss = 0.40970478\n",
      "Iteration 251, loss = 0.40953408\n",
      "Iteration 252, loss = 0.40956672\n",
      "Iteration 253, loss = 0.40935018\n",
      "Iteration 254, loss = 0.40936611\n",
      "Iteration 255, loss = 0.40925917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.40929533\n",
      "Iteration 257, loss = 0.40896828\n",
      "Iteration 258, loss = 0.40892909\n",
      "Iteration 259, loss = 0.40882924\n",
      "Iteration 260, loss = 0.40877762\n",
      "Iteration 261, loss = 0.40879551\n",
      "Iteration 262, loss = 0.40857950\n",
      "Iteration 263, loss = 0.40855528\n",
      "Iteration 264, loss = 0.40854124\n",
      "Iteration 265, loss = 0.40822027\n",
      "Iteration 266, loss = 0.40808403\n",
      "Iteration 267, loss = 0.40812407\n",
      "Iteration 268, loss = 0.40818691\n",
      "Iteration 269, loss = 0.40777466\n",
      "Iteration 270, loss = 0.40806745\n",
      "Iteration 271, loss = 0.40772345\n",
      "Iteration 272, loss = 0.40772688\n",
      "Iteration 273, loss = 0.40756909\n",
      "Iteration 274, loss = 0.40751224\n",
      "Iteration 275, loss = 0.40745420\n",
      "Iteration 276, loss = 0.40750279\n",
      "Iteration 277, loss = 0.40755164\n",
      "Iteration 278, loss = 0.40742681\n",
      "Iteration 279, loss = 0.40715814\n",
      "Iteration 280, loss = 0.40691632\n",
      "Iteration 281, loss = 0.40717337\n",
      "Iteration 282, loss = 0.40708559\n",
      "Iteration 283, loss = 0.40681878\n",
      "Iteration 284, loss = 0.40678293\n",
      "Iteration 285, loss = 0.40691808\n",
      "Iteration 286, loss = 0.40672351\n",
      "Iteration 287, loss = 0.40672833\n",
      "Iteration 288, loss = 0.40668407\n",
      "Iteration 289, loss = 0.40633983\n",
      "Iteration 290, loss = 0.40648967\n",
      "Iteration 291, loss = 0.40622126\n",
      "Iteration 292, loss = 0.40632381\n",
      "Iteration 293, loss = 0.40601466\n",
      "Iteration 294, loss = 0.40590924\n",
      "Iteration 295, loss = 0.40595739\n",
      "Iteration 296, loss = 0.40629553\n",
      "Iteration 297, loss = 0.40610517\n",
      "Iteration 298, loss = 0.40570082\n",
      "Iteration 299, loss = 0.40587029\n",
      "Iteration 300, loss = 0.40558399\n",
      "Iteration 301, loss = 0.40541492\n",
      "Iteration 302, loss = 0.40547762\n",
      "Iteration 303, loss = 0.40551175\n",
      "Iteration 304, loss = 0.40539515\n",
      "Iteration 305, loss = 0.40549583\n",
      "Iteration 306, loss = 0.40520036\n",
      "Iteration 307, loss = 0.40523629\n",
      "Iteration 308, loss = 0.40527485\n",
      "Iteration 309, loss = 0.40494943\n",
      "Iteration 310, loss = 0.40483336\n",
      "Iteration 311, loss = 0.40511101\n",
      "Iteration 312, loss = 0.40475225\n",
      "Iteration 313, loss = 0.40483810\n",
      "Iteration 314, loss = 0.40457518\n",
      "Iteration 315, loss = 0.40484745\n",
      "Iteration 316, loss = 0.40448240\n",
      "Iteration 317, loss = 0.40465979\n",
      "Iteration 318, loss = 0.40449055\n",
      "Iteration 319, loss = 0.40445220\n",
      "Iteration 320, loss = 0.40423180\n",
      "Iteration 321, loss = 0.40453931\n",
      "Iteration 322, loss = 0.40432456\n",
      "Iteration 323, loss = 0.40417671\n",
      "Iteration 324, loss = 0.40418395\n",
      "Iteration 325, loss = 0.40416977\n",
      "Iteration 326, loss = 0.40406002\n",
      "Iteration 327, loss = 0.40370550\n",
      "Iteration 328, loss = 0.40383860\n",
      "Iteration 329, loss = 0.40394373\n",
      "Iteration 330, loss = 0.40398977\n",
      "Iteration 331, loss = 0.40383393\n",
      "Iteration 332, loss = 0.40358624\n",
      "Iteration 333, loss = 0.40353123\n",
      "Iteration 334, loss = 0.40356004\n",
      "Iteration 335, loss = 0.40361223\n",
      "Iteration 336, loss = 0.40357152\n",
      "Iteration 337, loss = 0.40338380\n",
      "Iteration 338, loss = 0.40323580\n",
      "Iteration 339, loss = 0.40335629\n",
      "Iteration 340, loss = 0.40345877\n",
      "Iteration 341, loss = 0.40306575\n",
      "Iteration 342, loss = 0.40325175\n",
      "Iteration 343, loss = 0.40310458\n",
      "Iteration 344, loss = 0.40316757\n",
      "Iteration 345, loss = 0.40301526\n",
      "Iteration 346, loss = 0.40300138\n",
      "Iteration 347, loss = 0.40300068\n",
      "Iteration 348, loss = 0.40286111\n",
      "Iteration 349, loss = 0.40286374\n",
      "Iteration 350, loss = 0.40286370\n",
      "Iteration 351, loss = 0.40278337\n",
      "Iteration 352, loss = 0.40264387\n",
      "Iteration 353, loss = 0.40276582\n",
      "Iteration 354, loss = 0.40263786\n",
      "Iteration 355, loss = 0.40246685\n",
      "Iteration 356, loss = 0.40267858\n",
      "Iteration 357, loss = 0.40224855\n",
      "Iteration 358, loss = 0.40229704\n",
      "Iteration 359, loss = 0.40233975\n",
      "Iteration 360, loss = 0.40231510\n",
      "Iteration 361, loss = 0.40212760\n",
      "Iteration 362, loss = 0.40223009\n",
      "Iteration 363, loss = 0.40239293\n",
      "Iteration 364, loss = 0.40211458\n",
      "Iteration 365, loss = 0.40217178\n",
      "Iteration 366, loss = 0.40211019\n",
      "Iteration 367, loss = 0.40198714\n",
      "Iteration 368, loss = 0.40182452\n",
      "Iteration 369, loss = 0.40213844\n",
      "Iteration 370, loss = 0.40202498\n",
      "Iteration 371, loss = 0.40171519\n",
      "Iteration 372, loss = 0.40189378\n",
      "Iteration 373, loss = 0.40169989\n",
      "Iteration 374, loss = 0.40162972\n",
      "Iteration 375, loss = 0.40159715\n",
      "Iteration 376, loss = 0.40160494\n",
      "Iteration 377, loss = 0.40163024\n",
      "Iteration 378, loss = 0.40134128\n",
      "Iteration 379, loss = 0.40147421\n",
      "Iteration 380, loss = 0.40149119\n",
      "Iteration 381, loss = 0.40142179\n",
      "Iteration 382, loss = 0.40119716\n",
      "Iteration 383, loss = 0.40125033\n",
      "Iteration 384, loss = 0.40127508\n",
      "Iteration 385, loss = 0.40111220\n",
      "Iteration 386, loss = 0.40107553\n",
      "Iteration 387, loss = 0.40081095\n",
      "Iteration 388, loss = 0.40115002\n",
      "Iteration 389, loss = 0.40129207\n",
      "Iteration 390, loss = 0.40100878\n",
      "Iteration 391, loss = 0.40091804\n",
      "Iteration 392, loss = 0.40102000\n",
      "Iteration 393, loss = 0.40088752\n",
      "Iteration 394, loss = 0.40082441\n",
      "Iteration 395, loss = 0.40081823\n",
      "Iteration 396, loss = 0.40070168\n",
      "Iteration 397, loss = 0.40099855\n",
      "Iteration 398, loss = 0.40074685\n",
      "Iteration 399, loss = 0.40055078\n",
      "Iteration 400, loss = 0.40075594\n",
      "Iteration 401, loss = 0.40060268\n",
      "Iteration 402, loss = 0.40062878\n",
      "Iteration 403, loss = 0.40047021\n",
      "Iteration 404, loss = 0.40040783\n",
      "Iteration 405, loss = 0.40022973\n",
      "Iteration 406, loss = 0.40023446\n",
      "Iteration 407, loss = 0.40021628\n",
      "Iteration 408, loss = 0.40016453\n",
      "Iteration 409, loss = 0.40033871\n",
      "Iteration 410, loss = 0.40013451\n",
      "Iteration 411, loss = 0.40013474\n",
      "Iteration 412, loss = 0.40009190\n",
      "Iteration 413, loss = 0.40013597\n",
      "Iteration 414, loss = 0.40018162\n",
      "Iteration 415, loss = 0.40013642\n",
      "Iteration 416, loss = 0.39980698\n",
      "Iteration 417, loss = 0.39996320\n",
      "Iteration 418, loss = 0.40012420\n",
      "Iteration 419, loss = 0.39976457\n",
      "Iteration 420, loss = 0.39982759\n",
      "Iteration 421, loss = 0.39975235\n",
      "Iteration 422, loss = 0.39977342\n",
      "Iteration 423, loss = 0.39982712\n",
      "Iteration 424, loss = 0.39970692\n",
      "Iteration 425, loss = 0.39960730\n",
      "Iteration 426, loss = 0.39979492\n",
      "Iteration 427, loss = 0.39956374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05627280\n",
      "Iteration 2, loss = 1.83410368\n",
      "Iteration 3, loss = 1.66587451\n",
      "Iteration 4, loss = 1.52293959\n",
      "Iteration 5, loss = 1.37949613\n",
      "Iteration 6, loss = 1.23980407\n",
      "Iteration 7, loss = 1.12229294\n",
      "Iteration 8, loss = 1.02985057\n",
      "Iteration 9, loss = 0.95726732\n",
      "Iteration 10, loss = 0.89802803\n",
      "Iteration 11, loss = 0.84909390\n",
      "Iteration 12, loss = 0.80855877\n",
      "Iteration 13, loss = 0.77459250\n",
      "Iteration 14, loss = 0.74624056\n",
      "Iteration 15, loss = 0.72195163\n",
      "Iteration 16, loss = 0.70060984\n",
      "Iteration 17, loss = 0.68186490\n",
      "Iteration 18, loss = 0.66531569\n",
      "Iteration 19, loss = 0.65021379\n",
      "Iteration 20, loss = 0.63651420\n",
      "Iteration 21, loss = 0.62403641\n",
      "Iteration 22, loss = 0.61238145\n",
      "Iteration 23, loss = 0.60158239\n",
      "Iteration 24, loss = 0.59168812\n",
      "Iteration 25, loss = 0.58267326\n",
      "Iteration 26, loss = 0.57371800\n",
      "Iteration 27, loss = 0.56566663\n",
      "Iteration 28, loss = 0.55808563\n",
      "Iteration 29, loss = 0.55062196\n",
      "Iteration 30, loss = 0.54399525\n",
      "Iteration 31, loss = 0.53758146\n",
      "Iteration 32, loss = 0.53148120\n",
      "Iteration 33, loss = 0.52569180\n",
      "Iteration 34, loss = 0.52047270\n",
      "Iteration 35, loss = 0.51519225\n",
      "Iteration 36, loss = 0.51041852\n",
      "Iteration 37, loss = 0.50593200\n",
      "Iteration 38, loss = 0.50161093\n",
      "Iteration 39, loss = 0.49739884\n",
      "Iteration 40, loss = 0.49355126\n",
      "Iteration 41, loss = 0.49007255\n",
      "Iteration 42, loss = 0.48665684\n",
      "Iteration 43, loss = 0.48385688\n",
      "Iteration 44, loss = 0.48088629\n",
      "Iteration 45, loss = 0.47811111\n",
      "Iteration 46, loss = 0.47576407\n",
      "Iteration 47, loss = 0.47337149\n",
      "Iteration 48, loss = 0.47145334\n",
      "Iteration 49, loss = 0.46950018\n",
      "Iteration 50, loss = 0.46779734\n",
      "Iteration 51, loss = 0.46567580\n",
      "Iteration 52, loss = 0.46440124\n",
      "Iteration 53, loss = 0.46311515\n",
      "Iteration 54, loss = 0.46162163\n",
      "Iteration 55, loss = 0.46014278\n",
      "Iteration 56, loss = 0.45888414\n",
      "Iteration 57, loss = 0.45778256\n",
      "Iteration 58, loss = 0.45677903\n",
      "Iteration 59, loss = 0.45558470\n",
      "Iteration 60, loss = 0.45450918\n",
      "Iteration 61, loss = 0.45357835\n",
      "Iteration 62, loss = 0.45280454\n",
      "Iteration 63, loss = 0.45177389\n",
      "Iteration 64, loss = 0.45090494\n",
      "Iteration 65, loss = 0.45035078\n",
      "Iteration 66, loss = 0.44924368\n",
      "Iteration 67, loss = 0.44897143\n",
      "Iteration 68, loss = 0.44812890\n",
      "Iteration 69, loss = 0.44755172\n",
      "Iteration 70, loss = 0.44696665\n",
      "Iteration 71, loss = 0.44619332\n",
      "Iteration 72, loss = 0.44556191\n",
      "Iteration 73, loss = 0.44507302\n",
      "Iteration 74, loss = 0.44428643\n",
      "Iteration 75, loss = 0.44379759\n",
      "Iteration 76, loss = 0.44323847\n",
      "Iteration 77, loss = 0.44241879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.44246200\n",
      "Iteration 79, loss = 0.44158228\n",
      "Iteration 80, loss = 0.44131501\n",
      "Iteration 81, loss = 0.44088985\n",
      "Iteration 82, loss = 0.44049421\n",
      "Iteration 83, loss = 0.43995800\n",
      "Iteration 84, loss = 0.43956216\n",
      "Iteration 85, loss = 0.43890463\n",
      "Iteration 86, loss = 0.43876368\n",
      "Iteration 87, loss = 0.43836576\n",
      "Iteration 88, loss = 0.43791007\n",
      "Iteration 89, loss = 0.43749509\n",
      "Iteration 90, loss = 0.43730168\n",
      "Iteration 91, loss = 0.43688906\n",
      "Iteration 92, loss = 0.43629039\n",
      "Iteration 93, loss = 0.43636314\n",
      "Iteration 94, loss = 0.43609530\n",
      "Iteration 95, loss = 0.43560777\n",
      "Iteration 96, loss = 0.43507920\n",
      "Iteration 97, loss = 0.43503783\n",
      "Iteration 98, loss = 0.43437774\n",
      "Iteration 99, loss = 0.43429779\n",
      "Iteration 100, loss = 0.43391946\n",
      "Iteration 101, loss = 0.43359254\n",
      "Iteration 102, loss = 0.43371030\n",
      "Iteration 103, loss = 0.43307658\n",
      "Iteration 104, loss = 0.43292223\n",
      "Iteration 105, loss = 0.43265838\n",
      "Iteration 106, loss = 0.43233129\n",
      "Iteration 107, loss = 0.43202906\n",
      "Iteration 108, loss = 0.43184144\n",
      "Iteration 109, loss = 0.43161743\n",
      "Iteration 110, loss = 0.43121080\n",
      "Iteration 111, loss = 0.43075059\n",
      "Iteration 112, loss = 0.43082497\n",
      "Iteration 113, loss = 0.43045623\n",
      "Iteration 114, loss = 0.43033967\n",
      "Iteration 115, loss = 0.43002838\n",
      "Iteration 116, loss = 0.42996725\n",
      "Iteration 117, loss = 0.42954059\n",
      "Iteration 118, loss = 0.42925556\n",
      "Iteration 119, loss = 0.42898920\n",
      "Iteration 120, loss = 0.42884700\n",
      "Iteration 121, loss = 0.42894759\n",
      "Iteration 122, loss = 0.42830023\n",
      "Iteration 123, loss = 0.42809817\n",
      "Iteration 124, loss = 0.42797278\n",
      "Iteration 125, loss = 0.42789252\n",
      "Iteration 126, loss = 0.42771073\n",
      "Iteration 127, loss = 0.42752674\n",
      "Iteration 128, loss = 0.42727545\n",
      "Iteration 129, loss = 0.42711904\n",
      "Iteration 130, loss = 0.42687252\n",
      "Iteration 131, loss = 0.42667840\n",
      "Iteration 132, loss = 0.42667037\n",
      "Iteration 133, loss = 0.42632906\n",
      "Iteration 134, loss = 0.42609506\n",
      "Iteration 135, loss = 0.42590813\n",
      "Iteration 136, loss = 0.42569553\n",
      "Iteration 137, loss = 0.42558500\n",
      "Iteration 138, loss = 0.42548971\n",
      "Iteration 139, loss = 0.42545300\n",
      "Iteration 140, loss = 0.42519816\n",
      "Iteration 141, loss = 0.42505120\n",
      "Iteration 142, loss = 0.42488972\n",
      "Iteration 143, loss = 0.42451374\n",
      "Iteration 144, loss = 0.42431631\n",
      "Iteration 145, loss = 0.42437565\n",
      "Iteration 146, loss = 0.42416440\n",
      "Iteration 147, loss = 0.42404968\n",
      "Iteration 148, loss = 0.42381514\n",
      "Iteration 149, loss = 0.42368165\n",
      "Iteration 150, loss = 0.42339738\n",
      "Iteration 151, loss = 0.42344537\n",
      "Iteration 152, loss = 0.42331668\n",
      "Iteration 153, loss = 0.42302859\n",
      "Iteration 154, loss = 0.42284746\n",
      "Iteration 155, loss = 0.42289187\n",
      "Iteration 156, loss = 0.42270748\n",
      "Iteration 157, loss = 0.42250762\n",
      "Iteration 158, loss = 0.42226865\n",
      "Iteration 159, loss = 0.42216982\n",
      "Iteration 160, loss = 0.42228014\n",
      "Iteration 161, loss = 0.42179209\n",
      "Iteration 162, loss = 0.42174643\n",
      "Iteration 163, loss = 0.42152408\n",
      "Iteration 164, loss = 0.42161755\n",
      "Iteration 165, loss = 0.42158775\n",
      "Iteration 166, loss = 0.42125684\n",
      "Iteration 167, loss = 0.42118695\n",
      "Iteration 168, loss = 0.42093725\n",
      "Iteration 169, loss = 0.42093269\n",
      "Iteration 170, loss = 0.42063711\n",
      "Iteration 171, loss = 0.42064014\n",
      "Iteration 172, loss = 0.42054978\n",
      "Iteration 173, loss = 0.42048395\n",
      "Iteration 174, loss = 0.42027937\n",
      "Iteration 175, loss = 0.42009595\n",
      "Iteration 176, loss = 0.41994207\n",
      "Iteration 177, loss = 0.41997587\n",
      "Iteration 178, loss = 0.41948715\n",
      "Iteration 179, loss = 0.41962631\n",
      "Iteration 180, loss = 0.41959371\n",
      "Iteration 181, loss = 0.41910856\n",
      "Iteration 182, loss = 0.41918341\n",
      "Iteration 183, loss = 0.41924632\n",
      "Iteration 184, loss = 0.41874727\n",
      "Iteration 185, loss = 0.41893541\n",
      "Iteration 186, loss = 0.41889083\n",
      "Iteration 187, loss = 0.41868605\n",
      "Iteration 188, loss = 0.41862810\n",
      "Iteration 189, loss = 0.41843828\n",
      "Iteration 190, loss = 0.41792472\n",
      "Iteration 191, loss = 0.41798568\n",
      "Iteration 192, loss = 0.41827442\n",
      "Iteration 193, loss = 0.41794444\n",
      "Iteration 194, loss = 0.41779185\n",
      "Iteration 195, loss = 0.41756218\n",
      "Iteration 196, loss = 0.41743120\n",
      "Iteration 197, loss = 0.41733164\n",
      "Iteration 198, loss = 0.41739748\n",
      "Iteration 199, loss = 0.41717114\n",
      "Iteration 200, loss = 0.41711467\n",
      "Iteration 201, loss = 0.41689924\n",
      "Iteration 202, loss = 0.41678530\n",
      "Iteration 203, loss = 0.41685799\n",
      "Iteration 204, loss = 0.41662326\n",
      "Iteration 205, loss = 0.41655926\n",
      "Iteration 206, loss = 0.41663719\n",
      "Iteration 207, loss = 0.41631394\n",
      "Iteration 208, loss = 0.41629099\n",
      "Iteration 209, loss = 0.41602272\n",
      "Iteration 210, loss = 0.41597020\n",
      "Iteration 211, loss = 0.41593907\n",
      "Iteration 212, loss = 0.41558450\n",
      "Iteration 213, loss = 0.41572175\n",
      "Iteration 214, loss = 0.41557543\n",
      "Iteration 215, loss = 0.41558003\n",
      "Iteration 216, loss = 0.41533769\n",
      "Iteration 217, loss = 0.41518460\n",
      "Iteration 218, loss = 0.41521343\n",
      "Iteration 219, loss = 0.41495193\n",
      "Iteration 220, loss = 0.41492277\n",
      "Iteration 221, loss = 0.41494253\n",
      "Iteration 222, loss = 0.41456964\n",
      "Iteration 223, loss = 0.41458729\n",
      "Iteration 224, loss = 0.41454760\n",
      "Iteration 225, loss = 0.41443517\n",
      "Iteration 226, loss = 0.41415194\n",
      "Iteration 227, loss = 0.41451471\n",
      "Iteration 228, loss = 0.41411406\n",
      "Iteration 229, loss = 0.41395504\n",
      "Iteration 230, loss = 0.41397420\n",
      "Iteration 231, loss = 0.41384508\n",
      "Iteration 232, loss = 0.41375341\n",
      "Iteration 233, loss = 0.41371381\n",
      "Iteration 234, loss = 0.41337753\n",
      "Iteration 235, loss = 0.41373043\n",
      "Iteration 236, loss = 0.41312154\n",
      "Iteration 237, loss = 0.41314635\n",
      "Iteration 238, loss = 0.41322562\n",
      "Iteration 239, loss = 0.41315503\n",
      "Iteration 240, loss = 0.41288568\n",
      "Iteration 241, loss = 0.41286183\n",
      "Iteration 242, loss = 0.41286650\n",
      "Iteration 243, loss = 0.41265033\n",
      "Iteration 244, loss = 0.41251566\n",
      "Iteration 245, loss = 0.41267420\n",
      "Iteration 246, loss = 0.41259008\n",
      "Iteration 247, loss = 0.41228394\n",
      "Iteration 248, loss = 0.41241430\n",
      "Iteration 249, loss = 0.41226119\n",
      "Iteration 250, loss = 0.41208588\n",
      "Iteration 251, loss = 0.41186967\n",
      "Iteration 252, loss = 0.41217682\n",
      "Iteration 253, loss = 0.41186621\n",
      "Iteration 254, loss = 0.41169620\n",
      "Iteration 255, loss = 0.41179506\n",
      "Iteration 256, loss = 0.41174507\n",
      "Iteration 257, loss = 0.41163392\n",
      "Iteration 258, loss = 0.41150288\n",
      "Iteration 259, loss = 0.41151724\n",
      "Iteration 260, loss = 0.41108043\n",
      "Iteration 261, loss = 0.41114385\n",
      "Iteration 262, loss = 0.41111247\n",
      "Iteration 263, loss = 0.41089241\n",
      "Iteration 264, loss = 0.41091656\n",
      "Iteration 265, loss = 0.41059156\n",
      "Iteration 266, loss = 0.41081404\n",
      "Iteration 267, loss = 0.41071038\n",
      "Iteration 268, loss = 0.41049665\n",
      "Iteration 269, loss = 0.41050328\n",
      "Iteration 270, loss = 0.41044062\n",
      "Iteration 271, loss = 0.41022827\n",
      "Iteration 272, loss = 0.41061418\n",
      "Iteration 273, loss = 0.40998949\n",
      "Iteration 274, loss = 0.41023874\n",
      "Iteration 275, loss = 0.41012973\n",
      "Iteration 276, loss = 0.40989095\n",
      "Iteration 277, loss = 0.41003390\n",
      "Iteration 278, loss = 0.40969618\n",
      "Iteration 279, loss = 0.40994784\n",
      "Iteration 280, loss = 0.40959601\n",
      "Iteration 281, loss = 0.40951989\n",
      "Iteration 282, loss = 0.40929417\n",
      "Iteration 283, loss = 0.40942497\n",
      "Iteration 284, loss = 0.40933281\n",
      "Iteration 285, loss = 0.40896442\n",
      "Iteration 286, loss = 0.40880448\n",
      "Iteration 287, loss = 0.40880439\n",
      "Iteration 288, loss = 0.40891566\n",
      "Iteration 289, loss = 0.40887143\n",
      "Iteration 290, loss = 0.40856076\n",
      "Iteration 291, loss = 0.40830604\n",
      "Iteration 292, loss = 0.40859475\n",
      "Iteration 293, loss = 0.40816401\n",
      "Iteration 294, loss = 0.40831423\n",
      "Iteration 295, loss = 0.40824580\n",
      "Iteration 296, loss = 0.40808965\n",
      "Iteration 297, loss = 0.40822776\n",
      "Iteration 298, loss = 0.40799793\n",
      "Iteration 299, loss = 0.40803981\n",
      "Iteration 300, loss = 0.40800623\n",
      "Iteration 301, loss = 0.40771994\n",
      "Iteration 302, loss = 0.40761894\n",
      "Iteration 303, loss = 0.40797055\n",
      "Iteration 304, loss = 0.40758053\n",
      "Iteration 305, loss = 0.40737597\n",
      "Iteration 306, loss = 0.40744787\n",
      "Iteration 307, loss = 0.40771151\n",
      "Iteration 308, loss = 0.40752647\n",
      "Iteration 309, loss = 0.40735970\n",
      "Iteration 310, loss = 0.40715200\n",
      "Iteration 311, loss = 0.40697170\n",
      "Iteration 312, loss = 0.40689016\n",
      "Iteration 313, loss = 0.40718270\n",
      "Iteration 314, loss = 0.40692392\n",
      "Iteration 315, loss = 0.40706237\n",
      "Iteration 316, loss = 0.40669254\n",
      "Iteration 317, loss = 0.40683280\n",
      "Iteration 318, loss = 0.40657271\n",
      "Iteration 319, loss = 0.40669425\n",
      "Iteration 320, loss = 0.40660440\n",
      "Iteration 321, loss = 0.40701886\n",
      "Iteration 322, loss = 0.40656996\n",
      "Iteration 323, loss = 0.40624271\n",
      "Iteration 324, loss = 0.40620321\n",
      "Iteration 325, loss = 0.40609029\n",
      "Iteration 326, loss = 0.40639297\n",
      "Iteration 327, loss = 0.40614801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 328, loss = 0.40589796\n",
      "Iteration 329, loss = 0.40587831\n",
      "Iteration 330, loss = 0.40606935\n",
      "Iteration 331, loss = 0.40593947\n",
      "Iteration 332, loss = 0.40576089\n",
      "Iteration 333, loss = 0.40587874\n",
      "Iteration 334, loss = 0.40573428\n",
      "Iteration 335, loss = 0.40545033\n",
      "Iteration 336, loss = 0.40575068\n",
      "Iteration 337, loss = 0.40562471\n",
      "Iteration 338, loss = 0.40558630\n",
      "Iteration 339, loss = 0.40533281\n",
      "Iteration 340, loss = 0.40543686\n",
      "Iteration 341, loss = 0.40509045\n",
      "Iteration 342, loss = 0.40506855\n",
      "Iteration 343, loss = 0.40513562\n",
      "Iteration 344, loss = 0.40517929\n",
      "Iteration 345, loss = 0.40500571\n",
      "Iteration 346, loss = 0.40472172\n",
      "Iteration 347, loss = 0.40477788\n",
      "Iteration 348, loss = 0.40501289\n",
      "Iteration 349, loss = 0.40458210\n",
      "Iteration 350, loss = 0.40482211\n",
      "Iteration 351, loss = 0.40459401\n",
      "Iteration 352, loss = 0.40481419\n",
      "Iteration 353, loss = 0.40437727\n",
      "Iteration 354, loss = 0.40484332\n",
      "Iteration 355, loss = 0.40447737\n",
      "Iteration 356, loss = 0.40451609\n",
      "Iteration 357, loss = 0.40447014\n",
      "Iteration 358, loss = 0.40409139\n",
      "Iteration 359, loss = 0.40417004\n",
      "Iteration 360, loss = 0.40418779\n",
      "Iteration 361, loss = 0.40399244\n",
      "Iteration 362, loss = 0.40378528\n",
      "Iteration 363, loss = 0.40406162\n",
      "Iteration 364, loss = 0.40421136\n",
      "Iteration 365, loss = 0.40391454\n",
      "Iteration 366, loss = 0.40376199\n",
      "Iteration 367, loss = 0.40377380\n",
      "Iteration 368, loss = 0.40352888\n",
      "Iteration 369, loss = 0.40358840\n",
      "Iteration 370, loss = 0.40362814\n",
      "Iteration 371, loss = 0.40348362\n",
      "Iteration 372, loss = 0.40354969\n",
      "Iteration 373, loss = 0.40340975\n",
      "Iteration 374, loss = 0.40353042\n",
      "Iteration 375, loss = 0.40347533\n",
      "Iteration 376, loss = 0.40333507\n",
      "Iteration 377, loss = 0.40332317\n",
      "Iteration 378, loss = 0.40315544\n",
      "Iteration 379, loss = 0.40318847\n",
      "Iteration 380, loss = 0.40317708\n",
      "Iteration 381, loss = 0.40311323\n",
      "Iteration 382, loss = 0.40302194\n",
      "Iteration 383, loss = 0.40301071\n",
      "Iteration 384, loss = 0.40330035\n",
      "Iteration 385, loss = 0.40308467\n",
      "Iteration 386, loss = 0.40288770\n",
      "Iteration 387, loss = 0.40281352\n",
      "Iteration 388, loss = 0.40268657\n",
      "Iteration 389, loss = 0.40262401\n",
      "Iteration 390, loss = 0.40293682\n",
      "Iteration 391, loss = 0.40233609\n",
      "Iteration 392, loss = 0.40260585\n",
      "Iteration 393, loss = 0.40248920\n",
      "Iteration 394, loss = 0.40257415\n",
      "Iteration 395, loss = 0.40242175\n",
      "Iteration 396, loss = 0.40246032\n",
      "Iteration 397, loss = 0.40268233\n",
      "Iteration 398, loss = 0.40232042\n",
      "Iteration 399, loss = 0.40202779\n",
      "Iteration 400, loss = 0.40206752\n",
      "Iteration 401, loss = 0.40231726\n",
      "Iteration 402, loss = 0.40216144\n",
      "Iteration 403, loss = 0.40196068\n",
      "Iteration 404, loss = 0.40218039\n",
      "Iteration 405, loss = 0.40208217\n",
      "Iteration 406, loss = 0.40174751\n",
      "Iteration 407, loss = 0.40202524\n",
      "Iteration 408, loss = 0.40182352\n",
      "Iteration 409, loss = 0.40176267\n",
      "Iteration 410, loss = 0.40190703\n",
      "Iteration 411, loss = 0.40182672\n",
      "Iteration 412, loss = 0.40162382\n",
      "Iteration 413, loss = 0.40157470\n",
      "Iteration 414, loss = 0.40140474\n",
      "Iteration 415, loss = 0.40159046\n",
      "Iteration 416, loss = 0.40148864\n",
      "Iteration 417, loss = 0.40154537\n",
      "Iteration 418, loss = 0.40135582\n",
      "Iteration 419, loss = 0.40162643\n",
      "Iteration 420, loss = 0.40120186\n",
      "Iteration 421, loss = 0.40127038\n",
      "Iteration 422, loss = 0.40128169\n",
      "Iteration 423, loss = 0.40114841\n",
      "Iteration 424, loss = 0.40122192\n",
      "Iteration 425, loss = 0.40112211\n",
      "Iteration 426, loss = 0.40113922\n",
      "Iteration 427, loss = 0.40073259\n",
      "Iteration 428, loss = 0.40095574\n",
      "Iteration 429, loss = 0.40116698\n",
      "Iteration 430, loss = 0.40078735\n",
      "Iteration 431, loss = 0.40088395\n",
      "Iteration 432, loss = 0.40089704\n",
      "Iteration 433, loss = 0.40070631\n",
      "Iteration 434, loss = 0.40063808\n",
      "Iteration 435, loss = 0.40064948\n",
      "Iteration 436, loss = 0.40074680\n",
      "Iteration 437, loss = 0.40069599\n",
      "Iteration 438, loss = 0.40056055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05859233\n",
      "Iteration 2, loss = 1.83535392\n",
      "Iteration 3, loss = 1.66669125\n",
      "Iteration 4, loss = 1.52578477\n",
      "Iteration 5, loss = 1.38467705\n",
      "Iteration 6, loss = 1.24395946\n",
      "Iteration 7, loss = 1.12447155\n",
      "Iteration 8, loss = 1.03063018\n",
      "Iteration 9, loss = 0.95627538\n",
      "Iteration 10, loss = 0.89566043\n",
      "Iteration 11, loss = 0.84568082\n",
      "Iteration 12, loss = 0.80502828\n",
      "Iteration 13, loss = 0.77161267\n",
      "Iteration 14, loss = 0.74382741\n",
      "Iteration 15, loss = 0.71990937\n",
      "Iteration 16, loss = 0.69930136\n",
      "Iteration 17, loss = 0.68113879\n",
      "Iteration 18, loss = 0.66482454\n",
      "Iteration 19, loss = 0.65006653\n",
      "Iteration 20, loss = 0.63658768\n",
      "Iteration 21, loss = 0.62432173\n",
      "Iteration 22, loss = 0.61286716\n",
      "Iteration 23, loss = 0.60213641\n",
      "Iteration 24, loss = 0.59217607\n",
      "Iteration 25, loss = 0.58291294\n",
      "Iteration 26, loss = 0.57413871\n",
      "Iteration 27, loss = 0.56593179\n",
      "Iteration 28, loss = 0.55825811\n",
      "Iteration 29, loss = 0.55077774\n",
      "Iteration 30, loss = 0.54392093\n",
      "Iteration 31, loss = 0.53750311\n",
      "Iteration 32, loss = 0.53152339\n",
      "Iteration 33, loss = 0.52550994\n",
      "Iteration 34, loss = 0.52011015\n",
      "Iteration 35, loss = 0.51486448\n",
      "Iteration 36, loss = 0.51008341\n",
      "Iteration 37, loss = 0.50531981\n",
      "Iteration 38, loss = 0.50100899\n",
      "Iteration 39, loss = 0.49697206\n",
      "Iteration 40, loss = 0.49307372\n",
      "Iteration 41, loss = 0.48962341\n",
      "Iteration 42, loss = 0.48631072\n",
      "Iteration 43, loss = 0.48315790\n",
      "Iteration 44, loss = 0.48048853\n",
      "Iteration 45, loss = 0.47783491\n",
      "Iteration 46, loss = 0.47543214\n",
      "Iteration 47, loss = 0.47305014\n",
      "Iteration 48, loss = 0.47089514\n",
      "Iteration 49, loss = 0.46904626\n",
      "Iteration 50, loss = 0.46679905\n",
      "Iteration 51, loss = 0.46535482\n",
      "Iteration 52, loss = 0.46368996\n",
      "Iteration 53, loss = 0.46256406\n",
      "Iteration 54, loss = 0.46083839\n",
      "Iteration 55, loss = 0.45932420\n",
      "Iteration 56, loss = 0.45827077\n",
      "Iteration 57, loss = 0.45691922\n",
      "Iteration 58, loss = 0.45586387\n",
      "Iteration 59, loss = 0.45468929\n",
      "Iteration 60, loss = 0.45379837\n",
      "Iteration 61, loss = 0.45288305\n",
      "Iteration 62, loss = 0.45193857\n",
      "Iteration 63, loss = 0.45087920\n",
      "Iteration 64, loss = 0.45004289\n",
      "Iteration 65, loss = 0.44954649\n",
      "Iteration 66, loss = 0.44861159\n",
      "Iteration 67, loss = 0.44779432\n",
      "Iteration 68, loss = 0.44717430\n",
      "Iteration 69, loss = 0.44623854\n",
      "Iteration 70, loss = 0.44565035\n",
      "Iteration 71, loss = 0.44502424\n",
      "Iteration 72, loss = 0.44431848\n",
      "Iteration 73, loss = 0.44382536\n",
      "Iteration 74, loss = 0.44342582\n",
      "Iteration 75, loss = 0.44264166\n",
      "Iteration 76, loss = 0.44211955\n",
      "Iteration 77, loss = 0.44157601\n",
      "Iteration 78, loss = 0.44107615\n",
      "Iteration 79, loss = 0.44045133\n",
      "Iteration 80, loss = 0.44018425\n",
      "Iteration 81, loss = 0.43980689\n",
      "Iteration 82, loss = 0.43900665\n",
      "Iteration 83, loss = 0.43890433\n",
      "Iteration 84, loss = 0.43848028\n",
      "Iteration 85, loss = 0.43788212\n",
      "Iteration 86, loss = 0.43750538\n",
      "Iteration 87, loss = 0.43721180\n",
      "Iteration 88, loss = 0.43664210\n",
      "Iteration 89, loss = 0.43662475\n",
      "Iteration 90, loss = 0.43584618\n",
      "Iteration 91, loss = 0.43564341\n",
      "Iteration 92, loss = 0.43513934\n",
      "Iteration 93, loss = 0.43486379\n",
      "Iteration 94, loss = 0.43443019\n",
      "Iteration 95, loss = 0.43417165\n",
      "Iteration 96, loss = 0.43391853\n",
      "Iteration 97, loss = 0.43362113\n",
      "Iteration 98, loss = 0.43324622\n",
      "Iteration 99, loss = 0.43285232\n",
      "Iteration 100, loss = 0.43256598\n",
      "Iteration 101, loss = 0.43237318\n",
      "Iteration 102, loss = 0.43207655\n",
      "Iteration 103, loss = 0.43183429\n",
      "Iteration 104, loss = 0.43140657\n",
      "Iteration 105, loss = 0.43110327\n",
      "Iteration 106, loss = 0.43083404\n",
      "Iteration 107, loss = 0.43068086\n",
      "Iteration 108, loss = 0.43023550\n",
      "Iteration 109, loss = 0.42998943\n",
      "Iteration 110, loss = 0.42974290\n",
      "Iteration 111, loss = 0.42960885\n",
      "Iteration 112, loss = 0.42949878\n",
      "Iteration 113, loss = 0.42919366\n",
      "Iteration 114, loss = 0.42872312\n",
      "Iteration 115, loss = 0.42862908\n",
      "Iteration 116, loss = 0.42842395\n",
      "Iteration 117, loss = 0.42797016\n",
      "Iteration 118, loss = 0.42809597\n",
      "Iteration 119, loss = 0.42776712\n",
      "Iteration 120, loss = 0.42740022\n",
      "Iteration 121, loss = 0.42716300\n",
      "Iteration 122, loss = 0.42714976\n",
      "Iteration 123, loss = 0.42663962\n",
      "Iteration 124, loss = 0.42666813\n",
      "Iteration 125, loss = 0.42634338\n",
      "Iteration 126, loss = 0.42609169\n",
      "Iteration 127, loss = 0.42596339\n",
      "Iteration 128, loss = 0.42556351\n",
      "Iteration 129, loss = 0.42548997\n",
      "Iteration 130, loss = 0.42513066\n",
      "Iteration 131, loss = 0.42513866\n",
      "Iteration 132, loss = 0.42486300\n",
      "Iteration 133, loss = 0.42481627\n",
      "Iteration 134, loss = 0.42456164\n",
      "Iteration 135, loss = 0.42446173\n",
      "Iteration 136, loss = 0.42411571\n",
      "Iteration 137, loss = 0.42387330\n",
      "Iteration 138, loss = 0.42370646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 139, loss = 0.42376540\n",
      "Iteration 140, loss = 0.42340326\n",
      "Iteration 141, loss = 0.42314669\n",
      "Iteration 142, loss = 0.42309843\n",
      "Iteration 143, loss = 0.42279671\n",
      "Iteration 144, loss = 0.42280006\n",
      "Iteration 145, loss = 0.42256562\n",
      "Iteration 146, loss = 0.42227478\n",
      "Iteration 147, loss = 0.42210523\n",
      "Iteration 148, loss = 0.42189970\n",
      "Iteration 149, loss = 0.42184646\n",
      "Iteration 150, loss = 0.42155476\n",
      "Iteration 151, loss = 0.42160844\n",
      "Iteration 152, loss = 0.42111335\n",
      "Iteration 153, loss = 0.42090427\n",
      "Iteration 154, loss = 0.42101680\n",
      "Iteration 155, loss = 0.42105549\n",
      "Iteration 156, loss = 0.42056252\n",
      "Iteration 157, loss = 0.42060899\n",
      "Iteration 158, loss = 0.42039194\n",
      "Iteration 159, loss = 0.42030493\n",
      "Iteration 160, loss = 0.42014046\n",
      "Iteration 161, loss = 0.41989943\n",
      "Iteration 162, loss = 0.41976638\n",
      "Iteration 163, loss = 0.41951141\n",
      "Iteration 164, loss = 0.41938151\n",
      "Iteration 165, loss = 0.41944864\n",
      "Iteration 166, loss = 0.41945926\n",
      "Iteration 167, loss = 0.41905155\n",
      "Iteration 168, loss = 0.41877708\n",
      "Iteration 169, loss = 0.41873774\n",
      "Iteration 170, loss = 0.41865043\n",
      "Iteration 171, loss = 0.41861890\n",
      "Iteration 172, loss = 0.41821854\n",
      "Iteration 173, loss = 0.41814836\n",
      "Iteration 174, loss = 0.41833636\n",
      "Iteration 175, loss = 0.41811869\n",
      "Iteration 176, loss = 0.41759547\n",
      "Iteration 177, loss = 0.41758425\n",
      "Iteration 178, loss = 0.41730349\n",
      "Iteration 179, loss = 0.41744593\n",
      "Iteration 180, loss = 0.41719878\n",
      "Iteration 181, loss = 0.41742299\n",
      "Iteration 182, loss = 0.41701435\n",
      "Iteration 183, loss = 0.41682082\n",
      "Iteration 184, loss = 0.41650072\n",
      "Iteration 185, loss = 0.41668654\n",
      "Iteration 186, loss = 0.41651270\n",
      "Iteration 187, loss = 0.41636046\n",
      "Iteration 188, loss = 0.41606805\n",
      "Iteration 189, loss = 0.41612497\n",
      "Iteration 190, loss = 0.41613696\n",
      "Iteration 191, loss = 0.41592289\n",
      "Iteration 192, loss = 0.41583367\n",
      "Iteration 193, loss = 0.41566998\n",
      "Iteration 194, loss = 0.41554587\n",
      "Iteration 195, loss = 0.41533724\n",
      "Iteration 196, loss = 0.41532420\n",
      "Iteration 197, loss = 0.41529942\n",
      "Iteration 198, loss = 0.41494077\n",
      "Iteration 199, loss = 0.41487807\n",
      "Iteration 200, loss = 0.41492638\n",
      "Iteration 201, loss = 0.41484353\n",
      "Iteration 202, loss = 0.41473439\n",
      "Iteration 203, loss = 0.41457620\n",
      "Iteration 204, loss = 0.41437181\n",
      "Iteration 205, loss = 0.41420630\n",
      "Iteration 206, loss = 0.41416706\n",
      "Iteration 207, loss = 0.41403745\n",
      "Iteration 208, loss = 0.41390905\n",
      "Iteration 209, loss = 0.41366091\n",
      "Iteration 210, loss = 0.41381701\n",
      "Iteration 211, loss = 0.41334341\n",
      "Iteration 212, loss = 0.41388756\n",
      "Iteration 213, loss = 0.41337905\n",
      "Iteration 214, loss = 0.41333130\n",
      "Iteration 215, loss = 0.41326827\n",
      "Iteration 216, loss = 0.41300984\n",
      "Iteration 217, loss = 0.41292916\n",
      "Iteration 218, loss = 0.41303939\n",
      "Iteration 219, loss = 0.41262092\n",
      "Iteration 220, loss = 0.41247041\n",
      "Iteration 221, loss = 0.41240993\n",
      "Iteration 222, loss = 0.41261389\n",
      "Iteration 223, loss = 0.41219771\n",
      "Iteration 224, loss = 0.41234250\n",
      "Iteration 225, loss = 0.41203914\n",
      "Iteration 226, loss = 0.41219082\n",
      "Iteration 227, loss = 0.41201203\n",
      "Iteration 228, loss = 0.41179101\n",
      "Iteration 229, loss = 0.41170220\n",
      "Iteration 230, loss = 0.41180362\n",
      "Iteration 231, loss = 0.41160850\n",
      "Iteration 232, loss = 0.41138202\n",
      "Iteration 233, loss = 0.41147593\n",
      "Iteration 234, loss = 0.41124565\n",
      "Iteration 235, loss = 0.41128794\n",
      "Iteration 236, loss = 0.41107852\n",
      "Iteration 237, loss = 0.41127994\n",
      "Iteration 238, loss = 0.41085923\n",
      "Iteration 239, loss = 0.41077473\n",
      "Iteration 240, loss = 0.41076685\n",
      "Iteration 241, loss = 0.41052365\n",
      "Iteration 242, loss = 0.41068551\n",
      "Iteration 243, loss = 0.41039557\n",
      "Iteration 244, loss = 0.41037934\n",
      "Iteration 245, loss = 0.41028135\n",
      "Iteration 246, loss = 0.41028208\n",
      "Iteration 247, loss = 0.41003294\n",
      "Iteration 248, loss = 0.41015448\n",
      "Iteration 249, loss = 0.40999263\n",
      "Iteration 250, loss = 0.40975811\n",
      "Iteration 251, loss = 0.40962165\n",
      "Iteration 252, loss = 0.40962766\n",
      "Iteration 253, loss = 0.40950198\n",
      "Iteration 254, loss = 0.40930699\n",
      "Iteration 255, loss = 0.40933925\n",
      "Iteration 256, loss = 0.40925377\n",
      "Iteration 257, loss = 0.40920522\n",
      "Iteration 258, loss = 0.40925602\n",
      "Iteration 259, loss = 0.40917085\n",
      "Iteration 260, loss = 0.40904758\n",
      "Iteration 261, loss = 0.40894860\n",
      "Iteration 262, loss = 0.40901528\n",
      "Iteration 263, loss = 0.40874596\n",
      "Iteration 264, loss = 0.40862669\n",
      "Iteration 265, loss = 0.40862145\n",
      "Iteration 266, loss = 0.40838417\n",
      "Iteration 267, loss = 0.40827239\n",
      "Iteration 268, loss = 0.40836175\n",
      "Iteration 269, loss = 0.40824646\n",
      "Iteration 270, loss = 0.40833554\n",
      "Iteration 271, loss = 0.40790871\n",
      "Iteration 272, loss = 0.40790570\n",
      "Iteration 273, loss = 0.40800112\n",
      "Iteration 274, loss = 0.40801551\n",
      "Iteration 275, loss = 0.40765325\n",
      "Iteration 276, loss = 0.40764350\n",
      "Iteration 277, loss = 0.40784468\n",
      "Iteration 278, loss = 0.40758748\n",
      "Iteration 279, loss = 0.40735442\n",
      "Iteration 280, loss = 0.40719903\n",
      "Iteration 281, loss = 0.40716736\n",
      "Iteration 282, loss = 0.40744860\n",
      "Iteration 283, loss = 0.40727379\n",
      "Iteration 284, loss = 0.40714512\n",
      "Iteration 285, loss = 0.40690193\n",
      "Iteration 286, loss = 0.40697151\n",
      "Iteration 287, loss = 0.40677775\n",
      "Iteration 288, loss = 0.40660871\n",
      "Iteration 289, loss = 0.40678779\n",
      "Iteration 290, loss = 0.40660144\n",
      "Iteration 291, loss = 0.40643833\n",
      "Iteration 292, loss = 0.40665387\n",
      "Iteration 293, loss = 0.40655017\n",
      "Iteration 294, loss = 0.40616971\n",
      "Iteration 295, loss = 0.40630529\n",
      "Iteration 296, loss = 0.40613263\n",
      "Iteration 297, loss = 0.40608509\n",
      "Iteration 298, loss = 0.40624077\n",
      "Iteration 299, loss = 0.40610310\n",
      "Iteration 300, loss = 0.40588084\n",
      "Iteration 301, loss = 0.40602216\n",
      "Iteration 302, loss = 0.40573808\n",
      "Iteration 303, loss = 0.40586051\n",
      "Iteration 304, loss = 0.40577907\n",
      "Iteration 305, loss = 0.40573873\n",
      "Iteration 306, loss = 0.40542132\n",
      "Iteration 307, loss = 0.40543836\n",
      "Iteration 308, loss = 0.40560022\n",
      "Iteration 309, loss = 0.40519455\n",
      "Iteration 310, loss = 0.40527549\n",
      "Iteration 311, loss = 0.40526613\n",
      "Iteration 312, loss = 0.40512398\n",
      "Iteration 313, loss = 0.40500609\n",
      "Iteration 314, loss = 0.40501332\n",
      "Iteration 315, loss = 0.40512500\n",
      "Iteration 316, loss = 0.40498748\n",
      "Iteration 317, loss = 0.40469104\n",
      "Iteration 318, loss = 0.40490214\n",
      "Iteration 319, loss = 0.40481552\n",
      "Iteration 320, loss = 0.40453073\n",
      "Iteration 321, loss = 0.40480411\n",
      "Iteration 322, loss = 0.40446326\n",
      "Iteration 323, loss = 0.40458711\n",
      "Iteration 324, loss = 0.40442568\n",
      "Iteration 325, loss = 0.40446554\n",
      "Iteration 326, loss = 0.40474713\n",
      "Iteration 327, loss = 0.40423341\n",
      "Iteration 328, loss = 0.40390538\n",
      "Iteration 329, loss = 0.40416295\n",
      "Iteration 330, loss = 0.40397740\n",
      "Iteration 331, loss = 0.40418842\n",
      "Iteration 332, loss = 0.40386014\n",
      "Iteration 333, loss = 0.40404378\n",
      "Iteration 334, loss = 0.40387814\n",
      "Iteration 335, loss = 0.40377628\n",
      "Iteration 336, loss = 0.40367518\n",
      "Iteration 337, loss = 0.40389554\n",
      "Iteration 338, loss = 0.40361826\n",
      "Iteration 339, loss = 0.40358247\n",
      "Iteration 340, loss = 0.40356187\n",
      "Iteration 341, loss = 0.40351096\n",
      "Iteration 342, loss = 0.40338499\n",
      "Iteration 343, loss = 0.40351543\n",
      "Iteration 344, loss = 0.40333797\n",
      "Iteration 345, loss = 0.40324332\n",
      "Iteration 346, loss = 0.40323545\n",
      "Iteration 347, loss = 0.40303720\n",
      "Iteration 348, loss = 0.40314566\n",
      "Iteration 349, loss = 0.40328172\n",
      "Iteration 350, loss = 0.40305929\n",
      "Iteration 351, loss = 0.40307157\n",
      "Iteration 352, loss = 0.40295474\n",
      "Iteration 353, loss = 0.40278704\n",
      "Iteration 354, loss = 0.40293996\n",
      "Iteration 355, loss = 0.40311525\n",
      "Iteration 356, loss = 0.40293455\n",
      "Iteration 357, loss = 0.40263754\n",
      "Iteration 358, loss = 0.40284532\n",
      "Iteration 359, loss = 0.40248111\n",
      "Iteration 360, loss = 0.40248085\n",
      "Iteration 361, loss = 0.40253051\n",
      "Iteration 362, loss = 0.40247915\n",
      "Iteration 363, loss = 0.40246030\n",
      "Iteration 364, loss = 0.40235582\n",
      "Iteration 365, loss = 0.40253080\n",
      "Iteration 366, loss = 0.40229487\n",
      "Iteration 367, loss = 0.40222443\n",
      "Iteration 368, loss = 0.40210648\n",
      "Iteration 369, loss = 0.40214748\n",
      "Iteration 370, loss = 0.40208059\n",
      "Iteration 371, loss = 0.40218329\n",
      "Iteration 372, loss = 0.40191236\n",
      "Iteration 373, loss = 0.40207938\n",
      "Iteration 374, loss = 0.40195258\n",
      "Iteration 375, loss = 0.40190735\n",
      "Iteration 376, loss = 0.40191252\n",
      "Iteration 377, loss = 0.40202676\n",
      "Iteration 378, loss = 0.40179448\n",
      "Iteration 379, loss = 0.40162977\n",
      "Iteration 380, loss = 0.40182637\n",
      "Iteration 381, loss = 0.40156421\n",
      "Iteration 382, loss = 0.40174395\n",
      "Iteration 383, loss = 0.40147973\n",
      "Iteration 384, loss = 0.40172406\n",
      "Iteration 385, loss = 0.40137575\n",
      "Iteration 386, loss = 0.40149802\n",
      "Iteration 387, loss = 0.40158775\n",
      "Iteration 388, loss = 0.40134744\n",
      "Iteration 389, loss = 0.40153668\n",
      "Iteration 390, loss = 0.40141337\n",
      "Iteration 391, loss = 0.40104880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 392, loss = 0.40114796\n",
      "Iteration 393, loss = 0.40126304\n",
      "Iteration 394, loss = 0.40116164\n",
      "Iteration 395, loss = 0.40098399\n",
      "Iteration 396, loss = 0.40118964\n",
      "Iteration 397, loss = 0.40077266\n",
      "Iteration 398, loss = 0.40084353\n",
      "Iteration 399, loss = 0.40094103\n",
      "Iteration 400, loss = 0.40056979\n",
      "Iteration 401, loss = 0.40076772\n",
      "Iteration 402, loss = 0.40085411\n",
      "Iteration 403, loss = 0.40075783\n",
      "Iteration 404, loss = 0.40064306\n",
      "Iteration 405, loss = 0.40028522\n",
      "Iteration 406, loss = 0.40030419\n",
      "Iteration 407, loss = 0.40035071\n",
      "Iteration 408, loss = 0.40017550\n",
      "Iteration 409, loss = 0.40051622\n",
      "Iteration 410, loss = 0.40066764\n",
      "Iteration 411, loss = 0.40019839\n",
      "Iteration 412, loss = 0.40046843\n",
      "Iteration 413, loss = 0.40011271\n",
      "Iteration 414, loss = 0.40004487\n",
      "Iteration 415, loss = 0.40008409\n",
      "Iteration 416, loss = 0.39999124\n",
      "Iteration 417, loss = 0.39986646\n",
      "Iteration 418, loss = 0.39992593\n",
      "Iteration 419, loss = 0.39990840\n",
      "Iteration 420, loss = 0.40001478\n",
      "Iteration 421, loss = 0.39969256\n",
      "Iteration 422, loss = 0.39964314\n",
      "Iteration 423, loss = 0.39978518\n",
      "Iteration 424, loss = 0.39955446\n",
      "Iteration 425, loss = 0.39949748\n",
      "Iteration 426, loss = 0.39963896\n",
      "Iteration 427, loss = 0.39967182\n",
      "Iteration 428, loss = 0.39951581\n",
      "Iteration 429, loss = 0.39953077\n",
      "Iteration 430, loss = 0.39959872\n",
      "Iteration 431, loss = 0.39960191\n",
      "Iteration 432, loss = 0.39940549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05988594\n",
      "Iteration 2, loss = 1.83535885\n",
      "Iteration 3, loss = 1.66401760\n",
      "Iteration 4, loss = 1.51828004\n",
      "Iteration 5, loss = 1.37443215\n",
      "Iteration 6, loss = 1.23620025\n",
      "Iteration 7, loss = 1.12064132\n",
      "Iteration 8, loss = 1.02988664\n",
      "Iteration 9, loss = 0.95875622\n",
      "Iteration 10, loss = 0.90147064\n",
      "Iteration 11, loss = 0.85362407\n",
      "Iteration 12, loss = 0.81315758\n",
      "Iteration 13, loss = 0.77889697\n",
      "Iteration 14, loss = 0.74985505\n",
      "Iteration 15, loss = 0.72510022\n",
      "Iteration 16, loss = 0.70333537\n",
      "Iteration 17, loss = 0.68410797\n",
      "Iteration 18, loss = 0.66719681\n",
      "Iteration 19, loss = 0.65170070\n",
      "Iteration 20, loss = 0.63772087\n",
      "Iteration 21, loss = 0.62484785\n",
      "Iteration 22, loss = 0.61324768\n",
      "Iteration 23, loss = 0.60289662\n",
      "Iteration 24, loss = 0.59261748\n",
      "Iteration 25, loss = 0.58345446\n",
      "Iteration 26, loss = 0.57504334\n",
      "Iteration 27, loss = 0.56695141\n",
      "Iteration 28, loss = 0.55951964\n",
      "Iteration 29, loss = 0.55253982\n",
      "Iteration 30, loss = 0.54608983\n",
      "Iteration 31, loss = 0.53972602\n",
      "Iteration 32, loss = 0.53397600\n",
      "Iteration 33, loss = 0.52850302\n",
      "Iteration 34, loss = 0.52311509\n",
      "Iteration 35, loss = 0.51843540\n",
      "Iteration 36, loss = 0.51359528\n",
      "Iteration 37, loss = 0.50907570\n",
      "Iteration 38, loss = 0.50459990\n",
      "Iteration 39, loss = 0.50089547\n",
      "Iteration 40, loss = 0.49681333\n",
      "Iteration 41, loss = 0.49313151\n",
      "Iteration 42, loss = 0.48957771\n",
      "Iteration 43, loss = 0.48656769\n",
      "Iteration 44, loss = 0.48360062\n",
      "Iteration 45, loss = 0.48067856\n",
      "Iteration 46, loss = 0.47834568\n",
      "Iteration 47, loss = 0.47574376\n",
      "Iteration 48, loss = 0.47327168\n",
      "Iteration 49, loss = 0.47111015\n",
      "Iteration 50, loss = 0.46905202\n",
      "Iteration 51, loss = 0.46728974\n",
      "Iteration 52, loss = 0.46566497\n",
      "Iteration 53, loss = 0.46394650\n",
      "Iteration 54, loss = 0.46219659\n",
      "Iteration 55, loss = 0.46102756\n",
      "Iteration 56, loss = 0.45930608\n",
      "Iteration 57, loss = 0.45824749\n",
      "Iteration 58, loss = 0.45683991\n",
      "Iteration 59, loss = 0.45596980\n",
      "Iteration 60, loss = 0.45479548\n",
      "Iteration 61, loss = 0.45360547\n",
      "Iteration 62, loss = 0.45259878\n",
      "Iteration 63, loss = 0.45168377\n",
      "Iteration 64, loss = 0.45061356\n",
      "Iteration 65, loss = 0.44997860\n",
      "Iteration 66, loss = 0.44907896\n",
      "Iteration 67, loss = 0.44805898\n",
      "Iteration 68, loss = 0.44738637\n",
      "Iteration 69, loss = 0.44687294\n",
      "Iteration 70, loss = 0.44646980\n",
      "Iteration 71, loss = 0.44524300\n",
      "Iteration 72, loss = 0.44480726\n",
      "Iteration 73, loss = 0.44387725\n",
      "Iteration 74, loss = 0.44342118\n",
      "Iteration 75, loss = 0.44314058\n",
      "Iteration 76, loss = 0.44234810\n",
      "Iteration 77, loss = 0.44203327\n",
      "Iteration 78, loss = 0.44116767\n",
      "Iteration 79, loss = 0.44091063\n",
      "Iteration 80, loss = 0.44039045\n",
      "Iteration 81, loss = 0.43973830\n",
      "Iteration 82, loss = 0.43933753\n",
      "Iteration 83, loss = 0.43916720\n",
      "Iteration 84, loss = 0.43837061\n",
      "Iteration 85, loss = 0.43791871\n",
      "Iteration 86, loss = 0.43763384\n",
      "Iteration 87, loss = 0.43705675\n",
      "Iteration 88, loss = 0.43698887\n",
      "Iteration 89, loss = 0.43638656\n",
      "Iteration 90, loss = 0.43612357\n",
      "Iteration 91, loss = 0.43557935\n",
      "Iteration 92, loss = 0.43511143\n",
      "Iteration 93, loss = 0.43499912\n",
      "Iteration 94, loss = 0.43463986\n",
      "Iteration 95, loss = 0.43429317\n",
      "Iteration 96, loss = 0.43404126\n",
      "Iteration 97, loss = 0.43346078\n",
      "Iteration 98, loss = 0.43312746\n",
      "Iteration 99, loss = 0.43279324\n",
      "Iteration 100, loss = 0.43251038\n",
      "Iteration 101, loss = 0.43233741\n",
      "Iteration 102, loss = 0.43206655\n",
      "Iteration 103, loss = 0.43164134\n",
      "Iteration 104, loss = 0.43145269\n",
      "Iteration 105, loss = 0.43133178\n",
      "Iteration 106, loss = 0.43089458\n",
      "Iteration 107, loss = 0.43067728\n",
      "Iteration 108, loss = 0.43035034\n",
      "Iteration 109, loss = 0.42980563\n",
      "Iteration 110, loss = 0.42991487\n",
      "Iteration 111, loss = 0.42970021\n",
      "Iteration 112, loss = 0.42963501\n",
      "Iteration 113, loss = 0.42925921\n",
      "Iteration 114, loss = 0.42890691\n",
      "Iteration 115, loss = 0.42853589\n",
      "Iteration 116, loss = 0.42844054\n",
      "Iteration 117, loss = 0.42831612\n",
      "Iteration 118, loss = 0.42810136\n",
      "Iteration 119, loss = 0.42776788\n",
      "Iteration 120, loss = 0.42747694\n",
      "Iteration 121, loss = 0.42732090\n",
      "Iteration 122, loss = 0.42730180\n",
      "Iteration 123, loss = 0.42676826\n",
      "Iteration 124, loss = 0.42681666\n",
      "Iteration 125, loss = 0.42648069\n",
      "Iteration 126, loss = 0.42649938\n",
      "Iteration 127, loss = 0.42620139\n",
      "Iteration 128, loss = 0.42603137\n",
      "Iteration 129, loss = 0.42579185\n",
      "Iteration 130, loss = 0.42571971\n",
      "Iteration 131, loss = 0.42545463\n",
      "Iteration 132, loss = 0.42537116\n",
      "Iteration 133, loss = 0.42485593\n",
      "Iteration 134, loss = 0.42473358\n",
      "Iteration 135, loss = 0.42499277\n",
      "Iteration 136, loss = 0.42471152\n",
      "Iteration 137, loss = 0.42457769\n",
      "Iteration 138, loss = 0.42419304\n",
      "Iteration 139, loss = 0.42418898\n",
      "Iteration 140, loss = 0.42404319\n",
      "Iteration 141, loss = 0.42376927\n",
      "Iteration 142, loss = 0.42365716\n",
      "Iteration 143, loss = 0.42336048\n",
      "Iteration 144, loss = 0.42321862\n",
      "Iteration 145, loss = 0.42344987\n",
      "Iteration 146, loss = 0.42299725\n",
      "Iteration 147, loss = 0.42273930\n",
      "Iteration 148, loss = 0.42278070\n",
      "Iteration 149, loss = 0.42275980\n",
      "Iteration 150, loss = 0.42275972\n",
      "Iteration 151, loss = 0.42233238\n",
      "Iteration 152, loss = 0.42218699\n",
      "Iteration 153, loss = 0.42199924\n",
      "Iteration 154, loss = 0.42183801\n",
      "Iteration 155, loss = 0.42121770\n",
      "Iteration 156, loss = 0.42154690\n",
      "Iteration 157, loss = 0.42133791\n",
      "Iteration 158, loss = 0.42136654\n",
      "Iteration 159, loss = 0.42105207\n",
      "Iteration 160, loss = 0.42088758\n",
      "Iteration 161, loss = 0.42107123\n",
      "Iteration 162, loss = 0.42072331\n",
      "Iteration 163, loss = 0.42076054\n",
      "Iteration 164, loss = 0.42039553\n",
      "Iteration 165, loss = 0.42028347\n",
      "Iteration 166, loss = 0.42038462\n",
      "Iteration 167, loss = 0.42012411\n",
      "Iteration 168, loss = 0.41982469\n",
      "Iteration 169, loss = 0.42000261\n",
      "Iteration 170, loss = 0.41970560\n",
      "Iteration 171, loss = 0.41963317\n",
      "Iteration 172, loss = 0.41944037\n",
      "Iteration 173, loss = 0.41919118\n",
      "Iteration 174, loss = 0.41936366\n",
      "Iteration 175, loss = 0.41918772\n",
      "Iteration 176, loss = 0.41913764\n",
      "Iteration 177, loss = 0.41883207\n",
      "Iteration 178, loss = 0.41855611\n",
      "Iteration 179, loss = 0.41882307\n",
      "Iteration 180, loss = 0.41830481\n",
      "Iteration 181, loss = 0.41838421\n",
      "Iteration 182, loss = 0.41813999\n",
      "Iteration 183, loss = 0.41810452\n",
      "Iteration 184, loss = 0.41807865\n",
      "Iteration 185, loss = 0.41810747\n",
      "Iteration 186, loss = 0.41813915\n",
      "Iteration 187, loss = 0.41750868\n",
      "Iteration 188, loss = 0.41759195\n",
      "Iteration 189, loss = 0.41738071\n",
      "Iteration 190, loss = 0.41731589\n",
      "Iteration 191, loss = 0.41708264\n",
      "Iteration 192, loss = 0.41720294\n",
      "Iteration 193, loss = 0.41699821\n",
      "Iteration 194, loss = 0.41699033\n",
      "Iteration 195, loss = 0.41696310\n",
      "Iteration 196, loss = 0.41670948\n",
      "Iteration 197, loss = 0.41684827\n",
      "Iteration 198, loss = 0.41667589\n",
      "Iteration 199, loss = 0.41651747\n",
      "Iteration 200, loss = 0.41636916\n",
      "Iteration 201, loss = 0.41624211\n",
      "Iteration 202, loss = 0.41603977\n",
      "Iteration 203, loss = 0.41610820\n",
      "Iteration 204, loss = 0.41597800\n",
      "Iteration 205, loss = 0.41601481\n",
      "Iteration 206, loss = 0.41548534\n",
      "Iteration 207, loss = 0.41575634\n",
      "Iteration 208, loss = 0.41515650\n",
      "Iteration 209, loss = 0.41543562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210, loss = 0.41524987\n",
      "Iteration 211, loss = 0.41521688\n",
      "Iteration 212, loss = 0.41509832\n",
      "Iteration 213, loss = 0.41510099\n",
      "Iteration 214, loss = 0.41491085\n",
      "Iteration 215, loss = 0.41490817\n",
      "Iteration 216, loss = 0.41468910\n",
      "Iteration 217, loss = 0.41466889\n",
      "Iteration 218, loss = 0.41449540\n",
      "Iteration 219, loss = 0.41427592\n",
      "Iteration 220, loss = 0.41414469\n",
      "Iteration 221, loss = 0.41442127\n",
      "Iteration 222, loss = 0.41388997\n",
      "Iteration 223, loss = 0.41371018\n",
      "Iteration 224, loss = 0.41402658\n",
      "Iteration 225, loss = 0.41393266\n",
      "Iteration 226, loss = 0.41384218\n",
      "Iteration 227, loss = 0.41363934\n",
      "Iteration 228, loss = 0.41340337\n",
      "Iteration 229, loss = 0.41351763\n",
      "Iteration 230, loss = 0.41336061\n",
      "Iteration 231, loss = 0.41309419\n",
      "Iteration 232, loss = 0.41326510\n",
      "Iteration 233, loss = 0.41304132\n",
      "Iteration 234, loss = 0.41300713\n",
      "Iteration 235, loss = 0.41294114\n",
      "Iteration 236, loss = 0.41294801\n",
      "Iteration 237, loss = 0.41267649\n",
      "Iteration 238, loss = 0.41275436\n",
      "Iteration 239, loss = 0.41266098\n",
      "Iteration 240, loss = 0.41228852\n",
      "Iteration 241, loss = 0.41231205\n",
      "Iteration 242, loss = 0.41213235\n",
      "Iteration 243, loss = 0.41219461\n",
      "Iteration 244, loss = 0.41211648\n",
      "Iteration 245, loss = 0.41197625\n",
      "Iteration 246, loss = 0.41200499\n",
      "Iteration 247, loss = 0.41188693\n",
      "Iteration 248, loss = 0.41202910\n",
      "Iteration 249, loss = 0.41166418\n",
      "Iteration 250, loss = 0.41143093\n",
      "Iteration 251, loss = 0.41148370\n",
      "Iteration 252, loss = 0.41115801\n",
      "Iteration 253, loss = 0.41096083\n",
      "Iteration 254, loss = 0.41138466\n",
      "Iteration 255, loss = 0.41095060\n",
      "Iteration 256, loss = 0.41110038\n",
      "Iteration 257, loss = 0.41102520\n",
      "Iteration 258, loss = 0.41086524\n",
      "Iteration 259, loss = 0.41069654\n",
      "Iteration 260, loss = 0.41078811\n",
      "Iteration 261, loss = 0.41081881\n",
      "Iteration 262, loss = 0.41053083\n",
      "Iteration 263, loss = 0.41025076\n",
      "Iteration 264, loss = 0.41045473\n",
      "Iteration 265, loss = 0.41031662\n",
      "Iteration 266, loss = 0.41004456\n",
      "Iteration 267, loss = 0.41014648\n",
      "Iteration 268, loss = 0.41004239\n",
      "Iteration 269, loss = 0.40961654\n",
      "Iteration 270, loss = 0.40979962\n",
      "Iteration 271, loss = 0.40995705\n",
      "Iteration 272, loss = 0.40979922\n",
      "Iteration 273, loss = 0.40979805\n",
      "Iteration 274, loss = 0.40960989\n",
      "Iteration 275, loss = 0.40935850\n",
      "Iteration 276, loss = 0.40937423\n",
      "Iteration 277, loss = 0.40941815\n",
      "Iteration 278, loss = 0.40925184\n",
      "Iteration 279, loss = 0.40913111\n",
      "Iteration 280, loss = 0.40924048\n",
      "Iteration 281, loss = 0.40905264\n",
      "Iteration 282, loss = 0.40902154\n",
      "Iteration 283, loss = 0.40868705\n",
      "Iteration 284, loss = 0.40870398\n",
      "Iteration 285, loss = 0.40890373\n",
      "Iteration 286, loss = 0.40857898\n",
      "Iteration 287, loss = 0.40852207\n",
      "Iteration 288, loss = 0.40867455\n",
      "Iteration 289, loss = 0.40838513\n",
      "Iteration 290, loss = 0.40823171\n",
      "Iteration 291, loss = 0.40828955\n",
      "Iteration 292, loss = 0.40802213\n",
      "Iteration 293, loss = 0.40786285\n",
      "Iteration 294, loss = 0.40789529\n",
      "Iteration 295, loss = 0.40766619\n",
      "Iteration 296, loss = 0.40764582\n",
      "Iteration 297, loss = 0.40768338\n",
      "Iteration 298, loss = 0.40748500\n",
      "Iteration 299, loss = 0.40764654\n",
      "Iteration 300, loss = 0.40712631\n",
      "Iteration 301, loss = 0.40712116\n",
      "Iteration 302, loss = 0.40697457\n",
      "Iteration 303, loss = 0.40715662\n",
      "Iteration 304, loss = 0.40695723\n",
      "Iteration 305, loss = 0.40702384\n",
      "Iteration 306, loss = 0.40722979\n",
      "Iteration 307, loss = 0.40685655\n",
      "Iteration 308, loss = 0.40687605\n",
      "Iteration 309, loss = 0.40658665\n",
      "Iteration 310, loss = 0.40692405\n",
      "Iteration 311, loss = 0.40657304\n",
      "Iteration 312, loss = 0.40657683\n",
      "Iteration 313, loss = 0.40661322\n",
      "Iteration 314, loss = 0.40644675\n",
      "Iteration 315, loss = 0.40597938\n",
      "Iteration 316, loss = 0.40670940\n",
      "Iteration 317, loss = 0.40601683\n",
      "Iteration 318, loss = 0.40622262\n",
      "Iteration 319, loss = 0.40596927\n",
      "Iteration 320, loss = 0.40587330\n",
      "Iteration 321, loss = 0.40578295\n",
      "Iteration 322, loss = 0.40590161\n",
      "Iteration 323, loss = 0.40571442\n",
      "Iteration 324, loss = 0.40585151\n",
      "Iteration 325, loss = 0.40546241\n",
      "Iteration 326, loss = 0.40556837\n",
      "Iteration 327, loss = 0.40539438\n",
      "Iteration 328, loss = 0.40526019\n",
      "Iteration 329, loss = 0.40527458\n",
      "Iteration 330, loss = 0.40519148\n",
      "Iteration 331, loss = 0.40523052\n",
      "Iteration 332, loss = 0.40520992\n",
      "Iteration 333, loss = 0.40491293\n",
      "Iteration 334, loss = 0.40524556\n",
      "Iteration 335, loss = 0.40517254\n",
      "Iteration 336, loss = 0.40493372\n",
      "Iteration 337, loss = 0.40494124\n",
      "Iteration 338, loss = 0.40477433\n",
      "Iteration 339, loss = 0.40483543\n",
      "Iteration 340, loss = 0.40465721\n",
      "Iteration 341, loss = 0.40465640\n",
      "Iteration 342, loss = 0.40469418\n",
      "Iteration 343, loss = 0.40448373\n",
      "Iteration 344, loss = 0.40435419\n",
      "Iteration 345, loss = 0.40440632\n",
      "Iteration 346, loss = 0.40420373\n",
      "Iteration 347, loss = 0.40421312\n",
      "Iteration 348, loss = 0.40431505\n",
      "Iteration 349, loss = 0.40426206\n",
      "Iteration 350, loss = 0.40401581\n",
      "Iteration 351, loss = 0.40405788\n",
      "Iteration 352, loss = 0.40406293\n",
      "Iteration 353, loss = 0.40411232\n",
      "Iteration 354, loss = 0.40393168\n",
      "Iteration 355, loss = 0.40388160\n",
      "Iteration 356, loss = 0.40361894\n",
      "Iteration 357, loss = 0.40370500\n",
      "Iteration 358, loss = 0.40361913\n",
      "Iteration 359, loss = 0.40362802\n",
      "Iteration 360, loss = 0.40348322\n",
      "Iteration 361, loss = 0.40348424\n",
      "Iteration 362, loss = 0.40352502\n",
      "Iteration 363, loss = 0.40357751\n",
      "Iteration 364, loss = 0.40312269\n",
      "Iteration 365, loss = 0.40316628\n",
      "Iteration 366, loss = 0.40331531\n",
      "Iteration 367, loss = 0.40337797\n",
      "Iteration 368, loss = 0.40310153\n",
      "Iteration 369, loss = 0.40304856\n",
      "Iteration 370, loss = 0.40271572\n",
      "Iteration 371, loss = 0.40288948\n",
      "Iteration 372, loss = 0.40287515\n",
      "Iteration 373, loss = 0.40291683\n",
      "Iteration 374, loss = 0.40262820\n",
      "Iteration 375, loss = 0.40257863\n",
      "Iteration 376, loss = 0.40278504\n",
      "Iteration 377, loss = 0.40244696\n",
      "Iteration 378, loss = 0.40265441\n",
      "Iteration 379, loss = 0.40248298\n",
      "Iteration 380, loss = 0.40234344\n",
      "Iteration 381, loss = 0.40235407\n",
      "Iteration 382, loss = 0.40229317\n",
      "Iteration 383, loss = 0.40227515\n",
      "Iteration 384, loss = 0.40202851\n",
      "Iteration 385, loss = 0.40238415\n",
      "Iteration 386, loss = 0.40220135\n",
      "Iteration 387, loss = 0.40217323\n",
      "Iteration 388, loss = 0.40208339\n",
      "Iteration 389, loss = 0.40190403\n",
      "Iteration 390, loss = 0.40210652\n",
      "Iteration 391, loss = 0.40199103\n",
      "Iteration 392, loss = 0.40197196\n",
      "Iteration 393, loss = 0.40184425\n",
      "Iteration 394, loss = 0.40170045\n",
      "Iteration 395, loss = 0.40162895\n",
      "Iteration 396, loss = 0.40141663\n",
      "Iteration 397, loss = 0.40166951\n",
      "Iteration 398, loss = 0.40139970\n",
      "Iteration 399, loss = 0.40170501\n",
      "Iteration 400, loss = 0.40181226\n",
      "Iteration 401, loss = 0.40172694\n",
      "Iteration 402, loss = 0.40157232\n",
      "Iteration 403, loss = 0.40125146\n",
      "Iteration 404, loss = 0.40145670\n",
      "Iteration 405, loss = 0.40137518\n",
      "Iteration 406, loss = 0.40113344\n",
      "Iteration 407, loss = 0.40124987\n",
      "Iteration 408, loss = 0.40110483\n",
      "Iteration 409, loss = 0.40100911\n",
      "Iteration 410, loss = 0.40128999\n",
      "Iteration 411, loss = 0.40100382\n",
      "Iteration 412, loss = 0.40113988\n",
      "Iteration 413, loss = 0.40088676\n",
      "Iteration 414, loss = 0.40094375\n",
      "Iteration 415, loss = 0.40091317\n",
      "Iteration 416, loss = 0.40096506\n",
      "Iteration 417, loss = 0.40076951\n",
      "Iteration 418, loss = 0.40063002\n",
      "Iteration 419, loss = 0.40062964\n",
      "Iteration 420, loss = 0.40070295\n",
      "Iteration 421, loss = 0.40062557\n",
      "Iteration 422, loss = 0.40060091\n",
      "Iteration 423, loss = 0.40065263\n",
      "Iteration 424, loss = 0.40048369\n",
      "Iteration 425, loss = 0.40059466\n",
      "Iteration 426, loss = 0.40063501\n",
      "Iteration 427, loss = 0.40025723\n",
      "Iteration 428, loss = 0.40023559\n",
      "Iteration 429, loss = 0.40023903\n",
      "Iteration 430, loss = 0.40014162\n",
      "Iteration 431, loss = 0.40023635\n",
      "Iteration 432, loss = 0.39996524\n",
      "Iteration 433, loss = 0.40005255\n",
      "Iteration 434, loss = 0.40008510\n",
      "Iteration 435, loss = 0.40010123\n",
      "Iteration 436, loss = 0.39990676\n",
      "Iteration 437, loss = 0.40044141\n",
      "Iteration 438, loss = 0.39983597\n",
      "Iteration 439, loss = 0.39976452\n",
      "Iteration 440, loss = 0.39987651\n",
      "Iteration 441, loss = 0.39974187\n",
      "Iteration 442, loss = 0.39985127\n",
      "Iteration 443, loss = 0.39990292\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.06116375\n",
      "Iteration 2, loss = 1.83844593\n",
      "Iteration 3, loss = 1.66815098\n",
      "Iteration 4, loss = 1.52334638\n",
      "Iteration 5, loss = 1.37927908\n",
      "Iteration 6, loss = 1.23976898\n",
      "Iteration 7, loss = 1.12293895\n",
      "Iteration 8, loss = 1.03125676\n",
      "Iteration 9, loss = 0.95971259\n",
      "Iteration 10, loss = 0.90214324\n",
      "Iteration 11, loss = 0.85404101\n",
      "Iteration 12, loss = 0.81339140\n",
      "Iteration 13, loss = 0.77909812\n",
      "Iteration 14, loss = 0.74989167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.72493668\n",
      "Iteration 16, loss = 0.70312577\n",
      "Iteration 17, loss = 0.68381498\n",
      "Iteration 18, loss = 0.66660660\n",
      "Iteration 19, loss = 0.65118054\n",
      "Iteration 20, loss = 0.63725040\n",
      "Iteration 21, loss = 0.62457189\n",
      "Iteration 22, loss = 0.61254667\n",
      "Iteration 23, loss = 0.60184005\n",
      "Iteration 24, loss = 0.59170737\n",
      "Iteration 25, loss = 0.58238716\n",
      "Iteration 26, loss = 0.57368653\n",
      "Iteration 27, loss = 0.56585646\n",
      "Iteration 28, loss = 0.55800854\n",
      "Iteration 29, loss = 0.55090355\n",
      "Iteration 30, loss = 0.54439967\n",
      "Iteration 31, loss = 0.53806898\n",
      "Iteration 32, loss = 0.53193561\n",
      "Iteration 33, loss = 0.52622089\n",
      "Iteration 34, loss = 0.52122347\n",
      "Iteration 35, loss = 0.51578028\n",
      "Iteration 36, loss = 0.51111696\n",
      "Iteration 37, loss = 0.50646677\n",
      "Iteration 38, loss = 0.50208592\n",
      "Iteration 39, loss = 0.49810861\n",
      "Iteration 40, loss = 0.49416622\n",
      "Iteration 41, loss = 0.49032897\n",
      "Iteration 42, loss = 0.48690394\n",
      "Iteration 43, loss = 0.48364774\n",
      "Iteration 44, loss = 0.48073497\n",
      "Iteration 45, loss = 0.47778635\n",
      "Iteration 46, loss = 0.47526413\n",
      "Iteration 47, loss = 0.47312502\n",
      "Iteration 48, loss = 0.47077484\n",
      "Iteration 49, loss = 0.46861809\n",
      "Iteration 50, loss = 0.46651434\n",
      "Iteration 51, loss = 0.46483733\n",
      "Iteration 52, loss = 0.46308501\n",
      "Iteration 53, loss = 0.46128384\n",
      "Iteration 54, loss = 0.46006260\n",
      "Iteration 55, loss = 0.45859780\n",
      "Iteration 56, loss = 0.45716333\n",
      "Iteration 57, loss = 0.45603487\n",
      "Iteration 58, loss = 0.45489772\n",
      "Iteration 59, loss = 0.45364394\n",
      "Iteration 60, loss = 0.45246303\n",
      "Iteration 61, loss = 0.45167791\n",
      "Iteration 62, loss = 0.45073316\n",
      "Iteration 63, loss = 0.44973810\n",
      "Iteration 64, loss = 0.44882308\n",
      "Iteration 65, loss = 0.44811482\n",
      "Iteration 66, loss = 0.44715161\n",
      "Iteration 67, loss = 0.44660762\n",
      "Iteration 68, loss = 0.44568419\n",
      "Iteration 69, loss = 0.44483640\n",
      "Iteration 70, loss = 0.44433406\n",
      "Iteration 71, loss = 0.44356835\n",
      "Iteration 72, loss = 0.44326276\n",
      "Iteration 73, loss = 0.44238902\n",
      "Iteration 74, loss = 0.44175722\n",
      "Iteration 75, loss = 0.44129134\n",
      "Iteration 76, loss = 0.44074977\n",
      "Iteration 77, loss = 0.44028643\n",
      "Iteration 78, loss = 0.43966831\n",
      "Iteration 79, loss = 0.43909906\n",
      "Iteration 80, loss = 0.43856702\n",
      "Iteration 81, loss = 0.43817415\n",
      "Iteration 82, loss = 0.43778384\n",
      "Iteration 83, loss = 0.43732118\n",
      "Iteration 84, loss = 0.43693982\n",
      "Iteration 85, loss = 0.43636575\n",
      "Iteration 86, loss = 0.43611889\n",
      "Iteration 87, loss = 0.43575177\n",
      "Iteration 88, loss = 0.43514505\n",
      "Iteration 89, loss = 0.43487970\n",
      "Iteration 90, loss = 0.43447994\n",
      "Iteration 91, loss = 0.43408286\n",
      "Iteration 92, loss = 0.43366545\n",
      "Iteration 93, loss = 0.43355015\n",
      "Iteration 94, loss = 0.43288689\n",
      "Iteration 95, loss = 0.43285432\n",
      "Iteration 96, loss = 0.43239422\n",
      "Iteration 97, loss = 0.43218985\n",
      "Iteration 98, loss = 0.43171308\n",
      "Iteration 99, loss = 0.43169882\n",
      "Iteration 100, loss = 0.43116517\n",
      "Iteration 101, loss = 0.43098929\n",
      "Iteration 102, loss = 0.43047723\n",
      "Iteration 103, loss = 0.43035692\n",
      "Iteration 104, loss = 0.42970780\n",
      "Iteration 105, loss = 0.42943610\n",
      "Iteration 106, loss = 0.42924189\n",
      "Iteration 107, loss = 0.42900096\n",
      "Iteration 108, loss = 0.42888371\n",
      "Iteration 109, loss = 0.42856369\n",
      "Iteration 110, loss = 0.42868969\n",
      "Iteration 111, loss = 0.42809982\n",
      "Iteration 112, loss = 0.42807323\n",
      "Iteration 113, loss = 0.42746023\n",
      "Iteration 114, loss = 0.42716473\n",
      "Iteration 115, loss = 0.42704578\n",
      "Iteration 116, loss = 0.42703645\n",
      "Iteration 117, loss = 0.42674240\n",
      "Iteration 118, loss = 0.42655352\n",
      "Iteration 119, loss = 0.42618537\n",
      "Iteration 120, loss = 0.42596037\n",
      "Iteration 121, loss = 0.42587440\n",
      "Iteration 122, loss = 0.42554193\n",
      "Iteration 123, loss = 0.42549953\n",
      "Iteration 124, loss = 0.42542696\n",
      "Iteration 125, loss = 0.42528203\n",
      "Iteration 126, loss = 0.42489990\n",
      "Iteration 127, loss = 0.42474044\n",
      "Iteration 128, loss = 0.42459913\n",
      "Iteration 129, loss = 0.42436072\n",
      "Iteration 130, loss = 0.42421405\n",
      "Iteration 131, loss = 0.42394384\n",
      "Iteration 132, loss = 0.42384794\n",
      "Iteration 133, loss = 0.42348150\n",
      "Iteration 134, loss = 0.42346416\n",
      "Iteration 135, loss = 0.42339612\n",
      "Iteration 136, loss = 0.42348744\n",
      "Iteration 137, loss = 0.42288836\n",
      "Iteration 138, loss = 0.42265113\n",
      "Iteration 139, loss = 0.42263575\n",
      "Iteration 140, loss = 0.42236509\n",
      "Iteration 141, loss = 0.42245054\n",
      "Iteration 142, loss = 0.42209017\n",
      "Iteration 143, loss = 0.42219212\n",
      "Iteration 144, loss = 0.42185797\n",
      "Iteration 145, loss = 0.42184077\n",
      "Iteration 146, loss = 0.42159222\n",
      "Iteration 147, loss = 0.42146082\n",
      "Iteration 148, loss = 0.42156756\n",
      "Iteration 149, loss = 0.42120582\n",
      "Iteration 150, loss = 0.42117680\n",
      "Iteration 151, loss = 0.42058992\n",
      "Iteration 152, loss = 0.42061131\n",
      "Iteration 153, loss = 0.42049359\n",
      "Iteration 154, loss = 0.42064806\n",
      "Iteration 155, loss = 0.42049142\n",
      "Iteration 156, loss = 0.42003463\n",
      "Iteration 157, loss = 0.41998490\n",
      "Iteration 158, loss = 0.41995516\n",
      "Iteration 159, loss = 0.41967451\n",
      "Iteration 160, loss = 0.41962678\n",
      "Iteration 161, loss = 0.41942449\n",
      "Iteration 162, loss = 0.41934706\n",
      "Iteration 163, loss = 0.41931061\n",
      "Iteration 164, loss = 0.41920100\n",
      "Iteration 165, loss = 0.41867173\n",
      "Iteration 166, loss = 0.41871847\n",
      "Iteration 167, loss = 0.41865825\n",
      "Iteration 168, loss = 0.41843128\n",
      "Iteration 169, loss = 0.41838931\n",
      "Iteration 170, loss = 0.41827964\n",
      "Iteration 171, loss = 0.41812456\n",
      "Iteration 172, loss = 0.41798405\n",
      "Iteration 173, loss = 0.41811028\n",
      "Iteration 174, loss = 0.41785851\n",
      "Iteration 175, loss = 0.41757400\n",
      "Iteration 176, loss = 0.41752350\n",
      "Iteration 177, loss = 0.41761648\n",
      "Iteration 178, loss = 0.41724250\n",
      "Iteration 179, loss = 0.41712570\n",
      "Iteration 180, loss = 0.41709119\n",
      "Iteration 181, loss = 0.41721033\n",
      "Iteration 182, loss = 0.41671362\n",
      "Iteration 183, loss = 0.41683750\n",
      "Iteration 184, loss = 0.41681238\n",
      "Iteration 185, loss = 0.41667525\n",
      "Iteration 186, loss = 0.41623326\n",
      "Iteration 187, loss = 0.41626568\n",
      "Iteration 188, loss = 0.41625618\n",
      "Iteration 189, loss = 0.41595048\n",
      "Iteration 190, loss = 0.41601049\n",
      "Iteration 191, loss = 0.41587030\n",
      "Iteration 192, loss = 0.41603548\n",
      "Iteration 193, loss = 0.41551339\n",
      "Iteration 194, loss = 0.41548291\n",
      "Iteration 195, loss = 0.41541431\n",
      "Iteration 196, loss = 0.41521902\n",
      "Iteration 197, loss = 0.41518290\n",
      "Iteration 198, loss = 0.41519647\n",
      "Iteration 199, loss = 0.41485443\n",
      "Iteration 200, loss = 0.41486373\n",
      "Iteration 201, loss = 0.41496387\n",
      "Iteration 202, loss = 0.41458080\n",
      "Iteration 203, loss = 0.41439580\n",
      "Iteration 204, loss = 0.41448136\n",
      "Iteration 205, loss = 0.41431681\n",
      "Iteration 206, loss = 0.41416159\n",
      "Iteration 207, loss = 0.41416554\n",
      "Iteration 208, loss = 0.41403737\n",
      "Iteration 209, loss = 0.41399098\n",
      "Iteration 210, loss = 0.41368923\n",
      "Iteration 211, loss = 0.41368003\n",
      "Iteration 212, loss = 0.41369688\n",
      "Iteration 213, loss = 0.41350166\n",
      "Iteration 214, loss = 0.41319574\n",
      "Iteration 215, loss = 0.41319792\n",
      "Iteration 216, loss = 0.41318735\n",
      "Iteration 217, loss = 0.41325109\n",
      "Iteration 218, loss = 0.41293392\n",
      "Iteration 219, loss = 0.41279969\n",
      "Iteration 220, loss = 0.41266697\n",
      "Iteration 221, loss = 0.41247563\n",
      "Iteration 222, loss = 0.41263334\n",
      "Iteration 223, loss = 0.41243972\n",
      "Iteration 224, loss = 0.41237694\n",
      "Iteration 225, loss = 0.41203861\n",
      "Iteration 226, loss = 0.41212066\n",
      "Iteration 227, loss = 0.41211946\n",
      "Iteration 228, loss = 0.41213979\n",
      "Iteration 229, loss = 0.41175137\n",
      "Iteration 230, loss = 0.41156189\n",
      "Iteration 231, loss = 0.41188849\n",
      "Iteration 232, loss = 0.41154249\n",
      "Iteration 233, loss = 0.41137235\n",
      "Iteration 234, loss = 0.41140380\n",
      "Iteration 235, loss = 0.41151301\n",
      "Iteration 236, loss = 0.41137496\n",
      "Iteration 237, loss = 0.41099449\n",
      "Iteration 238, loss = 0.41093021\n",
      "Iteration 239, loss = 0.41086501\n",
      "Iteration 240, loss = 0.41072799\n",
      "Iteration 241, loss = 0.41094441\n",
      "Iteration 242, loss = 0.41048609\n",
      "Iteration 243, loss = 0.41062383\n",
      "Iteration 244, loss = 0.41031660\n",
      "Iteration 245, loss = 0.41029293\n",
      "Iteration 246, loss = 0.41020288\n",
      "Iteration 247, loss = 0.41014670\n",
      "Iteration 248, loss = 0.41006165\n",
      "Iteration 249, loss = 0.41008884\n",
      "Iteration 250, loss = 0.40979395\n",
      "Iteration 251, loss = 0.40998236\n",
      "Iteration 252, loss = 0.40975306\n",
      "Iteration 253, loss = 0.40948922\n",
      "Iteration 254, loss = 0.40938607\n",
      "Iteration 255, loss = 0.40925955\n",
      "Iteration 256, loss = 0.40948373\n",
      "Iteration 257, loss = 0.40926464\n",
      "Iteration 258, loss = 0.40905915\n",
      "Iteration 259, loss = 0.40915912\n",
      "Iteration 260, loss = 0.40926854\n",
      "Iteration 261, loss = 0.40913048\n",
      "Iteration 262, loss = 0.40875808\n",
      "Iteration 263, loss = 0.40876674\n",
      "Iteration 264, loss = 0.40853829\n",
      "Iteration 265, loss = 0.40844715\n",
      "Iteration 266, loss = 0.40864619\n",
      "Iteration 267, loss = 0.40846427\n",
      "Iteration 268, loss = 0.40830807\n",
      "Iteration 269, loss = 0.40842245\n",
      "Iteration 270, loss = 0.40812107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 271, loss = 0.40804806\n",
      "Iteration 272, loss = 0.40812598\n",
      "Iteration 273, loss = 0.40789740\n",
      "Iteration 274, loss = 0.40799678\n",
      "Iteration 275, loss = 0.40781497\n",
      "Iteration 276, loss = 0.40788041\n",
      "Iteration 277, loss = 0.40782129\n",
      "Iteration 278, loss = 0.40759039\n",
      "Iteration 279, loss = 0.40741253\n",
      "Iteration 280, loss = 0.40730306\n",
      "Iteration 281, loss = 0.40727149\n",
      "Iteration 282, loss = 0.40748268\n",
      "Iteration 283, loss = 0.40710285\n",
      "Iteration 284, loss = 0.40706958\n",
      "Iteration 285, loss = 0.40715012\n",
      "Iteration 286, loss = 0.40680022\n",
      "Iteration 287, loss = 0.40676427\n",
      "Iteration 288, loss = 0.40689529\n",
      "Iteration 289, loss = 0.40680412\n",
      "Iteration 290, loss = 0.40670820\n",
      "Iteration 291, loss = 0.40649258\n",
      "Iteration 292, loss = 0.40656343\n",
      "Iteration 293, loss = 0.40640517\n",
      "Iteration 294, loss = 0.40640081\n",
      "Iteration 295, loss = 0.40616876\n",
      "Iteration 296, loss = 0.40623585\n",
      "Iteration 297, loss = 0.40638397\n",
      "Iteration 298, loss = 0.40611426\n",
      "Iteration 299, loss = 0.40612967\n",
      "Iteration 300, loss = 0.40586381\n",
      "Iteration 301, loss = 0.40594887\n",
      "Iteration 302, loss = 0.40573860\n",
      "Iteration 303, loss = 0.40548691\n",
      "Iteration 304, loss = 0.40598961\n",
      "Iteration 305, loss = 0.40561593\n",
      "Iteration 306, loss = 0.40550159\n",
      "Iteration 307, loss = 0.40546523\n",
      "Iteration 308, loss = 0.40519173\n",
      "Iteration 309, loss = 0.40479117\n",
      "Iteration 310, loss = 0.40495434\n",
      "Iteration 311, loss = 0.40501418\n",
      "Iteration 312, loss = 0.40485682\n",
      "Iteration 313, loss = 0.40477542\n",
      "Iteration 314, loss = 0.40496168\n",
      "Iteration 315, loss = 0.40479803\n",
      "Iteration 316, loss = 0.40475746\n",
      "Iteration 317, loss = 0.40425177\n",
      "Iteration 318, loss = 0.40441729\n",
      "Iteration 319, loss = 0.40443524\n",
      "Iteration 320, loss = 0.40440046\n",
      "Iteration 321, loss = 0.40433373\n",
      "Iteration 322, loss = 0.40412664\n",
      "Iteration 323, loss = 0.40416229\n",
      "Iteration 324, loss = 0.40388062\n",
      "Iteration 325, loss = 0.40372886\n",
      "Iteration 326, loss = 0.40394935\n",
      "Iteration 327, loss = 0.40356855\n",
      "Iteration 328, loss = 0.40382959\n",
      "Iteration 329, loss = 0.40358350\n",
      "Iteration 330, loss = 0.40369642\n",
      "Iteration 331, loss = 0.40334566\n",
      "Iteration 332, loss = 0.40333739\n",
      "Iteration 333, loss = 0.40326696\n",
      "Iteration 334, loss = 0.40339346\n",
      "Iteration 335, loss = 0.40316828\n",
      "Iteration 336, loss = 0.40305266\n",
      "Iteration 337, loss = 0.40295462\n",
      "Iteration 338, loss = 0.40316081\n",
      "Iteration 339, loss = 0.40304569\n",
      "Iteration 340, loss = 0.40305942\n",
      "Iteration 341, loss = 0.40304608\n",
      "Iteration 342, loss = 0.40305820\n",
      "Iteration 343, loss = 0.40310416\n",
      "Iteration 344, loss = 0.40307160\n",
      "Iteration 345, loss = 0.40270764\n",
      "Iteration 346, loss = 0.40252183\n",
      "Iteration 347, loss = 0.40272958\n",
      "Iteration 348, loss = 0.40260068\n",
      "Iteration 349, loss = 0.40254153\n",
      "Iteration 350, loss = 0.40222990\n",
      "Iteration 351, loss = 0.40217859\n",
      "Iteration 352, loss = 0.40211968\n",
      "Iteration 353, loss = 0.40191860\n",
      "Iteration 354, loss = 0.40192795\n",
      "Iteration 355, loss = 0.40211070\n",
      "Iteration 356, loss = 0.40187788\n",
      "Iteration 357, loss = 0.40193858\n",
      "Iteration 358, loss = 0.40206250\n",
      "Iteration 359, loss = 0.40162091\n",
      "Iteration 360, loss = 0.40172307\n",
      "Iteration 361, loss = 0.40199811\n",
      "Iteration 362, loss = 0.40154068\n",
      "Iteration 363, loss = 0.40159534\n",
      "Iteration 364, loss = 0.40140986\n",
      "Iteration 365, loss = 0.40150608\n",
      "Iteration 366, loss = 0.40156574\n",
      "Iteration 367, loss = 0.40118189\n",
      "Iteration 368, loss = 0.40135306\n",
      "Iteration 369, loss = 0.40102305\n",
      "Iteration 370, loss = 0.40121834\n",
      "Iteration 371, loss = 0.40124707\n",
      "Iteration 372, loss = 0.40131560\n",
      "Iteration 373, loss = 0.40102681\n",
      "Iteration 374, loss = 0.40089400\n",
      "Iteration 375, loss = 0.40079365\n",
      "Iteration 376, loss = 0.40062531\n",
      "Iteration 377, loss = 0.40079605\n",
      "Iteration 378, loss = 0.40092239\n",
      "Iteration 379, loss = 0.40073724\n",
      "Iteration 380, loss = 0.40057366\n",
      "Iteration 381, loss = 0.40059032\n",
      "Iteration 382, loss = 0.40058960\n",
      "Iteration 383, loss = 0.40048209\n",
      "Iteration 384, loss = 0.40092034\n",
      "Iteration 385, loss = 0.40040874\n",
      "Iteration 386, loss = 0.40045378\n",
      "Iteration 387, loss = 0.40039985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05921413\n",
      "Iteration 2, loss = 1.83583935\n",
      "Iteration 3, loss = 1.66603204\n",
      "Iteration 4, loss = 1.52250292\n",
      "Iteration 5, loss = 1.37976018\n",
      "Iteration 6, loss = 1.23948153\n",
      "Iteration 7, loss = 1.12155640\n",
      "Iteration 8, loss = 1.02858585\n",
      "Iteration 9, loss = 0.95541893\n",
      "Iteration 10, loss = 0.89560642\n",
      "Iteration 11, loss = 0.84610078\n",
      "Iteration 12, loss = 0.80532021\n",
      "Iteration 13, loss = 0.77183062\n",
      "Iteration 14, loss = 0.74369992\n",
      "Iteration 15, loss = 0.71962505\n",
      "Iteration 16, loss = 0.69875485\n",
      "Iteration 17, loss = 0.68027393\n",
      "Iteration 18, loss = 0.66381786\n",
      "Iteration 19, loss = 0.64883293\n",
      "Iteration 20, loss = 0.63511762\n",
      "Iteration 21, loss = 0.62239676\n",
      "Iteration 22, loss = 0.61090394\n",
      "Iteration 23, loss = 0.60015877\n",
      "Iteration 24, loss = 0.58987620\n",
      "Iteration 25, loss = 0.58060859\n",
      "Iteration 26, loss = 0.57191644\n",
      "Iteration 27, loss = 0.56364548\n",
      "Iteration 28, loss = 0.55587468\n",
      "Iteration 29, loss = 0.54842728\n",
      "Iteration 30, loss = 0.54176115\n",
      "Iteration 31, loss = 0.53517774\n",
      "Iteration 32, loss = 0.52895097\n",
      "Iteration 33, loss = 0.52311048\n",
      "Iteration 34, loss = 0.51779866\n",
      "Iteration 35, loss = 0.51256647\n",
      "Iteration 36, loss = 0.50777553\n",
      "Iteration 37, loss = 0.50337495\n",
      "Iteration 38, loss = 0.49895137\n",
      "Iteration 39, loss = 0.49495813\n",
      "Iteration 40, loss = 0.49133381\n",
      "Iteration 41, loss = 0.48798211\n",
      "Iteration 42, loss = 0.48458034\n",
      "Iteration 43, loss = 0.48170301\n",
      "Iteration 44, loss = 0.47905090\n",
      "Iteration 45, loss = 0.47651672\n",
      "Iteration 46, loss = 0.47428459\n",
      "Iteration 47, loss = 0.47191817\n",
      "Iteration 48, loss = 0.46990791\n",
      "Iteration 49, loss = 0.46798507\n",
      "Iteration 50, loss = 0.46627818\n",
      "Iteration 51, loss = 0.46446548\n",
      "Iteration 52, loss = 0.46297803\n",
      "Iteration 53, loss = 0.46164582\n",
      "Iteration 54, loss = 0.46044639\n",
      "Iteration 55, loss = 0.45884335\n",
      "Iteration 56, loss = 0.45769522\n",
      "Iteration 57, loss = 0.45661230\n",
      "Iteration 58, loss = 0.45557591\n",
      "Iteration 59, loss = 0.45447232\n",
      "Iteration 60, loss = 0.45370705\n",
      "Iteration 61, loss = 0.45266208\n",
      "Iteration 62, loss = 0.45184204\n",
      "Iteration 63, loss = 0.45070665\n",
      "Iteration 64, loss = 0.45009462\n",
      "Iteration 65, loss = 0.44925236\n",
      "Iteration 66, loss = 0.44854180\n",
      "Iteration 67, loss = 0.44803530\n",
      "Iteration 68, loss = 0.44688129\n",
      "Iteration 69, loss = 0.44662864\n",
      "Iteration 70, loss = 0.44565321\n",
      "Iteration 71, loss = 0.44506895\n",
      "Iteration 72, loss = 0.44435868\n",
      "Iteration 73, loss = 0.44407795\n",
      "Iteration 74, loss = 0.44350844\n",
      "Iteration 75, loss = 0.44281104\n",
      "Iteration 76, loss = 0.44262230\n",
      "Iteration 77, loss = 0.44173600\n",
      "Iteration 78, loss = 0.44115113\n",
      "Iteration 79, loss = 0.44080238\n",
      "Iteration 80, loss = 0.44033284\n",
      "Iteration 81, loss = 0.43985163\n",
      "Iteration 82, loss = 0.43942860\n",
      "Iteration 83, loss = 0.43920964\n",
      "Iteration 84, loss = 0.43862590\n",
      "Iteration 85, loss = 0.43805603\n",
      "Iteration 86, loss = 0.43766727\n",
      "Iteration 87, loss = 0.43749193\n",
      "Iteration 88, loss = 0.43709140\n",
      "Iteration 89, loss = 0.43648251\n",
      "Iteration 90, loss = 0.43602221\n",
      "Iteration 91, loss = 0.43591087\n",
      "Iteration 92, loss = 0.43558963\n",
      "Iteration 93, loss = 0.43518222\n",
      "Iteration 94, loss = 0.43479892\n",
      "Iteration 95, loss = 0.43452411\n",
      "Iteration 96, loss = 0.43417629\n",
      "Iteration 97, loss = 0.43386161\n",
      "Iteration 98, loss = 0.43354231\n",
      "Iteration 99, loss = 0.43323419\n",
      "Iteration 100, loss = 0.43304763\n",
      "Iteration 101, loss = 0.43253277\n",
      "Iteration 102, loss = 0.43225739\n",
      "Iteration 103, loss = 0.43202867\n",
      "Iteration 104, loss = 0.43143825\n",
      "Iteration 105, loss = 0.43162477\n",
      "Iteration 106, loss = 0.43121383\n",
      "Iteration 107, loss = 0.43084551\n",
      "Iteration 108, loss = 0.43101219\n",
      "Iteration 109, loss = 0.43063552\n",
      "Iteration 110, loss = 0.43022043\n",
      "Iteration 111, loss = 0.42987852\n",
      "Iteration 112, loss = 0.42969560\n",
      "Iteration 113, loss = 0.42949531\n",
      "Iteration 114, loss = 0.42894185\n",
      "Iteration 115, loss = 0.42928448\n",
      "Iteration 116, loss = 0.42891209\n",
      "Iteration 117, loss = 0.42825809\n",
      "Iteration 118, loss = 0.42836713\n",
      "Iteration 119, loss = 0.42816350\n",
      "Iteration 120, loss = 0.42762555\n",
      "Iteration 121, loss = 0.42752017\n",
      "Iteration 122, loss = 0.42739332\n",
      "Iteration 123, loss = 0.42728283\n",
      "Iteration 124, loss = 0.42695660\n",
      "Iteration 125, loss = 0.42684569\n",
      "Iteration 126, loss = 0.42648934\n",
      "Iteration 127, loss = 0.42640372\n",
      "Iteration 128, loss = 0.42620359\n",
      "Iteration 129, loss = 0.42579395\n",
      "Iteration 130, loss = 0.42589004\n",
      "Iteration 131, loss = 0.42564883\n",
      "Iteration 132, loss = 0.42521548\n",
      "Iteration 133, loss = 0.42513096\n",
      "Iteration 134, loss = 0.42489643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 135, loss = 0.42507175\n",
      "Iteration 136, loss = 0.42482060\n",
      "Iteration 137, loss = 0.42451307\n",
      "Iteration 138, loss = 0.42415728\n",
      "Iteration 139, loss = 0.42407461\n",
      "Iteration 140, loss = 0.42366209\n",
      "Iteration 141, loss = 0.42370770\n",
      "Iteration 142, loss = 0.42359385\n",
      "Iteration 143, loss = 0.42351118\n",
      "Iteration 144, loss = 0.42324324\n",
      "Iteration 145, loss = 0.42332727\n",
      "Iteration 146, loss = 0.42291234\n",
      "Iteration 147, loss = 0.42282269\n",
      "Iteration 148, loss = 0.42257316\n",
      "Iteration 149, loss = 0.42252639\n",
      "Iteration 150, loss = 0.42211100\n",
      "Iteration 151, loss = 0.42225768\n",
      "Iteration 152, loss = 0.42176626\n",
      "Iteration 153, loss = 0.42187182\n",
      "Iteration 154, loss = 0.42170928\n",
      "Iteration 155, loss = 0.42162909\n",
      "Iteration 156, loss = 0.42106312\n",
      "Iteration 157, loss = 0.42137906\n",
      "Iteration 158, loss = 0.42104152\n",
      "Iteration 159, loss = 0.42077631\n",
      "Iteration 160, loss = 0.42086838\n",
      "Iteration 161, loss = 0.42060521\n",
      "Iteration 162, loss = 0.42057785\n",
      "Iteration 163, loss = 0.42031735\n",
      "Iteration 164, loss = 0.41998961\n",
      "Iteration 165, loss = 0.41985451\n",
      "Iteration 166, loss = 0.42008292\n",
      "Iteration 167, loss = 0.41978096\n",
      "Iteration 168, loss = 0.41968796\n",
      "Iteration 169, loss = 0.41972268\n",
      "Iteration 170, loss = 0.41944154\n",
      "Iteration 171, loss = 0.41916340\n",
      "Iteration 172, loss = 0.41910959\n",
      "Iteration 173, loss = 0.41875249\n",
      "Iteration 174, loss = 0.41876564\n",
      "Iteration 175, loss = 0.41874977\n",
      "Iteration 176, loss = 0.41859465\n",
      "Iteration 177, loss = 0.41851104\n",
      "Iteration 178, loss = 0.41821618\n",
      "Iteration 179, loss = 0.41796327\n",
      "Iteration 180, loss = 0.41774182\n",
      "Iteration 181, loss = 0.41814186\n",
      "Iteration 182, loss = 0.41779684\n",
      "Iteration 183, loss = 0.41769261\n",
      "Iteration 184, loss = 0.41768440\n",
      "Iteration 185, loss = 0.41725359\n",
      "Iteration 186, loss = 0.41729637\n",
      "Iteration 187, loss = 0.41724839\n",
      "Iteration 188, loss = 0.41706488\n",
      "Iteration 189, loss = 0.41695633\n",
      "Iteration 190, loss = 0.41674084\n",
      "Iteration 191, loss = 0.41662624\n",
      "Iteration 192, loss = 0.41671881\n",
      "Iteration 193, loss = 0.41616672\n",
      "Iteration 194, loss = 0.41634661\n",
      "Iteration 195, loss = 0.41621970\n",
      "Iteration 196, loss = 0.41600394\n",
      "Iteration 197, loss = 0.41609584\n",
      "Iteration 198, loss = 0.41574704\n",
      "Iteration 199, loss = 0.41578072\n",
      "Iteration 200, loss = 0.41576827\n",
      "Iteration 201, loss = 0.41539387\n",
      "Iteration 202, loss = 0.41541653\n",
      "Iteration 203, loss = 0.41511321\n",
      "Iteration 204, loss = 0.41515536\n",
      "Iteration 205, loss = 0.41502539\n",
      "Iteration 206, loss = 0.41485793\n",
      "Iteration 207, loss = 0.41480063\n",
      "Iteration 208, loss = 0.41464827\n",
      "Iteration 209, loss = 0.41464987\n",
      "Iteration 210, loss = 0.41448794\n",
      "Iteration 211, loss = 0.41450218\n",
      "Iteration 212, loss = 0.41416382\n",
      "Iteration 213, loss = 0.41430694\n",
      "Iteration 214, loss = 0.41387373\n",
      "Iteration 215, loss = 0.41392533\n",
      "Iteration 216, loss = 0.41367518\n",
      "Iteration 217, loss = 0.41352387\n",
      "Iteration 218, loss = 0.41361602\n",
      "Iteration 219, loss = 0.41358731\n",
      "Iteration 220, loss = 0.41359925\n",
      "Iteration 221, loss = 0.41330180\n",
      "Iteration 222, loss = 0.41315961\n",
      "Iteration 223, loss = 0.41309055\n",
      "Iteration 224, loss = 0.41300435\n",
      "Iteration 225, loss = 0.41275925\n",
      "Iteration 226, loss = 0.41267492\n",
      "Iteration 227, loss = 0.41272846\n",
      "Iteration 228, loss = 0.41244243\n",
      "Iteration 229, loss = 0.41239982\n",
      "Iteration 230, loss = 0.41236347\n",
      "Iteration 231, loss = 0.41240356\n",
      "Iteration 232, loss = 0.41212302\n",
      "Iteration 233, loss = 0.41218164\n",
      "Iteration 234, loss = 0.41203669\n",
      "Iteration 235, loss = 0.41177642\n",
      "Iteration 236, loss = 0.41177549\n",
      "Iteration 237, loss = 0.41193517\n",
      "Iteration 238, loss = 0.41130690\n",
      "Iteration 239, loss = 0.41142582\n",
      "Iteration 240, loss = 0.41138007\n",
      "Iteration 241, loss = 0.41146812\n",
      "Iteration 242, loss = 0.41130734\n",
      "Iteration 243, loss = 0.41109455\n",
      "Iteration 244, loss = 0.41097337\n",
      "Iteration 245, loss = 0.41103613\n",
      "Iteration 246, loss = 0.41084723\n",
      "Iteration 247, loss = 0.41056293\n",
      "Iteration 248, loss = 0.41077642\n",
      "Iteration 249, loss = 0.41040458\n",
      "Iteration 250, loss = 0.41036929\n",
      "Iteration 251, loss = 0.41039987\n",
      "Iteration 252, loss = 0.41062136\n",
      "Iteration 253, loss = 0.41038318\n",
      "Iteration 254, loss = 0.41007641\n",
      "Iteration 255, loss = 0.41008675\n",
      "Iteration 256, loss = 0.41017064\n",
      "Iteration 257, loss = 0.40990662\n",
      "Iteration 258, loss = 0.40994376\n",
      "Iteration 259, loss = 0.40952770\n",
      "Iteration 260, loss = 0.40950370\n",
      "Iteration 261, loss = 0.40939532\n",
      "Iteration 262, loss = 0.40922922\n",
      "Iteration 263, loss = 0.40913259\n",
      "Iteration 264, loss = 0.40887048\n",
      "Iteration 265, loss = 0.40886823\n",
      "Iteration 266, loss = 0.40890524\n",
      "Iteration 267, loss = 0.40906063\n",
      "Iteration 268, loss = 0.40862101\n",
      "Iteration 269, loss = 0.40843745\n",
      "Iteration 270, loss = 0.40836616\n",
      "Iteration 271, loss = 0.40850647\n",
      "Iteration 272, loss = 0.40823248\n",
      "Iteration 273, loss = 0.40834962\n",
      "Iteration 274, loss = 0.40822072\n",
      "Iteration 275, loss = 0.40808709\n",
      "Iteration 276, loss = 0.40786724\n",
      "Iteration 277, loss = 0.40800343\n",
      "Iteration 278, loss = 0.40792662\n",
      "Iteration 279, loss = 0.40799167\n",
      "Iteration 280, loss = 0.40758033\n",
      "Iteration 281, loss = 0.40780872\n",
      "Iteration 282, loss = 0.40732727\n",
      "Iteration 283, loss = 0.40726921\n",
      "Iteration 284, loss = 0.40720691\n",
      "Iteration 285, loss = 0.40710653\n",
      "Iteration 286, loss = 0.40728197\n",
      "Iteration 287, loss = 0.40718927\n",
      "Iteration 288, loss = 0.40687458\n",
      "Iteration 289, loss = 0.40700576\n",
      "Iteration 290, loss = 0.40693636\n",
      "Iteration 291, loss = 0.40680804\n",
      "Iteration 292, loss = 0.40698511\n",
      "Iteration 293, loss = 0.40656793\n",
      "Iteration 294, loss = 0.40673564\n",
      "Iteration 295, loss = 0.40642540\n",
      "Iteration 296, loss = 0.40660330\n",
      "Iteration 297, loss = 0.40642708\n",
      "Iteration 298, loss = 0.40627928\n",
      "Iteration 299, loss = 0.40624365\n",
      "Iteration 300, loss = 0.40605212\n",
      "Iteration 301, loss = 0.40601703\n",
      "Iteration 302, loss = 0.40592805\n",
      "Iteration 303, loss = 0.40593923\n",
      "Iteration 304, loss = 0.40571485\n",
      "Iteration 305, loss = 0.40574443\n",
      "Iteration 306, loss = 0.40584498\n",
      "Iteration 307, loss = 0.40584963\n",
      "Iteration 308, loss = 0.40535327\n",
      "Iteration 309, loss = 0.40534422\n",
      "Iteration 310, loss = 0.40508572\n",
      "Iteration 311, loss = 0.40530569\n",
      "Iteration 312, loss = 0.40518138\n",
      "Iteration 313, loss = 0.40531629\n",
      "Iteration 314, loss = 0.40525879\n",
      "Iteration 315, loss = 0.40509135\n",
      "Iteration 316, loss = 0.40497191\n",
      "Iteration 317, loss = 0.40481448\n",
      "Iteration 318, loss = 0.40492262\n",
      "Iteration 319, loss = 0.40489440\n",
      "Iteration 320, loss = 0.40468816\n",
      "Iteration 321, loss = 0.40453945\n",
      "Iteration 322, loss = 0.40456768\n",
      "Iteration 323, loss = 0.40463776\n",
      "Iteration 324, loss = 0.40472460\n",
      "Iteration 325, loss = 0.40454950\n",
      "Iteration 326, loss = 0.40426807\n",
      "Iteration 327, loss = 0.40436740\n",
      "Iteration 328, loss = 0.40415726\n",
      "Iteration 329, loss = 0.40428217\n",
      "Iteration 330, loss = 0.40384893\n",
      "Iteration 331, loss = 0.40405295\n",
      "Iteration 332, loss = 0.40386355\n",
      "Iteration 333, loss = 0.40389119\n",
      "Iteration 334, loss = 0.40402075\n",
      "Iteration 335, loss = 0.40366005\n",
      "Iteration 336, loss = 0.40369752\n",
      "Iteration 337, loss = 0.40364002\n",
      "Iteration 338, loss = 0.40358466\n",
      "Iteration 339, loss = 0.40337870\n",
      "Iteration 340, loss = 0.40342175\n",
      "Iteration 341, loss = 0.40341090\n",
      "Iteration 342, loss = 0.40332978\n",
      "Iteration 343, loss = 0.40346925\n",
      "Iteration 344, loss = 0.40321658\n",
      "Iteration 345, loss = 0.40303572\n",
      "Iteration 346, loss = 0.40320134\n",
      "Iteration 347, loss = 0.40309271\n",
      "Iteration 348, loss = 0.40290038\n",
      "Iteration 349, loss = 0.40268298\n",
      "Iteration 350, loss = 0.40275216\n",
      "Iteration 351, loss = 0.40292867\n",
      "Iteration 352, loss = 0.40274247\n",
      "Iteration 353, loss = 0.40249338\n",
      "Iteration 354, loss = 0.40280950\n",
      "Iteration 355, loss = 0.40274227\n",
      "Iteration 356, loss = 0.40266554\n",
      "Iteration 357, loss = 0.40237795\n",
      "Iteration 358, loss = 0.40238434\n",
      "Iteration 359, loss = 0.40253819\n",
      "Iteration 360, loss = 0.40229803\n",
      "Iteration 361, loss = 0.40237714\n",
      "Iteration 362, loss = 0.40219904\n",
      "Iteration 363, loss = 0.40211078\n",
      "Iteration 364, loss = 0.40205966\n",
      "Iteration 365, loss = 0.40218205\n",
      "Iteration 366, loss = 0.40211581\n",
      "Iteration 367, loss = 0.40187077\n",
      "Iteration 368, loss = 0.40198007\n",
      "Iteration 369, loss = 0.40182186\n",
      "Iteration 370, loss = 0.40178553\n",
      "Iteration 371, loss = 0.40185409\n",
      "Iteration 372, loss = 0.40187386\n",
      "Iteration 373, loss = 0.40179552\n",
      "Iteration 374, loss = 0.40180018\n",
      "Iteration 375, loss = 0.40167564\n",
      "Iteration 376, loss = 0.40159351\n",
      "Iteration 377, loss = 0.40138429\n",
      "Iteration 378, loss = 0.40142134\n",
      "Iteration 379, loss = 0.40137959\n",
      "Iteration 380, loss = 0.40134313\n",
      "Iteration 381, loss = 0.40130479\n",
      "Iteration 382, loss = 0.40113647\n",
      "Iteration 383, loss = 0.40134529\n",
      "Iteration 384, loss = 0.40134843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 385, loss = 0.40103757\n",
      "Iteration 386, loss = 0.40102271\n",
      "Iteration 387, loss = 0.40100046\n",
      "Iteration 388, loss = 0.40090083\n",
      "Iteration 389, loss = 0.40098751\n",
      "Iteration 390, loss = 0.40088348\n",
      "Iteration 391, loss = 0.40065469\n",
      "Iteration 392, loss = 0.40081192\n",
      "Iteration 393, loss = 0.40059118\n",
      "Iteration 394, loss = 0.40070889\n",
      "Iteration 395, loss = 0.40073890\n",
      "Iteration 396, loss = 0.40038059\n",
      "Iteration 397, loss = 0.40083409\n",
      "Iteration 398, loss = 0.40078407\n",
      "Iteration 399, loss = 0.40044118\n",
      "Iteration 400, loss = 0.40035741\n",
      "Iteration 401, loss = 0.40023945\n",
      "Iteration 402, loss = 0.40025930\n",
      "Iteration 403, loss = 0.40031418\n",
      "Iteration 404, loss = 0.40026836\n",
      "Iteration 405, loss = 0.40015648\n",
      "Iteration 406, loss = 0.40010937\n",
      "Iteration 407, loss = 0.40030780\n",
      "Iteration 408, loss = 0.40018124\n",
      "Iteration 409, loss = 0.39994043\n",
      "Iteration 410, loss = 0.39977203\n",
      "Iteration 411, loss = 0.39988755\n",
      "Iteration 412, loss = 0.40013438\n",
      "Iteration 413, loss = 0.39988112\n",
      "Iteration 414, loss = 0.39994682\n",
      "Iteration 415, loss = 0.39977624\n",
      "Iteration 416, loss = 0.39989668\n",
      "Iteration 417, loss = 0.39957805\n",
      "Iteration 418, loss = 0.39970365\n",
      "Iteration 419, loss = 0.39957020\n",
      "Iteration 420, loss = 0.39954402\n",
      "Iteration 421, loss = 0.39942062\n",
      "Iteration 422, loss = 0.39951538\n",
      "Iteration 423, loss = 0.39953142\n",
      "Iteration 424, loss = 0.39923312\n",
      "Iteration 425, loss = 0.39937996\n",
      "Iteration 426, loss = 0.39945008\n",
      "Iteration 427, loss = 0.39909119\n",
      "Iteration 428, loss = 0.39898476\n",
      "Iteration 429, loss = 0.39915672\n",
      "Iteration 430, loss = 0.39927748\n",
      "Iteration 431, loss = 0.39896679\n",
      "Iteration 432, loss = 0.39903667\n",
      "Iteration 433, loss = 0.39917718\n",
      "Iteration 434, loss = 0.39869904\n",
      "Iteration 435, loss = 0.39896708\n",
      "Iteration 436, loss = 0.39903286\n",
      "Iteration 437, loss = 0.39902485\n",
      "Iteration 438, loss = 0.39876417\n",
      "Iteration 439, loss = 0.39857359\n",
      "Iteration 440, loss = 0.39852903\n",
      "Iteration 441, loss = 0.39870034\n",
      "Iteration 442, loss = 0.39865025\n",
      "Iteration 443, loss = 0.39880039\n",
      "Iteration 444, loss = 0.39853367\n",
      "Iteration 445, loss = 0.39860133\n",
      "Iteration 446, loss = 0.39855092\n",
      "Iteration 447, loss = 0.39837698\n",
      "Iteration 448, loss = 0.39849802\n",
      "Iteration 449, loss = 0.39843432\n",
      "Iteration 450, loss = 0.39850569\n",
      "Iteration 451, loss = 0.39842626\n",
      "Iteration 452, loss = 0.39812506\n",
      "Iteration 453, loss = 0.39814351\n",
      "Iteration 454, loss = 0.39846562\n",
      "Iteration 455, loss = 0.39841655\n",
      "Iteration 456, loss = 0.39815447\n",
      "Iteration 457, loss = 0.39820239\n",
      "Iteration 458, loss = 0.39811220\n",
      "Iteration 459, loss = 0.39782888\n",
      "Iteration 460, loss = 0.39803448\n",
      "Iteration 461, loss = 0.39810982\n",
      "Iteration 462, loss = 0.39786715\n",
      "Iteration 463, loss = 0.39785440\n",
      "Iteration 464, loss = 0.39752010\n",
      "Iteration 465, loss = 0.39764178\n",
      "Iteration 466, loss = 0.39760967\n",
      "Iteration 467, loss = 0.39777689\n",
      "Iteration 468, loss = 0.39765031\n",
      "Iteration 469, loss = 0.39752774\n",
      "Iteration 470, loss = 0.39747739\n",
      "Iteration 471, loss = 0.39726804\n",
      "Iteration 472, loss = 0.39759512\n",
      "Iteration 473, loss = 0.39763389\n",
      "Iteration 474, loss = 0.39751941\n",
      "Iteration 475, loss = 0.39742759\n",
      "Iteration 476, loss = 0.39706637\n",
      "Iteration 477, loss = 0.39733223\n",
      "Iteration 478, loss = 0.39712196\n",
      "Iteration 479, loss = 0.39731847\n",
      "Iteration 480, loss = 0.39716032\n",
      "Iteration 481, loss = 0.39714596\n",
      "Iteration 482, loss = 0.39741307\n",
      "Iteration 483, loss = 0.39709253\n",
      "Iteration 484, loss = 0.39717348\n",
      "Iteration 485, loss = 0.39685894\n",
      "Iteration 486, loss = 0.39694268\n",
      "Iteration 487, loss = 0.39697224\n",
      "Iteration 488, loss = 0.39733313\n",
      "Iteration 489, loss = 0.39701445\n",
      "Iteration 490, loss = 0.39668449\n",
      "Iteration 491, loss = 0.39681045\n",
      "Iteration 492, loss = 0.39656813\n",
      "Iteration 493, loss = 0.39676748\n",
      "Iteration 494, loss = 0.39661413\n",
      "Iteration 495, loss = 0.39631719\n",
      "Iteration 496, loss = 0.39663579\n",
      "Iteration 497, loss = 0.39637346\n",
      "Iteration 498, loss = 0.39661797\n",
      "Iteration 499, loss = 0.39633325\n",
      "Iteration 500, loss = 0.39653432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.06007933\n",
      "Iteration 2, loss = 1.83832155\n",
      "Iteration 3, loss = 1.66909094\n",
      "Iteration 4, loss = 1.52641482\n",
      "Iteration 5, loss = 1.38393925\n",
      "Iteration 6, loss = 1.24343033\n",
      "Iteration 7, loss = 1.12518196\n",
      "Iteration 8, loss = 1.03183851\n",
      "Iteration 9, loss = 0.95851043\n",
      "Iteration 10, loss = 0.89872168\n",
      "Iteration 11, loss = 0.84905804\n",
      "Iteration 12, loss = 0.80784499\n",
      "Iteration 13, loss = 0.77403232\n",
      "Iteration 14, loss = 0.74533534\n",
      "Iteration 15, loss = 0.72082869\n",
      "Iteration 16, loss = 0.69947133\n",
      "Iteration 17, loss = 0.68077836\n",
      "Iteration 18, loss = 0.66403683\n",
      "Iteration 19, loss = 0.64874586\n",
      "Iteration 20, loss = 0.63504253\n",
      "Iteration 21, loss = 0.62239198\n",
      "Iteration 22, loss = 0.61065526\n",
      "Iteration 23, loss = 0.59960080\n",
      "Iteration 24, loss = 0.58930346\n",
      "Iteration 25, loss = 0.57989236\n",
      "Iteration 26, loss = 0.57096070\n",
      "Iteration 27, loss = 0.56246089\n",
      "Iteration 28, loss = 0.55464441\n",
      "Iteration 29, loss = 0.54725401\n",
      "Iteration 30, loss = 0.54012401\n",
      "Iteration 31, loss = 0.53347820\n",
      "Iteration 32, loss = 0.52718077\n",
      "Iteration 33, loss = 0.52138911\n",
      "Iteration 34, loss = 0.51558061\n",
      "Iteration 35, loss = 0.51039990\n",
      "Iteration 36, loss = 0.50569043\n",
      "Iteration 37, loss = 0.50102638\n",
      "Iteration 38, loss = 0.49649081\n",
      "Iteration 39, loss = 0.49246843\n",
      "Iteration 40, loss = 0.48865791\n",
      "Iteration 41, loss = 0.48539761\n",
      "Iteration 42, loss = 0.48209140\n",
      "Iteration 43, loss = 0.47926915\n",
      "Iteration 44, loss = 0.47644338\n",
      "Iteration 45, loss = 0.47393212\n",
      "Iteration 46, loss = 0.47151366\n",
      "Iteration 47, loss = 0.46966143\n",
      "Iteration 48, loss = 0.46749909\n",
      "Iteration 49, loss = 0.46575801\n",
      "Iteration 50, loss = 0.46383571\n",
      "Iteration 51, loss = 0.46210961\n",
      "Iteration 52, loss = 0.46068235\n",
      "Iteration 53, loss = 0.45909487\n",
      "Iteration 54, loss = 0.45792810\n",
      "Iteration 55, loss = 0.45720087\n",
      "Iteration 56, loss = 0.45570187\n",
      "Iteration 57, loss = 0.45463369\n",
      "Iteration 58, loss = 0.45349210\n",
      "Iteration 59, loss = 0.45229161\n",
      "Iteration 60, loss = 0.45161116\n",
      "Iteration 61, loss = 0.45059068\n",
      "Iteration 62, loss = 0.44977298\n",
      "Iteration 63, loss = 0.44894838\n",
      "Iteration 64, loss = 0.44832690\n",
      "Iteration 65, loss = 0.44733512\n",
      "Iteration 66, loss = 0.44680771\n",
      "Iteration 67, loss = 0.44584438\n",
      "Iteration 68, loss = 0.44519745\n",
      "Iteration 69, loss = 0.44474789\n",
      "Iteration 70, loss = 0.44432867\n",
      "Iteration 71, loss = 0.44342364\n",
      "Iteration 72, loss = 0.44268755\n",
      "Iteration 73, loss = 0.44219218\n",
      "Iteration 74, loss = 0.44175603\n",
      "Iteration 75, loss = 0.44136729\n",
      "Iteration 76, loss = 0.44077840\n",
      "Iteration 77, loss = 0.44041407\n",
      "Iteration 78, loss = 0.43964098\n",
      "Iteration 79, loss = 0.43917489\n",
      "Iteration 80, loss = 0.43926741\n",
      "Iteration 81, loss = 0.43827643\n",
      "Iteration 82, loss = 0.43807362\n",
      "Iteration 83, loss = 0.43773019\n",
      "Iteration 84, loss = 0.43718627\n",
      "Iteration 85, loss = 0.43666452\n",
      "Iteration 86, loss = 0.43633360\n",
      "Iteration 87, loss = 0.43605001\n",
      "Iteration 88, loss = 0.43542001\n",
      "Iteration 89, loss = 0.43524166\n",
      "Iteration 90, loss = 0.43509342\n",
      "Iteration 91, loss = 0.43443073\n",
      "Iteration 92, loss = 0.43431961\n",
      "Iteration 93, loss = 0.43366397\n",
      "Iteration 94, loss = 0.43367058\n",
      "Iteration 95, loss = 0.43350082\n",
      "Iteration 96, loss = 0.43306950\n",
      "Iteration 97, loss = 0.43265285\n",
      "Iteration 98, loss = 0.43248107\n",
      "Iteration 99, loss = 0.43222661\n",
      "Iteration 100, loss = 0.43161716\n",
      "Iteration 101, loss = 0.43128812\n",
      "Iteration 102, loss = 0.43108575\n",
      "Iteration 103, loss = 0.43093834\n",
      "Iteration 104, loss = 0.43054040\n",
      "Iteration 105, loss = 0.43023973\n",
      "Iteration 106, loss = 0.42987579\n",
      "Iteration 107, loss = 0.42977967\n",
      "Iteration 108, loss = 0.42953924\n",
      "Iteration 109, loss = 0.42935574\n",
      "Iteration 110, loss = 0.42915964\n",
      "Iteration 111, loss = 0.42890219\n",
      "Iteration 112, loss = 0.42842753\n",
      "Iteration 113, loss = 0.42847319\n",
      "Iteration 114, loss = 0.42822429\n",
      "Iteration 115, loss = 0.42774102\n",
      "Iteration 116, loss = 0.42778349\n",
      "Iteration 117, loss = 0.42747574\n",
      "Iteration 118, loss = 0.42740415\n",
      "Iteration 119, loss = 0.42703714\n",
      "Iteration 120, loss = 0.42706196\n",
      "Iteration 121, loss = 0.42694111\n",
      "Iteration 122, loss = 0.42659406\n",
      "Iteration 123, loss = 0.42629085\n",
      "Iteration 124, loss = 0.42612234\n",
      "Iteration 125, loss = 0.42605276\n",
      "Iteration 126, loss = 0.42593001\n",
      "Iteration 127, loss = 0.42567277\n",
      "Iteration 128, loss = 0.42544578\n",
      "Iteration 129, loss = 0.42536077\n",
      "Iteration 130, loss = 0.42502083\n",
      "Iteration 131, loss = 0.42484747\n",
      "Iteration 132, loss = 0.42471045\n",
      "Iteration 133, loss = 0.42473426\n",
      "Iteration 134, loss = 0.42431556\n",
      "Iteration 135, loss = 0.42437076\n",
      "Iteration 136, loss = 0.42391537\n",
      "Iteration 137, loss = 0.42374246\n",
      "Iteration 138, loss = 0.42364994\n",
      "Iteration 139, loss = 0.42373475\n",
      "Iteration 140, loss = 0.42338827\n",
      "Iteration 141, loss = 0.42328676\n",
      "Iteration 142, loss = 0.42313185\n",
      "Iteration 143, loss = 0.42281471\n",
      "Iteration 144, loss = 0.42275679\n",
      "Iteration 145, loss = 0.42251116\n",
      "Iteration 146, loss = 0.42250044\n",
      "Iteration 147, loss = 0.42248017\n",
      "Iteration 148, loss = 0.42240088\n",
      "Iteration 149, loss = 0.42200366\n",
      "Iteration 150, loss = 0.42190691\n",
      "Iteration 151, loss = 0.42179159\n",
      "Iteration 152, loss = 0.42148505\n",
      "Iteration 153, loss = 0.42149471\n",
      "Iteration 154, loss = 0.42128072\n",
      "Iteration 155, loss = 0.42129371\n",
      "Iteration 156, loss = 0.42098433\n",
      "Iteration 157, loss = 0.42087668\n",
      "Iteration 158, loss = 0.42072814\n",
      "Iteration 159, loss = 0.42046421\n",
      "Iteration 160, loss = 0.42081268\n",
      "Iteration 161, loss = 0.42057250\n",
      "Iteration 162, loss = 0.42027468\n",
      "Iteration 163, loss = 0.41997391\n",
      "Iteration 164, loss = 0.41992158\n",
      "Iteration 165, loss = 0.42002832\n",
      "Iteration 166, loss = 0.41964331\n",
      "Iteration 167, loss = 0.41952565\n",
      "Iteration 168, loss = 0.41955501\n",
      "Iteration 169, loss = 0.41934800\n",
      "Iteration 170, loss = 0.41927096\n",
      "Iteration 171, loss = 0.41903514\n",
      "Iteration 172, loss = 0.41928340\n",
      "Iteration 173, loss = 0.41895413\n",
      "Iteration 174, loss = 0.41891708\n",
      "Iteration 175, loss = 0.41873537\n",
      "Iteration 176, loss = 0.41858258\n",
      "Iteration 177, loss = 0.41852918\n",
      "Iteration 178, loss = 0.41839957\n",
      "Iteration 179, loss = 0.41824708\n",
      "Iteration 180, loss = 0.41813269\n",
      "Iteration 181, loss = 0.41814404\n",
      "Iteration 182, loss = 0.41812728\n",
      "Iteration 183, loss = 0.41778570\n",
      "Iteration 184, loss = 0.41730825\n",
      "Iteration 185, loss = 0.41742386\n",
      "Iteration 186, loss = 0.41741476\n",
      "Iteration 187, loss = 0.41720367\n",
      "Iteration 188, loss = 0.41723822\n",
      "Iteration 189, loss = 0.41704984\n",
      "Iteration 190, loss = 0.41672589\n",
      "Iteration 191, loss = 0.41692030\n",
      "Iteration 192, loss = 0.41695852\n",
      "Iteration 193, loss = 0.41682876\n",
      "Iteration 194, loss = 0.41674533\n",
      "Iteration 195, loss = 0.41641924\n",
      "Iteration 196, loss = 0.41632308\n",
      "Iteration 197, loss = 0.41614704\n",
      "Iteration 198, loss = 0.41620875\n",
      "Iteration 199, loss = 0.41596470\n",
      "Iteration 200, loss = 0.41591092\n",
      "Iteration 201, loss = 0.41582900\n",
      "Iteration 202, loss = 0.41577077\n",
      "Iteration 203, loss = 0.41572148\n",
      "Iteration 204, loss = 0.41536491\n",
      "Iteration 205, loss = 0.41536617\n",
      "Iteration 206, loss = 0.41541853\n",
      "Iteration 207, loss = 0.41533693\n",
      "Iteration 208, loss = 0.41503876\n",
      "Iteration 209, loss = 0.41510151\n",
      "Iteration 210, loss = 0.41506688\n",
      "Iteration 211, loss = 0.41472042\n",
      "Iteration 212, loss = 0.41463568\n",
      "Iteration 213, loss = 0.41454992\n",
      "Iteration 214, loss = 0.41451633\n",
      "Iteration 215, loss = 0.41444680\n",
      "Iteration 216, loss = 0.41429454\n",
      "Iteration 217, loss = 0.41410666\n",
      "Iteration 218, loss = 0.41395333\n",
      "Iteration 219, loss = 0.41403969\n",
      "Iteration 220, loss = 0.41377183\n",
      "Iteration 221, loss = 0.41374294\n",
      "Iteration 222, loss = 0.41352260\n",
      "Iteration 223, loss = 0.41386038\n",
      "Iteration 224, loss = 0.41364187\n",
      "Iteration 225, loss = 0.41352057\n",
      "Iteration 226, loss = 0.41319032\n",
      "Iteration 227, loss = 0.41314891\n",
      "Iteration 228, loss = 0.41315390\n",
      "Iteration 229, loss = 0.41296281\n",
      "Iteration 230, loss = 0.41305874\n",
      "Iteration 231, loss = 0.41283999\n",
      "Iteration 232, loss = 0.41278604\n",
      "Iteration 233, loss = 0.41280582\n",
      "Iteration 234, loss = 0.41271416\n",
      "Iteration 235, loss = 0.41244600\n",
      "Iteration 236, loss = 0.41253697\n",
      "Iteration 237, loss = 0.41236891\n",
      "Iteration 238, loss = 0.41224521\n",
      "Iteration 239, loss = 0.41207995\n",
      "Iteration 240, loss = 0.41207670\n",
      "Iteration 241, loss = 0.41199133\n",
      "Iteration 242, loss = 0.41182441\n",
      "Iteration 243, loss = 0.41170973\n",
      "Iteration 244, loss = 0.41193397\n",
      "Iteration 245, loss = 0.41163746\n",
      "Iteration 246, loss = 0.41172703\n",
      "Iteration 247, loss = 0.41177172\n",
      "Iteration 248, loss = 0.41146031\n",
      "Iteration 249, loss = 0.41147745\n",
      "Iteration 250, loss = 0.41128963\n",
      "Iteration 251, loss = 0.41105330\n",
      "Iteration 252, loss = 0.41109450\n",
      "Iteration 253, loss = 0.41096271\n",
      "Iteration 254, loss = 0.41078937\n",
      "Iteration 255, loss = 0.41087999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.41059717\n",
      "Iteration 257, loss = 0.41048187\n",
      "Iteration 258, loss = 0.41035233\n",
      "Iteration 259, loss = 0.41037530\n",
      "Iteration 260, loss = 0.41055608\n",
      "Iteration 261, loss = 0.41046404\n",
      "Iteration 262, loss = 0.41013331\n",
      "Iteration 263, loss = 0.41014008\n",
      "Iteration 264, loss = 0.40999453\n",
      "Iteration 265, loss = 0.41008760\n",
      "Iteration 266, loss = 0.40961352\n",
      "Iteration 267, loss = 0.41014263\n",
      "Iteration 268, loss = 0.40961632\n",
      "Iteration 269, loss = 0.40945107\n",
      "Iteration 270, loss = 0.40948837\n",
      "Iteration 271, loss = 0.40968166\n",
      "Iteration 272, loss = 0.40965161\n",
      "Iteration 273, loss = 0.40930617\n",
      "Iteration 274, loss = 0.40930589\n",
      "Iteration 275, loss = 0.40907348\n",
      "Iteration 276, loss = 0.40925600\n",
      "Iteration 277, loss = 0.40874651\n",
      "Iteration 278, loss = 0.40926631\n",
      "Iteration 279, loss = 0.40901894\n",
      "Iteration 280, loss = 0.40861340\n",
      "Iteration 281, loss = 0.40873928\n",
      "Iteration 282, loss = 0.40862650\n",
      "Iteration 283, loss = 0.40862669\n",
      "Iteration 284, loss = 0.40868318\n",
      "Iteration 285, loss = 0.40829091\n",
      "Iteration 286, loss = 0.40823527\n",
      "Iteration 287, loss = 0.40844364\n",
      "Iteration 288, loss = 0.40818657\n",
      "Iteration 289, loss = 0.40810937\n",
      "Iteration 290, loss = 0.40792841\n",
      "Iteration 291, loss = 0.40820179\n",
      "Iteration 292, loss = 0.40793054\n",
      "Iteration 293, loss = 0.40764205\n",
      "Iteration 294, loss = 0.40779066\n",
      "Iteration 295, loss = 0.40787524\n",
      "Iteration 296, loss = 0.40758974\n",
      "Iteration 297, loss = 0.40744633\n",
      "Iteration 298, loss = 0.40760734\n",
      "Iteration 299, loss = 0.40749097\n",
      "Iteration 300, loss = 0.40737977\n",
      "Iteration 301, loss = 0.40721324\n",
      "Iteration 302, loss = 0.40722425\n",
      "Iteration 303, loss = 0.40724897\n",
      "Iteration 304, loss = 0.40708233\n",
      "Iteration 305, loss = 0.40705600\n",
      "Iteration 306, loss = 0.40687532\n",
      "Iteration 307, loss = 0.40706919\n",
      "Iteration 308, loss = 0.40695575\n",
      "Iteration 309, loss = 0.40674702\n",
      "Iteration 310, loss = 0.40684383\n",
      "Iteration 311, loss = 0.40671992\n",
      "Iteration 312, loss = 0.40641409\n",
      "Iteration 313, loss = 0.40644148\n",
      "Iteration 314, loss = 0.40652525\n",
      "Iteration 315, loss = 0.40634292\n",
      "Iteration 316, loss = 0.40618038\n",
      "Iteration 317, loss = 0.40615849\n",
      "Iteration 318, loss = 0.40633501\n",
      "Iteration 319, loss = 0.40602743\n",
      "Iteration 320, loss = 0.40612734\n",
      "Iteration 321, loss = 0.40589474\n",
      "Iteration 322, loss = 0.40602132\n",
      "Iteration 323, loss = 0.40558377\n",
      "Iteration 324, loss = 0.40585473\n",
      "Iteration 325, loss = 0.40569952\n",
      "Iteration 326, loss = 0.40554794\n",
      "Iteration 327, loss = 0.40543415\n",
      "Iteration 328, loss = 0.40561226\n",
      "Iteration 329, loss = 0.40537398\n",
      "Iteration 330, loss = 0.40556352\n",
      "Iteration 331, loss = 0.40501093\n",
      "Iteration 332, loss = 0.40541400\n",
      "Iteration 333, loss = 0.40511164\n",
      "Iteration 334, loss = 0.40504843\n",
      "Iteration 335, loss = 0.40517301\n",
      "Iteration 336, loss = 0.40524523\n",
      "Iteration 337, loss = 0.40495893\n",
      "Iteration 338, loss = 0.40499230\n",
      "Iteration 339, loss = 0.40501999\n",
      "Iteration 340, loss = 0.40478424\n",
      "Iteration 341, loss = 0.40457853\n",
      "Iteration 342, loss = 0.40457628\n",
      "Iteration 343, loss = 0.40450542\n",
      "Iteration 344, loss = 0.40461210\n",
      "Iteration 345, loss = 0.40415960\n",
      "Iteration 346, loss = 0.40448599\n",
      "Iteration 347, loss = 0.40420491\n",
      "Iteration 348, loss = 0.40411984\n",
      "Iteration 349, loss = 0.40411161\n",
      "Iteration 350, loss = 0.40414733\n",
      "Iteration 351, loss = 0.40429979\n",
      "Iteration 352, loss = 0.40405355\n",
      "Iteration 353, loss = 0.40397257\n",
      "Iteration 354, loss = 0.40404605\n",
      "Iteration 355, loss = 0.40405014\n",
      "Iteration 356, loss = 0.40391656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05932565\n",
      "Iteration 2, loss = 1.83650843\n",
      "Iteration 3, loss = 1.66688442\n",
      "Iteration 4, loss = 1.52465798\n",
      "Iteration 5, loss = 1.38241742\n",
      "Iteration 6, loss = 1.24120349\n",
      "Iteration 7, loss = 1.12212280\n",
      "Iteration 8, loss = 1.02907750\n",
      "Iteration 9, loss = 0.95573080\n",
      "Iteration 10, loss = 0.89551522\n",
      "Iteration 11, loss = 0.84565102\n",
      "Iteration 12, loss = 0.80497604\n",
      "Iteration 13, loss = 0.77109208\n",
      "Iteration 14, loss = 0.74304682\n",
      "Iteration 15, loss = 0.71884319\n",
      "Iteration 16, loss = 0.69792961\n",
      "Iteration 17, loss = 0.67944355\n",
      "Iteration 18, loss = 0.66314280\n",
      "Iteration 19, loss = 0.64801922\n",
      "Iteration 20, loss = 0.63442641\n",
      "Iteration 21, loss = 0.62190663\n",
      "Iteration 22, loss = 0.61037308\n",
      "Iteration 23, loss = 0.59976027\n",
      "Iteration 24, loss = 0.58959353\n",
      "Iteration 25, loss = 0.58028813\n",
      "Iteration 26, loss = 0.57143255\n",
      "Iteration 27, loss = 0.56312982\n",
      "Iteration 28, loss = 0.55535316\n",
      "Iteration 29, loss = 0.54815162\n",
      "Iteration 30, loss = 0.54115452\n",
      "Iteration 31, loss = 0.53462184\n",
      "Iteration 32, loss = 0.52857866\n",
      "Iteration 33, loss = 0.52265091\n",
      "Iteration 34, loss = 0.51701543\n",
      "Iteration 35, loss = 0.51202263\n",
      "Iteration 36, loss = 0.50704933\n",
      "Iteration 37, loss = 0.50246831\n",
      "Iteration 38, loss = 0.49829373\n",
      "Iteration 39, loss = 0.49428896\n",
      "Iteration 40, loss = 0.49067891\n",
      "Iteration 41, loss = 0.48730406\n",
      "Iteration 42, loss = 0.48421965\n",
      "Iteration 43, loss = 0.48122658\n",
      "Iteration 44, loss = 0.47862128\n",
      "Iteration 45, loss = 0.47603850\n",
      "Iteration 46, loss = 0.47379702\n",
      "Iteration 47, loss = 0.47167922\n",
      "Iteration 48, loss = 0.46964519\n",
      "Iteration 49, loss = 0.46788011\n",
      "Iteration 50, loss = 0.46625204\n",
      "Iteration 51, loss = 0.46447114\n",
      "Iteration 52, loss = 0.46324385\n",
      "Iteration 53, loss = 0.46156477\n",
      "Iteration 54, loss = 0.46030321\n",
      "Iteration 55, loss = 0.45893101\n",
      "Iteration 56, loss = 0.45783119\n",
      "Iteration 57, loss = 0.45685083\n",
      "Iteration 58, loss = 0.45595565\n",
      "Iteration 59, loss = 0.45454332\n",
      "Iteration 60, loss = 0.45351549\n",
      "Iteration 61, loss = 0.45275804\n",
      "Iteration 62, loss = 0.45200430\n",
      "Iteration 63, loss = 0.45121057\n",
      "Iteration 64, loss = 0.45024030\n",
      "Iteration 65, loss = 0.44946640\n",
      "Iteration 66, loss = 0.44867906\n",
      "Iteration 67, loss = 0.44803194\n",
      "Iteration 68, loss = 0.44741779\n",
      "Iteration 69, loss = 0.44675185\n",
      "Iteration 70, loss = 0.44620516\n",
      "Iteration 71, loss = 0.44553376\n",
      "Iteration 72, loss = 0.44502391\n",
      "Iteration 73, loss = 0.44460726\n",
      "Iteration 74, loss = 0.44386035\n",
      "Iteration 75, loss = 0.44329069\n",
      "Iteration 76, loss = 0.44266928\n",
      "Iteration 77, loss = 0.44239464\n",
      "Iteration 78, loss = 0.44185778\n",
      "Iteration 79, loss = 0.44149559\n",
      "Iteration 80, loss = 0.44106663\n",
      "Iteration 81, loss = 0.44036161\n",
      "Iteration 82, loss = 0.44009735\n",
      "Iteration 83, loss = 0.43970214\n",
      "Iteration 84, loss = 0.43925040\n",
      "Iteration 85, loss = 0.43893819\n",
      "Iteration 86, loss = 0.43809227\n",
      "Iteration 87, loss = 0.43826184\n",
      "Iteration 88, loss = 0.43752971\n",
      "Iteration 89, loss = 0.43714862\n",
      "Iteration 90, loss = 0.43706213\n",
      "Iteration 91, loss = 0.43668820\n",
      "Iteration 92, loss = 0.43634179\n",
      "Iteration 93, loss = 0.43599302\n",
      "Iteration 94, loss = 0.43552469\n",
      "Iteration 95, loss = 0.43503979\n",
      "Iteration 96, loss = 0.43486260\n",
      "Iteration 97, loss = 0.43469669\n",
      "Iteration 98, loss = 0.43433451\n",
      "Iteration 99, loss = 0.43399674\n",
      "Iteration 100, loss = 0.43399623\n",
      "Iteration 101, loss = 0.43338976\n",
      "Iteration 102, loss = 0.43313905\n",
      "Iteration 103, loss = 0.43292877\n",
      "Iteration 104, loss = 0.43261790\n",
      "Iteration 105, loss = 0.43239435\n",
      "Iteration 106, loss = 0.43212510\n",
      "Iteration 107, loss = 0.43181579\n",
      "Iteration 108, loss = 0.43145010\n",
      "Iteration 109, loss = 0.43138946\n",
      "Iteration 110, loss = 0.43098507\n",
      "Iteration 111, loss = 0.43079758\n",
      "Iteration 112, loss = 0.43047921\n",
      "Iteration 113, loss = 0.43043217\n",
      "Iteration 114, loss = 0.43037565\n",
      "Iteration 115, loss = 0.42984281\n",
      "Iteration 116, loss = 0.42973034\n",
      "Iteration 117, loss = 0.42970251\n",
      "Iteration 118, loss = 0.42915835\n",
      "Iteration 119, loss = 0.42913952\n",
      "Iteration 120, loss = 0.42888681\n",
      "Iteration 121, loss = 0.42858497\n",
      "Iteration 122, loss = 0.42839395\n",
      "Iteration 123, loss = 0.42823828\n",
      "Iteration 124, loss = 0.42794765\n",
      "Iteration 125, loss = 0.42766107\n",
      "Iteration 126, loss = 0.42741635\n",
      "Iteration 127, loss = 0.42737670\n",
      "Iteration 128, loss = 0.42733047\n",
      "Iteration 129, loss = 0.42708996\n",
      "Iteration 130, loss = 0.42696892\n",
      "Iteration 131, loss = 0.42668333\n",
      "Iteration 132, loss = 0.42652674\n",
      "Iteration 133, loss = 0.42631081\n",
      "Iteration 134, loss = 0.42608180\n",
      "Iteration 135, loss = 0.42614728\n",
      "Iteration 136, loss = 0.42567158\n",
      "Iteration 137, loss = 0.42578082\n",
      "Iteration 138, loss = 0.42546598\n",
      "Iteration 139, loss = 0.42505856\n",
      "Iteration 140, loss = 0.42497404\n",
      "Iteration 141, loss = 0.42496592\n",
      "Iteration 142, loss = 0.42472822\n",
      "Iteration 143, loss = 0.42476070\n",
      "Iteration 144, loss = 0.42433199\n",
      "Iteration 145, loss = 0.42427350\n",
      "Iteration 146, loss = 0.42405360\n",
      "Iteration 147, loss = 0.42378532\n",
      "Iteration 148, loss = 0.42354965\n",
      "Iteration 149, loss = 0.42339283\n",
      "Iteration 150, loss = 0.42372071\n",
      "Iteration 151, loss = 0.42336860\n",
      "Iteration 152, loss = 0.42311217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 153, loss = 0.42311955\n",
      "Iteration 154, loss = 0.42264200\n",
      "Iteration 155, loss = 0.42273091\n",
      "Iteration 156, loss = 0.42234484\n",
      "Iteration 157, loss = 0.42237434\n",
      "Iteration 158, loss = 0.42200097\n",
      "Iteration 159, loss = 0.42196026\n",
      "Iteration 160, loss = 0.42193573\n",
      "Iteration 161, loss = 0.42188314\n",
      "Iteration 162, loss = 0.42182552\n",
      "Iteration 163, loss = 0.42156597\n",
      "Iteration 164, loss = 0.42122188\n",
      "Iteration 165, loss = 0.42141515\n",
      "Iteration 166, loss = 0.42090925\n",
      "Iteration 167, loss = 0.42119132\n",
      "Iteration 168, loss = 0.42051995\n",
      "Iteration 169, loss = 0.42075872\n",
      "Iteration 170, loss = 0.42051479\n",
      "Iteration 171, loss = 0.42022771\n",
      "Iteration 172, loss = 0.42001391\n",
      "Iteration 173, loss = 0.41998289\n",
      "Iteration 174, loss = 0.41996586\n",
      "Iteration 175, loss = 0.41976820\n",
      "Iteration 176, loss = 0.41973323\n",
      "Iteration 177, loss = 0.41964279\n",
      "Iteration 178, loss = 0.41933943\n",
      "Iteration 179, loss = 0.41923483\n",
      "Iteration 180, loss = 0.41902697\n",
      "Iteration 181, loss = 0.41887363\n",
      "Iteration 182, loss = 0.41890402\n",
      "Iteration 183, loss = 0.41864027\n",
      "Iteration 184, loss = 0.41869122\n",
      "Iteration 185, loss = 0.41858378\n",
      "Iteration 186, loss = 0.41834913\n",
      "Iteration 187, loss = 0.41830753\n",
      "Iteration 188, loss = 0.41804247\n",
      "Iteration 189, loss = 0.41792385\n",
      "Iteration 190, loss = 0.41809691\n",
      "Iteration 191, loss = 0.41766730\n",
      "Iteration 192, loss = 0.41764807\n",
      "Iteration 193, loss = 0.41750307\n",
      "Iteration 194, loss = 0.41735779\n",
      "Iteration 195, loss = 0.41721087\n",
      "Iteration 196, loss = 0.41724868\n",
      "Iteration 197, loss = 0.41688124\n",
      "Iteration 198, loss = 0.41689945\n",
      "Iteration 199, loss = 0.41685356\n",
      "Iteration 200, loss = 0.41676941\n",
      "Iteration 201, loss = 0.41641754\n",
      "Iteration 202, loss = 0.41643393\n",
      "Iteration 203, loss = 0.41641006\n",
      "Iteration 204, loss = 0.41607789\n",
      "Iteration 205, loss = 0.41626586\n",
      "Iteration 206, loss = 0.41610197\n",
      "Iteration 207, loss = 0.41609276\n",
      "Iteration 208, loss = 0.41575051\n",
      "Iteration 209, loss = 0.41594294\n",
      "Iteration 210, loss = 0.41556224\n",
      "Iteration 211, loss = 0.41543420\n",
      "Iteration 212, loss = 0.41532962\n",
      "Iteration 213, loss = 0.41508782\n",
      "Iteration 214, loss = 0.41515948\n",
      "Iteration 215, loss = 0.41488411\n",
      "Iteration 216, loss = 0.41478299\n",
      "Iteration 217, loss = 0.41503992\n",
      "Iteration 218, loss = 0.41466416\n",
      "Iteration 219, loss = 0.41462391\n",
      "Iteration 220, loss = 0.41458815\n",
      "Iteration 221, loss = 0.41434595\n",
      "Iteration 222, loss = 0.41461810\n",
      "Iteration 223, loss = 0.41410951\n",
      "Iteration 224, loss = 0.41401428\n",
      "Iteration 225, loss = 0.41405016\n",
      "Iteration 226, loss = 0.41385095\n",
      "Iteration 227, loss = 0.41370131\n",
      "Iteration 228, loss = 0.41361400\n",
      "Iteration 229, loss = 0.41349235\n",
      "Iteration 230, loss = 0.41353398\n",
      "Iteration 231, loss = 0.41334591\n",
      "Iteration 232, loss = 0.41336628\n",
      "Iteration 233, loss = 0.41301847\n",
      "Iteration 234, loss = 0.41328054\n",
      "Iteration 235, loss = 0.41287557\n",
      "Iteration 236, loss = 0.41281430\n",
      "Iteration 237, loss = 0.41288115\n",
      "Iteration 238, loss = 0.41273344\n",
      "Iteration 239, loss = 0.41246652\n",
      "Iteration 240, loss = 0.41238324\n",
      "Iteration 241, loss = 0.41240874\n",
      "Iteration 242, loss = 0.41221072\n",
      "Iteration 243, loss = 0.41220048\n",
      "Iteration 244, loss = 0.41193013\n",
      "Iteration 245, loss = 0.41185932\n",
      "Iteration 246, loss = 0.41211416\n",
      "Iteration 247, loss = 0.41187116\n",
      "Iteration 248, loss = 0.41155441\n",
      "Iteration 249, loss = 0.41185782\n",
      "Iteration 250, loss = 0.41177160\n",
      "Iteration 251, loss = 0.41134695\n",
      "Iteration 252, loss = 0.41151527\n",
      "Iteration 253, loss = 0.41135730\n",
      "Iteration 254, loss = 0.41111838\n",
      "Iteration 255, loss = 0.41138595\n",
      "Iteration 256, loss = 0.41128825\n",
      "Iteration 257, loss = 0.41101549\n",
      "Iteration 258, loss = 0.41083338\n",
      "Iteration 259, loss = 0.41060709\n",
      "Iteration 260, loss = 0.41084935\n",
      "Iteration 261, loss = 0.41042590\n",
      "Iteration 262, loss = 0.41055840\n",
      "Iteration 263, loss = 0.41042046\n",
      "Iteration 264, loss = 0.41034609\n",
      "Iteration 265, loss = 0.41013834\n",
      "Iteration 266, loss = 0.41037530\n",
      "Iteration 267, loss = 0.40996230\n",
      "Iteration 268, loss = 0.40984109\n",
      "Iteration 269, loss = 0.40999285\n",
      "Iteration 270, loss = 0.40957772\n",
      "Iteration 271, loss = 0.40970114\n",
      "Iteration 272, loss = 0.40956626\n",
      "Iteration 273, loss = 0.40962461\n",
      "Iteration 274, loss = 0.40955937\n",
      "Iteration 275, loss = 0.40966066\n",
      "Iteration 276, loss = 0.40948375\n",
      "Iteration 277, loss = 0.40921092\n",
      "Iteration 278, loss = 0.40930392\n",
      "Iteration 279, loss = 0.40924132\n",
      "Iteration 280, loss = 0.40905283\n",
      "Iteration 281, loss = 0.40871941\n",
      "Iteration 282, loss = 0.40905732\n",
      "Iteration 283, loss = 0.40881976\n",
      "Iteration 284, loss = 0.40842784\n",
      "Iteration 285, loss = 0.40877107\n",
      "Iteration 286, loss = 0.40866028\n",
      "Iteration 287, loss = 0.40843976\n",
      "Iteration 288, loss = 0.40819305\n",
      "Iteration 289, loss = 0.40848611\n",
      "Iteration 290, loss = 0.40811317\n",
      "Iteration 291, loss = 0.40806440\n",
      "Iteration 292, loss = 0.40817730\n",
      "Iteration 293, loss = 0.40788538\n",
      "Iteration 294, loss = 0.40774517\n",
      "Iteration 295, loss = 0.40774605\n",
      "Iteration 296, loss = 0.40772976\n",
      "Iteration 297, loss = 0.40769320\n",
      "Iteration 298, loss = 0.40746213\n",
      "Iteration 299, loss = 0.40748298\n",
      "Iteration 300, loss = 0.40742893\n",
      "Iteration 301, loss = 0.40725962\n",
      "Iteration 302, loss = 0.40717994\n",
      "Iteration 303, loss = 0.40708146\n",
      "Iteration 304, loss = 0.40709829\n",
      "Iteration 305, loss = 0.40718210\n",
      "Iteration 306, loss = 0.40724604\n",
      "Iteration 307, loss = 0.40661256\n",
      "Iteration 308, loss = 0.40707182\n",
      "Iteration 309, loss = 0.40681397\n",
      "Iteration 310, loss = 0.40680057\n",
      "Iteration 311, loss = 0.40646133\n",
      "Iteration 312, loss = 0.40661128\n",
      "Iteration 313, loss = 0.40650463\n",
      "Iteration 314, loss = 0.40649401\n",
      "Iteration 315, loss = 0.40627723\n",
      "Iteration 316, loss = 0.40614756\n",
      "Iteration 317, loss = 0.40608797\n",
      "Iteration 318, loss = 0.40600740\n",
      "Iteration 319, loss = 0.40597058\n",
      "Iteration 320, loss = 0.40596283\n",
      "Iteration 321, loss = 0.40617311\n",
      "Iteration 322, loss = 0.40595920\n",
      "Iteration 323, loss = 0.40564995\n",
      "Iteration 324, loss = 0.40568286\n",
      "Iteration 325, loss = 0.40598794\n",
      "Iteration 326, loss = 0.40547683\n",
      "Iteration 327, loss = 0.40545145\n",
      "Iteration 328, loss = 0.40528369\n",
      "Iteration 329, loss = 0.40546679\n",
      "Iteration 330, loss = 0.40542339\n",
      "Iteration 331, loss = 0.40541947\n",
      "Iteration 332, loss = 0.40540267\n",
      "Iteration 333, loss = 0.40485219\n",
      "Iteration 334, loss = 0.40517428\n",
      "Iteration 335, loss = 0.40505157\n",
      "Iteration 336, loss = 0.40512925\n",
      "Iteration 337, loss = 0.40485051\n",
      "Iteration 338, loss = 0.40489613\n",
      "Iteration 339, loss = 0.40480592\n",
      "Iteration 340, loss = 0.40467658\n",
      "Iteration 341, loss = 0.40465998\n",
      "Iteration 342, loss = 0.40446878\n",
      "Iteration 343, loss = 0.40446772\n",
      "Iteration 344, loss = 0.40460409\n",
      "Iteration 345, loss = 0.40448818\n",
      "Iteration 346, loss = 0.40438055\n",
      "Iteration 347, loss = 0.40444566\n",
      "Iteration 348, loss = 0.40402728\n",
      "Iteration 349, loss = 0.40426034\n",
      "Iteration 350, loss = 0.40420038\n",
      "Iteration 351, loss = 0.40402808\n",
      "Iteration 352, loss = 0.40391184\n",
      "Iteration 353, loss = 0.40397842\n",
      "Iteration 354, loss = 0.40384528\n",
      "Iteration 355, loss = 0.40395995\n",
      "Iteration 356, loss = 0.40389689\n",
      "Iteration 357, loss = 0.40380904\n",
      "Iteration 358, loss = 0.40369137\n",
      "Iteration 359, loss = 0.40379717\n",
      "Iteration 360, loss = 0.40356740\n",
      "Iteration 361, loss = 0.40349221\n",
      "Iteration 362, loss = 0.40361942\n",
      "Iteration 363, loss = 0.40333055\n",
      "Iteration 364, loss = 0.40341424\n",
      "Iteration 365, loss = 0.40357876\n",
      "Iteration 366, loss = 0.40339191\n",
      "Iteration 367, loss = 0.40339258\n",
      "Iteration 368, loss = 0.40318827\n",
      "Iteration 369, loss = 0.40312568\n",
      "Iteration 370, loss = 0.40307969\n",
      "Iteration 371, loss = 0.40294860\n",
      "Iteration 372, loss = 0.40311333\n",
      "Iteration 373, loss = 0.40327909\n",
      "Iteration 374, loss = 0.40292424\n",
      "Iteration 375, loss = 0.40288436\n",
      "Iteration 376, loss = 0.40282395\n",
      "Iteration 377, loss = 0.40279933\n",
      "Iteration 378, loss = 0.40286617\n",
      "Iteration 379, loss = 0.40249203\n",
      "Iteration 380, loss = 0.40256855\n",
      "Iteration 381, loss = 0.40245153\n",
      "Iteration 382, loss = 0.40238532\n",
      "Iteration 383, loss = 0.40260580\n",
      "Iteration 384, loss = 0.40251679\n",
      "Iteration 385, loss = 0.40229686\n",
      "Iteration 386, loss = 0.40205537\n",
      "Iteration 387, loss = 0.40240272\n",
      "Iteration 388, loss = 0.40247844\n",
      "Iteration 389, loss = 0.40221318\n",
      "Iteration 390, loss = 0.40226169\n",
      "Iteration 391, loss = 0.40233621\n",
      "Iteration 392, loss = 0.40186102\n",
      "Iteration 393, loss = 0.40230271\n",
      "Iteration 394, loss = 0.40199562\n",
      "Iteration 395, loss = 0.40195069\n",
      "Iteration 396, loss = 0.40182673\n",
      "Iteration 397, loss = 0.40179404\n",
      "Iteration 398, loss = 0.40168393\n",
      "Iteration 399, loss = 0.40161224\n",
      "Iteration 400, loss = 0.40170842\n",
      "Iteration 401, loss = 0.40190075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 402, loss = 0.40131305\n",
      "Iteration 403, loss = 0.40139779\n",
      "Iteration 404, loss = 0.40144282\n",
      "Iteration 405, loss = 0.40162534\n",
      "Iteration 406, loss = 0.40151330\n",
      "Iteration 407, loss = 0.40121644\n",
      "Iteration 408, loss = 0.40133594\n",
      "Iteration 409, loss = 0.40148287\n",
      "Iteration 410, loss = 0.40132209\n",
      "Iteration 411, loss = 0.40095160\n",
      "Iteration 412, loss = 0.40139001\n",
      "Iteration 413, loss = 0.40108431\n",
      "Iteration 414, loss = 0.40078756\n",
      "Iteration 415, loss = 0.40099970\n",
      "Iteration 416, loss = 0.40098124\n",
      "Iteration 417, loss = 0.40108504\n",
      "Iteration 418, loss = 0.40094423\n",
      "Iteration 419, loss = 0.40078737\n",
      "Iteration 420, loss = 0.40074943\n",
      "Iteration 421, loss = 0.40062951\n",
      "Iteration 422, loss = 0.40089465\n",
      "Iteration 423, loss = 0.40044672\n",
      "Iteration 424, loss = 0.40065288\n",
      "Iteration 425, loss = 0.40051070\n",
      "Iteration 426, loss = 0.40044510\n",
      "Iteration 427, loss = 0.40045067\n",
      "Iteration 428, loss = 0.40067999\n",
      "Iteration 429, loss = 0.40055860\n",
      "Iteration 430, loss = 0.40008723\n",
      "Iteration 431, loss = 0.40029344\n",
      "Iteration 432, loss = 0.40031577\n",
      "Iteration 433, loss = 0.40011030\n",
      "Iteration 434, loss = 0.40026885\n",
      "Iteration 435, loss = 0.40026882\n",
      "Iteration 436, loss = 0.39999098\n",
      "Iteration 437, loss = 0.40006337\n",
      "Iteration 438, loss = 0.40010179\n",
      "Iteration 439, loss = 0.40002326\n",
      "Iteration 440, loss = 0.39973582\n",
      "Iteration 441, loss = 0.39975634\n",
      "Iteration 442, loss = 0.39995377\n",
      "Iteration 443, loss = 0.39989205\n",
      "Iteration 444, loss = 0.39973365\n",
      "Iteration 445, loss = 0.39972994\n",
      "Iteration 446, loss = 0.40001954\n",
      "Iteration 447, loss = 0.39960539\n",
      "Iteration 448, loss = 0.39961567\n",
      "Iteration 449, loss = 0.39968855\n",
      "Iteration 450, loss = 0.39965967\n",
      "Iteration 451, loss = 0.39970521\n",
      "Iteration 452, loss = 0.39954308\n",
      "Iteration 453, loss = 0.39945799\n",
      "Iteration 454, loss = 0.39955857\n",
      "Iteration 455, loss = 0.39928725\n",
      "Iteration 456, loss = 0.39954870\n",
      "Iteration 457, loss = 0.39946906\n",
      "Iteration 458, loss = 0.39938049\n",
      "Iteration 459, loss = 0.39924097\n",
      "Iteration 460, loss = 0.39905537\n",
      "Iteration 461, loss = 0.39911432\n",
      "Iteration 462, loss = 0.39926001\n",
      "Iteration 463, loss = 0.39917467\n",
      "Iteration 464, loss = 0.39898644\n",
      "Iteration 465, loss = 0.39906055\n",
      "Iteration 466, loss = 0.39901152\n",
      "Iteration 467, loss = 0.39890586\n",
      "Iteration 468, loss = 0.39905027\n",
      "Iteration 469, loss = 0.39895677\n",
      "Iteration 470, loss = 0.39892833\n",
      "Iteration 471, loss = 0.39924014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05835521\n",
      "Iteration 2, loss = 1.83582334\n",
      "Iteration 3, loss = 1.66518818\n",
      "Iteration 4, loss = 1.51911017\n",
      "Iteration 5, loss = 1.37408685\n",
      "Iteration 6, loss = 1.23621671\n",
      "Iteration 7, loss = 1.12126299\n",
      "Iteration 8, loss = 1.03043311\n",
      "Iteration 9, loss = 0.95850879\n",
      "Iteration 10, loss = 0.89998193\n",
      "Iteration 11, loss = 0.85090395\n",
      "Iteration 12, loss = 0.81011829\n",
      "Iteration 13, loss = 0.77622760\n",
      "Iteration 14, loss = 0.74764100\n",
      "Iteration 15, loss = 0.72313393\n",
      "Iteration 16, loss = 0.70178609\n",
      "Iteration 17, loss = 0.68302072\n",
      "Iteration 18, loss = 0.66633807\n",
      "Iteration 19, loss = 0.65131008\n",
      "Iteration 20, loss = 0.63762176\n",
      "Iteration 21, loss = 0.62516626\n",
      "Iteration 22, loss = 0.61371078\n",
      "Iteration 23, loss = 0.60299659\n",
      "Iteration 24, loss = 0.59328881\n",
      "Iteration 25, loss = 0.58402760\n",
      "Iteration 26, loss = 0.57542815\n",
      "Iteration 27, loss = 0.56769021\n",
      "Iteration 28, loss = 0.55995223\n",
      "Iteration 29, loss = 0.55297459\n",
      "Iteration 30, loss = 0.54633584\n",
      "Iteration 31, loss = 0.54018545\n",
      "Iteration 32, loss = 0.53442614\n",
      "Iteration 33, loss = 0.52859647\n",
      "Iteration 34, loss = 0.52346564\n",
      "Iteration 35, loss = 0.51840822\n",
      "Iteration 36, loss = 0.51367240\n",
      "Iteration 37, loss = 0.50940951\n",
      "Iteration 38, loss = 0.50527400\n",
      "Iteration 39, loss = 0.50117089\n",
      "Iteration 40, loss = 0.49737891\n",
      "Iteration 41, loss = 0.49360862\n",
      "Iteration 42, loss = 0.49054427\n",
      "Iteration 43, loss = 0.48743919\n",
      "Iteration 44, loss = 0.48449340\n",
      "Iteration 45, loss = 0.48200391\n",
      "Iteration 46, loss = 0.47936475\n",
      "Iteration 47, loss = 0.47709348\n",
      "Iteration 48, loss = 0.47487583\n",
      "Iteration 49, loss = 0.47296613\n",
      "Iteration 50, loss = 0.47111119\n",
      "Iteration 51, loss = 0.46911209\n",
      "Iteration 52, loss = 0.46774563\n",
      "Iteration 53, loss = 0.46625525\n",
      "Iteration 54, loss = 0.46457708\n",
      "Iteration 55, loss = 0.46315363\n",
      "Iteration 56, loss = 0.46212403\n",
      "Iteration 57, loss = 0.46063220\n",
      "Iteration 58, loss = 0.45976196\n",
      "Iteration 59, loss = 0.45840274\n",
      "Iteration 60, loss = 0.45770218\n",
      "Iteration 61, loss = 0.45677797\n",
      "Iteration 62, loss = 0.45553419\n",
      "Iteration 63, loss = 0.45480438\n",
      "Iteration 64, loss = 0.45397626\n",
      "Iteration 65, loss = 0.45320884\n",
      "Iteration 66, loss = 0.45238307\n",
      "Iteration 67, loss = 0.45146933\n",
      "Iteration 68, loss = 0.45085459\n",
      "Iteration 69, loss = 0.45031093\n",
      "Iteration 70, loss = 0.44945394\n",
      "Iteration 71, loss = 0.44895218\n",
      "Iteration 72, loss = 0.44830896\n",
      "Iteration 73, loss = 0.44769282\n",
      "Iteration 74, loss = 0.44711394\n",
      "Iteration 75, loss = 0.44682529\n",
      "Iteration 76, loss = 0.44598749\n",
      "Iteration 77, loss = 0.44555258\n",
      "Iteration 78, loss = 0.44499091\n",
      "Iteration 79, loss = 0.44448551\n",
      "Iteration 80, loss = 0.44394149\n",
      "Iteration 81, loss = 0.44344038\n",
      "Iteration 82, loss = 0.44315665\n",
      "Iteration 83, loss = 0.44266465\n",
      "Iteration 84, loss = 0.44224647\n",
      "Iteration 85, loss = 0.44172104\n",
      "Iteration 86, loss = 0.44134716\n",
      "Iteration 87, loss = 0.44107787\n",
      "Iteration 88, loss = 0.44059638\n",
      "Iteration 89, loss = 0.44026350\n",
      "Iteration 90, loss = 0.44007751\n",
      "Iteration 91, loss = 0.43952631\n",
      "Iteration 92, loss = 0.43936685\n",
      "Iteration 93, loss = 0.43897668\n",
      "Iteration 94, loss = 0.43858556\n",
      "Iteration 95, loss = 0.43824724\n",
      "Iteration 96, loss = 0.43778802\n",
      "Iteration 97, loss = 0.43752369\n",
      "Iteration 98, loss = 0.43737957\n",
      "Iteration 99, loss = 0.43677064\n",
      "Iteration 100, loss = 0.43660759\n",
      "Iteration 101, loss = 0.43632935\n",
      "Iteration 102, loss = 0.43603359\n",
      "Iteration 103, loss = 0.43577023\n",
      "Iteration 104, loss = 0.43548943\n",
      "Iteration 105, loss = 0.43515977\n",
      "Iteration 106, loss = 0.43483509\n",
      "Iteration 107, loss = 0.43463942\n",
      "Iteration 108, loss = 0.43431483\n",
      "Iteration 109, loss = 0.43427354\n",
      "Iteration 110, loss = 0.43381701\n",
      "Iteration 111, loss = 0.43351062\n",
      "Iteration 112, loss = 0.43335316\n",
      "Iteration 113, loss = 0.43327816\n",
      "Iteration 114, loss = 0.43306027\n",
      "Iteration 115, loss = 0.43284623\n",
      "Iteration 116, loss = 0.43243052\n",
      "Iteration 117, loss = 0.43205402\n",
      "Iteration 118, loss = 0.43200473\n",
      "Iteration 119, loss = 0.43188808\n",
      "Iteration 120, loss = 0.43155156\n",
      "Iteration 121, loss = 0.43140677\n",
      "Iteration 122, loss = 0.43136647\n",
      "Iteration 123, loss = 0.43089213\n",
      "Iteration 124, loss = 0.43100001\n",
      "Iteration 125, loss = 0.43070896\n",
      "Iteration 126, loss = 0.43024344\n",
      "Iteration 127, loss = 0.43028045\n",
      "Iteration 128, loss = 0.43001040\n",
      "Iteration 129, loss = 0.42972285\n",
      "Iteration 130, loss = 0.42949301\n",
      "Iteration 131, loss = 0.42971025\n",
      "Iteration 132, loss = 0.42923667\n",
      "Iteration 133, loss = 0.42918614\n",
      "Iteration 134, loss = 0.42895024\n",
      "Iteration 135, loss = 0.42864083\n",
      "Iteration 136, loss = 0.42844869\n",
      "Iteration 137, loss = 0.42828413\n",
      "Iteration 138, loss = 0.42796909\n",
      "Iteration 139, loss = 0.42794441\n",
      "Iteration 140, loss = 0.42773742\n",
      "Iteration 141, loss = 0.42756231\n",
      "Iteration 142, loss = 0.42737419\n",
      "Iteration 143, loss = 0.42730044\n",
      "Iteration 144, loss = 0.42729213\n",
      "Iteration 145, loss = 0.42684272\n",
      "Iteration 146, loss = 0.42677708\n",
      "Iteration 147, loss = 0.42679785\n",
      "Iteration 148, loss = 0.42660517\n",
      "Iteration 149, loss = 0.42624905\n",
      "Iteration 150, loss = 0.42641360\n",
      "Iteration 151, loss = 0.42603722\n",
      "Iteration 152, loss = 0.42585305\n",
      "Iteration 153, loss = 0.42560429\n",
      "Iteration 154, loss = 0.42555577\n",
      "Iteration 155, loss = 0.42538164\n",
      "Iteration 156, loss = 0.42535384\n",
      "Iteration 157, loss = 0.42510238\n",
      "Iteration 158, loss = 0.42503095\n",
      "Iteration 159, loss = 0.42471249\n",
      "Iteration 160, loss = 0.42465117\n",
      "Iteration 161, loss = 0.42473874\n",
      "Iteration 162, loss = 0.42433239\n",
      "Iteration 163, loss = 0.42426300\n",
      "Iteration 164, loss = 0.42406318\n",
      "Iteration 165, loss = 0.42397885\n",
      "Iteration 166, loss = 0.42376932\n",
      "Iteration 167, loss = 0.42378519\n",
      "Iteration 168, loss = 0.42365142\n",
      "Iteration 169, loss = 0.42322540\n",
      "Iteration 170, loss = 0.42316613\n",
      "Iteration 171, loss = 0.42317618\n",
      "Iteration 172, loss = 0.42302671\n",
      "Iteration 173, loss = 0.42295449\n",
      "Iteration 174, loss = 0.42281112\n",
      "Iteration 175, loss = 0.42228963\n",
      "Iteration 176, loss = 0.42254836\n",
      "Iteration 177, loss = 0.42219512\n",
      "Iteration 178, loss = 0.42235763\n",
      "Iteration 179, loss = 0.42203015\n",
      "Iteration 180, loss = 0.42188402\n",
      "Iteration 181, loss = 0.42188269\n",
      "Iteration 182, loss = 0.42166091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 183, loss = 0.42172235\n",
      "Iteration 184, loss = 0.42143106\n",
      "Iteration 185, loss = 0.42132369\n",
      "Iteration 186, loss = 0.42113982\n",
      "Iteration 187, loss = 0.42093875\n",
      "Iteration 188, loss = 0.42115527\n",
      "Iteration 189, loss = 0.42090312\n",
      "Iteration 190, loss = 0.42076870\n",
      "Iteration 191, loss = 0.42065707\n",
      "Iteration 192, loss = 0.42069563\n",
      "Iteration 193, loss = 0.42027101\n",
      "Iteration 194, loss = 0.42022850\n",
      "Iteration 195, loss = 0.42015989\n",
      "Iteration 196, loss = 0.42003878\n",
      "Iteration 197, loss = 0.42004262\n",
      "Iteration 198, loss = 0.42019759\n",
      "Iteration 199, loss = 0.41964114\n",
      "Iteration 200, loss = 0.41960478\n",
      "Iteration 201, loss = 0.41941738\n",
      "Iteration 202, loss = 0.41923514\n",
      "Iteration 203, loss = 0.41934195\n",
      "Iteration 204, loss = 0.41911901\n",
      "Iteration 205, loss = 0.41903436\n",
      "Iteration 206, loss = 0.41892917\n",
      "Iteration 207, loss = 0.41892600\n",
      "Iteration 208, loss = 0.41886847\n",
      "Iteration 209, loss = 0.41838775\n",
      "Iteration 210, loss = 0.41840037\n",
      "Iteration 211, loss = 0.41863240\n",
      "Iteration 212, loss = 0.41813578\n",
      "Iteration 213, loss = 0.41825681\n",
      "Iteration 214, loss = 0.41799524\n",
      "Iteration 215, loss = 0.41810953\n",
      "Iteration 216, loss = 0.41791122\n",
      "Iteration 217, loss = 0.41769427\n",
      "Iteration 218, loss = 0.41745300\n",
      "Iteration 219, loss = 0.41768006\n",
      "Iteration 220, loss = 0.41761169\n",
      "Iteration 221, loss = 0.41710529\n",
      "Iteration 222, loss = 0.41713177\n",
      "Iteration 223, loss = 0.41719044\n",
      "Iteration 224, loss = 0.41691481\n",
      "Iteration 225, loss = 0.41709756\n",
      "Iteration 226, loss = 0.41677105\n",
      "Iteration 227, loss = 0.41667261\n",
      "Iteration 228, loss = 0.41670406\n",
      "Iteration 229, loss = 0.41670319\n",
      "Iteration 230, loss = 0.41673833\n",
      "Iteration 231, loss = 0.41624285\n",
      "Iteration 232, loss = 0.41619003\n",
      "Iteration 233, loss = 0.41622606\n",
      "Iteration 234, loss = 0.41590399\n",
      "Iteration 235, loss = 0.41593802\n",
      "Iteration 236, loss = 0.41579627\n",
      "Iteration 237, loss = 0.41586845\n",
      "Iteration 238, loss = 0.41572556\n",
      "Iteration 239, loss = 0.41554898\n",
      "Iteration 240, loss = 0.41554302\n",
      "Iteration 241, loss = 0.41520633\n",
      "Iteration 242, loss = 0.41560715\n",
      "Iteration 243, loss = 0.41531194\n",
      "Iteration 244, loss = 0.41526103\n",
      "Iteration 245, loss = 0.41524534\n",
      "Iteration 246, loss = 0.41513587\n",
      "Iteration 247, loss = 0.41482972\n",
      "Iteration 248, loss = 0.41489737\n",
      "Iteration 249, loss = 0.41476134\n",
      "Iteration 250, loss = 0.41468310\n",
      "Iteration 251, loss = 0.41464370\n",
      "Iteration 252, loss = 0.41441637\n",
      "Iteration 253, loss = 0.41446603\n",
      "Iteration 254, loss = 0.41443731\n",
      "Iteration 255, loss = 0.41420299\n",
      "Iteration 256, loss = 0.41419088\n",
      "Iteration 257, loss = 0.41416517\n",
      "Iteration 258, loss = 0.41395947\n",
      "Iteration 259, loss = 0.41388998\n",
      "Iteration 260, loss = 0.41393388\n",
      "Iteration 261, loss = 0.41371892\n",
      "Iteration 262, loss = 0.41363089\n",
      "Iteration 263, loss = 0.41360807\n",
      "Iteration 264, loss = 0.41316124\n",
      "Iteration 265, loss = 0.41350729\n",
      "Iteration 266, loss = 0.41314076\n",
      "Iteration 267, loss = 0.41318418\n",
      "Iteration 268, loss = 0.41301327\n",
      "Iteration 269, loss = 0.41309897\n",
      "Iteration 270, loss = 0.41286932\n",
      "Iteration 271, loss = 0.41298213\n",
      "Iteration 272, loss = 0.41278592\n",
      "Iteration 273, loss = 0.41284633\n",
      "Iteration 274, loss = 0.41266086\n",
      "Iteration 275, loss = 0.41248308\n",
      "Iteration 276, loss = 0.41313847\n",
      "Iteration 277, loss = 0.41258388\n",
      "Iteration 278, loss = 0.41245695\n",
      "Iteration 279, loss = 0.41223168\n",
      "Iteration 280, loss = 0.41211442\n",
      "Iteration 281, loss = 0.41209366\n",
      "Iteration 282, loss = 0.41185480\n",
      "Iteration 283, loss = 0.41186729\n",
      "Iteration 284, loss = 0.41187002\n",
      "Iteration 285, loss = 0.41164209\n",
      "Iteration 286, loss = 0.41179175\n",
      "Iteration 287, loss = 0.41164381\n",
      "Iteration 288, loss = 0.41178946\n",
      "Iteration 289, loss = 0.41119177\n",
      "Iteration 290, loss = 0.41140593\n",
      "Iteration 291, loss = 0.41144278\n",
      "Iteration 292, loss = 0.41117379\n",
      "Iteration 293, loss = 0.41106003\n",
      "Iteration 294, loss = 0.41084982\n",
      "Iteration 295, loss = 0.41092642\n",
      "Iteration 296, loss = 0.41069970\n",
      "Iteration 297, loss = 0.41060771\n",
      "Iteration 298, loss = 0.41088310\n",
      "Iteration 299, loss = 0.41081985\n",
      "Iteration 300, loss = 0.41041647\n",
      "Iteration 301, loss = 0.41022134\n",
      "Iteration 302, loss = 0.41011601\n",
      "Iteration 303, loss = 0.41045572\n",
      "Iteration 304, loss = 0.41036795\n",
      "Iteration 305, loss = 0.41000297\n",
      "Iteration 306, loss = 0.41007931\n",
      "Iteration 307, loss = 0.40987521\n",
      "Iteration 308, loss = 0.41008490\n",
      "Iteration 309, loss = 0.40962418\n",
      "Iteration 310, loss = 0.40979427\n",
      "Iteration 311, loss = 0.40978732\n",
      "Iteration 312, loss = 0.40981178\n",
      "Iteration 313, loss = 0.40963234\n",
      "Iteration 314, loss = 0.40942628\n",
      "Iteration 315, loss = 0.40924140\n",
      "Iteration 316, loss = 0.40923426\n",
      "Iteration 317, loss = 0.40929258\n",
      "Iteration 318, loss = 0.40926485\n",
      "Iteration 319, loss = 0.40910054\n",
      "Iteration 320, loss = 0.40905442\n",
      "Iteration 321, loss = 0.40934413\n",
      "Iteration 322, loss = 0.40866608\n",
      "Iteration 323, loss = 0.40879972\n",
      "Iteration 324, loss = 0.40871083\n",
      "Iteration 325, loss = 0.40906418\n",
      "Iteration 326, loss = 0.40881322\n",
      "Iteration 327, loss = 0.40884916\n",
      "Iteration 328, loss = 0.40864609\n",
      "Iteration 329, loss = 0.40851176\n",
      "Iteration 330, loss = 0.40855879\n",
      "Iteration 331, loss = 0.40833435\n",
      "Iteration 332, loss = 0.40859632\n",
      "Iteration 333, loss = 0.40835291\n",
      "Iteration 334, loss = 0.40829515\n",
      "Iteration 335, loss = 0.40819740\n",
      "Iteration 336, loss = 0.40805163\n",
      "Iteration 337, loss = 0.40807640\n",
      "Iteration 338, loss = 0.40810513\n",
      "Iteration 339, loss = 0.40808501\n",
      "Iteration 340, loss = 0.40798350\n",
      "Iteration 341, loss = 0.40793776\n",
      "Iteration 342, loss = 0.40784047\n",
      "Iteration 343, loss = 0.40765973\n",
      "Iteration 344, loss = 0.40778520\n",
      "Iteration 345, loss = 0.40791450\n",
      "Iteration 346, loss = 0.40747584\n",
      "Iteration 347, loss = 0.40753004\n",
      "Iteration 348, loss = 0.40751667\n",
      "Iteration 349, loss = 0.40735337\n",
      "Iteration 350, loss = 0.40730944\n",
      "Iteration 351, loss = 0.40727668\n",
      "Iteration 352, loss = 0.40732397\n",
      "Iteration 353, loss = 0.40724505\n",
      "Iteration 354, loss = 0.40708446\n",
      "Iteration 355, loss = 0.40711503\n",
      "Iteration 356, loss = 0.40678277\n",
      "Iteration 357, loss = 0.40709958\n",
      "Iteration 358, loss = 0.40680146\n",
      "Iteration 359, loss = 0.40715822\n",
      "Iteration 360, loss = 0.40684195\n",
      "Iteration 361, loss = 0.40685380\n",
      "Iteration 362, loss = 0.40694474\n",
      "Iteration 363, loss = 0.40680552\n",
      "Iteration 364, loss = 0.40655186\n",
      "Iteration 365, loss = 0.40642726\n",
      "Iteration 366, loss = 0.40682988\n",
      "Iteration 367, loss = 0.40661671\n",
      "Iteration 368, loss = 0.40637586\n",
      "Iteration 369, loss = 0.40643366\n",
      "Iteration 370, loss = 0.40640496\n",
      "Iteration 371, loss = 0.40647401\n",
      "Iteration 372, loss = 0.40649390\n",
      "Iteration 373, loss = 0.40647558\n",
      "Iteration 374, loss = 0.40633255\n",
      "Iteration 375, loss = 0.40613755\n",
      "Iteration 376, loss = 0.40600839\n",
      "Iteration 377, loss = 0.40609687\n",
      "Iteration 378, loss = 0.40587209\n",
      "Iteration 379, loss = 0.40584417\n",
      "Iteration 380, loss = 0.40587056\n",
      "Iteration 381, loss = 0.40583258\n",
      "Iteration 382, loss = 0.40587551\n",
      "Iteration 383, loss = 0.40604932\n",
      "Iteration 384, loss = 0.40556992\n",
      "Iteration 385, loss = 0.40558848\n",
      "Iteration 386, loss = 0.40553334\n",
      "Iteration 387, loss = 0.40540751\n",
      "Iteration 388, loss = 0.40535688\n",
      "Iteration 389, loss = 0.40543835\n",
      "Iteration 390, loss = 0.40535314\n",
      "Iteration 391, loss = 0.40539880\n",
      "Iteration 392, loss = 0.40543419\n",
      "Iteration 393, loss = 0.40534965\n",
      "Iteration 394, loss = 0.40524663\n",
      "Iteration 395, loss = 0.40549749\n",
      "Iteration 396, loss = 0.40512837\n",
      "Iteration 397, loss = 0.40519422\n",
      "Iteration 398, loss = 0.40505347\n",
      "Iteration 399, loss = 0.40540081\n",
      "Iteration 400, loss = 0.40510529\n",
      "Iteration 401, loss = 0.40487739\n",
      "Iteration 402, loss = 0.40498960\n",
      "Iteration 403, loss = 0.40476434\n",
      "Iteration 404, loss = 0.40482987\n",
      "Iteration 405, loss = 0.40483635\n",
      "Iteration 406, loss = 0.40451430\n",
      "Iteration 407, loss = 0.40464858\n",
      "Iteration 408, loss = 0.40463017\n",
      "Iteration 409, loss = 0.40454201\n",
      "Iteration 410, loss = 0.40470145\n",
      "Iteration 411, loss = 0.40466467\n",
      "Iteration 412, loss = 0.40439178\n",
      "Iteration 413, loss = 0.40457371\n",
      "Iteration 414, loss = 0.40422841\n",
      "Iteration 415, loss = 0.40456940\n",
      "Iteration 416, loss = 0.40435888\n",
      "Iteration 417, loss = 0.40440342\n",
      "Iteration 418, loss = 0.40428594\n",
      "Iteration 419, loss = 0.40442637\n",
      "Iteration 420, loss = 0.40421391\n",
      "Iteration 421, loss = 0.40387268\n",
      "Iteration 422, loss = 0.40407349\n",
      "Iteration 423, loss = 0.40403927\n",
      "Iteration 424, loss = 0.40405940\n",
      "Iteration 425, loss = 0.40396725\n",
      "Iteration 426, loss = 0.40384306\n",
      "Iteration 427, loss = 0.40402495\n",
      "Iteration 428, loss = 0.40384987\n",
      "Iteration 429, loss = 0.40385117\n",
      "Iteration 430, loss = 0.40381989\n",
      "Iteration 431, loss = 0.40373636\n",
      "Iteration 432, loss = 0.40370253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.06301534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.84077658\n",
      "Iteration 3, loss = 1.66984655\n",
      "Iteration 4, loss = 1.52230211\n",
      "Iteration 5, loss = 1.37630068\n",
      "Iteration 6, loss = 1.23839045\n",
      "Iteration 7, loss = 1.12315116\n",
      "Iteration 8, loss = 1.03191963\n",
      "Iteration 9, loss = 0.95992848\n",
      "Iteration 10, loss = 0.90122796\n",
      "Iteration 11, loss = 0.85189069\n",
      "Iteration 12, loss = 0.81051967\n",
      "Iteration 13, loss = 0.77609370\n",
      "Iteration 14, loss = 0.74724860\n",
      "Iteration 15, loss = 0.72253012\n",
      "Iteration 16, loss = 0.70112650\n",
      "Iteration 17, loss = 0.68222196\n",
      "Iteration 18, loss = 0.66523109\n",
      "Iteration 19, loss = 0.65009870\n",
      "Iteration 20, loss = 0.63631575\n",
      "Iteration 21, loss = 0.62367748\n",
      "Iteration 22, loss = 0.61222156\n",
      "Iteration 23, loss = 0.60161199\n",
      "Iteration 24, loss = 0.59169325\n",
      "Iteration 25, loss = 0.58283725\n",
      "Iteration 26, loss = 0.57449473\n",
      "Iteration 27, loss = 0.56652640\n",
      "Iteration 28, loss = 0.55933213\n",
      "Iteration 29, loss = 0.55255788\n",
      "Iteration 30, loss = 0.54616420\n",
      "Iteration 31, loss = 0.54015743\n",
      "Iteration 32, loss = 0.53441393\n",
      "Iteration 33, loss = 0.52908107\n",
      "Iteration 34, loss = 0.52403483\n",
      "Iteration 35, loss = 0.51931457\n",
      "Iteration 36, loss = 0.51478864\n",
      "Iteration 37, loss = 0.51042884\n",
      "Iteration 38, loss = 0.50645215\n",
      "Iteration 39, loss = 0.50265414\n",
      "Iteration 40, loss = 0.49892762\n",
      "Iteration 41, loss = 0.49527983\n",
      "Iteration 42, loss = 0.49198182\n",
      "Iteration 43, loss = 0.48882047\n",
      "Iteration 44, loss = 0.48568115\n",
      "Iteration 45, loss = 0.48298095\n",
      "Iteration 46, loss = 0.48018936\n",
      "Iteration 47, loss = 0.47759087\n",
      "Iteration 48, loss = 0.47532437\n",
      "Iteration 49, loss = 0.47311541\n",
      "Iteration 50, loss = 0.47107664\n",
      "Iteration 51, loss = 0.46909314\n",
      "Iteration 52, loss = 0.46715752\n",
      "Iteration 53, loss = 0.46560168\n",
      "Iteration 54, loss = 0.46399388\n",
      "Iteration 55, loss = 0.46203566\n",
      "Iteration 56, loss = 0.46062052\n",
      "Iteration 57, loss = 0.45928967\n",
      "Iteration 58, loss = 0.45813013\n",
      "Iteration 59, loss = 0.45678099\n",
      "Iteration 60, loss = 0.45548743\n",
      "Iteration 61, loss = 0.45450522\n",
      "Iteration 62, loss = 0.45343259\n",
      "Iteration 63, loss = 0.45243061\n",
      "Iteration 64, loss = 0.45162727\n",
      "Iteration 65, loss = 0.45077143\n",
      "Iteration 66, loss = 0.44975008\n",
      "Iteration 67, loss = 0.44871010\n",
      "Iteration 68, loss = 0.44810495\n",
      "Iteration 69, loss = 0.44726564\n",
      "Iteration 70, loss = 0.44650905\n",
      "Iteration 71, loss = 0.44586048\n",
      "Iteration 72, loss = 0.44488382\n",
      "Iteration 73, loss = 0.44431525\n",
      "Iteration 74, loss = 0.44387822\n",
      "Iteration 75, loss = 0.44318002\n",
      "Iteration 76, loss = 0.44264105\n",
      "Iteration 77, loss = 0.44207299\n",
      "Iteration 78, loss = 0.44131321\n",
      "Iteration 79, loss = 0.44083897\n",
      "Iteration 80, loss = 0.44033561\n",
      "Iteration 81, loss = 0.43972774\n",
      "Iteration 82, loss = 0.43944375\n",
      "Iteration 83, loss = 0.43869473\n",
      "Iteration 84, loss = 0.43838221\n",
      "Iteration 85, loss = 0.43798334\n",
      "Iteration 86, loss = 0.43767570\n",
      "Iteration 87, loss = 0.43717133\n",
      "Iteration 88, loss = 0.43643247\n",
      "Iteration 89, loss = 0.43606701\n",
      "Iteration 90, loss = 0.43582179\n",
      "Iteration 91, loss = 0.43526716\n",
      "Iteration 92, loss = 0.43511340\n",
      "Iteration 93, loss = 0.43477659\n",
      "Iteration 94, loss = 0.43422537\n",
      "Iteration 95, loss = 0.43405653\n",
      "Iteration 96, loss = 0.43369822\n",
      "Iteration 97, loss = 0.43306900\n",
      "Iteration 98, loss = 0.43282625\n",
      "Iteration 99, loss = 0.43237476\n",
      "Iteration 100, loss = 0.43245256\n",
      "Iteration 101, loss = 0.43169730\n",
      "Iteration 102, loss = 0.43123298\n",
      "Iteration 103, loss = 0.43102743\n",
      "Iteration 104, loss = 0.43094110\n",
      "Iteration 105, loss = 0.43063702\n",
      "Iteration 106, loss = 0.43007929\n",
      "Iteration 107, loss = 0.43033536\n",
      "Iteration 108, loss = 0.43011413\n",
      "Iteration 109, loss = 0.42953888\n",
      "Iteration 110, loss = 0.42932686\n",
      "Iteration 111, loss = 0.42892869\n",
      "Iteration 112, loss = 0.42867907\n",
      "Iteration 113, loss = 0.42844752\n",
      "Iteration 114, loss = 0.42829093\n",
      "Iteration 115, loss = 0.42792364\n",
      "Iteration 116, loss = 0.42752136\n",
      "Iteration 117, loss = 0.42717660\n",
      "Iteration 118, loss = 0.42713413\n",
      "Iteration 119, loss = 0.42678900\n",
      "Iteration 120, loss = 0.42661106\n",
      "Iteration 121, loss = 0.42628653\n",
      "Iteration 122, loss = 0.42616113\n",
      "Iteration 123, loss = 0.42581350\n",
      "Iteration 124, loss = 0.42550266\n",
      "Iteration 125, loss = 0.42567449\n",
      "Iteration 126, loss = 0.42524201\n",
      "Iteration 127, loss = 0.42501687\n",
      "Iteration 128, loss = 0.42480011\n",
      "Iteration 129, loss = 0.42450072\n",
      "Iteration 130, loss = 0.42452709\n",
      "Iteration 131, loss = 0.42442334\n",
      "Iteration 132, loss = 0.42409755\n",
      "Iteration 133, loss = 0.42385280\n",
      "Iteration 134, loss = 0.42371554\n",
      "Iteration 135, loss = 0.42340823\n",
      "Iteration 136, loss = 0.42333563\n",
      "Iteration 137, loss = 0.42306286\n",
      "Iteration 138, loss = 0.42297189\n",
      "Iteration 139, loss = 0.42275495\n",
      "Iteration 140, loss = 0.42247215\n",
      "Iteration 141, loss = 0.42231681\n",
      "Iteration 142, loss = 0.42232804\n",
      "Iteration 143, loss = 0.42209617\n",
      "Iteration 144, loss = 0.42176176\n",
      "Iteration 145, loss = 0.42150837\n",
      "Iteration 146, loss = 0.42152339\n",
      "Iteration 147, loss = 0.42102590\n",
      "Iteration 148, loss = 0.42122261\n",
      "Iteration 149, loss = 0.42097781\n",
      "Iteration 150, loss = 0.42120232\n",
      "Iteration 151, loss = 0.42055489\n",
      "Iteration 152, loss = 0.42042212\n",
      "Iteration 153, loss = 0.42028083\n",
      "Iteration 154, loss = 0.42017924\n",
      "Iteration 155, loss = 0.41994201\n",
      "Iteration 156, loss = 0.41997244\n",
      "Iteration 157, loss = 0.41960173\n",
      "Iteration 158, loss = 0.41966633\n",
      "Iteration 159, loss = 0.41948425\n",
      "Iteration 160, loss = 0.41931877\n",
      "Iteration 161, loss = 0.41905870\n",
      "Iteration 162, loss = 0.41915899\n",
      "Iteration 163, loss = 0.41902149\n",
      "Iteration 164, loss = 0.41860652\n",
      "Iteration 165, loss = 0.41859317\n",
      "Iteration 166, loss = 0.41845206\n",
      "Iteration 167, loss = 0.41806927\n",
      "Iteration 168, loss = 0.41799640\n",
      "Iteration 169, loss = 0.41822349\n",
      "Iteration 170, loss = 0.41800285\n",
      "Iteration 171, loss = 0.41755761\n",
      "Iteration 172, loss = 0.41768277\n",
      "Iteration 173, loss = 0.41757301\n",
      "Iteration 174, loss = 0.41719398\n",
      "Iteration 175, loss = 0.41739690\n",
      "Iteration 176, loss = 0.41704498\n",
      "Iteration 177, loss = 0.41672314\n",
      "Iteration 178, loss = 0.41675517\n",
      "Iteration 179, loss = 0.41636037\n",
      "Iteration 180, loss = 0.41638915\n",
      "Iteration 181, loss = 0.41615416\n",
      "Iteration 182, loss = 0.41630289\n",
      "Iteration 183, loss = 0.41636388\n",
      "Iteration 184, loss = 0.41600505\n",
      "Iteration 185, loss = 0.41580321\n",
      "Iteration 186, loss = 0.41585115\n",
      "Iteration 187, loss = 0.41560508\n",
      "Iteration 188, loss = 0.41569527\n",
      "Iteration 189, loss = 0.41560986\n",
      "Iteration 190, loss = 0.41544940\n",
      "Iteration 191, loss = 0.41535417\n",
      "Iteration 192, loss = 0.41491959\n",
      "Iteration 193, loss = 0.41492726\n",
      "Iteration 194, loss = 0.41480838\n",
      "Iteration 195, loss = 0.41467105\n",
      "Iteration 196, loss = 0.41481314\n",
      "Iteration 197, loss = 0.41466368\n",
      "Iteration 198, loss = 0.41440887\n",
      "Iteration 199, loss = 0.41428485\n",
      "Iteration 200, loss = 0.41426543\n",
      "Iteration 201, loss = 0.41398807\n",
      "Iteration 202, loss = 0.41408695\n",
      "Iteration 203, loss = 0.41403482\n",
      "Iteration 204, loss = 0.41387590\n",
      "Iteration 205, loss = 0.41371222\n",
      "Iteration 206, loss = 0.41350178\n",
      "Iteration 207, loss = 0.41320940\n",
      "Iteration 208, loss = 0.41300316\n",
      "Iteration 209, loss = 0.41314211\n",
      "Iteration 210, loss = 0.41290370\n",
      "Iteration 211, loss = 0.41269792\n",
      "Iteration 212, loss = 0.41277453\n",
      "Iteration 213, loss = 0.41270025\n",
      "Iteration 214, loss = 0.41255681\n",
      "Iteration 215, loss = 0.41243784\n",
      "Iteration 216, loss = 0.41229308\n",
      "Iteration 217, loss = 0.41225183\n",
      "Iteration 218, loss = 0.41214001\n",
      "Iteration 219, loss = 0.41210727\n",
      "Iteration 220, loss = 0.41196247\n",
      "Iteration 221, loss = 0.41182454\n",
      "Iteration 222, loss = 0.41164252\n",
      "Iteration 223, loss = 0.41164905\n",
      "Iteration 224, loss = 0.41144419\n",
      "Iteration 225, loss = 0.41141577\n",
      "Iteration 226, loss = 0.41141558\n",
      "Iteration 227, loss = 0.41135277\n",
      "Iteration 228, loss = 0.41102323\n",
      "Iteration 229, loss = 0.41102438\n",
      "Iteration 230, loss = 0.41094716\n",
      "Iteration 231, loss = 0.41090955\n",
      "Iteration 232, loss = 0.41056428\n",
      "Iteration 233, loss = 0.41040727\n",
      "Iteration 234, loss = 0.41058438\n",
      "Iteration 235, loss = 0.41037328\n",
      "Iteration 236, loss = 0.41033645\n",
      "Iteration 237, loss = 0.41017428\n",
      "Iteration 238, loss = 0.41035792\n",
      "Iteration 239, loss = 0.41015260\n",
      "Iteration 240, loss = 0.41002032\n",
      "Iteration 241, loss = 0.40987015\n",
      "Iteration 242, loss = 0.40985887\n",
      "Iteration 243, loss = 0.40981175\n",
      "Iteration 244, loss = 0.40963681\n",
      "Iteration 245, loss = 0.40960055\n",
      "Iteration 246, loss = 0.40954875\n",
      "Iteration 247, loss = 0.40938974\n",
      "Iteration 248, loss = 0.40936941\n",
      "Iteration 249, loss = 0.40942324\n",
      "Iteration 250, loss = 0.40913238\n",
      "Iteration 251, loss = 0.40913377\n",
      "Iteration 252, loss = 0.40905413\n",
      "Iteration 253, loss = 0.40904229\n",
      "Iteration 254, loss = 0.40867992\n",
      "Iteration 255, loss = 0.40869512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.40862008\n",
      "Iteration 257, loss = 0.40869071\n",
      "Iteration 258, loss = 0.40867059\n",
      "Iteration 259, loss = 0.40846047\n",
      "Iteration 260, loss = 0.40847968\n",
      "Iteration 261, loss = 0.40821798\n",
      "Iteration 262, loss = 0.40816115\n",
      "Iteration 263, loss = 0.40828033\n",
      "Iteration 264, loss = 0.40822945\n",
      "Iteration 265, loss = 0.40791332\n",
      "Iteration 266, loss = 0.40771089\n",
      "Iteration 267, loss = 0.40768752\n",
      "Iteration 268, loss = 0.40749366\n",
      "Iteration 269, loss = 0.40796833\n",
      "Iteration 270, loss = 0.40751152\n",
      "Iteration 271, loss = 0.40763275\n",
      "Iteration 272, loss = 0.40758420\n",
      "Iteration 273, loss = 0.40753183\n",
      "Iteration 274, loss = 0.40741351\n",
      "Iteration 275, loss = 0.40731022\n",
      "Iteration 276, loss = 0.40711535\n",
      "Iteration 277, loss = 0.40714224\n",
      "Iteration 278, loss = 0.40708559\n",
      "Iteration 279, loss = 0.40698330\n",
      "Iteration 280, loss = 0.40694290\n",
      "Iteration 281, loss = 0.40700803\n",
      "Iteration 282, loss = 0.40672918\n",
      "Iteration 283, loss = 0.40659328\n",
      "Iteration 284, loss = 0.40688413\n",
      "Iteration 285, loss = 0.40664858\n",
      "Iteration 286, loss = 0.40636727\n",
      "Iteration 287, loss = 0.40628252\n",
      "Iteration 288, loss = 0.40617539\n",
      "Iteration 289, loss = 0.40626986\n",
      "Iteration 290, loss = 0.40622494\n",
      "Iteration 291, loss = 0.40626915\n",
      "Iteration 292, loss = 0.40612355\n",
      "Iteration 293, loss = 0.40580599\n",
      "Iteration 294, loss = 0.40591355\n",
      "Iteration 295, loss = 0.40589158\n",
      "Iteration 296, loss = 0.40572360\n",
      "Iteration 297, loss = 0.40573739\n",
      "Iteration 298, loss = 0.40581672\n",
      "Iteration 299, loss = 0.40568220\n",
      "Iteration 300, loss = 0.40548543\n",
      "Iteration 301, loss = 0.40554043\n",
      "Iteration 302, loss = 0.40538923\n",
      "Iteration 303, loss = 0.40549277\n",
      "Iteration 304, loss = 0.40505570\n",
      "Iteration 305, loss = 0.40523300\n",
      "Iteration 306, loss = 0.40519760\n",
      "Iteration 307, loss = 0.40504147\n",
      "Iteration 308, loss = 0.40510919\n",
      "Iteration 309, loss = 0.40508039\n",
      "Iteration 310, loss = 0.40483997\n",
      "Iteration 311, loss = 0.40498811\n",
      "Iteration 312, loss = 0.40471873\n",
      "Iteration 313, loss = 0.40472380\n",
      "Iteration 314, loss = 0.40462398\n",
      "Iteration 315, loss = 0.40464265\n",
      "Iteration 316, loss = 0.40450213\n",
      "Iteration 317, loss = 0.40447467\n",
      "Iteration 318, loss = 0.40440178\n",
      "Iteration 319, loss = 0.40450723\n",
      "Iteration 320, loss = 0.40442949\n",
      "Iteration 321, loss = 0.40450463\n",
      "Iteration 322, loss = 0.40427777\n",
      "Iteration 323, loss = 0.40420221\n",
      "Iteration 324, loss = 0.40406497\n",
      "Iteration 325, loss = 0.40414185\n",
      "Iteration 326, loss = 0.40394609\n",
      "Iteration 327, loss = 0.40378945\n",
      "Iteration 328, loss = 0.40398697\n",
      "Iteration 329, loss = 0.40393586\n",
      "Iteration 330, loss = 0.40388252\n",
      "Iteration 331, loss = 0.40397318\n",
      "Iteration 332, loss = 0.40393492\n",
      "Iteration 333, loss = 0.40359684\n",
      "Iteration 334, loss = 0.40340251\n",
      "Iteration 335, loss = 0.40338185\n",
      "Iteration 336, loss = 0.40360040\n",
      "Iteration 337, loss = 0.40348398\n",
      "Iteration 338, loss = 0.40355972\n",
      "Iteration 339, loss = 0.40316506\n",
      "Iteration 340, loss = 0.40324125\n",
      "Iteration 341, loss = 0.40330013\n",
      "Iteration 342, loss = 0.40313544\n",
      "Iteration 343, loss = 0.40319391\n",
      "Iteration 344, loss = 0.40327576\n",
      "Iteration 345, loss = 0.40285377\n",
      "Iteration 346, loss = 0.40303822\n",
      "Iteration 347, loss = 0.40300802\n",
      "Iteration 348, loss = 0.40310264\n",
      "Iteration 349, loss = 0.40290115\n",
      "Iteration 350, loss = 0.40274628\n",
      "Iteration 351, loss = 0.40273107\n",
      "Iteration 352, loss = 0.40260266\n",
      "Iteration 353, loss = 0.40266121\n",
      "Iteration 354, loss = 0.40263607\n",
      "Iteration 355, loss = 0.40296351\n",
      "Iteration 356, loss = 0.40256308\n",
      "Iteration 357, loss = 0.40243358\n",
      "Iteration 358, loss = 0.40228718\n",
      "Iteration 359, loss = 0.40246163\n",
      "Iteration 360, loss = 0.40233814\n",
      "Iteration 361, loss = 0.40221815\n",
      "Iteration 362, loss = 0.40230704\n",
      "Iteration 363, loss = 0.40216681\n",
      "Iteration 364, loss = 0.40211817\n",
      "Iteration 365, loss = 0.40236733\n",
      "Iteration 366, loss = 0.40216399\n",
      "Iteration 367, loss = 0.40180537\n",
      "Iteration 368, loss = 0.40182188\n",
      "Iteration 369, loss = 0.40229369\n",
      "Iteration 370, loss = 0.40206509\n",
      "Iteration 371, loss = 0.40215619\n",
      "Iteration 372, loss = 0.40192628\n",
      "Iteration 373, loss = 0.40167605\n",
      "Iteration 374, loss = 0.40169015\n",
      "Iteration 375, loss = 0.40186889\n",
      "Iteration 376, loss = 0.40138397\n",
      "Iteration 377, loss = 0.40182900\n",
      "Iteration 378, loss = 0.40147712\n",
      "Iteration 379, loss = 0.40153385\n",
      "Iteration 380, loss = 0.40137136\n",
      "Iteration 381, loss = 0.40161606\n",
      "Iteration 382, loss = 0.40160586\n",
      "Iteration 383, loss = 0.40136595\n",
      "Iteration 384, loss = 0.40140033\n",
      "Iteration 385, loss = 0.40122200\n",
      "Iteration 386, loss = 0.40107550\n",
      "Iteration 387, loss = 0.40117309\n",
      "Iteration 388, loss = 0.40144235\n",
      "Iteration 389, loss = 0.40127607\n",
      "Iteration 390, loss = 0.40115660\n",
      "Iteration 391, loss = 0.40104101\n",
      "Iteration 392, loss = 0.40113677\n",
      "Iteration 393, loss = 0.40116890\n",
      "Iteration 394, loss = 0.40081937\n",
      "Iteration 395, loss = 0.40090600\n",
      "Iteration 396, loss = 0.40073724\n",
      "Iteration 397, loss = 0.40096319\n",
      "Iteration 398, loss = 0.40086676\n",
      "Iteration 399, loss = 0.40082626\n",
      "Iteration 400, loss = 0.40082820\n",
      "Iteration 401, loss = 0.40060034\n",
      "Iteration 402, loss = 0.40052043\n",
      "Iteration 403, loss = 0.40062440\n",
      "Iteration 404, loss = 0.40052342\n",
      "Iteration 405, loss = 0.40045961\n",
      "Iteration 406, loss = 0.40066412\n",
      "Iteration 407, loss = 0.40043509\n",
      "Iteration 408, loss = 0.40027698\n",
      "Iteration 409, loss = 0.40025697\n",
      "Iteration 410, loss = 0.40031649\n",
      "Iteration 411, loss = 0.40015818\n",
      "Iteration 412, loss = 0.40013277\n",
      "Iteration 413, loss = 0.40014762\n",
      "Iteration 414, loss = 0.40012604\n",
      "Iteration 415, loss = 0.40024022\n",
      "Iteration 416, loss = 0.39999936\n",
      "Iteration 417, loss = 0.40002413\n",
      "Iteration 418, loss = 0.40005840\n",
      "Iteration 419, loss = 0.40020811\n",
      "Iteration 420, loss = 0.39996272\n",
      "Iteration 421, loss = 0.39977119\n",
      "Iteration 422, loss = 0.39999609\n",
      "Iteration 423, loss = 0.40008288\n",
      "Iteration 424, loss = 0.39975696\n",
      "Iteration 425, loss = 0.39990931\n",
      "Iteration 426, loss = 0.39970439\n",
      "Iteration 427, loss = 0.39969806\n",
      "Iteration 428, loss = 0.39974060\n",
      "Iteration 429, loss = 0.39964758\n",
      "Iteration 430, loss = 0.39961611\n",
      "Iteration 431, loss = 0.39973741\n",
      "Iteration 432, loss = 0.39955839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11221737\n",
      "Iteration 2, loss = 1.94971627\n",
      "Iteration 3, loss = 1.83366700\n",
      "Iteration 4, loss = 1.74515920\n",
      "Iteration 5, loss = 1.67271757\n",
      "Iteration 6, loss = 1.60993629\n",
      "Iteration 7, loss = 1.55301551\n",
      "Iteration 8, loss = 1.49979203\n",
      "Iteration 9, loss = 1.44917063\n",
      "Iteration 10, loss = 1.40083156\n",
      "Iteration 11, loss = 1.35475274\n",
      "Iteration 12, loss = 1.31109690\n",
      "Iteration 13, loss = 1.27010186\n",
      "Iteration 14, loss = 1.23192828\n",
      "Iteration 15, loss = 1.19648507\n",
      "Iteration 16, loss = 1.16379439\n",
      "Iteration 17, loss = 1.13359196\n",
      "Iteration 18, loss = 1.10562237\n",
      "Iteration 19, loss = 1.07975289\n",
      "Iteration 20, loss = 1.05575905\n",
      "Iteration 21, loss = 1.03342847\n",
      "Iteration 22, loss = 1.01249735\n",
      "Iteration 23, loss = 0.99280750\n",
      "Iteration 24, loss = 0.97426792\n",
      "Iteration 25, loss = 0.95669624\n",
      "Iteration 26, loss = 0.93999374\n",
      "Iteration 27, loss = 0.92407646\n",
      "Iteration 28, loss = 0.90889968\n",
      "Iteration 29, loss = 0.89439790\n",
      "Iteration 30, loss = 0.88060140\n",
      "Iteration 31, loss = 0.86734336\n",
      "Iteration 32, loss = 0.85469261\n",
      "Iteration 33, loss = 0.84257367\n",
      "Iteration 34, loss = 0.83091792\n",
      "Iteration 35, loss = 0.81973202\n",
      "Iteration 36, loss = 0.80896902\n",
      "Iteration 37, loss = 0.79862188\n",
      "Iteration 38, loss = 0.78866535\n",
      "Iteration 39, loss = 0.77909265\n",
      "Iteration 40, loss = 0.76991142\n",
      "Iteration 41, loss = 0.76105968\n",
      "Iteration 42, loss = 0.75258186\n",
      "Iteration 43, loss = 0.74441179\n",
      "Iteration 44, loss = 0.73659263\n",
      "Iteration 45, loss = 0.72905110\n",
      "Iteration 46, loss = 0.72182098\n",
      "Iteration 47, loss = 0.71487349\n",
      "Iteration 48, loss = 0.70819704\n",
      "Iteration 49, loss = 0.70175258\n",
      "Iteration 50, loss = 0.69559412\n",
      "Iteration 51, loss = 0.68964945\n",
      "Iteration 52, loss = 0.68394951\n",
      "Iteration 53, loss = 0.67848563\n",
      "Iteration 54, loss = 0.67319870\n",
      "Iteration 55, loss = 0.66812320\n",
      "Iteration 56, loss = 0.66322102\n",
      "Iteration 57, loss = 0.65848197\n",
      "Iteration 58, loss = 0.65395424\n",
      "Iteration 59, loss = 0.64955358\n",
      "Iteration 60, loss = 0.64531022\n",
      "Iteration 61, loss = 0.64118163\n",
      "Iteration 62, loss = 0.63722060\n",
      "Iteration 63, loss = 0.63339364\n",
      "Iteration 64, loss = 0.62968762\n",
      "Iteration 65, loss = 0.62609239\n",
      "Iteration 66, loss = 0.62261267\n",
      "Iteration 67, loss = 0.61919253\n",
      "Iteration 68, loss = 0.61591757\n",
      "Iteration 69, loss = 0.61269220\n",
      "Iteration 70, loss = 0.60965182\n",
      "Iteration 71, loss = 0.60661311\n",
      "Iteration 72, loss = 0.60371992\n",
      "Iteration 73, loss = 0.60084645\n",
      "Iteration 74, loss = 0.59815408\n",
      "Iteration 75, loss = 0.59543669\n",
      "Iteration 76, loss = 0.59284612\n",
      "Iteration 77, loss = 0.59029537\n",
      "Iteration 78, loss = 0.58778957\n",
      "Iteration 79, loss = 0.58542549\n",
      "Iteration 80, loss = 0.58304207\n",
      "Iteration 81, loss = 0.58072090\n",
      "Iteration 82, loss = 0.57847445\n",
      "Iteration 83, loss = 0.57632428\n",
      "Iteration 84, loss = 0.57413727\n",
      "Iteration 85, loss = 0.57208971\n",
      "Iteration 86, loss = 0.57006467\n",
      "Iteration 87, loss = 0.56806152\n",
      "Iteration 88, loss = 0.56610474\n",
      "Iteration 89, loss = 0.56419956\n",
      "Iteration 90, loss = 0.56235506\n",
      "Iteration 91, loss = 0.56047380\n",
      "Iteration 92, loss = 0.55870130\n",
      "Iteration 93, loss = 0.55692737\n",
      "Iteration 94, loss = 0.55522100\n",
      "Iteration 95, loss = 0.55360899\n",
      "Iteration 96, loss = 0.55195788\n",
      "Iteration 97, loss = 0.55034572\n",
      "Iteration 98, loss = 0.54880531\n",
      "Iteration 99, loss = 0.54724001\n",
      "Iteration 100, loss = 0.54570964\n",
      "Iteration 101, loss = 0.54422196\n",
      "Iteration 102, loss = 0.54277494\n",
      "Iteration 103, loss = 0.54134655\n",
      "Iteration 104, loss = 0.53993367\n",
      "Iteration 105, loss = 0.53856283\n",
      "Iteration 106, loss = 0.53714319\n",
      "Iteration 107, loss = 0.53587686\n",
      "Iteration 108, loss = 0.53455350\n",
      "Iteration 109, loss = 0.53326612\n",
      "Iteration 110, loss = 0.53197805\n",
      "Iteration 111, loss = 0.53073840\n",
      "Iteration 112, loss = 0.52952689\n",
      "Iteration 113, loss = 0.52832766\n",
      "Iteration 114, loss = 0.52714819\n",
      "Iteration 115, loss = 0.52599446\n",
      "Iteration 116, loss = 0.52484639\n",
      "Iteration 117, loss = 0.52374615\n",
      "Iteration 118, loss = 0.52261346\n",
      "Iteration 119, loss = 0.52157173\n",
      "Iteration 120, loss = 0.52048159\n",
      "Iteration 121, loss = 0.51946838\n",
      "Iteration 122, loss = 0.51841600\n",
      "Iteration 123, loss = 0.51743468\n",
      "Iteration 124, loss = 0.51638859\n",
      "Iteration 125, loss = 0.51544091\n",
      "Iteration 126, loss = 0.51447357\n",
      "Iteration 127, loss = 0.51359523\n",
      "Iteration 128, loss = 0.51262822\n",
      "Iteration 129, loss = 0.51173080\n",
      "Iteration 130, loss = 0.51085106\n",
      "Iteration 131, loss = 0.50993623\n",
      "Iteration 132, loss = 0.50906531\n",
      "Iteration 133, loss = 0.50824987\n",
      "Iteration 134, loss = 0.50738260\n",
      "Iteration 135, loss = 0.50657519\n",
      "Iteration 136, loss = 0.50580254\n",
      "Iteration 137, loss = 0.50498599\n",
      "Iteration 138, loss = 0.50420397\n",
      "Iteration 139, loss = 0.50342286\n",
      "Iteration 140, loss = 0.50263007\n",
      "Iteration 141, loss = 0.50189301\n",
      "Iteration 142, loss = 0.50114869\n",
      "Iteration 143, loss = 0.50045134\n",
      "Iteration 144, loss = 0.49973593\n",
      "Iteration 145, loss = 0.49903855\n",
      "Iteration 146, loss = 0.49828789\n",
      "Iteration 147, loss = 0.49763548\n",
      "Iteration 148, loss = 0.49695545\n",
      "Iteration 149, loss = 0.49629283\n",
      "Iteration 150, loss = 0.49569023\n",
      "Iteration 151, loss = 0.49500279\n",
      "Iteration 152, loss = 0.49439915\n",
      "Iteration 153, loss = 0.49374331\n",
      "Iteration 154, loss = 0.49313572\n",
      "Iteration 155, loss = 0.49254929\n",
      "Iteration 156, loss = 0.49197737\n",
      "Iteration 157, loss = 0.49137950\n",
      "Iteration 158, loss = 0.49078410\n",
      "Iteration 159, loss = 0.49024107\n",
      "Iteration 160, loss = 0.48967277\n",
      "Iteration 161, loss = 0.48911129\n",
      "Iteration 162, loss = 0.48854996\n",
      "Iteration 163, loss = 0.48806474\n",
      "Iteration 164, loss = 0.48750824\n",
      "Iteration 165, loss = 0.48696230\n",
      "Iteration 166, loss = 0.48644958\n",
      "Iteration 167, loss = 0.48597891\n",
      "Iteration 168, loss = 0.48554829\n",
      "Iteration 169, loss = 0.48498350\n",
      "Iteration 170, loss = 0.48450164\n",
      "Iteration 171, loss = 0.48405030\n",
      "Iteration 172, loss = 0.48356844\n",
      "Iteration 173, loss = 0.48315296\n",
      "Iteration 174, loss = 0.48263633\n",
      "Iteration 175, loss = 0.48221298\n",
      "Iteration 176, loss = 0.48176330\n",
      "Iteration 177, loss = 0.48134940\n",
      "Iteration 178, loss = 0.48088753\n",
      "Iteration 179, loss = 0.48052626\n",
      "Iteration 180, loss = 0.48011252\n",
      "Iteration 181, loss = 0.47971308\n",
      "Iteration 182, loss = 0.47927869\n",
      "Iteration 183, loss = 0.47899320\n",
      "Iteration 184, loss = 0.47846565\n",
      "Iteration 185, loss = 0.47808264\n",
      "Iteration 186, loss = 0.47768825\n",
      "Iteration 187, loss = 0.47733809\n",
      "Iteration 188, loss = 0.47696895\n",
      "Iteration 189, loss = 0.47661545\n",
      "Iteration 190, loss = 0.47620578\n",
      "Iteration 191, loss = 0.47582710\n",
      "Iteration 192, loss = 0.47552747\n",
      "Iteration 193, loss = 0.47514560\n",
      "Iteration 194, loss = 0.47482793\n",
      "Iteration 195, loss = 0.47445486\n",
      "Iteration 196, loss = 0.47416144\n",
      "Iteration 197, loss = 0.47384043\n",
      "Iteration 198, loss = 0.47349216\n",
      "Iteration 199, loss = 0.47323056\n",
      "Iteration 200, loss = 0.47285560\n",
      "Iteration 201, loss = 0.47255087\n",
      "Iteration 202, loss = 0.47217360\n",
      "Iteration 203, loss = 0.47193791\n",
      "Iteration 204, loss = 0.47163144\n",
      "Iteration 205, loss = 0.47128755\n",
      "Iteration 206, loss = 0.47103017\n",
      "Iteration 207, loss = 0.47074939\n",
      "Iteration 208, loss = 0.47042055\n",
      "Iteration 209, loss = 0.47013320\n",
      "Iteration 210, loss = 0.46988297\n",
      "Iteration 211, loss = 0.46952943\n",
      "Iteration 212, loss = 0.46932083\n",
      "Iteration 213, loss = 0.46907001\n",
      "Iteration 214, loss = 0.46875909\n",
      "Iteration 215, loss = 0.46852936\n",
      "Iteration 216, loss = 0.46825584\n",
      "Iteration 217, loss = 0.46803436\n",
      "Iteration 218, loss = 0.46769844\n",
      "Iteration 219, loss = 0.46757086\n",
      "Iteration 220, loss = 0.46728572\n",
      "Iteration 221, loss = 0.46698641\n",
      "Iteration 222, loss = 0.46675055\n",
      "Iteration 223, loss = 0.46652031\n",
      "Iteration 224, loss = 0.46626310\n",
      "Iteration 225, loss = 0.46607412\n",
      "Iteration 226, loss = 0.46583279\n",
      "Iteration 227, loss = 0.46565923\n",
      "Iteration 228, loss = 0.46540519\n",
      "Iteration 229, loss = 0.46524913\n",
      "Iteration 230, loss = 0.46493742\n",
      "Iteration 231, loss = 0.46470515\n",
      "Iteration 232, loss = 0.46453128\n",
      "Iteration 233, loss = 0.46426927\n",
      "Iteration 234, loss = 0.46405990\n",
      "Iteration 235, loss = 0.46383511\n",
      "Iteration 236, loss = 0.46363406\n",
      "Iteration 237, loss = 0.46339911\n",
      "Iteration 238, loss = 0.46327737\n",
      "Iteration 239, loss = 0.46305760\n",
      "Iteration 240, loss = 0.46286473\n",
      "Iteration 241, loss = 0.46262900\n",
      "Iteration 242, loss = 0.46240611\n",
      "Iteration 243, loss = 0.46217962\n",
      "Iteration 244, loss = 0.46203216\n",
      "Iteration 245, loss = 0.46191375\n",
      "Iteration 246, loss = 0.46168948\n",
      "Iteration 247, loss = 0.46157652\n",
      "Iteration 248, loss = 0.46135734\n",
      "Iteration 249, loss = 0.46109877\n",
      "Iteration 250, loss = 0.46094836\n",
      "Iteration 251, loss = 0.46077158\n",
      "Iteration 252, loss = 0.46060655\n",
      "Iteration 253, loss = 0.46040311\n",
      "Iteration 254, loss = 0.46030134\n",
      "Iteration 255, loss = 0.46009781\n",
      "Iteration 256, loss = 0.45994211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.45971249\n",
      "Iteration 258, loss = 0.45959387\n",
      "Iteration 259, loss = 0.45940558\n",
      "Iteration 260, loss = 0.45919353\n",
      "Iteration 261, loss = 0.45911165\n",
      "Iteration 262, loss = 0.45894765\n",
      "Iteration 263, loss = 0.45879980\n",
      "Iteration 264, loss = 0.45867429\n",
      "Iteration 265, loss = 0.45839937\n",
      "Iteration 266, loss = 0.45826516\n",
      "Iteration 267, loss = 0.45815843\n",
      "Iteration 268, loss = 0.45800153\n",
      "Iteration 269, loss = 0.45779408\n",
      "Iteration 270, loss = 0.45771407\n",
      "Iteration 271, loss = 0.45750712\n",
      "Iteration 272, loss = 0.45734806\n",
      "Iteration 273, loss = 0.45722927\n",
      "Iteration 274, loss = 0.45703058\n",
      "Iteration 275, loss = 0.45692222\n",
      "Iteration 276, loss = 0.45680121\n",
      "Iteration 277, loss = 0.45670100\n",
      "Iteration 278, loss = 0.45654711\n",
      "Iteration 279, loss = 0.45647934\n",
      "Iteration 280, loss = 0.45620121\n",
      "Iteration 281, loss = 0.45612764\n",
      "Iteration 282, loss = 0.45600914\n",
      "Iteration 283, loss = 0.45586546\n",
      "Iteration 284, loss = 0.45569265\n",
      "Iteration 285, loss = 0.45566573\n",
      "Iteration 286, loss = 0.45548660\n",
      "Iteration 287, loss = 0.45534175\n",
      "Iteration 288, loss = 0.45525466\n",
      "Iteration 289, loss = 0.45506821\n",
      "Iteration 290, loss = 0.45493047\n",
      "Iteration 291, loss = 0.45481911\n",
      "Iteration 292, loss = 0.45472050\n",
      "Iteration 293, loss = 0.45456204\n",
      "Iteration 294, loss = 0.45438854\n",
      "Iteration 295, loss = 0.45434971\n",
      "Iteration 296, loss = 0.45434662\n",
      "Iteration 297, loss = 0.45416527\n",
      "Iteration 298, loss = 0.45400551\n",
      "Iteration 299, loss = 0.45391533\n",
      "Iteration 300, loss = 0.45376214\n",
      "Iteration 301, loss = 0.45357389\n",
      "Iteration 302, loss = 0.45348859\n",
      "Iteration 303, loss = 0.45338055\n",
      "Iteration 304, loss = 0.45332078\n",
      "Iteration 305, loss = 0.45320215\n",
      "Iteration 306, loss = 0.45302649\n",
      "Iteration 307, loss = 0.45296909\n",
      "Iteration 308, loss = 0.45288282\n",
      "Iteration 309, loss = 0.45271225\n",
      "Iteration 310, loss = 0.45260144\n",
      "Iteration 311, loss = 0.45257523\n",
      "Iteration 312, loss = 0.45238281\n",
      "Iteration 313, loss = 0.45227422\n",
      "Iteration 314, loss = 0.45218000\n",
      "Iteration 315, loss = 0.45211622\n",
      "Iteration 316, loss = 0.45199278\n",
      "Iteration 317, loss = 0.45190461\n",
      "Iteration 318, loss = 0.45173220\n",
      "Iteration 319, loss = 0.45171475\n",
      "Iteration 320, loss = 0.45150665\n",
      "Iteration 321, loss = 0.45150315\n",
      "Iteration 322, loss = 0.45141340\n",
      "Iteration 323, loss = 0.45129808\n",
      "Iteration 324, loss = 0.45120169\n",
      "Iteration 325, loss = 0.45109619\n",
      "Iteration 326, loss = 0.45096713\n",
      "Iteration 327, loss = 0.45088173\n",
      "Iteration 328, loss = 0.45076470\n",
      "Iteration 329, loss = 0.45078536\n",
      "Iteration 330, loss = 0.45072691\n",
      "Iteration 331, loss = 0.45057770\n",
      "Iteration 332, loss = 0.45041226\n",
      "Iteration 333, loss = 0.45026717\n",
      "Iteration 334, loss = 0.45023921\n",
      "Iteration 335, loss = 0.45013840\n",
      "Iteration 336, loss = 0.45009380\n",
      "Iteration 337, loss = 0.44996974\n",
      "Iteration 338, loss = 0.44987603\n",
      "Iteration 339, loss = 0.44975923\n",
      "Iteration 340, loss = 0.44969148\n",
      "Iteration 341, loss = 0.44961726\n",
      "Iteration 342, loss = 0.44949508\n",
      "Iteration 343, loss = 0.44941309\n",
      "Iteration 344, loss = 0.44935031\n",
      "Iteration 345, loss = 0.44923828\n",
      "Iteration 346, loss = 0.44920027\n",
      "Iteration 347, loss = 0.44912512\n",
      "Iteration 348, loss = 0.44896623\n",
      "Iteration 349, loss = 0.44891100\n",
      "Iteration 350, loss = 0.44882959\n",
      "Iteration 351, loss = 0.44876807\n",
      "Iteration 352, loss = 0.44871538\n",
      "Iteration 353, loss = 0.44861569\n",
      "Iteration 354, loss = 0.44851934\n",
      "Iteration 355, loss = 0.44840534\n",
      "Iteration 356, loss = 0.44839872\n",
      "Iteration 357, loss = 0.44818878\n",
      "Iteration 358, loss = 0.44813747\n",
      "Iteration 359, loss = 0.44810652\n",
      "Iteration 360, loss = 0.44799235\n",
      "Iteration 361, loss = 0.44789021\n",
      "Iteration 362, loss = 0.44787217\n",
      "Iteration 363, loss = 0.44782132\n",
      "Iteration 364, loss = 0.44765032\n",
      "Iteration 365, loss = 0.44765415\n",
      "Iteration 366, loss = 0.44757731\n",
      "Iteration 367, loss = 0.44742782\n",
      "Iteration 368, loss = 0.44736877\n",
      "Iteration 369, loss = 0.44736863\n",
      "Iteration 370, loss = 0.44728695\n",
      "Iteration 371, loss = 0.44711652\n",
      "Iteration 372, loss = 0.44711358\n",
      "Iteration 373, loss = 0.44697379\n",
      "Iteration 374, loss = 0.44691812\n",
      "Iteration 375, loss = 0.44689638\n",
      "Iteration 376, loss = 0.44675992\n",
      "Iteration 377, loss = 0.44671358\n",
      "Iteration 378, loss = 0.44664311\n",
      "Iteration 379, loss = 0.44657755\n",
      "Iteration 380, loss = 0.44659371\n",
      "Iteration 381, loss = 0.44645489\n",
      "Iteration 382, loss = 0.44633691\n",
      "Iteration 383, loss = 0.44632506\n",
      "Iteration 384, loss = 0.44623168\n",
      "Iteration 385, loss = 0.44615399\n",
      "Iteration 386, loss = 0.44612316\n",
      "Iteration 387, loss = 0.44599197\n",
      "Iteration 388, loss = 0.44600319\n",
      "Iteration 389, loss = 0.44594207\n",
      "Iteration 390, loss = 0.44580100\n",
      "Iteration 391, loss = 0.44572696\n",
      "Iteration 392, loss = 0.44573427\n",
      "Iteration 393, loss = 0.44565455\n",
      "Iteration 394, loss = 0.44554758\n",
      "Iteration 395, loss = 0.44550070\n",
      "Iteration 396, loss = 0.44535313\n",
      "Iteration 397, loss = 0.44543623\n",
      "Iteration 398, loss = 0.44532356\n",
      "Iteration 399, loss = 0.44520966\n",
      "Iteration 400, loss = 0.44518219\n",
      "Iteration 401, loss = 0.44513064\n",
      "Iteration 402, loss = 0.44504995\n",
      "Iteration 403, loss = 0.44494656\n",
      "Iteration 404, loss = 0.44488932\n",
      "Iteration 405, loss = 0.44486115\n",
      "Iteration 406, loss = 0.44467734\n",
      "Iteration 407, loss = 0.44473781\n",
      "Iteration 408, loss = 0.44461848\n",
      "Iteration 409, loss = 0.44461620\n",
      "Iteration 410, loss = 0.44448576\n",
      "Iteration 411, loss = 0.44444495\n",
      "Iteration 412, loss = 0.44442157\n",
      "Iteration 413, loss = 0.44434399\n",
      "Iteration 414, loss = 0.44430983\n",
      "Iteration 415, loss = 0.44427481\n",
      "Iteration 416, loss = 0.44415775\n",
      "Iteration 417, loss = 0.44414252\n",
      "Iteration 418, loss = 0.44406403\n",
      "Iteration 419, loss = 0.44395558\n",
      "Iteration 420, loss = 0.44395084\n",
      "Iteration 421, loss = 0.44387433\n",
      "Iteration 422, loss = 0.44378241\n",
      "Iteration 423, loss = 0.44370103\n",
      "Iteration 424, loss = 0.44379209\n",
      "Iteration 425, loss = 0.44360662\n",
      "Iteration 426, loss = 0.44364848\n",
      "Iteration 427, loss = 0.44349886\n",
      "Iteration 428, loss = 0.44351006\n",
      "Iteration 429, loss = 0.44343591\n",
      "Iteration 430, loss = 0.44336319\n",
      "Iteration 431, loss = 0.44330466\n",
      "Iteration 432, loss = 0.44321060\n",
      "Iteration 433, loss = 0.44324089\n",
      "Iteration 434, loss = 0.44313459\n",
      "Iteration 435, loss = 0.44304595\n",
      "Iteration 436, loss = 0.44303896\n",
      "Iteration 437, loss = 0.44291433\n",
      "Iteration 438, loss = 0.44291615\n",
      "Iteration 439, loss = 0.44287599\n",
      "Iteration 440, loss = 0.44279241\n",
      "Iteration 441, loss = 0.44271534\n",
      "Iteration 442, loss = 0.44265038\n",
      "Iteration 443, loss = 0.44262672\n",
      "Iteration 444, loss = 0.44260674\n",
      "Iteration 445, loss = 0.44256199\n",
      "Iteration 446, loss = 0.44248601\n",
      "Iteration 447, loss = 0.44243491\n",
      "Iteration 448, loss = 0.44241143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10854204\n",
      "Iteration 2, loss = 1.94652680\n",
      "Iteration 3, loss = 1.83081906\n",
      "Iteration 4, loss = 1.74263479\n",
      "Iteration 5, loss = 1.67055232\n",
      "Iteration 6, loss = 1.60824952\n",
      "Iteration 7, loss = 1.55175563\n",
      "Iteration 8, loss = 1.49891539\n",
      "Iteration 9, loss = 1.44870242\n",
      "Iteration 10, loss = 1.40064603\n",
      "Iteration 11, loss = 1.35483922\n",
      "Iteration 12, loss = 1.31140364\n",
      "Iteration 13, loss = 1.27045795\n",
      "Iteration 14, loss = 1.23222536\n",
      "Iteration 15, loss = 1.19672997\n",
      "Iteration 16, loss = 1.16385045\n",
      "Iteration 17, loss = 1.13342613\n",
      "Iteration 18, loss = 1.10523546\n",
      "Iteration 19, loss = 1.07910205\n",
      "Iteration 20, loss = 1.05487941\n",
      "Iteration 21, loss = 1.03227346\n",
      "Iteration 22, loss = 1.01112350\n",
      "Iteration 23, loss = 0.99125185\n",
      "Iteration 24, loss = 0.97247635\n",
      "Iteration 25, loss = 0.95470036\n",
      "Iteration 26, loss = 0.93779499\n",
      "Iteration 27, loss = 0.92176310\n",
      "Iteration 28, loss = 0.90648468\n",
      "Iteration 29, loss = 0.89193256\n",
      "Iteration 30, loss = 0.87808336\n",
      "Iteration 31, loss = 0.86485151\n",
      "Iteration 32, loss = 0.85218454\n",
      "Iteration 33, loss = 0.84005982\n",
      "Iteration 34, loss = 0.82846473\n",
      "Iteration 35, loss = 0.81732798\n",
      "Iteration 36, loss = 0.80663866\n",
      "Iteration 37, loss = 0.79638121\n",
      "Iteration 38, loss = 0.78653884\n",
      "Iteration 39, loss = 0.77703475\n",
      "Iteration 40, loss = 0.76790093\n",
      "Iteration 41, loss = 0.75912222\n",
      "Iteration 42, loss = 0.75070382\n",
      "Iteration 43, loss = 0.74260442\n",
      "Iteration 44, loss = 0.73482131\n",
      "Iteration 45, loss = 0.72731991\n",
      "Iteration 46, loss = 0.72012982\n",
      "Iteration 47, loss = 0.71322637\n",
      "Iteration 48, loss = 0.70660560\n",
      "Iteration 49, loss = 0.70023844\n",
      "Iteration 50, loss = 0.69413649\n",
      "Iteration 51, loss = 0.68825499\n",
      "Iteration 52, loss = 0.68257157\n",
      "Iteration 53, loss = 0.67718488\n",
      "Iteration 54, loss = 0.67194633\n",
      "Iteration 55, loss = 0.66690635\n",
      "Iteration 56, loss = 0.66204654\n",
      "Iteration 57, loss = 0.65735402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 0.65286809\n",
      "Iteration 59, loss = 0.64847064\n",
      "Iteration 60, loss = 0.64429288\n",
      "Iteration 61, loss = 0.64023261\n",
      "Iteration 62, loss = 0.63627663\n",
      "Iteration 63, loss = 0.63246279\n",
      "Iteration 64, loss = 0.62877406\n",
      "Iteration 65, loss = 0.62521927\n",
      "Iteration 66, loss = 0.62173486\n",
      "Iteration 67, loss = 0.61845933\n",
      "Iteration 68, loss = 0.61517905\n",
      "Iteration 69, loss = 0.61201686\n",
      "Iteration 70, loss = 0.60896878\n",
      "Iteration 71, loss = 0.60596062\n",
      "Iteration 72, loss = 0.60312744\n",
      "Iteration 73, loss = 0.60029505\n",
      "Iteration 74, loss = 0.59751386\n",
      "Iteration 75, loss = 0.59489710\n",
      "Iteration 76, loss = 0.59225105\n",
      "Iteration 77, loss = 0.58972390\n",
      "Iteration 78, loss = 0.58730530\n",
      "Iteration 79, loss = 0.58486911\n",
      "Iteration 80, loss = 0.58257706\n",
      "Iteration 81, loss = 0.58027809\n",
      "Iteration 82, loss = 0.57804147\n",
      "Iteration 83, loss = 0.57584371\n",
      "Iteration 84, loss = 0.57375942\n",
      "Iteration 85, loss = 0.57165460\n",
      "Iteration 86, loss = 0.56965294\n",
      "Iteration 87, loss = 0.56769684\n",
      "Iteration 88, loss = 0.56569997\n",
      "Iteration 89, loss = 0.56381663\n",
      "Iteration 90, loss = 0.56197882\n",
      "Iteration 91, loss = 0.56013876\n",
      "Iteration 92, loss = 0.55838282\n",
      "Iteration 93, loss = 0.55667169\n",
      "Iteration 94, loss = 0.55493932\n",
      "Iteration 95, loss = 0.55326508\n",
      "Iteration 96, loss = 0.55161675\n",
      "Iteration 97, loss = 0.55007080\n",
      "Iteration 98, loss = 0.54842957\n",
      "Iteration 99, loss = 0.54692215\n",
      "Iteration 100, loss = 0.54538572\n",
      "Iteration 101, loss = 0.54391874\n",
      "Iteration 102, loss = 0.54252991\n",
      "Iteration 103, loss = 0.54104732\n",
      "Iteration 104, loss = 0.53963396\n",
      "Iteration 105, loss = 0.53825213\n",
      "Iteration 106, loss = 0.53693648\n",
      "Iteration 107, loss = 0.53556918\n",
      "Iteration 108, loss = 0.53426412\n",
      "Iteration 109, loss = 0.53296124\n",
      "Iteration 110, loss = 0.53171499\n",
      "Iteration 111, loss = 0.53045832\n",
      "Iteration 112, loss = 0.52929347\n",
      "Iteration 113, loss = 0.52807262\n",
      "Iteration 114, loss = 0.52688745\n",
      "Iteration 115, loss = 0.52572488\n",
      "Iteration 116, loss = 0.52462667\n",
      "Iteration 117, loss = 0.52344366\n",
      "Iteration 118, loss = 0.52237697\n",
      "Iteration 119, loss = 0.52128728\n",
      "Iteration 120, loss = 0.52022313\n",
      "Iteration 121, loss = 0.51918534\n",
      "Iteration 122, loss = 0.51814037\n",
      "Iteration 123, loss = 0.51711758\n",
      "Iteration 124, loss = 0.51618998\n",
      "Iteration 125, loss = 0.51521895\n",
      "Iteration 126, loss = 0.51418873\n",
      "Iteration 127, loss = 0.51329953\n",
      "Iteration 128, loss = 0.51234773\n",
      "Iteration 129, loss = 0.51148702\n",
      "Iteration 130, loss = 0.51057702\n",
      "Iteration 131, loss = 0.50966920\n",
      "Iteration 132, loss = 0.50885498\n",
      "Iteration 133, loss = 0.50796336\n",
      "Iteration 134, loss = 0.50710130\n",
      "Iteration 135, loss = 0.50629080\n",
      "Iteration 136, loss = 0.50545979\n",
      "Iteration 137, loss = 0.50466865\n",
      "Iteration 138, loss = 0.50387175\n",
      "Iteration 139, loss = 0.50311568\n",
      "Iteration 140, loss = 0.50237987\n",
      "Iteration 141, loss = 0.50162199\n",
      "Iteration 142, loss = 0.50085212\n",
      "Iteration 143, loss = 0.50010476\n",
      "Iteration 144, loss = 0.49938658\n",
      "Iteration 145, loss = 0.49873912\n",
      "Iteration 146, loss = 0.49803009\n",
      "Iteration 147, loss = 0.49732910\n",
      "Iteration 148, loss = 0.49667611\n",
      "Iteration 149, loss = 0.49599570\n",
      "Iteration 150, loss = 0.49535671\n",
      "Iteration 151, loss = 0.49472942\n",
      "Iteration 152, loss = 0.49411341\n",
      "Iteration 153, loss = 0.49347806\n",
      "Iteration 154, loss = 0.49284345\n",
      "Iteration 155, loss = 0.49227062\n",
      "Iteration 156, loss = 0.49169263\n",
      "Iteration 157, loss = 0.49109827\n",
      "Iteration 158, loss = 0.49049840\n",
      "Iteration 159, loss = 0.48990832\n",
      "Iteration 160, loss = 0.48937482\n",
      "Iteration 161, loss = 0.48877715\n",
      "Iteration 162, loss = 0.48827236\n",
      "Iteration 163, loss = 0.48773724\n",
      "Iteration 164, loss = 0.48727355\n",
      "Iteration 165, loss = 0.48672657\n",
      "Iteration 166, loss = 0.48617905\n",
      "Iteration 167, loss = 0.48572485\n",
      "Iteration 168, loss = 0.48518182\n",
      "Iteration 169, loss = 0.48477038\n",
      "Iteration 170, loss = 0.48424777\n",
      "Iteration 171, loss = 0.48375883\n",
      "Iteration 172, loss = 0.48329253\n",
      "Iteration 173, loss = 0.48287308\n",
      "Iteration 174, loss = 0.48240064\n",
      "Iteration 175, loss = 0.48192711\n",
      "Iteration 176, loss = 0.48155376\n",
      "Iteration 177, loss = 0.48109165\n",
      "Iteration 178, loss = 0.48063514\n",
      "Iteration 179, loss = 0.48021972\n",
      "Iteration 180, loss = 0.47979141\n",
      "Iteration 181, loss = 0.47933935\n",
      "Iteration 182, loss = 0.47898449\n",
      "Iteration 183, loss = 0.47857323\n",
      "Iteration 184, loss = 0.47813302\n",
      "Iteration 185, loss = 0.47785225\n",
      "Iteration 186, loss = 0.47745074\n",
      "Iteration 187, loss = 0.47707754\n",
      "Iteration 188, loss = 0.47664911\n",
      "Iteration 189, loss = 0.47629361\n",
      "Iteration 190, loss = 0.47586025\n",
      "Iteration 191, loss = 0.47557944\n",
      "Iteration 192, loss = 0.47525138\n",
      "Iteration 193, loss = 0.47488829\n",
      "Iteration 194, loss = 0.47452905\n",
      "Iteration 195, loss = 0.47415573\n",
      "Iteration 196, loss = 0.47383825\n",
      "Iteration 197, loss = 0.47352334\n",
      "Iteration 198, loss = 0.47320165\n",
      "Iteration 199, loss = 0.47291427\n",
      "Iteration 200, loss = 0.47254994\n",
      "Iteration 201, loss = 0.47225337\n",
      "Iteration 202, loss = 0.47189305\n",
      "Iteration 203, loss = 0.47161877\n",
      "Iteration 204, loss = 0.47130791\n",
      "Iteration 205, loss = 0.47104801\n",
      "Iteration 206, loss = 0.47077892\n",
      "Iteration 207, loss = 0.47041913\n",
      "Iteration 208, loss = 0.47017783\n",
      "Iteration 209, loss = 0.46987574\n",
      "Iteration 210, loss = 0.46960970\n",
      "Iteration 211, loss = 0.46934824\n",
      "Iteration 212, loss = 0.46902304\n",
      "Iteration 213, loss = 0.46875449\n",
      "Iteration 214, loss = 0.46848757\n",
      "Iteration 215, loss = 0.46824648\n",
      "Iteration 216, loss = 0.46798318\n",
      "Iteration 217, loss = 0.46775698\n",
      "Iteration 218, loss = 0.46745439\n",
      "Iteration 219, loss = 0.46718227\n",
      "Iteration 220, loss = 0.46692038\n",
      "Iteration 221, loss = 0.46671842\n",
      "Iteration 222, loss = 0.46643866\n",
      "Iteration 223, loss = 0.46621514\n",
      "Iteration 224, loss = 0.46601500\n",
      "Iteration 225, loss = 0.46576600\n",
      "Iteration 226, loss = 0.46550493\n",
      "Iteration 227, loss = 0.46537317\n",
      "Iteration 228, loss = 0.46503412\n",
      "Iteration 229, loss = 0.46482356\n",
      "Iteration 230, loss = 0.46463555\n",
      "Iteration 231, loss = 0.46440416\n",
      "Iteration 232, loss = 0.46416893\n",
      "Iteration 233, loss = 0.46398800\n",
      "Iteration 234, loss = 0.46372342\n",
      "Iteration 235, loss = 0.46366529\n",
      "Iteration 236, loss = 0.46330206\n",
      "Iteration 237, loss = 0.46310319\n",
      "Iteration 238, loss = 0.46293437\n",
      "Iteration 239, loss = 0.46270055\n",
      "Iteration 240, loss = 0.46249930\n",
      "Iteration 241, loss = 0.46228170\n",
      "Iteration 242, loss = 0.46211287\n",
      "Iteration 243, loss = 0.46190979\n",
      "Iteration 244, loss = 0.46175464\n",
      "Iteration 245, loss = 0.46162795\n",
      "Iteration 246, loss = 0.46141635\n",
      "Iteration 247, loss = 0.46116113\n",
      "Iteration 248, loss = 0.46105414\n",
      "Iteration 249, loss = 0.46084188\n",
      "Iteration 250, loss = 0.46060150\n",
      "Iteration 251, loss = 0.46047015\n",
      "Iteration 252, loss = 0.46033350\n",
      "Iteration 253, loss = 0.46011711\n",
      "Iteration 254, loss = 0.45996114\n",
      "Iteration 255, loss = 0.45977717\n",
      "Iteration 256, loss = 0.45970400\n",
      "Iteration 257, loss = 0.45951484\n",
      "Iteration 258, loss = 0.45931784\n",
      "Iteration 259, loss = 0.45915694\n",
      "Iteration 260, loss = 0.45891561\n",
      "Iteration 261, loss = 0.45878202\n",
      "Iteration 262, loss = 0.45868183\n",
      "Iteration 263, loss = 0.45847875\n",
      "Iteration 264, loss = 0.45825556\n",
      "Iteration 265, loss = 0.45811265\n",
      "Iteration 266, loss = 0.45804385\n",
      "Iteration 267, loss = 0.45788606\n",
      "Iteration 268, loss = 0.45773976\n",
      "Iteration 269, loss = 0.45751325\n",
      "Iteration 270, loss = 0.45739285\n",
      "Iteration 271, loss = 0.45727973\n",
      "Iteration 272, loss = 0.45716691\n",
      "Iteration 273, loss = 0.45686876\n",
      "Iteration 274, loss = 0.45685072\n",
      "Iteration 275, loss = 0.45669884\n",
      "Iteration 276, loss = 0.45659052\n",
      "Iteration 277, loss = 0.45641913\n",
      "Iteration 278, loss = 0.45626090\n",
      "Iteration 279, loss = 0.45620277\n",
      "Iteration 280, loss = 0.45595562\n",
      "Iteration 281, loss = 0.45586117\n",
      "Iteration 282, loss = 0.45573552\n",
      "Iteration 283, loss = 0.45561337\n",
      "Iteration 284, loss = 0.45552672\n",
      "Iteration 285, loss = 0.45531201\n",
      "Iteration 286, loss = 0.45521308\n",
      "Iteration 287, loss = 0.45507097\n",
      "Iteration 288, loss = 0.45495154\n",
      "Iteration 289, loss = 0.45485327\n",
      "Iteration 290, loss = 0.45466880\n",
      "Iteration 291, loss = 0.45454665\n",
      "Iteration 292, loss = 0.45449349\n",
      "Iteration 293, loss = 0.45425323\n",
      "Iteration 294, loss = 0.45416440\n",
      "Iteration 295, loss = 0.45411193\n",
      "Iteration 296, loss = 0.45397408\n",
      "Iteration 297, loss = 0.45385644\n",
      "Iteration 298, loss = 0.45370868\n",
      "Iteration 299, loss = 0.45365891\n",
      "Iteration 300, loss = 0.45355822\n",
      "Iteration 301, loss = 0.45336139\n",
      "Iteration 302, loss = 0.45324617\n",
      "Iteration 303, loss = 0.45319461\n",
      "Iteration 304, loss = 0.45301220\n",
      "Iteration 305, loss = 0.45287346\n",
      "Iteration 306, loss = 0.45283777\n",
      "Iteration 307, loss = 0.45279467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 308, loss = 0.45260739\n",
      "Iteration 309, loss = 0.45248700\n",
      "Iteration 310, loss = 0.45237133\n",
      "Iteration 311, loss = 0.45227318\n",
      "Iteration 312, loss = 0.45218995\n",
      "Iteration 313, loss = 0.45213309\n",
      "Iteration 314, loss = 0.45197578\n",
      "Iteration 315, loss = 0.45193054\n",
      "Iteration 316, loss = 0.45177420\n",
      "Iteration 317, loss = 0.45171191\n",
      "Iteration 318, loss = 0.45154119\n",
      "Iteration 319, loss = 0.45145583\n",
      "Iteration 320, loss = 0.45143316\n",
      "Iteration 321, loss = 0.45131504\n",
      "Iteration 322, loss = 0.45118888\n",
      "Iteration 323, loss = 0.45106862\n",
      "Iteration 324, loss = 0.45094594\n",
      "Iteration 325, loss = 0.45083584\n",
      "Iteration 326, loss = 0.45083069\n",
      "Iteration 327, loss = 0.45065314\n",
      "Iteration 328, loss = 0.45053178\n",
      "Iteration 329, loss = 0.45049453\n",
      "Iteration 330, loss = 0.45043391\n",
      "Iteration 331, loss = 0.45028726\n",
      "Iteration 332, loss = 0.45025847\n",
      "Iteration 333, loss = 0.45012409\n",
      "Iteration 334, loss = 0.45002894\n",
      "Iteration 335, loss = 0.44989753\n",
      "Iteration 336, loss = 0.44995486\n",
      "Iteration 337, loss = 0.44979949\n",
      "Iteration 338, loss = 0.44970749\n",
      "Iteration 339, loss = 0.44958237\n",
      "Iteration 340, loss = 0.44955416\n",
      "Iteration 341, loss = 0.44934132\n",
      "Iteration 342, loss = 0.44927436\n",
      "Iteration 343, loss = 0.44923941\n",
      "Iteration 344, loss = 0.44919175\n",
      "Iteration 345, loss = 0.44906088\n",
      "Iteration 346, loss = 0.44890422\n",
      "Iteration 347, loss = 0.44885499\n",
      "Iteration 348, loss = 0.44887076\n",
      "Iteration 349, loss = 0.44869284\n",
      "Iteration 350, loss = 0.44868773\n",
      "Iteration 351, loss = 0.44854175\n",
      "Iteration 352, loss = 0.44855126\n",
      "Iteration 353, loss = 0.44832807\n",
      "Iteration 354, loss = 0.44839692\n",
      "Iteration 355, loss = 0.44828232\n",
      "Iteration 356, loss = 0.44823947\n",
      "Iteration 357, loss = 0.44808385\n",
      "Iteration 358, loss = 0.44796590\n",
      "Iteration 359, loss = 0.44789681\n",
      "Iteration 360, loss = 0.44783009\n",
      "Iteration 361, loss = 0.44775116\n",
      "Iteration 362, loss = 0.44766099\n",
      "Iteration 363, loss = 0.44760771\n",
      "Iteration 364, loss = 0.44756816\n",
      "Iteration 365, loss = 0.44743114\n",
      "Iteration 366, loss = 0.44741984\n",
      "Iteration 367, loss = 0.44730404\n",
      "Iteration 368, loss = 0.44721395\n",
      "Iteration 369, loss = 0.44714686\n",
      "Iteration 370, loss = 0.44708251\n",
      "Iteration 371, loss = 0.44703240\n",
      "Iteration 372, loss = 0.44695431\n",
      "Iteration 373, loss = 0.44688931\n",
      "Iteration 374, loss = 0.44684118\n",
      "Iteration 375, loss = 0.44681468\n",
      "Iteration 376, loss = 0.44662580\n",
      "Iteration 377, loss = 0.44661037\n",
      "Iteration 378, loss = 0.44652733\n",
      "Iteration 379, loss = 0.44638828\n",
      "Iteration 380, loss = 0.44641338\n",
      "Iteration 381, loss = 0.44629335\n",
      "Iteration 382, loss = 0.44628541\n",
      "Iteration 383, loss = 0.44616181\n",
      "Iteration 384, loss = 0.44621686\n",
      "Iteration 385, loss = 0.44616447\n",
      "Iteration 386, loss = 0.44600869\n",
      "Iteration 387, loss = 0.44595373\n",
      "Iteration 388, loss = 0.44583512\n",
      "Iteration 389, loss = 0.44576234\n",
      "Iteration 390, loss = 0.44580623\n",
      "Iteration 391, loss = 0.44561153\n",
      "Iteration 392, loss = 0.44559874\n",
      "Iteration 393, loss = 0.44548191\n",
      "Iteration 394, loss = 0.44547706\n",
      "Iteration 395, loss = 0.44535139\n",
      "Iteration 396, loss = 0.44535966\n",
      "Iteration 397, loss = 0.44533611\n",
      "Iteration 398, loss = 0.44519576\n",
      "Iteration 399, loss = 0.44508834\n",
      "Iteration 400, loss = 0.44504896\n",
      "Iteration 401, loss = 0.44503006\n",
      "Iteration 402, loss = 0.44500446\n",
      "Iteration 403, loss = 0.44484412\n",
      "Iteration 404, loss = 0.44491577\n",
      "Iteration 405, loss = 0.44478219\n",
      "Iteration 406, loss = 0.44469782\n",
      "Iteration 407, loss = 0.44469759\n",
      "Iteration 408, loss = 0.44458845\n",
      "Iteration 409, loss = 0.44446522\n",
      "Iteration 410, loss = 0.44449215\n",
      "Iteration 411, loss = 0.44444934\n",
      "Iteration 412, loss = 0.44436923\n",
      "Iteration 413, loss = 0.44428375\n",
      "Iteration 414, loss = 0.44420006\n",
      "Iteration 415, loss = 0.44422194\n",
      "Iteration 416, loss = 0.44411651\n",
      "Iteration 417, loss = 0.44411762\n",
      "Iteration 418, loss = 0.44400128\n",
      "Iteration 419, loss = 0.44400889\n",
      "Iteration 420, loss = 0.44382194\n",
      "Iteration 421, loss = 0.44380885\n",
      "Iteration 422, loss = 0.44378363\n",
      "Iteration 423, loss = 0.44370299\n",
      "Iteration 424, loss = 0.44364398\n",
      "Iteration 425, loss = 0.44358540\n",
      "Iteration 426, loss = 0.44357615\n",
      "Iteration 427, loss = 0.44345201\n",
      "Iteration 428, loss = 0.44344312\n",
      "Iteration 429, loss = 0.44339915\n",
      "Iteration 430, loss = 0.44332694\n",
      "Iteration 431, loss = 0.44325719\n",
      "Iteration 432, loss = 0.44326517\n",
      "Iteration 433, loss = 0.44313055\n",
      "Iteration 434, loss = 0.44312920\n",
      "Iteration 435, loss = 0.44305187\n",
      "Iteration 436, loss = 0.44300651\n",
      "Iteration 437, loss = 0.44300484\n",
      "Iteration 438, loss = 0.44281829\n",
      "Iteration 439, loss = 0.44286375\n",
      "Iteration 440, loss = 0.44281452\n",
      "Iteration 441, loss = 0.44270999\n",
      "Iteration 442, loss = 0.44265757\n",
      "Iteration 443, loss = 0.44270281\n",
      "Iteration 444, loss = 0.44262898\n",
      "Iteration 445, loss = 0.44254489\n",
      "Iteration 446, loss = 0.44246670\n",
      "Iteration 447, loss = 0.44241667\n",
      "Iteration 448, loss = 0.44233022\n",
      "Iteration 449, loss = 0.44234081\n",
      "Iteration 450, loss = 0.44231514\n",
      "Iteration 451, loss = 0.44217332\n",
      "Iteration 452, loss = 0.44215651\n",
      "Iteration 453, loss = 0.44210324\n",
      "Iteration 454, loss = 0.44208596\n",
      "Iteration 455, loss = 0.44206930\n",
      "Iteration 456, loss = 0.44197655\n",
      "Iteration 457, loss = 0.44189478\n",
      "Iteration 458, loss = 0.44189851\n",
      "Iteration 459, loss = 0.44180180\n",
      "Iteration 460, loss = 0.44184902\n",
      "Iteration 461, loss = 0.44171929\n",
      "Iteration 462, loss = 0.44163672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10882012\n",
      "Iteration 2, loss = 1.94718396\n",
      "Iteration 3, loss = 1.83192487\n",
      "Iteration 4, loss = 1.74383452\n",
      "Iteration 5, loss = 1.67184985\n",
      "Iteration 6, loss = 1.60954458\n",
      "Iteration 7, loss = 1.55304877\n",
      "Iteration 8, loss = 1.50011535\n",
      "Iteration 9, loss = 1.44978349\n",
      "Iteration 10, loss = 1.40165168\n",
      "Iteration 11, loss = 1.35561826\n",
      "Iteration 12, loss = 1.31206132\n",
      "Iteration 13, loss = 1.27110011\n",
      "Iteration 14, loss = 1.23288536\n",
      "Iteration 15, loss = 1.19746853\n",
      "Iteration 16, loss = 1.16467846\n",
      "Iteration 17, loss = 1.13441065\n",
      "Iteration 18, loss = 1.10639520\n",
      "Iteration 19, loss = 1.08040936\n",
      "Iteration 20, loss = 1.05631989\n",
      "Iteration 21, loss = 1.03385100\n",
      "Iteration 22, loss = 1.01283052\n",
      "Iteration 23, loss = 0.99300185\n",
      "Iteration 24, loss = 0.97436114\n",
      "Iteration 25, loss = 0.95669171\n",
      "Iteration 26, loss = 0.93994303\n",
      "Iteration 27, loss = 0.92397779\n",
      "Iteration 28, loss = 0.90878593\n",
      "Iteration 29, loss = 0.89425975\n",
      "Iteration 30, loss = 0.88037343\n",
      "Iteration 31, loss = 0.86710438\n",
      "Iteration 32, loss = 0.85440286\n",
      "Iteration 33, loss = 0.84220731\n",
      "Iteration 34, loss = 0.83052926\n",
      "Iteration 35, loss = 0.81932005\n",
      "Iteration 36, loss = 0.80854051\n",
      "Iteration 37, loss = 0.79816335\n",
      "Iteration 38, loss = 0.78817264\n",
      "Iteration 39, loss = 0.77859625\n",
      "Iteration 40, loss = 0.76934455\n",
      "Iteration 41, loss = 0.76051401\n",
      "Iteration 42, loss = 0.75200384\n",
      "Iteration 43, loss = 0.74385529\n",
      "Iteration 44, loss = 0.73604598\n",
      "Iteration 45, loss = 0.72853513\n",
      "Iteration 46, loss = 0.72130706\n",
      "Iteration 47, loss = 0.71438523\n",
      "Iteration 48, loss = 0.70776308\n",
      "Iteration 49, loss = 0.70137398\n",
      "Iteration 50, loss = 0.69521534\n",
      "Iteration 51, loss = 0.68932705\n",
      "Iteration 52, loss = 0.68366064\n",
      "Iteration 53, loss = 0.67823765\n",
      "Iteration 54, loss = 0.67296996\n",
      "Iteration 55, loss = 0.66788825\n",
      "Iteration 56, loss = 0.66303619\n",
      "Iteration 57, loss = 0.65834419\n",
      "Iteration 58, loss = 0.65383932\n",
      "Iteration 59, loss = 0.64944186\n",
      "Iteration 60, loss = 0.64523852\n",
      "Iteration 61, loss = 0.64114745\n",
      "Iteration 62, loss = 0.63719606\n",
      "Iteration 63, loss = 0.63338518\n",
      "Iteration 64, loss = 0.62966095\n",
      "Iteration 65, loss = 0.62608998\n",
      "Iteration 66, loss = 0.62265259\n",
      "Iteration 67, loss = 0.61923811\n",
      "Iteration 68, loss = 0.61600085\n",
      "Iteration 69, loss = 0.61279602\n",
      "Iteration 70, loss = 0.60971465\n",
      "Iteration 71, loss = 0.60671857\n",
      "Iteration 72, loss = 0.60382285\n",
      "Iteration 73, loss = 0.60100231\n",
      "Iteration 74, loss = 0.59827549\n",
      "Iteration 75, loss = 0.59555244\n",
      "Iteration 76, loss = 0.59296458\n",
      "Iteration 77, loss = 0.59043702\n",
      "Iteration 78, loss = 0.58793289\n",
      "Iteration 79, loss = 0.58549793\n",
      "Iteration 80, loss = 0.58323377\n",
      "Iteration 81, loss = 0.58090448\n",
      "Iteration 82, loss = 0.57867977\n",
      "Iteration 83, loss = 0.57650551\n",
      "Iteration 84, loss = 0.57435735\n",
      "Iteration 85, loss = 0.57227613\n",
      "Iteration 86, loss = 0.57025953\n",
      "Iteration 87, loss = 0.56822887\n",
      "Iteration 88, loss = 0.56630996\n",
      "Iteration 89, loss = 0.56440846\n",
      "Iteration 90, loss = 0.56252439\n",
      "Iteration 91, loss = 0.56073923\n",
      "Iteration 92, loss = 0.55895507\n",
      "Iteration 93, loss = 0.55717695\n",
      "Iteration 94, loss = 0.55549225\n",
      "Iteration 95, loss = 0.55376713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96, loss = 0.55215944\n",
      "Iteration 97, loss = 0.55054239\n",
      "Iteration 98, loss = 0.54894340\n",
      "Iteration 99, loss = 0.54737631\n",
      "Iteration 100, loss = 0.54583265\n",
      "Iteration 101, loss = 0.54437903\n",
      "Iteration 102, loss = 0.54292010\n",
      "Iteration 103, loss = 0.54148450\n",
      "Iteration 104, loss = 0.54005461\n",
      "Iteration 105, loss = 0.53864872\n",
      "Iteration 106, loss = 0.53728466\n",
      "Iteration 107, loss = 0.53597835\n",
      "Iteration 108, loss = 0.53463990\n",
      "Iteration 109, loss = 0.53331559\n",
      "Iteration 110, loss = 0.53206424\n",
      "Iteration 111, loss = 0.53084455\n",
      "Iteration 112, loss = 0.52959499\n",
      "Iteration 113, loss = 0.52839462\n",
      "Iteration 114, loss = 0.52718476\n",
      "Iteration 115, loss = 0.52601955\n",
      "Iteration 116, loss = 0.52489662\n",
      "Iteration 117, loss = 0.52372393\n",
      "Iteration 118, loss = 0.52269543\n",
      "Iteration 119, loss = 0.52158871\n",
      "Iteration 120, loss = 0.52048208\n",
      "Iteration 121, loss = 0.51945757\n",
      "Iteration 122, loss = 0.51843298\n",
      "Iteration 123, loss = 0.51737369\n",
      "Iteration 124, loss = 0.51644010\n",
      "Iteration 125, loss = 0.51544273\n",
      "Iteration 126, loss = 0.51446424\n",
      "Iteration 127, loss = 0.51354116\n",
      "Iteration 128, loss = 0.51255720\n",
      "Iteration 129, loss = 0.51165981\n",
      "Iteration 130, loss = 0.51073493\n",
      "Iteration 131, loss = 0.50985911\n",
      "Iteration 132, loss = 0.50899549\n",
      "Iteration 133, loss = 0.50815569\n",
      "Iteration 134, loss = 0.50726549\n",
      "Iteration 135, loss = 0.50647874\n",
      "Iteration 136, loss = 0.50560365\n",
      "Iteration 137, loss = 0.50483205\n",
      "Iteration 138, loss = 0.50401560\n",
      "Iteration 139, loss = 0.50331492\n",
      "Iteration 140, loss = 0.50248714\n",
      "Iteration 141, loss = 0.50173176\n",
      "Iteration 142, loss = 0.50102707\n",
      "Iteration 143, loss = 0.50024086\n",
      "Iteration 144, loss = 0.49953073\n",
      "Iteration 145, loss = 0.49881655\n",
      "Iteration 146, loss = 0.49809267\n",
      "Iteration 147, loss = 0.49744872\n",
      "Iteration 148, loss = 0.49672495\n",
      "Iteration 149, loss = 0.49610494\n",
      "Iteration 150, loss = 0.49539870\n",
      "Iteration 151, loss = 0.49477892\n",
      "Iteration 152, loss = 0.49409548\n",
      "Iteration 153, loss = 0.49346924\n",
      "Iteration 154, loss = 0.49288307\n",
      "Iteration 155, loss = 0.49228573\n",
      "Iteration 156, loss = 0.49162211\n",
      "Iteration 157, loss = 0.49110824\n",
      "Iteration 158, loss = 0.49050799\n",
      "Iteration 159, loss = 0.48992033\n",
      "Iteration 160, loss = 0.48938287\n",
      "Iteration 161, loss = 0.48879403\n",
      "Iteration 162, loss = 0.48823184\n",
      "Iteration 163, loss = 0.48767984\n",
      "Iteration 164, loss = 0.48716726\n",
      "Iteration 165, loss = 0.48665437\n",
      "Iteration 166, loss = 0.48620601\n",
      "Iteration 167, loss = 0.48561984\n",
      "Iteration 168, loss = 0.48510679\n",
      "Iteration 169, loss = 0.48465225\n",
      "Iteration 170, loss = 0.48423474\n",
      "Iteration 171, loss = 0.48370321\n",
      "Iteration 172, loss = 0.48313824\n",
      "Iteration 173, loss = 0.48275303\n",
      "Iteration 174, loss = 0.48227860\n",
      "Iteration 175, loss = 0.48185624\n",
      "Iteration 176, loss = 0.48133954\n",
      "Iteration 177, loss = 0.48090074\n",
      "Iteration 178, loss = 0.48043376\n",
      "Iteration 179, loss = 0.48007503\n",
      "Iteration 180, loss = 0.47962350\n",
      "Iteration 181, loss = 0.47930148\n",
      "Iteration 182, loss = 0.47884723\n",
      "Iteration 183, loss = 0.47836638\n",
      "Iteration 184, loss = 0.47792852\n",
      "Iteration 185, loss = 0.47759585\n",
      "Iteration 186, loss = 0.47723437\n",
      "Iteration 187, loss = 0.47682241\n",
      "Iteration 188, loss = 0.47646939\n",
      "Iteration 189, loss = 0.47608561\n",
      "Iteration 190, loss = 0.47572786\n",
      "Iteration 191, loss = 0.47534911\n",
      "Iteration 192, loss = 0.47502828\n",
      "Iteration 193, loss = 0.47465301\n",
      "Iteration 194, loss = 0.47428410\n",
      "Iteration 195, loss = 0.47398178\n",
      "Iteration 196, loss = 0.47363641\n",
      "Iteration 197, loss = 0.47329102\n",
      "Iteration 198, loss = 0.47290151\n",
      "Iteration 199, loss = 0.47258946\n",
      "Iteration 200, loss = 0.47229797\n",
      "Iteration 201, loss = 0.47199814\n",
      "Iteration 202, loss = 0.47171086\n",
      "Iteration 203, loss = 0.47137239\n",
      "Iteration 204, loss = 0.47100900\n",
      "Iteration 205, loss = 0.47072708\n",
      "Iteration 206, loss = 0.47047587\n",
      "Iteration 207, loss = 0.47014121\n",
      "Iteration 208, loss = 0.46985089\n",
      "Iteration 209, loss = 0.46957268\n",
      "Iteration 210, loss = 0.46928517\n",
      "Iteration 211, loss = 0.46896821\n",
      "Iteration 212, loss = 0.46880410\n",
      "Iteration 213, loss = 0.46845060\n",
      "Iteration 214, loss = 0.46818669\n",
      "Iteration 215, loss = 0.46785933\n",
      "Iteration 216, loss = 0.46763900\n",
      "Iteration 217, loss = 0.46741796\n",
      "Iteration 218, loss = 0.46713970\n",
      "Iteration 219, loss = 0.46680561\n",
      "Iteration 220, loss = 0.46658846\n",
      "Iteration 221, loss = 0.46630465\n",
      "Iteration 222, loss = 0.46616651\n",
      "Iteration 223, loss = 0.46584762\n",
      "Iteration 224, loss = 0.46563856\n",
      "Iteration 225, loss = 0.46540363\n",
      "Iteration 226, loss = 0.46511714\n",
      "Iteration 227, loss = 0.46493161\n",
      "Iteration 228, loss = 0.46467428\n",
      "Iteration 229, loss = 0.46446966\n",
      "Iteration 230, loss = 0.46431498\n",
      "Iteration 231, loss = 0.46404639\n",
      "Iteration 232, loss = 0.46376544\n",
      "Iteration 233, loss = 0.46362169\n",
      "Iteration 234, loss = 0.46338017\n",
      "Iteration 235, loss = 0.46320138\n",
      "Iteration 236, loss = 0.46295356\n",
      "Iteration 237, loss = 0.46279472\n",
      "Iteration 238, loss = 0.46251939\n",
      "Iteration 239, loss = 0.46232862\n",
      "Iteration 240, loss = 0.46216688\n",
      "Iteration 241, loss = 0.46189962\n",
      "Iteration 242, loss = 0.46171568\n",
      "Iteration 243, loss = 0.46149900\n",
      "Iteration 244, loss = 0.46138128\n",
      "Iteration 245, loss = 0.46114433\n",
      "Iteration 246, loss = 0.46100926\n",
      "Iteration 247, loss = 0.46080255\n",
      "Iteration 248, loss = 0.46060792\n",
      "Iteration 249, loss = 0.46044045\n",
      "Iteration 250, loss = 0.46021712\n",
      "Iteration 251, loss = 0.46000744\n",
      "Iteration 252, loss = 0.45989478\n",
      "Iteration 253, loss = 0.45969604\n",
      "Iteration 254, loss = 0.45955735\n",
      "Iteration 255, loss = 0.45933708\n",
      "Iteration 256, loss = 0.45915640\n",
      "Iteration 257, loss = 0.45907707\n",
      "Iteration 258, loss = 0.45880848\n",
      "Iteration 259, loss = 0.45865646\n",
      "Iteration 260, loss = 0.45856350\n",
      "Iteration 261, loss = 0.45836276\n",
      "Iteration 262, loss = 0.45825202\n",
      "Iteration 263, loss = 0.45805480\n",
      "Iteration 264, loss = 0.45790242\n",
      "Iteration 265, loss = 0.45778896\n",
      "Iteration 266, loss = 0.45753319\n",
      "Iteration 267, loss = 0.45738197\n",
      "Iteration 268, loss = 0.45728930\n",
      "Iteration 269, loss = 0.45708502\n",
      "Iteration 270, loss = 0.45696594\n",
      "Iteration 271, loss = 0.45675610\n",
      "Iteration 272, loss = 0.45664737\n",
      "Iteration 273, loss = 0.45652696\n",
      "Iteration 274, loss = 0.45642430\n",
      "Iteration 275, loss = 0.45621177\n",
      "Iteration 276, loss = 0.45610469\n",
      "Iteration 277, loss = 0.45602410\n",
      "Iteration 278, loss = 0.45583413\n",
      "Iteration 279, loss = 0.45564220\n",
      "Iteration 280, loss = 0.45548661\n",
      "Iteration 281, loss = 0.45534845\n",
      "Iteration 282, loss = 0.45530625\n",
      "Iteration 283, loss = 0.45512015\n",
      "Iteration 284, loss = 0.45496379\n",
      "Iteration 285, loss = 0.45481255\n",
      "Iteration 286, loss = 0.45474972\n",
      "Iteration 287, loss = 0.45455793\n",
      "Iteration 288, loss = 0.45439925\n",
      "Iteration 289, loss = 0.45433441\n",
      "Iteration 290, loss = 0.45420878\n",
      "Iteration 291, loss = 0.45399766\n",
      "Iteration 292, loss = 0.45401244\n",
      "Iteration 293, loss = 0.45388973\n",
      "Iteration 294, loss = 0.45366654\n",
      "Iteration 295, loss = 0.45356019\n",
      "Iteration 296, loss = 0.45343840\n",
      "Iteration 297, loss = 0.45332257\n",
      "Iteration 298, loss = 0.45325941\n",
      "Iteration 299, loss = 0.45323122\n",
      "Iteration 300, loss = 0.45299332\n",
      "Iteration 301, loss = 0.45286513\n",
      "Iteration 302, loss = 0.45272161\n",
      "Iteration 303, loss = 0.45270230\n",
      "Iteration 304, loss = 0.45254736\n",
      "Iteration 305, loss = 0.45242977\n",
      "Iteration 306, loss = 0.45226371\n",
      "Iteration 307, loss = 0.45213628\n",
      "Iteration 308, loss = 0.45214033\n",
      "Iteration 309, loss = 0.45191978\n",
      "Iteration 310, loss = 0.45187465\n",
      "Iteration 311, loss = 0.45173805\n",
      "Iteration 312, loss = 0.45166220\n",
      "Iteration 313, loss = 0.45152656\n",
      "Iteration 314, loss = 0.45147671\n",
      "Iteration 315, loss = 0.45138194\n",
      "Iteration 316, loss = 0.45124250\n",
      "Iteration 317, loss = 0.45107116\n",
      "Iteration 318, loss = 0.45103390\n",
      "Iteration 319, loss = 0.45094576\n",
      "Iteration 320, loss = 0.45076098\n",
      "Iteration 321, loss = 0.45074398\n",
      "Iteration 322, loss = 0.45055413\n",
      "Iteration 323, loss = 0.45054365\n",
      "Iteration 324, loss = 0.45045732\n",
      "Iteration 325, loss = 0.45035401\n",
      "Iteration 326, loss = 0.45030889\n",
      "Iteration 327, loss = 0.45014529\n",
      "Iteration 328, loss = 0.44994001\n",
      "Iteration 329, loss = 0.44990902\n",
      "Iteration 330, loss = 0.44976404\n",
      "Iteration 331, loss = 0.44981257\n",
      "Iteration 332, loss = 0.44962277\n",
      "Iteration 333, loss = 0.44952787\n",
      "Iteration 334, loss = 0.44946374\n",
      "Iteration 335, loss = 0.44934815\n",
      "Iteration 336, loss = 0.44928296\n",
      "Iteration 337, loss = 0.44922270\n",
      "Iteration 338, loss = 0.44907140\n",
      "Iteration 339, loss = 0.44902963\n",
      "Iteration 340, loss = 0.44891338\n",
      "Iteration 341, loss = 0.44884553\n",
      "Iteration 342, loss = 0.44869771\n",
      "Iteration 343, loss = 0.44867572\n",
      "Iteration 344, loss = 0.44854237\n",
      "Iteration 345, loss = 0.44844569\n",
      "Iteration 346, loss = 0.44837602\n",
      "Iteration 347, loss = 0.44828312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 348, loss = 0.44823872\n",
      "Iteration 349, loss = 0.44816598\n",
      "Iteration 350, loss = 0.44798560\n",
      "Iteration 351, loss = 0.44803128\n",
      "Iteration 352, loss = 0.44783490\n",
      "Iteration 353, loss = 0.44773159\n",
      "Iteration 354, loss = 0.44774088\n",
      "Iteration 355, loss = 0.44770128\n",
      "Iteration 356, loss = 0.44765096\n",
      "Iteration 357, loss = 0.44746045\n",
      "Iteration 358, loss = 0.44740464\n",
      "Iteration 359, loss = 0.44724817\n",
      "Iteration 360, loss = 0.44721381\n",
      "Iteration 361, loss = 0.44714217\n",
      "Iteration 362, loss = 0.44711279\n",
      "Iteration 363, loss = 0.44698530\n",
      "Iteration 364, loss = 0.44693229\n",
      "Iteration 365, loss = 0.44687578\n",
      "Iteration 366, loss = 0.44677495\n",
      "Iteration 367, loss = 0.44667216\n",
      "Iteration 368, loss = 0.44659877\n",
      "Iteration 369, loss = 0.44655410\n",
      "Iteration 370, loss = 0.44648536\n",
      "Iteration 371, loss = 0.44640933\n",
      "Iteration 372, loss = 0.44624246\n",
      "Iteration 373, loss = 0.44625420\n",
      "Iteration 374, loss = 0.44618857\n",
      "Iteration 375, loss = 0.44610312\n",
      "Iteration 376, loss = 0.44600990\n",
      "Iteration 377, loss = 0.44603112\n",
      "Iteration 378, loss = 0.44589492\n",
      "Iteration 379, loss = 0.44580355\n",
      "Iteration 380, loss = 0.44578286\n",
      "Iteration 381, loss = 0.44571771\n",
      "Iteration 382, loss = 0.44559285\n",
      "Iteration 383, loss = 0.44555282\n",
      "Iteration 384, loss = 0.44549491\n",
      "Iteration 385, loss = 0.44540779\n",
      "Iteration 386, loss = 0.44536247\n",
      "Iteration 387, loss = 0.44524692\n",
      "Iteration 388, loss = 0.44514495\n",
      "Iteration 389, loss = 0.44517644\n",
      "Iteration 390, loss = 0.44508924\n",
      "Iteration 391, loss = 0.44496536\n",
      "Iteration 392, loss = 0.44493483\n",
      "Iteration 393, loss = 0.44498152\n",
      "Iteration 394, loss = 0.44480076\n",
      "Iteration 395, loss = 0.44466761\n",
      "Iteration 396, loss = 0.44470880\n",
      "Iteration 397, loss = 0.44461142\n",
      "Iteration 398, loss = 0.44458081\n",
      "Iteration 399, loss = 0.44452478\n",
      "Iteration 400, loss = 0.44438607\n",
      "Iteration 401, loss = 0.44435061\n",
      "Iteration 402, loss = 0.44424816\n",
      "Iteration 403, loss = 0.44427512\n",
      "Iteration 404, loss = 0.44415472\n",
      "Iteration 405, loss = 0.44401870\n",
      "Iteration 406, loss = 0.44400433\n",
      "Iteration 407, loss = 0.44392004\n",
      "Iteration 408, loss = 0.44386964\n",
      "Iteration 409, loss = 0.44383371\n",
      "Iteration 410, loss = 0.44388416\n",
      "Iteration 411, loss = 0.44370353\n",
      "Iteration 412, loss = 0.44368696\n",
      "Iteration 413, loss = 0.44354452\n",
      "Iteration 414, loss = 0.44354727\n",
      "Iteration 415, loss = 0.44341893\n",
      "Iteration 416, loss = 0.44339747\n",
      "Iteration 417, loss = 0.44331933\n",
      "Iteration 418, loss = 0.44331268\n",
      "Iteration 419, loss = 0.44320075\n",
      "Iteration 420, loss = 0.44320258\n",
      "Iteration 421, loss = 0.44308240\n",
      "Iteration 422, loss = 0.44299236\n",
      "Iteration 423, loss = 0.44295432\n",
      "Iteration 424, loss = 0.44291208\n",
      "Iteration 425, loss = 0.44288288\n",
      "Iteration 426, loss = 0.44282253\n",
      "Iteration 427, loss = 0.44273947\n",
      "Iteration 428, loss = 0.44268079\n",
      "Iteration 429, loss = 0.44263463\n",
      "Iteration 430, loss = 0.44266346\n",
      "Iteration 431, loss = 0.44258789\n",
      "Iteration 432, loss = 0.44247319\n",
      "Iteration 433, loss = 0.44238223\n",
      "Iteration 434, loss = 0.44234570\n",
      "Iteration 435, loss = 0.44233041\n",
      "Iteration 436, loss = 0.44224913\n",
      "Iteration 437, loss = 0.44214120\n",
      "Iteration 438, loss = 0.44217125\n",
      "Iteration 439, loss = 0.44211209\n",
      "Iteration 440, loss = 0.44201386\n",
      "Iteration 441, loss = 0.44193505\n",
      "Iteration 442, loss = 0.44198331\n",
      "Iteration 443, loss = 0.44192348\n",
      "Iteration 444, loss = 0.44185433\n",
      "Iteration 445, loss = 0.44173686\n",
      "Iteration 446, loss = 0.44173578\n",
      "Iteration 447, loss = 0.44166017\n",
      "Iteration 448, loss = 0.44166732\n",
      "Iteration 449, loss = 0.44159549\n",
      "Iteration 450, loss = 0.44149337\n",
      "Iteration 451, loss = 0.44140669\n",
      "Iteration 452, loss = 0.44138218\n",
      "Iteration 453, loss = 0.44132648\n",
      "Iteration 454, loss = 0.44129532\n",
      "Iteration 455, loss = 0.44129868\n",
      "Iteration 456, loss = 0.44114886\n",
      "Iteration 457, loss = 0.44120332\n",
      "Iteration 458, loss = 0.44104207\n",
      "Iteration 459, loss = 0.44107812\n",
      "Iteration 460, loss = 0.44097345\n",
      "Iteration 461, loss = 0.44081970\n",
      "Iteration 462, loss = 0.44086333\n",
      "Iteration 463, loss = 0.44077265\n",
      "Iteration 464, loss = 0.44077424\n",
      "Iteration 465, loss = 0.44070400\n",
      "Iteration 466, loss = 0.44073019\n",
      "Iteration 467, loss = 0.44068182\n",
      "Iteration 468, loss = 0.44059268\n",
      "Iteration 469, loss = 0.44052610\n",
      "Iteration 470, loss = 0.44049591\n",
      "Iteration 471, loss = 0.44039144\n",
      "Iteration 472, loss = 0.44047451\n",
      "Iteration 473, loss = 0.44038313\n",
      "Iteration 474, loss = 0.44030137\n",
      "Iteration 475, loss = 0.44015434\n",
      "Iteration 476, loss = 0.44025171\n",
      "Iteration 477, loss = 0.44011006\n",
      "Iteration 478, loss = 0.44012414\n",
      "Iteration 479, loss = 0.44000638\n",
      "Iteration 480, loss = 0.43995449\n",
      "Iteration 481, loss = 0.43996140\n",
      "Iteration 482, loss = 0.43981290\n",
      "Iteration 483, loss = 0.43980873\n",
      "Iteration 484, loss = 0.43979180\n",
      "Iteration 485, loss = 0.43969158\n",
      "Iteration 486, loss = 0.43968745\n",
      "Iteration 487, loss = 0.43967751\n",
      "Iteration 488, loss = 0.43960310\n",
      "Iteration 489, loss = 0.43954935\n",
      "Iteration 490, loss = 0.43948811\n",
      "Iteration 491, loss = 0.43949949\n",
      "Iteration 492, loss = 0.43934658\n",
      "Iteration 493, loss = 0.43932491\n",
      "Iteration 494, loss = 0.43930614\n",
      "Iteration 495, loss = 0.43923348\n",
      "Iteration 496, loss = 0.43914934\n",
      "Iteration 497, loss = 0.43917045\n",
      "Iteration 498, loss = 0.43911099\n",
      "Iteration 499, loss = 0.43904540\n",
      "Iteration 500, loss = 0.43892179\n",
      "Iteration 1, loss = 2.11028249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.94775083\n",
      "Iteration 3, loss = 1.83147685\n",
      "Iteration 4, loss = 1.74262983\n",
      "Iteration 5, loss = 1.67002612\n",
      "Iteration 6, loss = 1.60721366\n",
      "Iteration 7, loss = 1.55035471\n",
      "Iteration 8, loss = 1.49729560\n",
      "Iteration 9, loss = 1.44688252\n",
      "Iteration 10, loss = 1.39874172\n",
      "Iteration 11, loss = 1.35293511\n",
      "Iteration 12, loss = 1.30956943\n",
      "Iteration 13, loss = 1.26885552\n",
      "Iteration 14, loss = 1.23091787\n",
      "Iteration 15, loss = 1.19573264\n",
      "Iteration 16, loss = 1.16314513\n",
      "Iteration 17, loss = 1.13299671\n",
      "Iteration 18, loss = 1.10506013\n",
      "Iteration 19, loss = 1.07918109\n",
      "Iteration 20, loss = 1.05515303\n",
      "Iteration 21, loss = 1.03272901\n",
      "Iteration 22, loss = 1.01173010\n",
      "Iteration 23, loss = 0.99195597\n",
      "Iteration 24, loss = 0.97323286\n",
      "Iteration 25, loss = 0.95550312\n",
      "Iteration 26, loss = 0.93865898\n",
      "Iteration 27, loss = 0.92259464\n",
      "Iteration 28, loss = 0.90733314\n",
      "Iteration 29, loss = 0.89276159\n",
      "Iteration 30, loss = 0.87889326\n",
      "Iteration 31, loss = 0.86561379\n",
      "Iteration 32, loss = 0.85288144\n",
      "Iteration 33, loss = 0.84069369\n",
      "Iteration 34, loss = 0.82899776\n",
      "Iteration 35, loss = 0.81778208\n",
      "Iteration 36, loss = 0.80700611\n",
      "Iteration 37, loss = 0.79662478\n",
      "Iteration 38, loss = 0.78664771\n",
      "Iteration 39, loss = 0.77704009\n",
      "Iteration 40, loss = 0.76781258\n",
      "Iteration 41, loss = 0.75895635\n",
      "Iteration 42, loss = 0.75045947\n",
      "Iteration 43, loss = 0.74229217\n",
      "Iteration 44, loss = 0.73446240\n",
      "Iteration 45, loss = 0.72696313\n",
      "Iteration 46, loss = 0.71974432\n",
      "Iteration 47, loss = 0.71278532\n",
      "Iteration 48, loss = 0.70612929\n",
      "Iteration 49, loss = 0.69974518\n",
      "Iteration 50, loss = 0.69356974\n",
      "Iteration 51, loss = 0.68767826\n",
      "Iteration 52, loss = 0.68198423\n",
      "Iteration 53, loss = 0.67649449\n",
      "Iteration 54, loss = 0.67122251\n",
      "Iteration 55, loss = 0.66616809\n",
      "Iteration 56, loss = 0.66125340\n",
      "Iteration 57, loss = 0.65655730\n",
      "Iteration 58, loss = 0.65199267\n",
      "Iteration 59, loss = 0.64762081\n",
      "Iteration 60, loss = 0.64338777\n",
      "Iteration 61, loss = 0.63926159\n",
      "Iteration 62, loss = 0.63530726\n",
      "Iteration 63, loss = 0.63147548\n",
      "Iteration 64, loss = 0.62775668\n",
      "Iteration 65, loss = 0.62416952\n",
      "Iteration 66, loss = 0.62068099\n",
      "Iteration 67, loss = 0.61729048\n",
      "Iteration 68, loss = 0.61402677\n",
      "Iteration 69, loss = 0.61085793\n",
      "Iteration 70, loss = 0.60779201\n",
      "Iteration 71, loss = 0.60475109\n",
      "Iteration 72, loss = 0.60185951\n",
      "Iteration 73, loss = 0.59900533\n",
      "Iteration 74, loss = 0.59623724\n",
      "Iteration 75, loss = 0.59361627\n",
      "Iteration 76, loss = 0.59098507\n",
      "Iteration 77, loss = 0.58846960\n",
      "Iteration 78, loss = 0.58593021\n",
      "Iteration 79, loss = 0.58356003\n",
      "Iteration 80, loss = 0.58120482\n",
      "Iteration 81, loss = 0.57889350\n",
      "Iteration 82, loss = 0.57665592\n",
      "Iteration 83, loss = 0.57446945\n",
      "Iteration 84, loss = 0.57232134\n",
      "Iteration 85, loss = 0.57021704\n",
      "Iteration 86, loss = 0.56816839\n",
      "Iteration 87, loss = 0.56618919\n",
      "Iteration 88, loss = 0.56428546\n",
      "Iteration 89, loss = 0.56235111\n",
      "Iteration 90, loss = 0.56048894\n",
      "Iteration 91, loss = 0.55864255\n",
      "Iteration 92, loss = 0.55684619\n",
      "Iteration 93, loss = 0.55511153\n",
      "Iteration 94, loss = 0.55343907\n",
      "Iteration 95, loss = 0.55170678\n",
      "Iteration 96, loss = 0.55013661\n",
      "Iteration 97, loss = 0.54845911\n",
      "Iteration 98, loss = 0.54688034\n",
      "Iteration 99, loss = 0.54531922\n",
      "Iteration 100, loss = 0.54386170\n",
      "Iteration 101, loss = 0.54235054\n",
      "Iteration 102, loss = 0.54089740\n",
      "Iteration 103, loss = 0.53943264\n",
      "Iteration 104, loss = 0.53804962\n",
      "Iteration 105, loss = 0.53665355\n",
      "Iteration 106, loss = 0.53528719\n",
      "Iteration 107, loss = 0.53398640\n",
      "Iteration 108, loss = 0.53265803\n",
      "Iteration 109, loss = 0.53133791\n",
      "Iteration 110, loss = 0.53015303\n",
      "Iteration 111, loss = 0.52887691\n",
      "Iteration 112, loss = 0.52771129\n",
      "Iteration 113, loss = 0.52645813\n",
      "Iteration 114, loss = 0.52527175\n",
      "Iteration 115, loss = 0.52409050\n",
      "Iteration 116, loss = 0.52299855\n",
      "Iteration 117, loss = 0.52189632\n",
      "Iteration 118, loss = 0.52076505\n",
      "Iteration 119, loss = 0.51965992\n",
      "Iteration 120, loss = 0.51857284\n",
      "Iteration 121, loss = 0.51752074\n",
      "Iteration 122, loss = 0.51652482\n",
      "Iteration 123, loss = 0.51546996\n",
      "Iteration 124, loss = 0.51452419\n",
      "Iteration 125, loss = 0.51349690\n",
      "Iteration 126, loss = 0.51258035\n",
      "Iteration 127, loss = 0.51162817\n",
      "Iteration 128, loss = 0.51070067\n",
      "Iteration 129, loss = 0.50973035\n",
      "Iteration 130, loss = 0.50886706\n",
      "Iteration 131, loss = 0.50800265\n",
      "Iteration 132, loss = 0.50717713\n",
      "Iteration 133, loss = 0.50623691\n",
      "Iteration 134, loss = 0.50542979\n",
      "Iteration 135, loss = 0.50467416\n",
      "Iteration 136, loss = 0.50384090\n",
      "Iteration 137, loss = 0.50301349\n",
      "Iteration 138, loss = 0.50224948\n",
      "Iteration 139, loss = 0.50148053\n",
      "Iteration 140, loss = 0.50066981\n",
      "Iteration 141, loss = 0.49995976\n",
      "Iteration 142, loss = 0.49919064\n",
      "Iteration 143, loss = 0.49844905\n",
      "Iteration 144, loss = 0.49774775\n",
      "Iteration 145, loss = 0.49714980\n",
      "Iteration 146, loss = 0.49634746\n",
      "Iteration 147, loss = 0.49565119\n",
      "Iteration 148, loss = 0.49507219\n",
      "Iteration 149, loss = 0.49438250\n",
      "Iteration 150, loss = 0.49373139\n",
      "Iteration 151, loss = 0.49304866\n",
      "Iteration 152, loss = 0.49245739\n",
      "Iteration 153, loss = 0.49181239\n",
      "Iteration 154, loss = 0.49121245\n",
      "Iteration 155, loss = 0.49053961\n",
      "Iteration 156, loss = 0.49001820\n",
      "Iteration 157, loss = 0.48940283\n",
      "Iteration 158, loss = 0.48886445\n",
      "Iteration 159, loss = 0.48824420\n",
      "Iteration 160, loss = 0.48768459\n",
      "Iteration 161, loss = 0.48722924\n",
      "Iteration 162, loss = 0.48661678\n",
      "Iteration 163, loss = 0.48609467\n",
      "Iteration 164, loss = 0.48556432\n",
      "Iteration 165, loss = 0.48501960\n",
      "Iteration 166, loss = 0.48455193\n",
      "Iteration 167, loss = 0.48406476\n",
      "Iteration 168, loss = 0.48350188\n",
      "Iteration 169, loss = 0.48315337\n",
      "Iteration 170, loss = 0.48257165\n",
      "Iteration 171, loss = 0.48215721\n",
      "Iteration 172, loss = 0.48164318\n",
      "Iteration 173, loss = 0.48113378\n",
      "Iteration 174, loss = 0.48076599\n",
      "Iteration 175, loss = 0.48027773\n",
      "Iteration 176, loss = 0.47986849\n",
      "Iteration 177, loss = 0.47940219\n",
      "Iteration 178, loss = 0.47894587\n",
      "Iteration 179, loss = 0.47860643\n",
      "Iteration 180, loss = 0.47810842\n",
      "Iteration 181, loss = 0.47773408\n",
      "Iteration 182, loss = 0.47732713\n",
      "Iteration 183, loss = 0.47695111\n",
      "Iteration 184, loss = 0.47656299\n",
      "Iteration 185, loss = 0.47616743\n",
      "Iteration 186, loss = 0.47586510\n",
      "Iteration 187, loss = 0.47532053\n",
      "Iteration 188, loss = 0.47504783\n",
      "Iteration 189, loss = 0.47456491\n",
      "Iteration 190, loss = 0.47428176\n",
      "Iteration 191, loss = 0.47391762\n",
      "Iteration 192, loss = 0.47358716\n",
      "Iteration 193, loss = 0.47320727\n",
      "Iteration 194, loss = 0.47292520\n",
      "Iteration 195, loss = 0.47258335\n",
      "Iteration 196, loss = 0.47225221\n",
      "Iteration 197, loss = 0.47194403\n",
      "Iteration 198, loss = 0.47158920\n",
      "Iteration 199, loss = 0.47126028\n",
      "Iteration 200, loss = 0.47097385\n",
      "Iteration 201, loss = 0.47061910\n",
      "Iteration 202, loss = 0.47029554\n",
      "Iteration 203, loss = 0.47002704\n",
      "Iteration 204, loss = 0.46973358\n",
      "Iteration 205, loss = 0.46943583\n",
      "Iteration 206, loss = 0.46908046\n",
      "Iteration 207, loss = 0.46887043\n",
      "Iteration 208, loss = 0.46846466\n",
      "Iteration 209, loss = 0.46824158\n",
      "Iteration 210, loss = 0.46797510\n",
      "Iteration 211, loss = 0.46768018\n",
      "Iteration 212, loss = 0.46740293\n",
      "Iteration 213, loss = 0.46715418\n",
      "Iteration 214, loss = 0.46688108\n",
      "Iteration 215, loss = 0.46662258\n",
      "Iteration 216, loss = 0.46633018\n",
      "Iteration 217, loss = 0.46607006\n",
      "Iteration 218, loss = 0.46587238\n",
      "Iteration 219, loss = 0.46557645\n",
      "Iteration 220, loss = 0.46530617\n",
      "Iteration 221, loss = 0.46513750\n",
      "Iteration 222, loss = 0.46482303\n",
      "Iteration 223, loss = 0.46453518\n",
      "Iteration 224, loss = 0.46439520\n",
      "Iteration 225, loss = 0.46415666\n",
      "Iteration 226, loss = 0.46395079\n",
      "Iteration 227, loss = 0.46368635\n",
      "Iteration 228, loss = 0.46344948\n",
      "Iteration 229, loss = 0.46324177\n",
      "Iteration 230, loss = 0.46303982\n",
      "Iteration 231, loss = 0.46274929\n",
      "Iteration 232, loss = 0.46258814\n",
      "Iteration 233, loss = 0.46235343\n",
      "Iteration 234, loss = 0.46214381\n",
      "Iteration 235, loss = 0.46192616\n",
      "Iteration 236, loss = 0.46176254\n",
      "Iteration 237, loss = 0.46150371\n",
      "Iteration 238, loss = 0.46138438\n",
      "Iteration 239, loss = 0.46114973\n",
      "Iteration 240, loss = 0.46086074\n",
      "Iteration 241, loss = 0.46071031\n",
      "Iteration 242, loss = 0.46047576\n",
      "Iteration 243, loss = 0.46034825\n",
      "Iteration 244, loss = 0.46008222\n",
      "Iteration 245, loss = 0.45991441\n",
      "Iteration 246, loss = 0.45973370\n",
      "Iteration 247, loss = 0.45960758\n",
      "Iteration 248, loss = 0.45944989\n",
      "Iteration 249, loss = 0.45920202\n",
      "Iteration 250, loss = 0.45904058\n",
      "Iteration 251, loss = 0.45884103\n",
      "Iteration 252, loss = 0.45859023\n",
      "Iteration 253, loss = 0.45839239\n",
      "Iteration 254, loss = 0.45836044\n",
      "Iteration 255, loss = 0.45812098\n",
      "Iteration 256, loss = 0.45798332\n",
      "Iteration 257, loss = 0.45778525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.45766821\n",
      "Iteration 259, loss = 0.45748281\n",
      "Iteration 260, loss = 0.45735218\n",
      "Iteration 261, loss = 0.45717372\n",
      "Iteration 262, loss = 0.45699444\n",
      "Iteration 263, loss = 0.45673824\n",
      "Iteration 264, loss = 0.45665956\n",
      "Iteration 265, loss = 0.45649860\n",
      "Iteration 266, loss = 0.45632753\n",
      "Iteration 267, loss = 0.45618148\n",
      "Iteration 268, loss = 0.45601002\n",
      "Iteration 269, loss = 0.45580192\n",
      "Iteration 270, loss = 0.45580131\n",
      "Iteration 271, loss = 0.45562417\n",
      "Iteration 272, loss = 0.45542888\n",
      "Iteration 273, loss = 0.45532514\n",
      "Iteration 274, loss = 0.45513023\n",
      "Iteration 275, loss = 0.45497736\n",
      "Iteration 276, loss = 0.45483066\n",
      "Iteration 277, loss = 0.45476440\n",
      "Iteration 278, loss = 0.45457867\n",
      "Iteration 279, loss = 0.45443383\n",
      "Iteration 280, loss = 0.45434568\n",
      "Iteration 281, loss = 0.45420406\n",
      "Iteration 282, loss = 0.45405536\n",
      "Iteration 283, loss = 0.45387435\n",
      "Iteration 284, loss = 0.45372872\n",
      "Iteration 285, loss = 0.45366721\n",
      "Iteration 286, loss = 0.45346988\n",
      "Iteration 287, loss = 0.45333523\n",
      "Iteration 288, loss = 0.45333536\n",
      "Iteration 289, loss = 0.45307466\n",
      "Iteration 290, loss = 0.45294495\n",
      "Iteration 291, loss = 0.45286787\n",
      "Iteration 292, loss = 0.45273269\n",
      "Iteration 293, loss = 0.45257856\n",
      "Iteration 294, loss = 0.45247793\n",
      "Iteration 295, loss = 0.45237136\n",
      "Iteration 296, loss = 0.45222648\n",
      "Iteration 297, loss = 0.45213403\n",
      "Iteration 298, loss = 0.45198701\n",
      "Iteration 299, loss = 0.45190651\n",
      "Iteration 300, loss = 0.45166042\n",
      "Iteration 301, loss = 0.45160917\n",
      "Iteration 302, loss = 0.45144405\n",
      "Iteration 303, loss = 0.45141075\n",
      "Iteration 304, loss = 0.45126066\n",
      "Iteration 305, loss = 0.45120012\n",
      "Iteration 306, loss = 0.45110586\n",
      "Iteration 307, loss = 0.45091247\n",
      "Iteration 308, loss = 0.45082225\n",
      "Iteration 309, loss = 0.45072110\n",
      "Iteration 310, loss = 0.45073045\n",
      "Iteration 311, loss = 0.45051964\n",
      "Iteration 312, loss = 0.45040830\n",
      "Iteration 313, loss = 0.45035207\n",
      "Iteration 314, loss = 0.45027407\n",
      "Iteration 315, loss = 0.45004999\n",
      "Iteration 316, loss = 0.45018951\n",
      "Iteration 317, loss = 0.44988237\n",
      "Iteration 318, loss = 0.44982199\n",
      "Iteration 319, loss = 0.44965839\n",
      "Iteration 320, loss = 0.44954580\n",
      "Iteration 321, loss = 0.44945237\n",
      "Iteration 322, loss = 0.44941559\n",
      "Iteration 323, loss = 0.44922011\n",
      "Iteration 324, loss = 0.44923812\n",
      "Iteration 325, loss = 0.44902996\n",
      "Iteration 326, loss = 0.44896242\n",
      "Iteration 327, loss = 0.44888108\n",
      "Iteration 328, loss = 0.44875586\n",
      "Iteration 329, loss = 0.44865489\n",
      "Iteration 330, loss = 0.44850181\n",
      "Iteration 331, loss = 0.44849373\n",
      "Iteration 332, loss = 0.44840693\n",
      "Iteration 333, loss = 0.44823090\n",
      "Iteration 334, loss = 0.44829433\n",
      "Iteration 335, loss = 0.44819102\n",
      "Iteration 336, loss = 0.44803493\n",
      "Iteration 337, loss = 0.44796154\n",
      "Iteration 338, loss = 0.44780631\n",
      "Iteration 339, loss = 0.44777447\n",
      "Iteration 340, loss = 0.44766841\n",
      "Iteration 341, loss = 0.44758240\n",
      "Iteration 342, loss = 0.44750738\n",
      "Iteration 343, loss = 0.44737769\n",
      "Iteration 344, loss = 0.44728006\n",
      "Iteration 345, loss = 0.44722555\n",
      "Iteration 346, loss = 0.44715026\n",
      "Iteration 347, loss = 0.44702042\n",
      "Iteration 348, loss = 0.44694835\n",
      "Iteration 349, loss = 0.44690425\n",
      "Iteration 350, loss = 0.44675652\n",
      "Iteration 351, loss = 0.44673077\n",
      "Iteration 352, loss = 0.44663139\n",
      "Iteration 353, loss = 0.44651689\n",
      "Iteration 354, loss = 0.44645723\n",
      "Iteration 355, loss = 0.44640253\n",
      "Iteration 356, loss = 0.44627342\n",
      "Iteration 357, loss = 0.44622616\n",
      "Iteration 358, loss = 0.44611866\n",
      "Iteration 359, loss = 0.44605516\n",
      "Iteration 360, loss = 0.44597553\n",
      "Iteration 361, loss = 0.44590114\n",
      "Iteration 362, loss = 0.44590079\n",
      "Iteration 363, loss = 0.44583034\n",
      "Iteration 364, loss = 0.44561921\n",
      "Iteration 365, loss = 0.44552768\n",
      "Iteration 366, loss = 0.44553574\n",
      "Iteration 367, loss = 0.44544141\n",
      "Iteration 368, loss = 0.44527582\n",
      "Iteration 369, loss = 0.44523179\n",
      "Iteration 370, loss = 0.44516717\n",
      "Iteration 371, loss = 0.44515441\n",
      "Iteration 372, loss = 0.44499025\n",
      "Iteration 373, loss = 0.44499071\n",
      "Iteration 374, loss = 0.44486287\n",
      "Iteration 375, loss = 0.44478122\n",
      "Iteration 376, loss = 0.44478660\n",
      "Iteration 377, loss = 0.44464832\n",
      "Iteration 378, loss = 0.44459804\n",
      "Iteration 379, loss = 0.44453638\n",
      "Iteration 380, loss = 0.44442605\n",
      "Iteration 381, loss = 0.44436744\n",
      "Iteration 382, loss = 0.44429515\n",
      "Iteration 383, loss = 0.44423179\n",
      "Iteration 384, loss = 0.44418500\n",
      "Iteration 385, loss = 0.44412450\n",
      "Iteration 386, loss = 0.44398448\n",
      "Iteration 387, loss = 0.44402854\n",
      "Iteration 388, loss = 0.44393091\n",
      "Iteration 389, loss = 0.44379348\n",
      "Iteration 390, loss = 0.44386360\n",
      "Iteration 391, loss = 0.44367582\n",
      "Iteration 392, loss = 0.44366412\n",
      "Iteration 393, loss = 0.44355676\n",
      "Iteration 394, loss = 0.44344705\n",
      "Iteration 395, loss = 0.44335961\n",
      "Iteration 396, loss = 0.44323248\n",
      "Iteration 397, loss = 0.44333905\n",
      "Iteration 398, loss = 0.44317997\n",
      "Iteration 399, loss = 0.44320254\n",
      "Iteration 400, loss = 0.44317335\n",
      "Iteration 401, loss = 0.44303226\n",
      "Iteration 402, loss = 0.44292535\n",
      "Iteration 403, loss = 0.44284145\n",
      "Iteration 404, loss = 0.44288394\n",
      "Iteration 405, loss = 0.44282436\n",
      "Iteration 406, loss = 0.44271931\n",
      "Iteration 407, loss = 0.44263212\n",
      "Iteration 408, loss = 0.44257889\n",
      "Iteration 409, loss = 0.44251796\n",
      "Iteration 410, loss = 0.44251297\n",
      "Iteration 411, loss = 0.44234384\n",
      "Iteration 412, loss = 0.44237822\n",
      "Iteration 413, loss = 0.44224482\n",
      "Iteration 414, loss = 0.44224880\n",
      "Iteration 415, loss = 0.44220253\n",
      "Iteration 416, loss = 0.44212023\n",
      "Iteration 417, loss = 0.44202822\n",
      "Iteration 418, loss = 0.44192607\n",
      "Iteration 419, loss = 0.44191144\n",
      "Iteration 420, loss = 0.44190204\n",
      "Iteration 421, loss = 0.44176110\n",
      "Iteration 422, loss = 0.44173051\n",
      "Iteration 423, loss = 0.44166698\n",
      "Iteration 424, loss = 0.44162499\n",
      "Iteration 425, loss = 0.44153852\n",
      "Iteration 426, loss = 0.44150888\n",
      "Iteration 427, loss = 0.44140176\n",
      "Iteration 428, loss = 0.44135508\n",
      "Iteration 429, loss = 0.44132410\n",
      "Iteration 430, loss = 0.44120813\n",
      "Iteration 431, loss = 0.44121678\n",
      "Iteration 432, loss = 0.44107531\n",
      "Iteration 433, loss = 0.44109911\n",
      "Iteration 434, loss = 0.44103019\n",
      "Iteration 435, loss = 0.44095762\n",
      "Iteration 436, loss = 0.44095383\n",
      "Iteration 437, loss = 0.44094505\n",
      "Iteration 438, loss = 0.44082451\n",
      "Iteration 439, loss = 0.44070958\n",
      "Iteration 440, loss = 0.44065677\n",
      "Iteration 441, loss = 0.44069619\n",
      "Iteration 442, loss = 0.44064068\n",
      "Iteration 443, loss = 0.44057454\n",
      "Iteration 444, loss = 0.44053462\n",
      "Iteration 445, loss = 0.44049799\n",
      "Iteration 446, loss = 0.44036319\n",
      "Iteration 447, loss = 0.44029977\n",
      "Iteration 448, loss = 0.44034750\n",
      "Iteration 449, loss = 0.44025009\n",
      "Iteration 450, loss = 0.44015404\n",
      "Iteration 451, loss = 0.44012439\n",
      "Iteration 452, loss = 0.44008509\n",
      "Iteration 453, loss = 0.44010682\n",
      "Iteration 454, loss = 0.44004477\n",
      "Iteration 455, loss = 0.43984907\n",
      "Iteration 456, loss = 0.43986783\n",
      "Iteration 457, loss = 0.43981858\n",
      "Iteration 458, loss = 0.43966937\n",
      "Iteration 459, loss = 0.43978642\n",
      "Iteration 460, loss = 0.43965081\n",
      "Iteration 461, loss = 0.43963555\n",
      "Iteration 462, loss = 0.43955857\n",
      "Iteration 463, loss = 0.43950756\n",
      "Iteration 464, loss = 0.43944286\n",
      "Iteration 465, loss = 0.43937235\n",
      "Iteration 466, loss = 0.43926912\n",
      "Iteration 467, loss = 0.43923060\n",
      "Iteration 468, loss = 0.43916842\n",
      "Iteration 469, loss = 0.43920208\n",
      "Iteration 470, loss = 0.43903682\n",
      "Iteration 471, loss = 0.43898711\n",
      "Iteration 472, loss = 0.43901752\n",
      "Iteration 473, loss = 0.43887535\n",
      "Iteration 474, loss = 0.43888824\n",
      "Iteration 475, loss = 0.43875233\n",
      "Iteration 476, loss = 0.43877487\n",
      "Iteration 477, loss = 0.43866254\n",
      "Iteration 478, loss = 0.43855770\n",
      "Iteration 479, loss = 0.43850523\n",
      "Iteration 480, loss = 0.43849777\n",
      "Iteration 481, loss = 0.43835793\n",
      "Iteration 482, loss = 0.43833491\n",
      "Iteration 483, loss = 0.43827052\n",
      "Iteration 484, loss = 0.43821441\n",
      "Iteration 485, loss = 0.43807991\n",
      "Iteration 486, loss = 0.43798917\n",
      "Iteration 487, loss = 0.43792035\n",
      "Iteration 488, loss = 0.43785667\n",
      "Iteration 489, loss = 0.43769868\n",
      "Iteration 490, loss = 0.43771237\n",
      "Iteration 491, loss = 0.43756330\n",
      "Iteration 492, loss = 0.43740957\n",
      "Iteration 493, loss = 0.43732826\n",
      "Iteration 494, loss = 0.43734681\n",
      "Iteration 495, loss = 0.43717232\n",
      "Iteration 496, loss = 0.43697179\n",
      "Iteration 497, loss = 0.43696128\n",
      "Iteration 498, loss = 0.43681252\n",
      "Iteration 499, loss = 0.43670290\n",
      "Iteration 500, loss = 0.43661185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11212037\n",
      "Iteration 2, loss = 1.94956044\n",
      "Iteration 3, loss = 1.83327281\n",
      "Iteration 4, loss = 1.74455382\n",
      "Iteration 5, loss = 1.67202586\n",
      "Iteration 6, loss = 1.60911898\n",
      "Iteration 7, loss = 1.55207035\n",
      "Iteration 8, loss = 1.49874050\n",
      "Iteration 9, loss = 1.44806241\n",
      "Iteration 10, loss = 1.39973179\n",
      "Iteration 11, loss = 1.35365077\n",
      "Iteration 12, loss = 1.31005727\n",
      "Iteration 13, loss = 1.26909924\n",
      "Iteration 14, loss = 1.23091663\n",
      "Iteration 15, loss = 1.19553272\n",
      "Iteration 16, loss = 1.16283501\n",
      "Iteration 17, loss = 1.13260685\n",
      "Iteration 18, loss = 1.10463180\n",
      "Iteration 19, loss = 1.07869746\n",
      "Iteration 20, loss = 1.05460296\n",
      "Iteration 21, loss = 1.03210051\n",
      "Iteration 22, loss = 1.01102753\n",
      "Iteration 23, loss = 0.99122111\n",
      "Iteration 24, loss = 0.97250360\n",
      "Iteration 25, loss = 0.95475956\n",
      "Iteration 26, loss = 0.93790048\n",
      "Iteration 27, loss = 0.92187790\n",
      "Iteration 28, loss = 0.90659890\n",
      "Iteration 29, loss = 0.89202242\n",
      "Iteration 30, loss = 0.87815500\n",
      "Iteration 31, loss = 0.86487810\n",
      "Iteration 32, loss = 0.85218520\n",
      "Iteration 33, loss = 0.84002983\n",
      "Iteration 34, loss = 0.82839964\n",
      "Iteration 35, loss = 0.81718357\n",
      "Iteration 36, loss = 0.80645547\n",
      "Iteration 37, loss = 0.79613076\n",
      "Iteration 38, loss = 0.78620611\n",
      "Iteration 39, loss = 0.77667944\n",
      "Iteration 40, loss = 0.76750224\n",
      "Iteration 41, loss = 0.75866422\n",
      "Iteration 42, loss = 0.75014683\n",
      "Iteration 43, loss = 0.74199871\n",
      "Iteration 44, loss = 0.73416635\n",
      "Iteration 45, loss = 0.72660946\n",
      "Iteration 46, loss = 0.71941754\n",
      "Iteration 47, loss = 0.71246824\n",
      "Iteration 48, loss = 0.70577888\n",
      "Iteration 49, loss = 0.69933953\n",
      "Iteration 50, loss = 0.69317356\n",
      "Iteration 51, loss = 0.68725276\n",
      "Iteration 52, loss = 0.68154297\n",
      "Iteration 53, loss = 0.67606955\n",
      "Iteration 54, loss = 0.67080631\n",
      "Iteration 55, loss = 0.66571267\n",
      "Iteration 56, loss = 0.66080720\n",
      "Iteration 57, loss = 0.65609639\n",
      "Iteration 58, loss = 0.65154814\n",
      "Iteration 59, loss = 0.64712980\n",
      "Iteration 60, loss = 0.64286900\n",
      "Iteration 61, loss = 0.63880312\n",
      "Iteration 62, loss = 0.63484032\n",
      "Iteration 63, loss = 0.63100246\n",
      "Iteration 64, loss = 0.62729406\n",
      "Iteration 65, loss = 0.62371110\n",
      "Iteration 66, loss = 0.62019911\n",
      "Iteration 67, loss = 0.61682507\n",
      "Iteration 68, loss = 0.61353894\n",
      "Iteration 69, loss = 0.61034448\n",
      "Iteration 70, loss = 0.60725978\n",
      "Iteration 71, loss = 0.60425639\n",
      "Iteration 72, loss = 0.60141998\n",
      "Iteration 73, loss = 0.59848802\n",
      "Iteration 74, loss = 0.59577271\n",
      "Iteration 75, loss = 0.59308772\n",
      "Iteration 76, loss = 0.59048813\n",
      "Iteration 77, loss = 0.58794797\n",
      "Iteration 78, loss = 0.58545626\n",
      "Iteration 79, loss = 0.58301760\n",
      "Iteration 80, loss = 0.58066216\n",
      "Iteration 81, loss = 0.57836399\n",
      "Iteration 82, loss = 0.57614374\n",
      "Iteration 83, loss = 0.57395636\n",
      "Iteration 84, loss = 0.57181028\n",
      "Iteration 85, loss = 0.56971521\n",
      "Iteration 86, loss = 0.56770892\n",
      "Iteration 87, loss = 0.56569957\n",
      "Iteration 88, loss = 0.56371325\n",
      "Iteration 89, loss = 0.56181094\n",
      "Iteration 90, loss = 0.55996675\n",
      "Iteration 91, loss = 0.55807892\n",
      "Iteration 92, loss = 0.55631560\n",
      "Iteration 93, loss = 0.55456206\n",
      "Iteration 94, loss = 0.55279165\n",
      "Iteration 95, loss = 0.55114565\n",
      "Iteration 96, loss = 0.54950643\n",
      "Iteration 97, loss = 0.54787573\n",
      "Iteration 98, loss = 0.54626074\n",
      "Iteration 99, loss = 0.54474890\n",
      "Iteration 100, loss = 0.54317729\n",
      "Iteration 101, loss = 0.54170010\n",
      "Iteration 102, loss = 0.54020609\n",
      "Iteration 103, loss = 0.53881762\n",
      "Iteration 104, loss = 0.53734384\n",
      "Iteration 105, loss = 0.53597621\n",
      "Iteration 106, loss = 0.53458687\n",
      "Iteration 107, loss = 0.53326477\n",
      "Iteration 108, loss = 0.53196267\n",
      "Iteration 109, loss = 0.53064744\n",
      "Iteration 110, loss = 0.52942902\n",
      "Iteration 111, loss = 0.52817641\n",
      "Iteration 112, loss = 0.52693490\n",
      "Iteration 113, loss = 0.52569077\n",
      "Iteration 114, loss = 0.52448966\n",
      "Iteration 115, loss = 0.52330559\n",
      "Iteration 116, loss = 0.52221326\n",
      "Iteration 117, loss = 0.52106647\n",
      "Iteration 118, loss = 0.51991736\n",
      "Iteration 119, loss = 0.51880187\n",
      "Iteration 120, loss = 0.51772437\n",
      "Iteration 121, loss = 0.51668088\n",
      "Iteration 122, loss = 0.51562510\n",
      "Iteration 123, loss = 0.51463460\n",
      "Iteration 124, loss = 0.51362489\n",
      "Iteration 125, loss = 0.51265626\n",
      "Iteration 126, loss = 0.51164402\n",
      "Iteration 127, loss = 0.51067384\n",
      "Iteration 128, loss = 0.50975684\n",
      "Iteration 129, loss = 0.50887654\n",
      "Iteration 130, loss = 0.50792137\n",
      "Iteration 131, loss = 0.50705163\n",
      "Iteration 132, loss = 0.50615286\n",
      "Iteration 133, loss = 0.50521474\n",
      "Iteration 134, loss = 0.50444821\n",
      "Iteration 135, loss = 0.50358298\n",
      "Iteration 136, loss = 0.50282537\n",
      "Iteration 137, loss = 0.50192607\n",
      "Iteration 138, loss = 0.50108821\n",
      "Iteration 139, loss = 0.50037363\n",
      "Iteration 140, loss = 0.49954427\n",
      "Iteration 141, loss = 0.49885988\n",
      "Iteration 142, loss = 0.49805454\n",
      "Iteration 143, loss = 0.49736844\n",
      "Iteration 144, loss = 0.49661108\n",
      "Iteration 145, loss = 0.49594145\n",
      "Iteration 146, loss = 0.49516712\n",
      "Iteration 147, loss = 0.49453569\n",
      "Iteration 148, loss = 0.49385552\n",
      "Iteration 149, loss = 0.49320311\n",
      "Iteration 150, loss = 0.49255789\n",
      "Iteration 151, loss = 0.49178261\n",
      "Iteration 152, loss = 0.49115410\n",
      "Iteration 153, loss = 0.49058434\n",
      "Iteration 154, loss = 0.49000006\n",
      "Iteration 155, loss = 0.48938953\n",
      "Iteration 156, loss = 0.48870793\n",
      "Iteration 157, loss = 0.48813517\n",
      "Iteration 158, loss = 0.48760404\n",
      "Iteration 159, loss = 0.48692959\n",
      "Iteration 160, loss = 0.48639880\n",
      "Iteration 161, loss = 0.48584339\n",
      "Iteration 162, loss = 0.48528431\n",
      "Iteration 163, loss = 0.48477893\n",
      "Iteration 164, loss = 0.48418086\n",
      "Iteration 165, loss = 0.48363757\n",
      "Iteration 166, loss = 0.48309291\n",
      "Iteration 167, loss = 0.48265206\n",
      "Iteration 168, loss = 0.48214313\n",
      "Iteration 169, loss = 0.48166367\n",
      "Iteration 170, loss = 0.48113087\n",
      "Iteration 171, loss = 0.48065716\n",
      "Iteration 172, loss = 0.48014692\n",
      "Iteration 173, loss = 0.47974261\n",
      "Iteration 174, loss = 0.47925409\n",
      "Iteration 175, loss = 0.47876177\n",
      "Iteration 176, loss = 0.47837616\n",
      "Iteration 177, loss = 0.47794142\n",
      "Iteration 178, loss = 0.47748159\n",
      "Iteration 179, loss = 0.47700877\n",
      "Iteration 180, loss = 0.47660764\n",
      "Iteration 181, loss = 0.47626675\n",
      "Iteration 182, loss = 0.47572365\n",
      "Iteration 183, loss = 0.47534437\n",
      "Iteration 184, loss = 0.47497864\n",
      "Iteration 185, loss = 0.47459880\n",
      "Iteration 186, loss = 0.47417448\n",
      "Iteration 187, loss = 0.47377355\n",
      "Iteration 188, loss = 0.47339698\n",
      "Iteration 189, loss = 0.47300394\n",
      "Iteration 190, loss = 0.47271975\n",
      "Iteration 191, loss = 0.47229309\n",
      "Iteration 192, loss = 0.47199636\n",
      "Iteration 193, loss = 0.47158014\n",
      "Iteration 194, loss = 0.47121078\n",
      "Iteration 195, loss = 0.47083935\n",
      "Iteration 196, loss = 0.47051686\n",
      "Iteration 197, loss = 0.47021754\n",
      "Iteration 198, loss = 0.46986277\n",
      "Iteration 199, loss = 0.46956121\n",
      "Iteration 200, loss = 0.46923483\n",
      "Iteration 201, loss = 0.46893560\n",
      "Iteration 202, loss = 0.46853141\n",
      "Iteration 203, loss = 0.46824126\n",
      "Iteration 204, loss = 0.46796934\n",
      "Iteration 205, loss = 0.46762670\n",
      "Iteration 206, loss = 0.46731955\n",
      "Iteration 207, loss = 0.46701953\n",
      "Iteration 208, loss = 0.46676740\n",
      "Iteration 209, loss = 0.46646950\n",
      "Iteration 210, loss = 0.46622294\n",
      "Iteration 211, loss = 0.46585650\n",
      "Iteration 212, loss = 0.46562030\n",
      "Iteration 213, loss = 0.46534892\n",
      "Iteration 214, loss = 0.46502935\n",
      "Iteration 215, loss = 0.46476817\n",
      "Iteration 216, loss = 0.46452186\n",
      "Iteration 217, loss = 0.46430412\n",
      "Iteration 218, loss = 0.46397285\n",
      "Iteration 219, loss = 0.46373105\n",
      "Iteration 220, loss = 0.46344504\n",
      "Iteration 221, loss = 0.46316933\n",
      "Iteration 222, loss = 0.46298574\n",
      "Iteration 223, loss = 0.46272464\n",
      "Iteration 224, loss = 0.46249018\n",
      "Iteration 225, loss = 0.46219590\n",
      "Iteration 226, loss = 0.46201578\n",
      "Iteration 227, loss = 0.46179508\n",
      "Iteration 228, loss = 0.46157308\n",
      "Iteration 229, loss = 0.46129222\n",
      "Iteration 230, loss = 0.46107188\n",
      "Iteration 231, loss = 0.46089408\n",
      "Iteration 232, loss = 0.46062706\n",
      "Iteration 233, loss = 0.46036903\n",
      "Iteration 234, loss = 0.46019678\n",
      "Iteration 235, loss = 0.46005890\n",
      "Iteration 236, loss = 0.45979364\n",
      "Iteration 237, loss = 0.45955411\n",
      "Iteration 238, loss = 0.45936981\n",
      "Iteration 239, loss = 0.45915403\n",
      "Iteration 240, loss = 0.45892015\n",
      "Iteration 241, loss = 0.45880402\n",
      "Iteration 242, loss = 0.45849278\n",
      "Iteration 243, loss = 0.45838844\n",
      "Iteration 244, loss = 0.45814201\n",
      "Iteration 245, loss = 0.45796420\n",
      "Iteration 246, loss = 0.45779264\n",
      "Iteration 247, loss = 0.45756776\n",
      "Iteration 248, loss = 0.45744477\n",
      "Iteration 249, loss = 0.45722288\n",
      "Iteration 250, loss = 0.45699236\n",
      "Iteration 251, loss = 0.45684395\n",
      "Iteration 252, loss = 0.45662542\n",
      "Iteration 253, loss = 0.45649416\n",
      "Iteration 254, loss = 0.45625254\n",
      "Iteration 255, loss = 0.45609013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.45598351\n",
      "Iteration 257, loss = 0.45576778\n",
      "Iteration 258, loss = 0.45563414\n",
      "Iteration 259, loss = 0.45545343\n",
      "Iteration 260, loss = 0.45535918\n",
      "Iteration 261, loss = 0.45517207\n",
      "Iteration 262, loss = 0.45496385\n",
      "Iteration 263, loss = 0.45479755\n",
      "Iteration 264, loss = 0.45458853\n",
      "Iteration 265, loss = 0.45445041\n",
      "Iteration 266, loss = 0.45437722\n",
      "Iteration 267, loss = 0.45416712\n",
      "Iteration 268, loss = 0.45402996\n",
      "Iteration 269, loss = 0.45385269\n",
      "Iteration 270, loss = 0.45370446\n",
      "Iteration 271, loss = 0.45354395\n",
      "Iteration 272, loss = 0.45337064\n",
      "Iteration 273, loss = 0.45323231\n",
      "Iteration 274, loss = 0.45314388\n",
      "Iteration 275, loss = 0.45299596\n",
      "Iteration 276, loss = 0.45285746\n",
      "Iteration 277, loss = 0.45271239\n",
      "Iteration 278, loss = 0.45253012\n",
      "Iteration 279, loss = 0.45237339\n",
      "Iteration 280, loss = 0.45219674\n",
      "Iteration 281, loss = 0.45214747\n",
      "Iteration 282, loss = 0.45203122\n",
      "Iteration 283, loss = 0.45177978\n",
      "Iteration 284, loss = 0.45166071\n",
      "Iteration 285, loss = 0.45159560\n",
      "Iteration 286, loss = 0.45136979\n",
      "Iteration 287, loss = 0.45131953\n",
      "Iteration 288, loss = 0.45113670\n",
      "Iteration 289, loss = 0.45105969\n",
      "Iteration 290, loss = 0.45090638\n",
      "Iteration 291, loss = 0.45080206\n",
      "Iteration 292, loss = 0.45068432\n",
      "Iteration 293, loss = 0.45056952\n",
      "Iteration 294, loss = 0.45038922\n",
      "Iteration 295, loss = 0.45024214\n",
      "Iteration 296, loss = 0.45016846\n",
      "Iteration 297, loss = 0.45007647\n",
      "Iteration 298, loss = 0.44998610\n",
      "Iteration 299, loss = 0.44976728\n",
      "Iteration 300, loss = 0.44971262\n",
      "Iteration 301, loss = 0.44959588\n",
      "Iteration 302, loss = 0.44942463\n",
      "Iteration 303, loss = 0.44926673\n",
      "Iteration 304, loss = 0.44935559\n",
      "Iteration 305, loss = 0.44913598\n",
      "Iteration 306, loss = 0.44896714\n",
      "Iteration 307, loss = 0.44890112\n",
      "Iteration 308, loss = 0.44877183\n",
      "Iteration 309, loss = 0.44861303\n",
      "Iteration 310, loss = 0.44855361\n",
      "Iteration 311, loss = 0.44844451\n",
      "Iteration 312, loss = 0.44837448\n",
      "Iteration 313, loss = 0.44823058\n",
      "Iteration 314, loss = 0.44820193\n",
      "Iteration 315, loss = 0.44803047\n",
      "Iteration 316, loss = 0.44797440\n",
      "Iteration 317, loss = 0.44775633\n",
      "Iteration 318, loss = 0.44772181\n",
      "Iteration 319, loss = 0.44766677\n",
      "Iteration 320, loss = 0.44754038\n",
      "Iteration 321, loss = 0.44747842\n",
      "Iteration 322, loss = 0.44730327\n",
      "Iteration 323, loss = 0.44720961\n",
      "Iteration 324, loss = 0.44708469\n",
      "Iteration 325, loss = 0.44696698\n",
      "Iteration 326, loss = 0.44697229\n",
      "Iteration 327, loss = 0.44672582\n",
      "Iteration 328, loss = 0.44678367\n",
      "Iteration 329, loss = 0.44660812\n",
      "Iteration 330, loss = 0.44653091\n",
      "Iteration 331, loss = 0.44635790\n",
      "Iteration 332, loss = 0.44629842\n",
      "Iteration 333, loss = 0.44623052\n",
      "Iteration 334, loss = 0.44613637\n",
      "Iteration 335, loss = 0.44601050\n",
      "Iteration 336, loss = 0.44592550\n",
      "Iteration 337, loss = 0.44579473\n",
      "Iteration 338, loss = 0.44579627\n",
      "Iteration 339, loss = 0.44569202\n",
      "Iteration 340, loss = 0.44559995\n",
      "Iteration 341, loss = 0.44556514\n",
      "Iteration 342, loss = 0.44546571\n",
      "Iteration 343, loss = 0.44535206\n",
      "Iteration 344, loss = 0.44527805\n",
      "Iteration 345, loss = 0.44519415\n",
      "Iteration 346, loss = 0.44501755\n",
      "Iteration 347, loss = 0.44502921\n",
      "Iteration 348, loss = 0.44493768\n",
      "Iteration 349, loss = 0.44487147\n",
      "Iteration 350, loss = 0.44472775\n",
      "Iteration 351, loss = 0.44460649\n",
      "Iteration 352, loss = 0.44453302\n",
      "Iteration 353, loss = 0.44440335\n",
      "Iteration 354, loss = 0.44434681\n",
      "Iteration 355, loss = 0.44437959\n",
      "Iteration 356, loss = 0.44419736\n",
      "Iteration 357, loss = 0.44413361\n",
      "Iteration 358, loss = 0.44412152\n",
      "Iteration 359, loss = 0.44393917\n",
      "Iteration 360, loss = 0.44390290\n",
      "Iteration 361, loss = 0.44392518\n",
      "Iteration 362, loss = 0.44372684\n",
      "Iteration 363, loss = 0.44366467\n",
      "Iteration 364, loss = 0.44355691\n",
      "Iteration 365, loss = 0.44351696\n",
      "Iteration 366, loss = 0.44344994\n",
      "Iteration 367, loss = 0.44336321\n",
      "Iteration 368, loss = 0.44327391\n",
      "Iteration 369, loss = 0.44314758\n",
      "Iteration 370, loss = 0.44311650\n",
      "Iteration 371, loss = 0.44301267\n",
      "Iteration 372, loss = 0.44304135\n",
      "Iteration 373, loss = 0.44290494\n",
      "Iteration 374, loss = 0.44279499\n",
      "Iteration 375, loss = 0.44272484\n",
      "Iteration 376, loss = 0.44261561\n",
      "Iteration 377, loss = 0.44267574\n",
      "Iteration 378, loss = 0.44259124\n",
      "Iteration 379, loss = 0.44249121\n",
      "Iteration 380, loss = 0.44237736\n",
      "Iteration 381, loss = 0.44233160\n",
      "Iteration 382, loss = 0.44228842\n",
      "Iteration 383, loss = 0.44214178\n",
      "Iteration 384, loss = 0.44227379\n",
      "Iteration 385, loss = 0.44202466\n",
      "Iteration 386, loss = 0.44204921\n",
      "Iteration 387, loss = 0.44186286\n",
      "Iteration 388, loss = 0.44192688\n",
      "Iteration 389, loss = 0.44174421\n",
      "Iteration 390, loss = 0.44172576\n",
      "Iteration 391, loss = 0.44162924\n",
      "Iteration 392, loss = 0.44160716\n",
      "Iteration 393, loss = 0.44148814\n",
      "Iteration 394, loss = 0.44142739\n",
      "Iteration 395, loss = 0.44138361\n",
      "Iteration 396, loss = 0.44126789\n",
      "Iteration 397, loss = 0.44125836\n",
      "Iteration 398, loss = 0.44107478\n",
      "Iteration 399, loss = 0.44112697\n",
      "Iteration 400, loss = 0.44102725\n",
      "Iteration 401, loss = 0.44109400\n",
      "Iteration 402, loss = 0.44091312\n",
      "Iteration 403, loss = 0.44077419\n",
      "Iteration 404, loss = 0.44078226\n",
      "Iteration 405, loss = 0.44070239\n",
      "Iteration 406, loss = 0.44067840\n",
      "Iteration 407, loss = 0.44058145\n",
      "Iteration 408, loss = 0.44052538\n",
      "Iteration 409, loss = 0.44058284\n",
      "Iteration 410, loss = 0.44035025\n",
      "Iteration 411, loss = 0.44037425\n",
      "Iteration 412, loss = 0.44029317\n",
      "Iteration 413, loss = 0.44018919\n",
      "Iteration 414, loss = 0.44007951\n",
      "Iteration 415, loss = 0.44007046\n",
      "Iteration 416, loss = 0.44001699\n",
      "Iteration 417, loss = 0.44000346\n",
      "Iteration 418, loss = 0.43988872\n",
      "Iteration 419, loss = 0.43984663\n",
      "Iteration 420, loss = 0.43979399\n",
      "Iteration 421, loss = 0.43973317\n",
      "Iteration 422, loss = 0.43967981\n",
      "Iteration 423, loss = 0.43965195\n",
      "Iteration 424, loss = 0.43956553\n",
      "Iteration 425, loss = 0.43945928\n",
      "Iteration 426, loss = 0.43947155\n",
      "Iteration 427, loss = 0.43941253\n",
      "Iteration 428, loss = 0.43939488\n",
      "Iteration 429, loss = 0.43928229\n",
      "Iteration 430, loss = 0.43924985\n",
      "Iteration 431, loss = 0.43922527\n",
      "Iteration 432, loss = 0.43907289\n",
      "Iteration 433, loss = 0.43906762\n",
      "Iteration 434, loss = 0.43914107\n",
      "Iteration 435, loss = 0.43897542\n",
      "Iteration 436, loss = 0.43896141\n",
      "Iteration 437, loss = 0.43877991\n",
      "Iteration 438, loss = 0.43870072\n",
      "Iteration 439, loss = 0.43875535\n",
      "Iteration 440, loss = 0.43870086\n",
      "Iteration 441, loss = 0.43857999\n",
      "Iteration 442, loss = 0.43862937\n",
      "Iteration 443, loss = 0.43852212\n",
      "Iteration 444, loss = 0.43840358\n",
      "Iteration 445, loss = 0.43838640\n",
      "Iteration 446, loss = 0.43833681\n",
      "Iteration 447, loss = 0.43826213\n",
      "Iteration 448, loss = 0.43827754\n",
      "Iteration 449, loss = 0.43818709\n",
      "Iteration 450, loss = 0.43809528\n",
      "Iteration 451, loss = 0.43803457\n",
      "Iteration 452, loss = 0.43808672\n",
      "Iteration 453, loss = 0.43798301\n",
      "Iteration 454, loss = 0.43784965\n",
      "Iteration 455, loss = 0.43781091\n",
      "Iteration 456, loss = 0.43780733\n",
      "Iteration 457, loss = 0.43780568\n",
      "Iteration 458, loss = 0.43775358\n",
      "Iteration 459, loss = 0.43767437\n",
      "Iteration 460, loss = 0.43759861\n",
      "Iteration 461, loss = 0.43762257\n",
      "Iteration 462, loss = 0.43757193\n",
      "Iteration 463, loss = 0.43757342\n",
      "Iteration 464, loss = 0.43746231\n",
      "Iteration 465, loss = 0.43739766\n",
      "Iteration 466, loss = 0.43730197\n",
      "Iteration 467, loss = 0.43727514\n",
      "Iteration 468, loss = 0.43724261\n",
      "Iteration 469, loss = 0.43719502\n",
      "Iteration 470, loss = 0.43717155\n",
      "Iteration 471, loss = 0.43703884\n",
      "Iteration 472, loss = 0.43697751\n",
      "Iteration 473, loss = 0.43698561\n",
      "Iteration 474, loss = 0.43688417\n",
      "Iteration 475, loss = 0.43685055\n",
      "Iteration 476, loss = 0.43683698\n",
      "Iteration 477, loss = 0.43668539\n",
      "Iteration 478, loss = 0.43663598\n",
      "Iteration 479, loss = 0.43667112\n",
      "Iteration 480, loss = 0.43661167\n",
      "Iteration 481, loss = 0.43655690\n",
      "Iteration 482, loss = 0.43654854\n",
      "Iteration 483, loss = 0.43645362\n",
      "Iteration 484, loss = 0.43639942\n",
      "Iteration 485, loss = 0.43632246\n",
      "Iteration 486, loss = 0.43631961\n",
      "Iteration 487, loss = 0.43635429\n",
      "Iteration 488, loss = 0.43621436\n",
      "Iteration 489, loss = 0.43616659\n",
      "Iteration 490, loss = 0.43604744\n",
      "Iteration 491, loss = 0.43596258\n",
      "Iteration 492, loss = 0.43604105\n",
      "Iteration 493, loss = 0.43590552\n",
      "Iteration 494, loss = 0.43592460\n",
      "Iteration 495, loss = 0.43578963\n",
      "Iteration 496, loss = 0.43572939\n",
      "Iteration 497, loss = 0.43570425\n",
      "Iteration 498, loss = 0.43560752\n",
      "Iteration 499, loss = 0.43548188\n",
      "Iteration 500, loss = 0.43548600\n",
      "Iteration 1, loss = 2.10971618\n",
      "Iteration 2, loss = 1.94779156\n",
      "Iteration 3, loss = 1.83214671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 1.74386814\n",
      "Iteration 5, loss = 1.67160289\n",
      "Iteration 6, loss = 1.60894071\n",
      "Iteration 7, loss = 1.55210654\n",
      "Iteration 8, loss = 1.49902760\n",
      "Iteration 9, loss = 1.44857260\n",
      "Iteration 10, loss = 1.40040272\n",
      "Iteration 11, loss = 1.35448597\n",
      "Iteration 12, loss = 1.31098903\n",
      "Iteration 13, loss = 1.27013250\n",
      "Iteration 14, loss = 1.23203044\n",
      "Iteration 15, loss = 1.19668783\n",
      "Iteration 16, loss = 1.16404196\n",
      "Iteration 17, loss = 1.13390000\n",
      "Iteration 18, loss = 1.10596385\n",
      "Iteration 19, loss = 1.08009975\n",
      "Iteration 20, loss = 1.05608922\n",
      "Iteration 21, loss = 1.03369422\n",
      "Iteration 22, loss = 1.01269516\n",
      "Iteration 23, loss = 0.99299436\n",
      "Iteration 24, loss = 0.97434403\n",
      "Iteration 25, loss = 0.95670691\n",
      "Iteration 26, loss = 0.93993535\n",
      "Iteration 27, loss = 0.92396019\n",
      "Iteration 28, loss = 0.90878214\n",
      "Iteration 29, loss = 0.89431759\n",
      "Iteration 30, loss = 0.88049078\n",
      "Iteration 31, loss = 0.86724517\n",
      "Iteration 32, loss = 0.85459028\n",
      "Iteration 33, loss = 0.84240843\n",
      "Iteration 34, loss = 0.83071332\n",
      "Iteration 35, loss = 0.81947980\n",
      "Iteration 36, loss = 0.80868636\n",
      "Iteration 37, loss = 0.79830694\n",
      "Iteration 38, loss = 0.78829121\n",
      "Iteration 39, loss = 0.77866302\n",
      "Iteration 40, loss = 0.76936625\n",
      "Iteration 41, loss = 0.76044715\n",
      "Iteration 42, loss = 0.75182958\n",
      "Iteration 43, loss = 0.74358758\n",
      "Iteration 44, loss = 0.73566219\n",
      "Iteration 45, loss = 0.72805620\n",
      "Iteration 46, loss = 0.72073899\n",
      "Iteration 47, loss = 0.71371671\n",
      "Iteration 48, loss = 0.70699895\n",
      "Iteration 49, loss = 0.70053271\n",
      "Iteration 50, loss = 0.69429285\n",
      "Iteration 51, loss = 0.68830132\n",
      "Iteration 52, loss = 0.68254372\n",
      "Iteration 53, loss = 0.67701806\n",
      "Iteration 54, loss = 0.67171601\n",
      "Iteration 55, loss = 0.66657382\n",
      "Iteration 56, loss = 0.66162304\n",
      "Iteration 57, loss = 0.65685221\n",
      "Iteration 58, loss = 0.65224103\n",
      "Iteration 59, loss = 0.64781818\n",
      "Iteration 60, loss = 0.64353798\n",
      "Iteration 61, loss = 0.63936926\n",
      "Iteration 62, loss = 0.63537862\n",
      "Iteration 63, loss = 0.63149343\n",
      "Iteration 64, loss = 0.62774776\n",
      "Iteration 65, loss = 0.62410444\n",
      "Iteration 66, loss = 0.62060161\n",
      "Iteration 67, loss = 0.61716417\n",
      "Iteration 68, loss = 0.61382348\n",
      "Iteration 69, loss = 0.61062908\n",
      "Iteration 70, loss = 0.60749819\n",
      "Iteration 71, loss = 0.60450114\n",
      "Iteration 72, loss = 0.60151498\n",
      "Iteration 73, loss = 0.59867176\n",
      "Iteration 74, loss = 0.59591496\n",
      "Iteration 75, loss = 0.59318478\n",
      "Iteration 76, loss = 0.59059445\n",
      "Iteration 77, loss = 0.58796123\n",
      "Iteration 78, loss = 0.58547265\n",
      "Iteration 79, loss = 0.58303342\n",
      "Iteration 80, loss = 0.58069456\n",
      "Iteration 81, loss = 0.57833426\n",
      "Iteration 82, loss = 0.57611712\n",
      "Iteration 83, loss = 0.57390789\n",
      "Iteration 84, loss = 0.57172503\n",
      "Iteration 85, loss = 0.56963372\n",
      "Iteration 86, loss = 0.56753635\n",
      "Iteration 87, loss = 0.56563135\n",
      "Iteration 88, loss = 0.56362090\n",
      "Iteration 89, loss = 0.56166580\n",
      "Iteration 90, loss = 0.55980012\n",
      "Iteration 91, loss = 0.55796545\n",
      "Iteration 92, loss = 0.55617771\n",
      "Iteration 93, loss = 0.55439607\n",
      "Iteration 94, loss = 0.55267536\n",
      "Iteration 95, loss = 0.55101138\n",
      "Iteration 96, loss = 0.54934144\n",
      "Iteration 97, loss = 0.54771874\n",
      "Iteration 98, loss = 0.54614365\n",
      "Iteration 99, loss = 0.54457397\n",
      "Iteration 100, loss = 0.54306969\n",
      "Iteration 101, loss = 0.54156214\n",
      "Iteration 102, loss = 0.54009902\n",
      "Iteration 103, loss = 0.53860772\n",
      "Iteration 104, loss = 0.53719446\n",
      "Iteration 105, loss = 0.53586272\n",
      "Iteration 106, loss = 0.53449887\n",
      "Iteration 107, loss = 0.53314689\n",
      "Iteration 108, loss = 0.53191394\n",
      "Iteration 109, loss = 0.53060998\n",
      "Iteration 110, loss = 0.52932090\n",
      "Iteration 111, loss = 0.52806118\n",
      "Iteration 112, loss = 0.52686129\n",
      "Iteration 113, loss = 0.52565146\n",
      "Iteration 114, loss = 0.52445556\n",
      "Iteration 115, loss = 0.52332413\n",
      "Iteration 116, loss = 0.52222336\n",
      "Iteration 117, loss = 0.52101449\n",
      "Iteration 118, loss = 0.51999889\n",
      "Iteration 119, loss = 0.51889794\n",
      "Iteration 120, loss = 0.51781565\n",
      "Iteration 121, loss = 0.51679231\n",
      "Iteration 122, loss = 0.51575859\n",
      "Iteration 123, loss = 0.51481806\n",
      "Iteration 124, loss = 0.51379868\n",
      "Iteration 125, loss = 0.51287180\n",
      "Iteration 126, loss = 0.51188513\n",
      "Iteration 127, loss = 0.51093227\n",
      "Iteration 128, loss = 0.51003800\n",
      "Iteration 129, loss = 0.50911041\n",
      "Iteration 130, loss = 0.50821781\n",
      "Iteration 131, loss = 0.50736586\n",
      "Iteration 132, loss = 0.50648327\n",
      "Iteration 133, loss = 0.50565088\n",
      "Iteration 134, loss = 0.50483888\n",
      "Iteration 135, loss = 0.50402640\n",
      "Iteration 136, loss = 0.50323614\n",
      "Iteration 137, loss = 0.50241092\n",
      "Iteration 138, loss = 0.50159190\n",
      "Iteration 139, loss = 0.50082812\n",
      "Iteration 140, loss = 0.50005289\n",
      "Iteration 141, loss = 0.49929704\n",
      "Iteration 142, loss = 0.49863911\n",
      "Iteration 143, loss = 0.49793032\n",
      "Iteration 144, loss = 0.49716905\n",
      "Iteration 145, loss = 0.49649967\n",
      "Iteration 146, loss = 0.49580189\n",
      "Iteration 147, loss = 0.49515426\n",
      "Iteration 148, loss = 0.49447016\n",
      "Iteration 149, loss = 0.49383684\n",
      "Iteration 150, loss = 0.49312881\n",
      "Iteration 151, loss = 0.49258389\n",
      "Iteration 152, loss = 0.49188166\n",
      "Iteration 153, loss = 0.49133950\n",
      "Iteration 154, loss = 0.49070326\n",
      "Iteration 155, loss = 0.49018386\n",
      "Iteration 156, loss = 0.48947594\n",
      "Iteration 157, loss = 0.48898224\n",
      "Iteration 158, loss = 0.48841108\n",
      "Iteration 159, loss = 0.48785705\n",
      "Iteration 160, loss = 0.48725433\n",
      "Iteration 161, loss = 0.48673824\n",
      "Iteration 162, loss = 0.48619181\n",
      "Iteration 163, loss = 0.48566906\n",
      "Iteration 164, loss = 0.48515081\n",
      "Iteration 165, loss = 0.48462911\n",
      "Iteration 166, loss = 0.48417512\n",
      "Iteration 167, loss = 0.48366514\n",
      "Iteration 168, loss = 0.48319222\n",
      "Iteration 169, loss = 0.48276857\n",
      "Iteration 170, loss = 0.48226723\n",
      "Iteration 171, loss = 0.48173582\n",
      "Iteration 172, loss = 0.48131999\n",
      "Iteration 173, loss = 0.48079499\n",
      "Iteration 174, loss = 0.48039283\n",
      "Iteration 175, loss = 0.47999583\n",
      "Iteration 176, loss = 0.47955731\n",
      "Iteration 177, loss = 0.47912191\n",
      "Iteration 178, loss = 0.47869523\n",
      "Iteration 179, loss = 0.47821116\n",
      "Iteration 180, loss = 0.47778120\n",
      "Iteration 181, loss = 0.47751066\n",
      "Iteration 182, loss = 0.47706198\n",
      "Iteration 183, loss = 0.47666187\n",
      "Iteration 184, loss = 0.47627031\n",
      "Iteration 185, loss = 0.47590200\n",
      "Iteration 186, loss = 0.47551290\n",
      "Iteration 187, loss = 0.47518666\n",
      "Iteration 188, loss = 0.47481416\n",
      "Iteration 189, loss = 0.47448632\n",
      "Iteration 190, loss = 0.47404966\n",
      "Iteration 191, loss = 0.47372882\n",
      "Iteration 192, loss = 0.47344442\n",
      "Iteration 193, loss = 0.47301104\n",
      "Iteration 194, loss = 0.47267966\n",
      "Iteration 195, loss = 0.47242624\n",
      "Iteration 196, loss = 0.47204035\n",
      "Iteration 197, loss = 0.47175182\n",
      "Iteration 198, loss = 0.47139019\n",
      "Iteration 199, loss = 0.47112569\n",
      "Iteration 200, loss = 0.47084863\n",
      "Iteration 201, loss = 0.47045314\n",
      "Iteration 202, loss = 0.47017233\n",
      "Iteration 203, loss = 0.46984490\n",
      "Iteration 204, loss = 0.46957366\n",
      "Iteration 205, loss = 0.46931467\n",
      "Iteration 206, loss = 0.46898724\n",
      "Iteration 207, loss = 0.46872820\n",
      "Iteration 208, loss = 0.46843120\n",
      "Iteration 209, loss = 0.46812968\n",
      "Iteration 210, loss = 0.46787494\n",
      "Iteration 211, loss = 0.46761828\n",
      "Iteration 212, loss = 0.46732739\n",
      "Iteration 213, loss = 0.46715823\n",
      "Iteration 214, loss = 0.46680668\n",
      "Iteration 215, loss = 0.46655263\n",
      "Iteration 216, loss = 0.46631072\n",
      "Iteration 217, loss = 0.46607312\n",
      "Iteration 218, loss = 0.46578613\n",
      "Iteration 219, loss = 0.46557495\n",
      "Iteration 220, loss = 0.46538921\n",
      "Iteration 221, loss = 0.46503808\n",
      "Iteration 222, loss = 0.46483640\n",
      "Iteration 223, loss = 0.46460839\n",
      "Iteration 224, loss = 0.46441948\n",
      "Iteration 225, loss = 0.46416292\n",
      "Iteration 226, loss = 0.46388910\n",
      "Iteration 227, loss = 0.46373480\n",
      "Iteration 228, loss = 0.46346022\n",
      "Iteration 229, loss = 0.46326662\n",
      "Iteration 230, loss = 0.46301566\n",
      "Iteration 231, loss = 0.46286403\n",
      "Iteration 232, loss = 0.46261787\n",
      "Iteration 233, loss = 0.46243768\n",
      "Iteration 234, loss = 0.46219143\n",
      "Iteration 235, loss = 0.46198632\n",
      "Iteration 236, loss = 0.46179379\n",
      "Iteration 237, loss = 0.46166235\n",
      "Iteration 238, loss = 0.46135133\n",
      "Iteration 239, loss = 0.46123278\n",
      "Iteration 240, loss = 0.46104039\n",
      "Iteration 241, loss = 0.46084299\n",
      "Iteration 242, loss = 0.46070080\n",
      "Iteration 243, loss = 0.46047960\n",
      "Iteration 244, loss = 0.46031987\n",
      "Iteration 245, loss = 0.46007201\n",
      "Iteration 246, loss = 0.45987236\n",
      "Iteration 247, loss = 0.45971759\n",
      "Iteration 248, loss = 0.45956995\n",
      "Iteration 249, loss = 0.45933694\n",
      "Iteration 250, loss = 0.45923794\n",
      "Iteration 251, loss = 0.45902356\n",
      "Iteration 252, loss = 0.45893209\n",
      "Iteration 253, loss = 0.45869961\n",
      "Iteration 254, loss = 0.45846716\n",
      "Iteration 255, loss = 0.45835398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.45829935\n",
      "Iteration 257, loss = 0.45805868\n",
      "Iteration 258, loss = 0.45792516\n",
      "Iteration 259, loss = 0.45767694\n",
      "Iteration 260, loss = 0.45758960\n",
      "Iteration 261, loss = 0.45746416\n",
      "Iteration 262, loss = 0.45723217\n",
      "Iteration 263, loss = 0.45710272\n",
      "Iteration 264, loss = 0.45690983\n",
      "Iteration 265, loss = 0.45678374\n",
      "Iteration 266, loss = 0.45673104\n",
      "Iteration 267, loss = 0.45655611\n",
      "Iteration 268, loss = 0.45632589\n",
      "Iteration 269, loss = 0.45615866\n",
      "Iteration 270, loss = 0.45600231\n",
      "Iteration 271, loss = 0.45593047\n",
      "Iteration 272, loss = 0.45573879\n",
      "Iteration 273, loss = 0.45567793\n",
      "Iteration 274, loss = 0.45552045\n",
      "Iteration 275, loss = 0.45531122\n",
      "Iteration 276, loss = 0.45519471\n",
      "Iteration 277, loss = 0.45509021\n",
      "Iteration 278, loss = 0.45498595\n",
      "Iteration 279, loss = 0.45491181\n",
      "Iteration 280, loss = 0.45466544\n",
      "Iteration 281, loss = 0.45456263\n",
      "Iteration 282, loss = 0.45437605\n",
      "Iteration 283, loss = 0.45425957\n",
      "Iteration 284, loss = 0.45412414\n",
      "Iteration 285, loss = 0.45401189\n",
      "Iteration 286, loss = 0.45397094\n",
      "Iteration 287, loss = 0.45385922\n",
      "Iteration 288, loss = 0.45363556\n",
      "Iteration 289, loss = 0.45355174\n",
      "Iteration 290, loss = 0.45339327\n",
      "Iteration 291, loss = 0.45335070\n",
      "Iteration 292, loss = 0.45324048\n",
      "Iteration 293, loss = 0.45300642\n",
      "Iteration 294, loss = 0.45301155\n",
      "Iteration 295, loss = 0.45284198\n",
      "Iteration 296, loss = 0.45273183\n",
      "Iteration 297, loss = 0.45262754\n",
      "Iteration 298, loss = 0.45244397\n",
      "Iteration 299, loss = 0.45238307\n",
      "Iteration 300, loss = 0.45222301\n",
      "Iteration 301, loss = 0.45207836\n",
      "Iteration 302, loss = 0.45203199\n",
      "Iteration 303, loss = 0.45188680\n",
      "Iteration 304, loss = 0.45180431\n",
      "Iteration 305, loss = 0.45166919\n",
      "Iteration 306, loss = 0.45163864\n",
      "Iteration 307, loss = 0.45155902\n",
      "Iteration 308, loss = 0.45132162\n",
      "Iteration 309, loss = 0.45123194\n",
      "Iteration 310, loss = 0.45115614\n",
      "Iteration 311, loss = 0.45105608\n",
      "Iteration 312, loss = 0.45093529\n",
      "Iteration 313, loss = 0.45088078\n",
      "Iteration 314, loss = 0.45079740\n",
      "Iteration 315, loss = 0.45060554\n",
      "Iteration 316, loss = 0.45054583\n",
      "Iteration 317, loss = 0.45039462\n",
      "Iteration 318, loss = 0.45039213\n",
      "Iteration 319, loss = 0.45031325\n",
      "Iteration 320, loss = 0.45016241\n",
      "Iteration 321, loss = 0.45003111\n",
      "Iteration 322, loss = 0.44994130\n",
      "Iteration 323, loss = 0.44987897\n",
      "Iteration 324, loss = 0.44980095\n",
      "Iteration 325, loss = 0.44969533\n",
      "Iteration 326, loss = 0.44955271\n",
      "Iteration 327, loss = 0.44953569\n",
      "Iteration 328, loss = 0.44943434\n",
      "Iteration 329, loss = 0.44933510\n",
      "Iteration 330, loss = 0.44915080\n",
      "Iteration 331, loss = 0.44910848\n",
      "Iteration 332, loss = 0.44899809\n",
      "Iteration 333, loss = 0.44893712\n",
      "Iteration 334, loss = 0.44888019\n",
      "Iteration 335, loss = 0.44874638\n",
      "Iteration 336, loss = 0.44868369\n",
      "Iteration 337, loss = 0.44855254\n",
      "Iteration 338, loss = 0.44850399\n",
      "Iteration 339, loss = 0.44835016\n",
      "Iteration 340, loss = 0.44837957\n",
      "Iteration 341, loss = 0.44824297\n",
      "Iteration 342, loss = 0.44810997\n",
      "Iteration 343, loss = 0.44815350\n",
      "Iteration 344, loss = 0.44796971\n",
      "Iteration 345, loss = 0.44784334\n",
      "Iteration 346, loss = 0.44778428\n",
      "Iteration 347, loss = 0.44770808\n",
      "Iteration 348, loss = 0.44765836\n",
      "Iteration 349, loss = 0.44749503\n",
      "Iteration 350, loss = 0.44739590\n",
      "Iteration 351, loss = 0.44747775\n",
      "Iteration 352, loss = 0.44723656\n",
      "Iteration 353, loss = 0.44724467\n",
      "Iteration 354, loss = 0.44722617\n",
      "Iteration 355, loss = 0.44704977\n",
      "Iteration 356, loss = 0.44700509\n",
      "Iteration 357, loss = 0.44688407\n",
      "Iteration 358, loss = 0.44682868\n",
      "Iteration 359, loss = 0.44684945\n",
      "Iteration 360, loss = 0.44669478\n",
      "Iteration 361, loss = 0.44670540\n",
      "Iteration 362, loss = 0.44656008\n",
      "Iteration 363, loss = 0.44642189\n",
      "Iteration 364, loss = 0.44639924\n",
      "Iteration 365, loss = 0.44633468\n",
      "Iteration 366, loss = 0.44634535\n",
      "Iteration 367, loss = 0.44620498\n",
      "Iteration 368, loss = 0.44608725\n",
      "Iteration 369, loss = 0.44602201\n",
      "Iteration 370, loss = 0.44596029\n",
      "Iteration 371, loss = 0.44590992\n",
      "Iteration 372, loss = 0.44584642\n",
      "Iteration 373, loss = 0.44578316\n",
      "Iteration 374, loss = 0.44571908\n",
      "Iteration 375, loss = 0.44565308\n",
      "Iteration 376, loss = 0.44557026\n",
      "Iteration 377, loss = 0.44541909\n",
      "Iteration 378, loss = 0.44538761\n",
      "Iteration 379, loss = 0.44534572\n",
      "Iteration 380, loss = 0.44524277\n",
      "Iteration 381, loss = 0.44520298\n",
      "Iteration 382, loss = 0.44512543\n",
      "Iteration 383, loss = 0.44511901\n",
      "Iteration 384, loss = 0.44503847\n",
      "Iteration 385, loss = 0.44489056\n",
      "Iteration 386, loss = 0.44488220\n",
      "Iteration 387, loss = 0.44474549\n",
      "Iteration 388, loss = 0.44470692\n",
      "Iteration 389, loss = 0.44465421\n",
      "Iteration 390, loss = 0.44457739\n",
      "Iteration 391, loss = 0.44445882\n",
      "Iteration 392, loss = 0.44447391\n",
      "Iteration 393, loss = 0.44432152\n",
      "Iteration 394, loss = 0.44429904\n",
      "Iteration 395, loss = 0.44431849\n",
      "Iteration 396, loss = 0.44417017\n",
      "Iteration 397, loss = 0.44426050\n",
      "Iteration 398, loss = 0.44410192\n",
      "Iteration 399, loss = 0.44403831\n",
      "Iteration 400, loss = 0.44396720\n",
      "Iteration 401, loss = 0.44383898\n",
      "Iteration 402, loss = 0.44384336\n",
      "Iteration 403, loss = 0.44380241\n",
      "Iteration 404, loss = 0.44367384\n",
      "Iteration 405, loss = 0.44366454\n",
      "Iteration 406, loss = 0.44360585\n",
      "Iteration 407, loss = 0.44362350\n",
      "Iteration 408, loss = 0.44352351\n",
      "Iteration 409, loss = 0.44336865\n",
      "Iteration 410, loss = 0.44329979\n",
      "Iteration 411, loss = 0.44329351\n",
      "Iteration 412, loss = 0.44330361\n",
      "Iteration 413, loss = 0.44318319\n",
      "Iteration 414, loss = 0.44317933\n",
      "Iteration 415, loss = 0.44308520\n",
      "Iteration 416, loss = 0.44305540\n",
      "Iteration 417, loss = 0.44290377\n",
      "Iteration 418, loss = 0.44287796\n",
      "Iteration 419, loss = 0.44278274\n",
      "Iteration 420, loss = 0.44279491\n",
      "Iteration 421, loss = 0.44265724\n",
      "Iteration 422, loss = 0.44269163\n",
      "Iteration 423, loss = 0.44260949\n",
      "Iteration 424, loss = 0.44255667\n",
      "Iteration 425, loss = 0.44248500\n",
      "Iteration 426, loss = 0.44246939\n",
      "Iteration 427, loss = 0.44234661\n",
      "Iteration 428, loss = 0.44234226\n",
      "Iteration 429, loss = 0.44225190\n",
      "Iteration 430, loss = 0.44223014\n",
      "Iteration 431, loss = 0.44220889\n",
      "Iteration 432, loss = 0.44211156\n",
      "Iteration 433, loss = 0.44210481\n",
      "Iteration 434, loss = 0.44197495\n",
      "Iteration 435, loss = 0.44196357\n",
      "Iteration 436, loss = 0.44198922\n",
      "Iteration 437, loss = 0.44189842\n",
      "Iteration 438, loss = 0.44175497\n",
      "Iteration 439, loss = 0.44173218\n",
      "Iteration 440, loss = 0.44164863\n",
      "Iteration 441, loss = 0.44165427\n",
      "Iteration 442, loss = 0.44160988\n",
      "Iteration 443, loss = 0.44158245\n",
      "Iteration 444, loss = 0.44142690\n",
      "Iteration 445, loss = 0.44143592\n",
      "Iteration 446, loss = 0.44137286\n",
      "Iteration 447, loss = 0.44128162\n",
      "Iteration 448, loss = 0.44127775\n",
      "Iteration 449, loss = 0.44127208\n",
      "Iteration 450, loss = 0.44122768\n",
      "Iteration 451, loss = 0.44116026\n",
      "Iteration 452, loss = 0.44101418\n",
      "Iteration 453, loss = 0.44098824\n",
      "Iteration 454, loss = 0.44105894\n",
      "Iteration 455, loss = 0.44090155\n",
      "Iteration 456, loss = 0.44091926\n",
      "Iteration 457, loss = 0.44090284\n",
      "Iteration 458, loss = 0.44077627\n",
      "Iteration 459, loss = 0.44067779\n",
      "Iteration 460, loss = 0.44070310\n",
      "Iteration 461, loss = 0.44070278\n",
      "Iteration 462, loss = 0.44055497\n",
      "Iteration 463, loss = 0.44057405\n",
      "Iteration 464, loss = 0.44042807\n",
      "Iteration 465, loss = 0.44037445\n",
      "Iteration 466, loss = 0.44033709\n",
      "Iteration 467, loss = 0.44038683\n",
      "Iteration 468, loss = 0.44027135\n",
      "Iteration 469, loss = 0.44024009\n",
      "Iteration 470, loss = 0.44010984\n",
      "Iteration 471, loss = 0.44006123\n",
      "Iteration 472, loss = 0.44016703\n",
      "Iteration 473, loss = 0.44008632\n",
      "Iteration 474, loss = 0.43996755\n",
      "Iteration 475, loss = 0.43990796\n",
      "Iteration 476, loss = 0.43982214\n",
      "Iteration 477, loss = 0.43984099\n",
      "Iteration 478, loss = 0.43979599\n",
      "Iteration 479, loss = 0.43976666\n",
      "Iteration 480, loss = 0.43966528\n",
      "Iteration 481, loss = 0.43963561\n",
      "Iteration 482, loss = 0.43960607\n",
      "Iteration 483, loss = 0.43954010\n",
      "Iteration 484, loss = 0.43949069\n",
      "Iteration 485, loss = 0.43945458\n",
      "Iteration 486, loss = 0.43939467\n",
      "Iteration 487, loss = 0.43935447\n",
      "Iteration 488, loss = 0.43941101\n",
      "Iteration 489, loss = 0.43933524\n",
      "Iteration 490, loss = 0.43915987\n",
      "Iteration 491, loss = 0.43916783\n",
      "Iteration 492, loss = 0.43907368\n",
      "Iteration 493, loss = 0.43901898\n",
      "Iteration 494, loss = 0.43896337\n",
      "Iteration 495, loss = 0.43882813\n",
      "Iteration 496, loss = 0.43881919\n",
      "Iteration 497, loss = 0.43875583\n",
      "Iteration 498, loss = 0.43864628\n",
      "Iteration 499, loss = 0.43866298\n",
      "Iteration 500, loss = 0.43860796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11023316\n",
      "Iteration 2, loss = 1.94839930\n",
      "Iteration 3, loss = 1.83271405\n",
      "Iteration 4, loss = 1.74437347\n",
      "Iteration 5, loss = 1.67211967\n",
      "Iteration 6, loss = 1.60940792\n",
      "Iteration 7, loss = 1.55250328\n",
      "Iteration 8, loss = 1.49926212\n",
      "Iteration 9, loss = 1.44874593\n",
      "Iteration 10, loss = 1.40054517\n",
      "Iteration 11, loss = 1.35462105\n",
      "Iteration 12, loss = 1.31124766\n",
      "Iteration 13, loss = 1.27053201\n",
      "Iteration 14, loss = 1.23260828\n",
      "Iteration 15, loss = 1.19746649\n",
      "Iteration 16, loss = 1.16497026\n",
      "Iteration 17, loss = 1.13488544\n",
      "Iteration 18, loss = 1.10699824\n",
      "Iteration 19, loss = 1.08114401\n",
      "Iteration 20, loss = 1.05711219\n",
      "Iteration 21, loss = 1.03473036\n",
      "Iteration 22, loss = 1.01373853\n",
      "Iteration 23, loss = 0.99397151\n",
      "Iteration 24, loss = 0.97535158\n",
      "Iteration 25, loss = 0.95772628\n",
      "Iteration 26, loss = 0.94096943\n",
      "Iteration 27, loss = 0.92499653\n",
      "Iteration 28, loss = 0.90978035\n",
      "Iteration 29, loss = 0.89526109\n",
      "Iteration 30, loss = 0.88140659\n",
      "Iteration 31, loss = 0.86814870\n",
      "Iteration 32, loss = 0.85543347\n",
      "Iteration 33, loss = 0.84325279\n",
      "Iteration 34, loss = 0.83152192\n",
      "Iteration 35, loss = 0.82025310\n",
      "Iteration 36, loss = 0.80943201\n",
      "Iteration 37, loss = 0.79902073\n",
      "Iteration 38, loss = 0.78898782\n",
      "Iteration 39, loss = 0.77930474\n",
      "Iteration 40, loss = 0.77002189\n",
      "Iteration 41, loss = 0.76107052\n",
      "Iteration 42, loss = 0.75245656\n",
      "Iteration 43, loss = 0.74420513\n",
      "Iteration 44, loss = 0.73624952\n",
      "Iteration 45, loss = 0.72862784\n",
      "Iteration 46, loss = 0.72131319\n",
      "Iteration 47, loss = 0.71428761\n",
      "Iteration 48, loss = 0.70753687\n",
      "Iteration 49, loss = 0.70106413\n",
      "Iteration 50, loss = 0.69481302\n",
      "Iteration 51, loss = 0.68881614\n",
      "Iteration 52, loss = 0.68305315\n",
      "Iteration 53, loss = 0.67749965\n",
      "Iteration 54, loss = 0.67219295\n",
      "Iteration 55, loss = 0.66707242\n",
      "Iteration 56, loss = 0.66208861\n",
      "Iteration 57, loss = 0.65733653\n",
      "Iteration 58, loss = 0.65270635\n",
      "Iteration 59, loss = 0.64825353\n",
      "Iteration 60, loss = 0.64396755\n",
      "Iteration 61, loss = 0.63981803\n",
      "Iteration 62, loss = 0.63582474\n",
      "Iteration 63, loss = 0.63191773\n",
      "Iteration 64, loss = 0.62819232\n",
      "Iteration 65, loss = 0.62453433\n",
      "Iteration 66, loss = 0.62100397\n",
      "Iteration 67, loss = 0.61755840\n",
      "Iteration 68, loss = 0.61425761\n",
      "Iteration 69, loss = 0.61104529\n",
      "Iteration 70, loss = 0.60795171\n",
      "Iteration 71, loss = 0.60487052\n",
      "Iteration 72, loss = 0.60190477\n",
      "Iteration 73, loss = 0.59907050\n",
      "Iteration 74, loss = 0.59628224\n",
      "Iteration 75, loss = 0.59356583\n",
      "Iteration 76, loss = 0.59089165\n",
      "Iteration 77, loss = 0.58833524\n",
      "Iteration 78, loss = 0.58582790\n",
      "Iteration 79, loss = 0.58337629\n",
      "Iteration 80, loss = 0.58102966\n",
      "Iteration 81, loss = 0.57869288\n",
      "Iteration 82, loss = 0.57643288\n",
      "Iteration 83, loss = 0.57420265\n",
      "Iteration 84, loss = 0.57202739\n",
      "Iteration 85, loss = 0.56990916\n",
      "Iteration 86, loss = 0.56782838\n",
      "Iteration 87, loss = 0.56585351\n",
      "Iteration 88, loss = 0.56385174\n",
      "Iteration 89, loss = 0.56196278\n",
      "Iteration 90, loss = 0.56011247\n",
      "Iteration 91, loss = 0.55822243\n",
      "Iteration 92, loss = 0.55642654\n",
      "Iteration 93, loss = 0.55464577\n",
      "Iteration 94, loss = 0.55293433\n",
      "Iteration 95, loss = 0.55125834\n",
      "Iteration 96, loss = 0.54955948\n",
      "Iteration 97, loss = 0.54792308\n",
      "Iteration 98, loss = 0.54636203\n",
      "Iteration 99, loss = 0.54483906\n",
      "Iteration 100, loss = 0.54323522\n",
      "Iteration 101, loss = 0.54172648\n",
      "Iteration 102, loss = 0.54026689\n",
      "Iteration 103, loss = 0.53883265\n",
      "Iteration 104, loss = 0.53740128\n",
      "Iteration 105, loss = 0.53600493\n",
      "Iteration 106, loss = 0.53462712\n",
      "Iteration 107, loss = 0.53325992\n",
      "Iteration 108, loss = 0.53197147\n",
      "Iteration 109, loss = 0.53067186\n",
      "Iteration 110, loss = 0.52940140\n",
      "Iteration 111, loss = 0.52809503\n",
      "Iteration 112, loss = 0.52684398\n",
      "Iteration 113, loss = 0.52569374\n",
      "Iteration 114, loss = 0.52446033\n",
      "Iteration 115, loss = 0.52328561\n",
      "Iteration 116, loss = 0.52216514\n",
      "Iteration 117, loss = 0.52099834\n",
      "Iteration 118, loss = 0.51990037\n",
      "Iteration 119, loss = 0.51880572\n",
      "Iteration 120, loss = 0.51777201\n",
      "Iteration 121, loss = 0.51670474\n",
      "Iteration 122, loss = 0.51563750\n",
      "Iteration 123, loss = 0.51459903\n",
      "Iteration 124, loss = 0.51361010\n",
      "Iteration 125, loss = 0.51266511\n",
      "Iteration 126, loss = 0.51170583\n",
      "Iteration 127, loss = 0.51070477\n",
      "Iteration 128, loss = 0.50976441\n",
      "Iteration 129, loss = 0.50885164\n",
      "Iteration 130, loss = 0.50791635\n",
      "Iteration 131, loss = 0.50704026\n",
      "Iteration 132, loss = 0.50620904\n",
      "Iteration 133, loss = 0.50535721\n",
      "Iteration 134, loss = 0.50446102\n",
      "Iteration 135, loss = 0.50366589\n",
      "Iteration 136, loss = 0.50282121\n",
      "Iteration 137, loss = 0.50197068\n",
      "Iteration 138, loss = 0.50121027\n",
      "Iteration 139, loss = 0.50044626\n",
      "Iteration 140, loss = 0.49966071\n",
      "Iteration 141, loss = 0.49886809\n",
      "Iteration 142, loss = 0.49816793\n",
      "Iteration 143, loss = 0.49741254\n",
      "Iteration 144, loss = 0.49671879\n",
      "Iteration 145, loss = 0.49600845\n",
      "Iteration 146, loss = 0.49532500\n",
      "Iteration 147, loss = 0.49464791\n",
      "Iteration 148, loss = 0.49398334\n",
      "Iteration 149, loss = 0.49323884\n",
      "Iteration 150, loss = 0.49262007\n",
      "Iteration 151, loss = 0.49198423\n",
      "Iteration 152, loss = 0.49133181\n",
      "Iteration 153, loss = 0.49071607\n",
      "Iteration 154, loss = 0.49005132\n",
      "Iteration 155, loss = 0.48949699\n",
      "Iteration 156, loss = 0.48885221\n",
      "Iteration 157, loss = 0.48828977\n",
      "Iteration 158, loss = 0.48772371\n",
      "Iteration 159, loss = 0.48710371\n",
      "Iteration 160, loss = 0.48663488\n",
      "Iteration 161, loss = 0.48605670\n",
      "Iteration 162, loss = 0.48544606\n",
      "Iteration 163, loss = 0.48488143\n",
      "Iteration 164, loss = 0.48436099\n",
      "Iteration 165, loss = 0.48391704\n",
      "Iteration 166, loss = 0.48333764\n",
      "Iteration 167, loss = 0.48285876\n",
      "Iteration 168, loss = 0.48234454\n",
      "Iteration 169, loss = 0.48185013\n",
      "Iteration 170, loss = 0.48142872\n",
      "Iteration 171, loss = 0.48093364\n",
      "Iteration 172, loss = 0.48047205\n",
      "Iteration 173, loss = 0.48000703\n",
      "Iteration 174, loss = 0.47953916\n",
      "Iteration 175, loss = 0.47907845\n",
      "Iteration 176, loss = 0.47867590\n",
      "Iteration 177, loss = 0.47821546\n",
      "Iteration 178, loss = 0.47778624\n",
      "Iteration 179, loss = 0.47733044\n",
      "Iteration 180, loss = 0.47693647\n",
      "Iteration 181, loss = 0.47656752\n",
      "Iteration 182, loss = 0.47615100\n",
      "Iteration 183, loss = 0.47568252\n",
      "Iteration 184, loss = 0.47520894\n",
      "Iteration 185, loss = 0.47491219\n",
      "Iteration 186, loss = 0.47453240\n",
      "Iteration 187, loss = 0.47411171\n",
      "Iteration 188, loss = 0.47375336\n",
      "Iteration 189, loss = 0.47335778\n",
      "Iteration 190, loss = 0.47297215\n",
      "Iteration 191, loss = 0.47270255\n",
      "Iteration 192, loss = 0.47232720\n",
      "Iteration 193, loss = 0.47206772\n",
      "Iteration 194, loss = 0.47168624\n",
      "Iteration 195, loss = 0.47125935\n",
      "Iteration 196, loss = 0.47095616\n",
      "Iteration 197, loss = 0.47059183\n",
      "Iteration 198, loss = 0.47029415\n",
      "Iteration 199, loss = 0.46996619\n",
      "Iteration 200, loss = 0.46968753\n",
      "Iteration 201, loss = 0.46933650\n",
      "Iteration 202, loss = 0.46902451\n",
      "Iteration 203, loss = 0.46874757\n",
      "Iteration 204, loss = 0.46839843\n",
      "Iteration 205, loss = 0.46810222\n",
      "Iteration 206, loss = 0.46786616\n",
      "Iteration 207, loss = 0.46759587\n",
      "Iteration 208, loss = 0.46723834\n",
      "Iteration 209, loss = 0.46704144\n",
      "Iteration 210, loss = 0.46673466\n",
      "Iteration 211, loss = 0.46638819\n",
      "Iteration 212, loss = 0.46618577\n",
      "Iteration 213, loss = 0.46587276\n",
      "Iteration 214, loss = 0.46558268\n",
      "Iteration 215, loss = 0.46535219\n",
      "Iteration 216, loss = 0.46509948\n",
      "Iteration 217, loss = 0.46479464\n",
      "Iteration 218, loss = 0.46453468\n",
      "Iteration 219, loss = 0.46433439\n",
      "Iteration 220, loss = 0.46403814\n",
      "Iteration 221, loss = 0.46379132\n",
      "Iteration 222, loss = 0.46354710\n",
      "Iteration 223, loss = 0.46339622\n",
      "Iteration 224, loss = 0.46311887\n",
      "Iteration 225, loss = 0.46291345\n",
      "Iteration 226, loss = 0.46260033\n",
      "Iteration 227, loss = 0.46238424\n",
      "Iteration 228, loss = 0.46223185\n",
      "Iteration 229, loss = 0.46199543\n",
      "Iteration 230, loss = 0.46175842\n",
      "Iteration 231, loss = 0.46151359\n",
      "Iteration 232, loss = 0.46133276\n",
      "Iteration 233, loss = 0.46111580\n",
      "Iteration 234, loss = 0.46097002\n",
      "Iteration 235, loss = 0.46068660\n",
      "Iteration 236, loss = 0.46053953\n",
      "Iteration 237, loss = 0.46033991\n",
      "Iteration 238, loss = 0.46011623\n",
      "Iteration 239, loss = 0.45986819\n",
      "Iteration 240, loss = 0.45964637\n",
      "Iteration 241, loss = 0.45948852\n",
      "Iteration 242, loss = 0.45928128\n",
      "Iteration 243, loss = 0.45909918\n",
      "Iteration 244, loss = 0.45899078\n",
      "Iteration 245, loss = 0.45870455\n",
      "Iteration 246, loss = 0.45860067\n",
      "Iteration 247, loss = 0.45845687\n",
      "Iteration 248, loss = 0.45818497\n",
      "Iteration 249, loss = 0.45804340\n",
      "Iteration 250, loss = 0.45780619\n",
      "Iteration 251, loss = 0.45762478\n",
      "Iteration 252, loss = 0.45752413\n",
      "Iteration 253, loss = 0.45734524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.45710105\n",
      "Iteration 255, loss = 0.45699136\n",
      "Iteration 256, loss = 0.45675419\n",
      "Iteration 257, loss = 0.45661383\n",
      "Iteration 258, loss = 0.45643871\n",
      "Iteration 259, loss = 0.45628130\n",
      "Iteration 260, loss = 0.45616133\n",
      "Iteration 261, loss = 0.45605914\n",
      "Iteration 262, loss = 0.45587824\n",
      "Iteration 263, loss = 0.45564928\n",
      "Iteration 264, loss = 0.45552499\n",
      "Iteration 265, loss = 0.45543644\n",
      "Iteration 266, loss = 0.45514441\n",
      "Iteration 267, loss = 0.45515833\n",
      "Iteration 268, loss = 0.45490988\n",
      "Iteration 269, loss = 0.45472960\n",
      "Iteration 270, loss = 0.45461160\n",
      "Iteration 271, loss = 0.45450238\n",
      "Iteration 272, loss = 0.45432848\n",
      "Iteration 273, loss = 0.45416570\n",
      "Iteration 274, loss = 0.45406220\n",
      "Iteration 275, loss = 0.45386734\n",
      "Iteration 276, loss = 0.45383029\n",
      "Iteration 277, loss = 0.45354218\n",
      "Iteration 278, loss = 0.45359658\n",
      "Iteration 279, loss = 0.45339939\n",
      "Iteration 280, loss = 0.45319743\n",
      "Iteration 281, loss = 0.45305785\n",
      "Iteration 282, loss = 0.45296221\n",
      "Iteration 283, loss = 0.45284698\n",
      "Iteration 284, loss = 0.45274338\n",
      "Iteration 285, loss = 0.45249514\n",
      "Iteration 286, loss = 0.45236400\n",
      "Iteration 287, loss = 0.45234687\n",
      "Iteration 288, loss = 0.45220855\n",
      "Iteration 289, loss = 0.45202101\n",
      "Iteration 290, loss = 0.45193591\n",
      "Iteration 291, loss = 0.45182590\n",
      "Iteration 292, loss = 0.45167860\n",
      "Iteration 293, loss = 0.45153977\n",
      "Iteration 294, loss = 0.45141548\n",
      "Iteration 295, loss = 0.45133600\n",
      "Iteration 296, loss = 0.45120715\n",
      "Iteration 297, loss = 0.45106809\n",
      "Iteration 298, loss = 0.45100668\n",
      "Iteration 299, loss = 0.45088410\n",
      "Iteration 300, loss = 0.45071437\n",
      "Iteration 301, loss = 0.45061585\n",
      "Iteration 302, loss = 0.45051302\n",
      "Iteration 303, loss = 0.45047852\n",
      "Iteration 304, loss = 0.45028683\n",
      "Iteration 305, loss = 0.45022032\n",
      "Iteration 306, loss = 0.45009258\n",
      "Iteration 307, loss = 0.45003318\n",
      "Iteration 308, loss = 0.44990641\n",
      "Iteration 309, loss = 0.44979915\n",
      "Iteration 310, loss = 0.44966294\n",
      "Iteration 311, loss = 0.44954822\n",
      "Iteration 312, loss = 0.44941531\n",
      "Iteration 313, loss = 0.44933758\n",
      "Iteration 314, loss = 0.44923904\n",
      "Iteration 315, loss = 0.44910703\n",
      "Iteration 316, loss = 0.44902023\n",
      "Iteration 317, loss = 0.44895713\n",
      "Iteration 318, loss = 0.44889300\n",
      "Iteration 319, loss = 0.44871657\n",
      "Iteration 320, loss = 0.44866558\n",
      "Iteration 321, loss = 0.44852233\n",
      "Iteration 322, loss = 0.44845603\n",
      "Iteration 323, loss = 0.44827980\n",
      "Iteration 324, loss = 0.44821682\n",
      "Iteration 325, loss = 0.44814948\n",
      "Iteration 326, loss = 0.44800889\n",
      "Iteration 327, loss = 0.44800290\n",
      "Iteration 328, loss = 0.44783117\n",
      "Iteration 329, loss = 0.44774166\n",
      "Iteration 330, loss = 0.44774329\n",
      "Iteration 331, loss = 0.44750191\n",
      "Iteration 332, loss = 0.44749916\n",
      "Iteration 333, loss = 0.44736170\n",
      "Iteration 334, loss = 0.44726356\n",
      "Iteration 335, loss = 0.44727216\n",
      "Iteration 336, loss = 0.44716380\n",
      "Iteration 337, loss = 0.44703885\n",
      "Iteration 338, loss = 0.44706468\n",
      "Iteration 339, loss = 0.44689776\n",
      "Iteration 340, loss = 0.44681838\n",
      "Iteration 341, loss = 0.44662268\n",
      "Iteration 342, loss = 0.44659508\n",
      "Iteration 343, loss = 0.44648191\n",
      "Iteration 344, loss = 0.44645114\n",
      "Iteration 345, loss = 0.44629313\n",
      "Iteration 346, loss = 0.44626934\n",
      "Iteration 347, loss = 0.44614795\n",
      "Iteration 348, loss = 0.44603271\n",
      "Iteration 349, loss = 0.44595349\n",
      "Iteration 350, loss = 0.44599269\n",
      "Iteration 351, loss = 0.44590404\n",
      "Iteration 352, loss = 0.44580612\n",
      "Iteration 353, loss = 0.44565810\n",
      "Iteration 354, loss = 0.44565831\n",
      "Iteration 355, loss = 0.44554246\n",
      "Iteration 356, loss = 0.44545859\n",
      "Iteration 357, loss = 0.44533171\n",
      "Iteration 358, loss = 0.44529225\n",
      "Iteration 359, loss = 0.44530742\n",
      "Iteration 360, loss = 0.44522804\n",
      "Iteration 361, loss = 0.44504599\n",
      "Iteration 362, loss = 0.44490742\n",
      "Iteration 363, loss = 0.44494002\n",
      "Iteration 364, loss = 0.44487363\n",
      "Iteration 365, loss = 0.44473787\n",
      "Iteration 366, loss = 0.44463723\n",
      "Iteration 367, loss = 0.44458442\n",
      "Iteration 368, loss = 0.44450377\n",
      "Iteration 369, loss = 0.44455933\n",
      "Iteration 370, loss = 0.44438997\n",
      "Iteration 371, loss = 0.44435599\n",
      "Iteration 372, loss = 0.44419028\n",
      "Iteration 373, loss = 0.44417568\n",
      "Iteration 374, loss = 0.44411281\n",
      "Iteration 375, loss = 0.44404412\n",
      "Iteration 376, loss = 0.44395650\n",
      "Iteration 377, loss = 0.44390750\n",
      "Iteration 378, loss = 0.44376932\n",
      "Iteration 379, loss = 0.44374269\n",
      "Iteration 380, loss = 0.44381399\n",
      "Iteration 381, loss = 0.44362205\n",
      "Iteration 382, loss = 0.44348089\n",
      "Iteration 383, loss = 0.44353199\n",
      "Iteration 384, loss = 0.44332237\n",
      "Iteration 385, loss = 0.44338545\n",
      "Iteration 386, loss = 0.44325004\n",
      "Iteration 387, loss = 0.44324944\n",
      "Iteration 388, loss = 0.44306614\n",
      "Iteration 389, loss = 0.44309238\n",
      "Iteration 390, loss = 0.44297292\n",
      "Iteration 391, loss = 0.44295416\n",
      "Iteration 392, loss = 0.44288116\n",
      "Iteration 393, loss = 0.44280248\n",
      "Iteration 394, loss = 0.44269479\n",
      "Iteration 395, loss = 0.44263559\n",
      "Iteration 396, loss = 0.44260674\n",
      "Iteration 397, loss = 0.44250244\n",
      "Iteration 398, loss = 0.44246186\n",
      "Iteration 399, loss = 0.44243100\n",
      "Iteration 400, loss = 0.44232284\n",
      "Iteration 401, loss = 0.44223005\n",
      "Iteration 402, loss = 0.44226375\n",
      "Iteration 403, loss = 0.44217308\n",
      "Iteration 404, loss = 0.44205485\n",
      "Iteration 405, loss = 0.44210439\n",
      "Iteration 406, loss = 0.44195484\n",
      "Iteration 407, loss = 0.44193325\n",
      "Iteration 408, loss = 0.44181683\n",
      "Iteration 409, loss = 0.44184052\n",
      "Iteration 410, loss = 0.44171070\n",
      "Iteration 411, loss = 0.44166378\n",
      "Iteration 412, loss = 0.44157743\n",
      "Iteration 413, loss = 0.44154011\n",
      "Iteration 414, loss = 0.44147425\n",
      "Iteration 415, loss = 0.44143391\n",
      "Iteration 416, loss = 0.44139213\n",
      "Iteration 417, loss = 0.44129867\n",
      "Iteration 418, loss = 0.44123515\n",
      "Iteration 419, loss = 0.44127593\n",
      "Iteration 420, loss = 0.44123856\n",
      "Iteration 421, loss = 0.44109749\n",
      "Iteration 422, loss = 0.44106348\n",
      "Iteration 423, loss = 0.44102590\n",
      "Iteration 424, loss = 0.44096252\n",
      "Iteration 425, loss = 0.44089141\n",
      "Iteration 426, loss = 0.44081989\n",
      "Iteration 427, loss = 0.44072959\n",
      "Iteration 428, loss = 0.44065759\n",
      "Iteration 429, loss = 0.44063017\n",
      "Iteration 430, loss = 0.44058806\n",
      "Iteration 431, loss = 0.44050481\n",
      "Iteration 432, loss = 0.44051776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10982085\n",
      "Iteration 2, loss = 1.94780716\n",
      "Iteration 3, loss = 1.83199087\n",
      "Iteration 4, loss = 1.74361224\n",
      "Iteration 5, loss = 1.67140131\n",
      "Iteration 6, loss = 1.60889794\n",
      "Iteration 7, loss = 1.55233401\n",
      "Iteration 8, loss = 1.49953600\n",
      "Iteration 9, loss = 1.44934278\n",
      "Iteration 10, loss = 1.40141131\n",
      "Iteration 11, loss = 1.35568073\n",
      "Iteration 12, loss = 1.31233828\n",
      "Iteration 13, loss = 1.27162219\n",
      "Iteration 14, loss = 1.23363684\n",
      "Iteration 15, loss = 1.19838476\n",
      "Iteration 16, loss = 1.16577816\n",
      "Iteration 17, loss = 1.13565156\n",
      "Iteration 18, loss = 1.10783673\n",
      "Iteration 19, loss = 1.08202796\n",
      "Iteration 20, loss = 1.05808790\n",
      "Iteration 21, loss = 1.03579518\n",
      "Iteration 22, loss = 1.01487700\n",
      "Iteration 23, loss = 0.99523475\n",
      "Iteration 24, loss = 0.97664433\n",
      "Iteration 25, loss = 0.95905453\n",
      "Iteration 26, loss = 0.94231354\n",
      "Iteration 27, loss = 0.92638296\n",
      "Iteration 28, loss = 0.91118003\n",
      "Iteration 29, loss = 0.89664412\n",
      "Iteration 30, loss = 0.88272403\n",
      "Iteration 31, loss = 0.86940959\n",
      "Iteration 32, loss = 0.85666618\n",
      "Iteration 33, loss = 0.84442283\n",
      "Iteration 34, loss = 0.83266843\n",
      "Iteration 35, loss = 0.82137989\n",
      "Iteration 36, loss = 0.81053845\n",
      "Iteration 37, loss = 0.80011140\n",
      "Iteration 38, loss = 0.79004963\n",
      "Iteration 39, loss = 0.78040527\n",
      "Iteration 40, loss = 0.77109943\n",
      "Iteration 41, loss = 0.76212662\n",
      "Iteration 42, loss = 0.75352445\n",
      "Iteration 43, loss = 0.74525785\n",
      "Iteration 44, loss = 0.73731912\n",
      "Iteration 45, loss = 0.72967557\n",
      "Iteration 46, loss = 0.72235071\n",
      "Iteration 47, loss = 0.71529434\n",
      "Iteration 48, loss = 0.70854374\n",
      "Iteration 49, loss = 0.70205621\n",
      "Iteration 50, loss = 0.69581955\n",
      "Iteration 51, loss = 0.68980332\n",
      "Iteration 52, loss = 0.68406621\n",
      "Iteration 53, loss = 0.67847656\n",
      "Iteration 54, loss = 0.67314518\n",
      "Iteration 55, loss = 0.66801786\n",
      "Iteration 56, loss = 0.66304870\n",
      "Iteration 57, loss = 0.65827857\n",
      "Iteration 58, loss = 0.65371440\n",
      "Iteration 59, loss = 0.64923891\n",
      "Iteration 60, loss = 0.64496738\n",
      "Iteration 61, loss = 0.64082501\n",
      "Iteration 62, loss = 0.63685095\n",
      "Iteration 63, loss = 0.63297746\n",
      "Iteration 64, loss = 0.62921035\n",
      "Iteration 65, loss = 0.62560012\n",
      "Iteration 66, loss = 0.62209442\n",
      "Iteration 67, loss = 0.61866940\n",
      "Iteration 68, loss = 0.61538756\n",
      "Iteration 69, loss = 0.61217067\n",
      "Iteration 70, loss = 0.60908136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.60606324\n",
      "Iteration 72, loss = 0.60313598\n",
      "Iteration 73, loss = 0.60027776\n",
      "Iteration 74, loss = 0.59748687\n",
      "Iteration 75, loss = 0.59477908\n",
      "Iteration 76, loss = 0.59216005\n",
      "Iteration 77, loss = 0.58962534\n",
      "Iteration 78, loss = 0.58716595\n",
      "Iteration 79, loss = 0.58470211\n",
      "Iteration 80, loss = 0.58234289\n",
      "Iteration 81, loss = 0.58002183\n",
      "Iteration 82, loss = 0.57783145\n",
      "Iteration 83, loss = 0.57558837\n",
      "Iteration 84, loss = 0.57345417\n",
      "Iteration 85, loss = 0.57136508\n",
      "Iteration 86, loss = 0.56928305\n",
      "Iteration 87, loss = 0.56733320\n",
      "Iteration 88, loss = 0.56534144\n",
      "Iteration 89, loss = 0.56344159\n",
      "Iteration 90, loss = 0.56161144\n",
      "Iteration 91, loss = 0.55979023\n",
      "Iteration 92, loss = 0.55796834\n",
      "Iteration 93, loss = 0.55621844\n",
      "Iteration 94, loss = 0.55450715\n",
      "Iteration 95, loss = 0.55277372\n",
      "Iteration 96, loss = 0.55115436\n",
      "Iteration 97, loss = 0.54955795\n",
      "Iteration 98, loss = 0.54795369\n",
      "Iteration 99, loss = 0.54637968\n",
      "Iteration 100, loss = 0.54490494\n",
      "Iteration 101, loss = 0.54338063\n",
      "Iteration 102, loss = 0.54192831\n",
      "Iteration 103, loss = 0.54044870\n",
      "Iteration 104, loss = 0.53904074\n",
      "Iteration 105, loss = 0.53768452\n",
      "Iteration 106, loss = 0.53631206\n",
      "Iteration 107, loss = 0.53497273\n",
      "Iteration 108, loss = 0.53364605\n",
      "Iteration 109, loss = 0.53239129\n",
      "Iteration 110, loss = 0.53108125\n",
      "Iteration 111, loss = 0.52984995\n",
      "Iteration 112, loss = 0.52859817\n",
      "Iteration 113, loss = 0.52743133\n",
      "Iteration 114, loss = 0.52627721\n",
      "Iteration 115, loss = 0.52506843\n",
      "Iteration 116, loss = 0.52392731\n",
      "Iteration 117, loss = 0.52282537\n",
      "Iteration 118, loss = 0.52169034\n",
      "Iteration 119, loss = 0.52059936\n",
      "Iteration 120, loss = 0.51952839\n",
      "Iteration 121, loss = 0.51845943\n",
      "Iteration 122, loss = 0.51745015\n",
      "Iteration 123, loss = 0.51640313\n",
      "Iteration 124, loss = 0.51540529\n",
      "Iteration 125, loss = 0.51437542\n",
      "Iteration 126, loss = 0.51340539\n",
      "Iteration 127, loss = 0.51248735\n",
      "Iteration 128, loss = 0.51159008\n",
      "Iteration 129, loss = 0.51065603\n",
      "Iteration 130, loss = 0.50973340\n",
      "Iteration 131, loss = 0.50885169\n",
      "Iteration 132, loss = 0.50799087\n",
      "Iteration 133, loss = 0.50708935\n",
      "Iteration 134, loss = 0.50623960\n",
      "Iteration 135, loss = 0.50548442\n",
      "Iteration 136, loss = 0.50459387\n",
      "Iteration 137, loss = 0.50382447\n",
      "Iteration 138, loss = 0.50303351\n",
      "Iteration 139, loss = 0.50219847\n",
      "Iteration 140, loss = 0.50144840\n",
      "Iteration 141, loss = 0.50071913\n",
      "Iteration 142, loss = 0.50000906\n",
      "Iteration 143, loss = 0.49928286\n",
      "Iteration 144, loss = 0.49850213\n",
      "Iteration 145, loss = 0.49781738\n",
      "Iteration 146, loss = 0.49714734\n",
      "Iteration 147, loss = 0.49642264\n",
      "Iteration 148, loss = 0.49574587\n",
      "Iteration 149, loss = 0.49510043\n",
      "Iteration 150, loss = 0.49454235\n",
      "Iteration 151, loss = 0.49382683\n",
      "Iteration 152, loss = 0.49321498\n",
      "Iteration 153, loss = 0.49261485\n",
      "Iteration 154, loss = 0.49194019\n",
      "Iteration 155, loss = 0.49136281\n",
      "Iteration 156, loss = 0.49075181\n",
      "Iteration 157, loss = 0.49020543\n",
      "Iteration 158, loss = 0.48960191\n",
      "Iteration 159, loss = 0.48902842\n",
      "Iteration 160, loss = 0.48850792\n",
      "Iteration 161, loss = 0.48799414\n",
      "Iteration 162, loss = 0.48748404\n",
      "Iteration 163, loss = 0.48688138\n",
      "Iteration 164, loss = 0.48636921\n",
      "Iteration 165, loss = 0.48591103\n",
      "Iteration 166, loss = 0.48531011\n",
      "Iteration 167, loss = 0.48487382\n",
      "Iteration 168, loss = 0.48433633\n",
      "Iteration 169, loss = 0.48387077\n",
      "Iteration 170, loss = 0.48338073\n",
      "Iteration 171, loss = 0.48290461\n",
      "Iteration 172, loss = 0.48245080\n",
      "Iteration 173, loss = 0.48198513\n",
      "Iteration 174, loss = 0.48153795\n",
      "Iteration 175, loss = 0.48110748\n",
      "Iteration 176, loss = 0.48067520\n",
      "Iteration 177, loss = 0.48021977\n",
      "Iteration 178, loss = 0.47981088\n",
      "Iteration 179, loss = 0.47935114\n",
      "Iteration 180, loss = 0.47892011\n",
      "Iteration 181, loss = 0.47853417\n",
      "Iteration 182, loss = 0.47813195\n",
      "Iteration 183, loss = 0.47775825\n",
      "Iteration 184, loss = 0.47739989\n",
      "Iteration 185, loss = 0.47695102\n",
      "Iteration 186, loss = 0.47658619\n",
      "Iteration 187, loss = 0.47628521\n",
      "Iteration 188, loss = 0.47585761\n",
      "Iteration 189, loss = 0.47549902\n",
      "Iteration 190, loss = 0.47520585\n",
      "Iteration 191, loss = 0.47476382\n",
      "Iteration 192, loss = 0.47445343\n",
      "Iteration 193, loss = 0.47408190\n",
      "Iteration 194, loss = 0.47373125\n",
      "Iteration 195, loss = 0.47344300\n",
      "Iteration 196, loss = 0.47310562\n",
      "Iteration 197, loss = 0.47273394\n",
      "Iteration 198, loss = 0.47243922\n",
      "Iteration 199, loss = 0.47213575\n",
      "Iteration 200, loss = 0.47183697\n",
      "Iteration 201, loss = 0.47147430\n",
      "Iteration 202, loss = 0.47117353\n",
      "Iteration 203, loss = 0.47094944\n",
      "Iteration 204, loss = 0.47054434\n",
      "Iteration 205, loss = 0.47036132\n",
      "Iteration 206, loss = 0.47000459\n",
      "Iteration 207, loss = 0.46981711\n",
      "Iteration 208, loss = 0.46944630\n",
      "Iteration 209, loss = 0.46917009\n",
      "Iteration 210, loss = 0.46892030\n",
      "Iteration 211, loss = 0.46861209\n",
      "Iteration 212, loss = 0.46831978\n",
      "Iteration 213, loss = 0.46805042\n",
      "Iteration 214, loss = 0.46782429\n",
      "Iteration 215, loss = 0.46753339\n",
      "Iteration 216, loss = 0.46730796\n",
      "Iteration 217, loss = 0.46710611\n",
      "Iteration 218, loss = 0.46677856\n",
      "Iteration 219, loss = 0.46656753\n",
      "Iteration 220, loss = 0.46631903\n",
      "Iteration 221, loss = 0.46608764\n",
      "Iteration 222, loss = 0.46588477\n",
      "Iteration 223, loss = 0.46560816\n",
      "Iteration 224, loss = 0.46530433\n",
      "Iteration 225, loss = 0.46519223\n",
      "Iteration 226, loss = 0.46488386\n",
      "Iteration 227, loss = 0.46473356\n",
      "Iteration 228, loss = 0.46444797\n",
      "Iteration 229, loss = 0.46420854\n",
      "Iteration 230, loss = 0.46403406\n",
      "Iteration 231, loss = 0.46384432\n",
      "Iteration 232, loss = 0.46361416\n",
      "Iteration 233, loss = 0.46343500\n",
      "Iteration 234, loss = 0.46314900\n",
      "Iteration 235, loss = 0.46296801\n",
      "Iteration 236, loss = 0.46274881\n",
      "Iteration 237, loss = 0.46264555\n",
      "Iteration 238, loss = 0.46237743\n",
      "Iteration 239, loss = 0.46215509\n",
      "Iteration 240, loss = 0.46197824\n",
      "Iteration 241, loss = 0.46175611\n",
      "Iteration 242, loss = 0.46158867\n",
      "Iteration 243, loss = 0.46137574\n",
      "Iteration 244, loss = 0.46122329\n",
      "Iteration 245, loss = 0.46102204\n",
      "Iteration 246, loss = 0.46086952\n",
      "Iteration 247, loss = 0.46067813\n",
      "Iteration 248, loss = 0.46050613\n",
      "Iteration 249, loss = 0.46040309\n",
      "Iteration 250, loss = 0.46019387\n",
      "Iteration 251, loss = 0.45994512\n",
      "Iteration 252, loss = 0.45987466\n",
      "Iteration 253, loss = 0.45968849\n",
      "Iteration 254, loss = 0.45948805\n",
      "Iteration 255, loss = 0.45935903\n",
      "Iteration 256, loss = 0.45919572\n",
      "Iteration 257, loss = 0.45899218\n",
      "Iteration 258, loss = 0.45876616\n",
      "Iteration 259, loss = 0.45860223\n",
      "Iteration 260, loss = 0.45852446\n",
      "Iteration 261, loss = 0.45837095\n",
      "Iteration 262, loss = 0.45820079\n",
      "Iteration 263, loss = 0.45802444\n",
      "Iteration 264, loss = 0.45785914\n",
      "Iteration 265, loss = 0.45774296\n",
      "Iteration 266, loss = 0.45761357\n",
      "Iteration 267, loss = 0.45738517\n",
      "Iteration 268, loss = 0.45723397\n",
      "Iteration 269, loss = 0.45714072\n",
      "Iteration 270, loss = 0.45694700\n",
      "Iteration 271, loss = 0.45684975\n",
      "Iteration 272, loss = 0.45669899\n",
      "Iteration 273, loss = 0.45659247\n",
      "Iteration 274, loss = 0.45644990\n",
      "Iteration 275, loss = 0.45635647\n",
      "Iteration 276, loss = 0.45619283\n",
      "Iteration 277, loss = 0.45596983\n",
      "Iteration 278, loss = 0.45594304\n",
      "Iteration 279, loss = 0.45578548\n",
      "Iteration 280, loss = 0.45562250\n",
      "Iteration 281, loss = 0.45548240\n",
      "Iteration 282, loss = 0.45544465\n",
      "Iteration 283, loss = 0.45532508\n",
      "Iteration 284, loss = 0.45510127\n",
      "Iteration 285, loss = 0.45498213\n",
      "Iteration 286, loss = 0.45489555\n",
      "Iteration 287, loss = 0.45475061\n",
      "Iteration 288, loss = 0.45462537\n",
      "Iteration 289, loss = 0.45456048\n",
      "Iteration 290, loss = 0.45435102\n",
      "Iteration 291, loss = 0.45427420\n",
      "Iteration 292, loss = 0.45416791\n",
      "Iteration 293, loss = 0.45404384\n",
      "Iteration 294, loss = 0.45388085\n",
      "Iteration 295, loss = 0.45379774\n",
      "Iteration 296, loss = 0.45368561\n",
      "Iteration 297, loss = 0.45358846\n",
      "Iteration 298, loss = 0.45338506\n",
      "Iteration 299, loss = 0.45335442\n",
      "Iteration 300, loss = 0.45324964\n",
      "Iteration 301, loss = 0.45313194\n",
      "Iteration 302, loss = 0.45297185\n",
      "Iteration 303, loss = 0.45285935\n",
      "Iteration 304, loss = 0.45280063\n",
      "Iteration 305, loss = 0.45270731\n",
      "Iteration 306, loss = 0.45261963\n",
      "Iteration 307, loss = 0.45242925\n",
      "Iteration 308, loss = 0.45239601\n",
      "Iteration 309, loss = 0.45230683\n",
      "Iteration 310, loss = 0.45216271\n",
      "Iteration 311, loss = 0.45198937\n",
      "Iteration 312, loss = 0.45195120\n",
      "Iteration 313, loss = 0.45185039\n",
      "Iteration 314, loss = 0.45178810\n",
      "Iteration 315, loss = 0.45165200\n",
      "Iteration 316, loss = 0.45148785\n",
      "Iteration 317, loss = 0.45138064\n",
      "Iteration 318, loss = 0.45137981\n",
      "Iteration 319, loss = 0.45118821\n",
      "Iteration 320, loss = 0.45113311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 321, loss = 0.45104952\n",
      "Iteration 322, loss = 0.45096744\n",
      "Iteration 323, loss = 0.45083463\n",
      "Iteration 324, loss = 0.45075838\n",
      "Iteration 325, loss = 0.45067731\n",
      "Iteration 326, loss = 0.45054195\n",
      "Iteration 327, loss = 0.45048595\n",
      "Iteration 328, loss = 0.45038169\n",
      "Iteration 329, loss = 0.45029747\n",
      "Iteration 330, loss = 0.45021898\n",
      "Iteration 331, loss = 0.45016759\n",
      "Iteration 332, loss = 0.45005744\n",
      "Iteration 333, loss = 0.44989389\n",
      "Iteration 334, loss = 0.44992018\n",
      "Iteration 335, loss = 0.44980170\n",
      "Iteration 336, loss = 0.44968734\n",
      "Iteration 337, loss = 0.44961296\n",
      "Iteration 338, loss = 0.44947739\n",
      "Iteration 339, loss = 0.44943554\n",
      "Iteration 340, loss = 0.44937418\n",
      "Iteration 341, loss = 0.44921423\n",
      "Iteration 342, loss = 0.44914917\n",
      "Iteration 343, loss = 0.44910319\n",
      "Iteration 344, loss = 0.44899169\n",
      "Iteration 345, loss = 0.44887087\n",
      "Iteration 346, loss = 0.44894962\n",
      "Iteration 347, loss = 0.44878733\n",
      "Iteration 348, loss = 0.44863872\n",
      "Iteration 349, loss = 0.44859051\n",
      "Iteration 350, loss = 0.44854319\n",
      "Iteration 351, loss = 0.44845777\n",
      "Iteration 352, loss = 0.44837583\n",
      "Iteration 353, loss = 0.44830483\n",
      "Iteration 354, loss = 0.44819728\n",
      "Iteration 355, loss = 0.44819171\n",
      "Iteration 356, loss = 0.44804427\n",
      "Iteration 357, loss = 0.44799722\n",
      "Iteration 358, loss = 0.44793670\n",
      "Iteration 359, loss = 0.44791223\n",
      "Iteration 360, loss = 0.44774789\n",
      "Iteration 361, loss = 0.44764393\n",
      "Iteration 362, loss = 0.44764827\n",
      "Iteration 363, loss = 0.44752655\n",
      "Iteration 364, loss = 0.44750366\n",
      "Iteration 365, loss = 0.44747056\n",
      "Iteration 366, loss = 0.44734067\n",
      "Iteration 367, loss = 0.44731580\n",
      "Iteration 368, loss = 0.44717393\n",
      "Iteration 369, loss = 0.44710114\n",
      "Iteration 370, loss = 0.44703011\n",
      "Iteration 371, loss = 0.44694022\n",
      "Iteration 372, loss = 0.44693026\n",
      "Iteration 373, loss = 0.44685906\n",
      "Iteration 374, loss = 0.44679927\n",
      "Iteration 375, loss = 0.44667935\n",
      "Iteration 376, loss = 0.44661631\n",
      "Iteration 377, loss = 0.44654773\n",
      "Iteration 378, loss = 0.44657821\n",
      "Iteration 379, loss = 0.44635316\n",
      "Iteration 380, loss = 0.44631995\n",
      "Iteration 381, loss = 0.44625754\n",
      "Iteration 382, loss = 0.44624584\n",
      "Iteration 383, loss = 0.44617837\n",
      "Iteration 384, loss = 0.44613678\n",
      "Iteration 385, loss = 0.44599904\n",
      "Iteration 386, loss = 0.44592800\n",
      "Iteration 387, loss = 0.44587763\n",
      "Iteration 388, loss = 0.44588289\n",
      "Iteration 389, loss = 0.44580027\n",
      "Iteration 390, loss = 0.44574659\n",
      "Iteration 391, loss = 0.44570742\n",
      "Iteration 392, loss = 0.44554337\n",
      "Iteration 393, loss = 0.44556147\n",
      "Iteration 394, loss = 0.44541065\n",
      "Iteration 395, loss = 0.44542965\n",
      "Iteration 396, loss = 0.44529501\n",
      "Iteration 397, loss = 0.44526013\n",
      "Iteration 398, loss = 0.44520218\n",
      "Iteration 399, loss = 0.44506398\n",
      "Iteration 400, loss = 0.44498397\n",
      "Iteration 401, loss = 0.44511077\n",
      "Iteration 402, loss = 0.44488536\n",
      "Iteration 403, loss = 0.44486159\n",
      "Iteration 404, loss = 0.44483823\n",
      "Iteration 405, loss = 0.44481242\n",
      "Iteration 406, loss = 0.44474639\n",
      "Iteration 407, loss = 0.44471293\n",
      "Iteration 408, loss = 0.44458639\n",
      "Iteration 409, loss = 0.44455489\n",
      "Iteration 410, loss = 0.44453826\n",
      "Iteration 411, loss = 0.44437305\n",
      "Iteration 412, loss = 0.44442078\n",
      "Iteration 413, loss = 0.44427249\n",
      "Iteration 414, loss = 0.44415352\n",
      "Iteration 415, loss = 0.44414579\n",
      "Iteration 416, loss = 0.44410200\n",
      "Iteration 417, loss = 0.44411136\n",
      "Iteration 418, loss = 0.44402779\n",
      "Iteration 419, loss = 0.44395173\n",
      "Iteration 420, loss = 0.44384239\n",
      "Iteration 421, loss = 0.44383392\n",
      "Iteration 422, loss = 0.44379678\n",
      "Iteration 423, loss = 0.44370537\n",
      "Iteration 424, loss = 0.44369651\n",
      "Iteration 425, loss = 0.44358439\n",
      "Iteration 426, loss = 0.44356660\n",
      "Iteration 427, loss = 0.44352442\n",
      "Iteration 428, loss = 0.44357870\n",
      "Iteration 429, loss = 0.44342392\n",
      "Iteration 430, loss = 0.44328532\n",
      "Iteration 431, loss = 0.44330960\n",
      "Iteration 432, loss = 0.44329495\n",
      "Iteration 433, loss = 0.44314651\n",
      "Iteration 434, loss = 0.44311374\n",
      "Iteration 435, loss = 0.44309749\n",
      "Iteration 436, loss = 0.44300882\n",
      "Iteration 437, loss = 0.44299470\n",
      "Iteration 438, loss = 0.44289369\n",
      "Iteration 439, loss = 0.44286338\n",
      "Iteration 440, loss = 0.44278641\n",
      "Iteration 441, loss = 0.44272836\n",
      "Iteration 442, loss = 0.44270529\n",
      "Iteration 443, loss = 0.44264647\n",
      "Iteration 444, loss = 0.44256001\n",
      "Iteration 445, loss = 0.44266443\n",
      "Iteration 446, loss = 0.44257206\n",
      "Iteration 447, loss = 0.44243502\n",
      "Iteration 448, loss = 0.44244072\n",
      "Iteration 449, loss = 0.44238557\n",
      "Iteration 450, loss = 0.44226526\n",
      "Iteration 451, loss = 0.44232921\n",
      "Iteration 452, loss = 0.44218162\n",
      "Iteration 453, loss = 0.44218886\n",
      "Iteration 454, loss = 0.44211504\n",
      "Iteration 455, loss = 0.44209295\n",
      "Iteration 456, loss = 0.44205477\n",
      "Iteration 457, loss = 0.44203637\n",
      "Iteration 458, loss = 0.44198867\n",
      "Iteration 459, loss = 0.44183585\n",
      "Iteration 460, loss = 0.44178570\n",
      "Iteration 461, loss = 0.44174778\n",
      "Iteration 462, loss = 0.44174987\n",
      "Iteration 463, loss = 0.44175747\n",
      "Iteration 464, loss = 0.44159799\n",
      "Iteration 465, loss = 0.44157940\n",
      "Iteration 466, loss = 0.44152914\n",
      "Iteration 467, loss = 0.44148257\n",
      "Iteration 468, loss = 0.44147327\n",
      "Iteration 469, loss = 0.44135438\n",
      "Iteration 470, loss = 0.44140570\n",
      "Iteration 471, loss = 0.44137272\n",
      "Iteration 472, loss = 0.44127443\n",
      "Iteration 473, loss = 0.44116411\n",
      "Iteration 474, loss = 0.44118458\n",
      "Iteration 475, loss = 0.44104444\n",
      "Iteration 476, loss = 0.44100767\n",
      "Iteration 477, loss = 0.44096832\n",
      "Iteration 478, loss = 0.44090013\n",
      "Iteration 479, loss = 0.44088384\n",
      "Iteration 480, loss = 0.44083934\n",
      "Iteration 481, loss = 0.44082496\n",
      "Iteration 482, loss = 0.44072396\n",
      "Iteration 483, loss = 0.44078981\n",
      "Iteration 484, loss = 0.44060034\n",
      "Iteration 485, loss = 0.44056918\n",
      "Iteration 486, loss = 0.44056925\n",
      "Iteration 487, loss = 0.44050658\n",
      "Iteration 488, loss = 0.44040375\n",
      "Iteration 489, loss = 0.44045175\n",
      "Iteration 490, loss = 0.44031761\n",
      "Iteration 491, loss = 0.44029992\n",
      "Iteration 492, loss = 0.44014796\n",
      "Iteration 493, loss = 0.44005923\n",
      "Iteration 494, loss = 0.44006893\n",
      "Iteration 495, loss = 0.43989173\n",
      "Iteration 496, loss = 0.43989518\n",
      "Iteration 497, loss = 0.43975563\n",
      "Iteration 498, loss = 0.43973656\n",
      "Iteration 499, loss = 0.43963492\n",
      "Iteration 500, loss = 0.43947302\n",
      "Iteration 1, loss = 2.10868033\n",
      "Iteration 2, loss = 1.94688248\n",
      "Iteration 3, loss = 1.83122062\n",
      "Iteration 4, loss = 1.74297466"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 5, loss = 1.67076865\n",
      "Iteration 6, loss = 1.60823389\n",
      "Iteration 7, loss = 1.55148130\n",
      "Iteration 8, loss = 1.49843168\n",
      "Iteration 9, loss = 1.44800723\n",
      "Iteration 10, loss = 1.39988153\n",
      "Iteration 11, loss = 1.35408304\n",
      "Iteration 12, loss = 1.31073057\n",
      "Iteration 13, loss = 1.27007664\n",
      "Iteration 14, loss = 1.23221438\n",
      "Iteration 15, loss = 1.19714232\n",
      "Iteration 16, loss = 1.16467948\n",
      "Iteration 17, loss = 1.13466189\n",
      "Iteration 18, loss = 1.10684370\n",
      "Iteration 19, loss = 1.08108090\n",
      "Iteration 20, loss = 1.05708671\n",
      "Iteration 21, loss = 1.03471523\n",
      "Iteration 22, loss = 1.01370911\n",
      "Iteration 23, loss = 0.99397975\n",
      "Iteration 24, loss = 0.97539019\n",
      "Iteration 25, loss = 0.95773798\n",
      "Iteration 26, loss = 0.94099209\n",
      "Iteration 27, loss = 0.92509535\n",
      "Iteration 28, loss = 0.90994839\n",
      "Iteration 29, loss = 0.89552537\n",
      "Iteration 30, loss = 0.88171867\n",
      "Iteration 31, loss = 0.86852906\n",
      "Iteration 32, loss = 0.85590845\n",
      "Iteration 33, loss = 0.84378098\n",
      "Iteration 34, loss = 0.83215310\n",
      "Iteration 35, loss = 0.82099574\n",
      "Iteration 36, loss = 0.81025726\n",
      "Iteration 37, loss = 0.79995206\n",
      "Iteration 38, loss = 0.79002569\n",
      "Iteration 39, loss = 0.78043038\n",
      "Iteration 40, loss = 0.77122393\n",
      "Iteration 41, loss = 0.76238373\n",
      "Iteration 42, loss = 0.75387143\n",
      "Iteration 43, loss = 0.74569084\n",
      "Iteration 44, loss = 0.73781691\n",
      "Iteration 45, loss = 0.73027432\n",
      "Iteration 46, loss = 0.72303714\n",
      "Iteration 47, loss = 0.71608284\n",
      "Iteration 48, loss = 0.70939651\n",
      "Iteration 49, loss = 0.70300012\n",
      "Iteration 50, loss = 0.69685392\n",
      "Iteration 51, loss = 0.69090422\n",
      "Iteration 52, loss = 0.68521469\n",
      "Iteration 53, loss = 0.67973813\n",
      "Iteration 54, loss = 0.67443425\n",
      "Iteration 55, loss = 0.66932841\n",
      "Iteration 56, loss = 0.66445223\n",
      "Iteration 57, loss = 0.65973309\n",
      "Iteration 58, loss = 0.65517521\n",
      "Iteration 59, loss = 0.65078575\n",
      "Iteration 60, loss = 0.64654832\n",
      "Iteration 61, loss = 0.64245641\n",
      "Iteration 62, loss = 0.63847830\n",
      "Iteration 63, loss = 0.63464785\n",
      "Iteration 64, loss = 0.63093644\n",
      "Iteration 65, loss = 0.62733288\n",
      "Iteration 66, loss = 0.62383733\n",
      "Iteration 67, loss = 0.62044595\n",
      "Iteration 68, loss = 0.61721186\n",
      "Iteration 69, loss = 0.61400911\n",
      "Iteration 70, loss = 0.61092593\n",
      "Iteration 71, loss = 0.60792295\n",
      "Iteration 72, loss = 0.60502094\n",
      "Iteration 73, loss = 0.60220709\n",
      "Iteration 74, loss = 0.59941697\n",
      "Iteration 75, loss = 0.59676618\n",
      "Iteration 76, loss = 0.59412794\n",
      "Iteration 77, loss = 0.59158179\n",
      "Iteration 78, loss = 0.58914378\n",
      "Iteration 79, loss = 0.58669733\n",
      "Iteration 80, loss = 0.58437862\n",
      "Iteration 81, loss = 0.58204603\n",
      "Iteration 82, loss = 0.57983158\n",
      "Iteration 83, loss = 0.57764801\n",
      "Iteration 84, loss = 0.57552721\n",
      "Iteration 85, loss = 0.57338146\n",
      "Iteration 86, loss = 0.57136340\n",
      "Iteration 87, loss = 0.56939793\n",
      "Iteration 88, loss = 0.56741251\n",
      "Iteration 89, loss = 0.56552121\n",
      "Iteration 90, loss = 0.56369273\n",
      "Iteration 91, loss = 0.56184804\n",
      "Iteration 92, loss = 0.56006677\n",
      "Iteration 93, loss = 0.55831942\n",
      "Iteration 94, loss = 0.55662718\n",
      "Iteration 95, loss = 0.55496936\n",
      "Iteration 96, loss = 0.55328313\n",
      "Iteration 97, loss = 0.55172051\n",
      "Iteration 98, loss = 0.55014915\n",
      "Iteration 99, loss = 0.54854963\n",
      "Iteration 100, loss = 0.54705127\n",
      "Iteration 101, loss = 0.54559402\n",
      "Iteration 102, loss = 0.54410793\n",
      "Iteration 103, loss = 0.54269054\n",
      "Iteration 104, loss = 0.54129283\n",
      "Iteration 105, loss = 0.53986243\n",
      "Iteration 106, loss = 0.53850398\n",
      "Iteration 107, loss = 0.53721249\n",
      "Iteration 108, loss = 0.53588383\n",
      "Iteration 109, loss = 0.53462110\n",
      "Iteration 110, loss = 0.53335182\n",
      "Iteration 111, loss = 0.53209947\n",
      "Iteration 112, loss = 0.53086896\n",
      "Iteration 113, loss = 0.52972635\n",
      "Iteration 114, loss = 0.52855227\n",
      "Iteration 115, loss = 0.52738841\n",
      "Iteration 116, loss = 0.52623187\n",
      "Iteration 117, loss = 0.52509835\n",
      "Iteration 118, loss = 0.52405262\n",
      "Iteration 119, loss = 0.52298185\n",
      "Iteration 120, loss = 0.52188701\n",
      "Iteration 121, loss = 0.52085470\n",
      "Iteration 122, loss = 0.51985181\n",
      "Iteration 123, loss = 0.51880250\n",
      "Iteration 124, loss = 0.51788645\n",
      "Iteration 125, loss = 0.51686722\n",
      "Iteration 126, loss = 0.51588770\n",
      "Iteration 127, loss = 0.51499937\n",
      "Iteration 128, loss = 0.51406293\n",
      "Iteration 129, loss = 0.51309938\n",
      "Iteration 130, loss = 0.51225012\n",
      "Iteration 131, loss = 0.51142794\n",
      "Iteration 132, loss = 0.51057451\n",
      "Iteration 133, loss = 0.50968461\n",
      "Iteration 134, loss = 0.50883100\n",
      "Iteration 135, loss = 0.50798403\n",
      "Iteration 136, loss = 0.50724464\n",
      "Iteration 137, loss = 0.50641729\n",
      "Iteration 138, loss = 0.50561562\n",
      "Iteration 139, loss = 0.50488830\n",
      "Iteration 140, loss = 0.50409500\n",
      "Iteration 141, loss = 0.50338849\n",
      "Iteration 142, loss = 0.50260581\n",
      "Iteration 143, loss = 0.50188728\n",
      "Iteration 144, loss = 0.50126510\n",
      "Iteration 145, loss = 0.50051177\n",
      "Iteration 146, loss = 0.49983038\n",
      "Iteration 147, loss = 0.49917151\n",
      "Iteration 148, loss = 0.49854940\n",
      "Iteration 149, loss = 0.49780687\n",
      "Iteration 150, loss = 0.49720881\n",
      "Iteration 151, loss = 0.49654870\n",
      "Iteration 152, loss = 0.49591229\n",
      "Iteration 153, loss = 0.49530657\n",
      "Iteration 154, loss = 0.49473127\n",
      "Iteration 155, loss = 0.49408449\n",
      "Iteration 156, loss = 0.49357992\n",
      "Iteration 157, loss = 0.49297501\n",
      "Iteration 158, loss = 0.49236310\n",
      "Iteration 159, loss = 0.49186327\n",
      "Iteration 160, loss = 0.49128063\n",
      "Iteration 161, loss = 0.49077271\n",
      "Iteration 162, loss = 0.49019544\n",
      "Iteration 163, loss = 0.48965095\n",
      "Iteration 164, loss = 0.48914414\n",
      "Iteration 165, loss = 0.48864266\n",
      "Iteration 166, loss = 0.48809701\n",
      "Iteration 167, loss = 0.48769303\n",
      "Iteration 168, loss = 0.48718220\n",
      "Iteration 169, loss = 0.48663348\n",
      "Iteration 170, loss = 0.48619740\n",
      "Iteration 171, loss = 0.48574767\n",
      "Iteration 172, loss = 0.48523624\n",
      "Iteration 173, loss = 0.48484413\n",
      "Iteration 174, loss = 0.48441122\n",
      "Iteration 175, loss = 0.48385118\n",
      "Iteration 176, loss = 0.48349994\n",
      "Iteration 177, loss = 0.48303801\n",
      "Iteration 178, loss = 0.48270730\n",
      "Iteration 179, loss = 0.48223534\n",
      "Iteration 180, loss = 0.48178523\n",
      "Iteration 181, loss = 0.48142315\n",
      "Iteration 182, loss = 0.48100079\n",
      "Iteration 183, loss = 0.48066538\n",
      "Iteration 184, loss = 0.48031822\n",
      "Iteration 185, loss = 0.47981629\n",
      "Iteration 186, loss = 0.47942046\n",
      "Iteration 187, loss = 0.47906553\n",
      "Iteration 188, loss = 0.47880350\n",
      "Iteration 189, loss = 0.47841433\n",
      "Iteration 190, loss = 0.47802973\n",
      "Iteration 191, loss = 0.47771310\n",
      "Iteration 192, loss = 0.47733573\n",
      "Iteration 193, loss = 0.47694777\n",
      "Iteration 194, loss = 0.47660393\n",
      "Iteration 195, loss = 0.47629965\n",
      "Iteration 196, loss = 0.47599212\n",
      "Iteration 197, loss = 0.47564959\n",
      "Iteration 198, loss = 0.47542662\n",
      "Iteration 199, loss = 0.47498935\n",
      "Iteration 200, loss = 0.47468265\n",
      "Iteration 201, loss = 0.47436577\n",
      "Iteration 202, loss = 0.47409344\n",
      "Iteration 203, loss = 0.47378587\n",
      "Iteration 204, loss = 0.47353658\n",
      "Iteration 205, loss = 0.47319082\n",
      "Iteration 206, loss = 0.47295546\n",
      "Iteration 207, loss = 0.47265254\n",
      "Iteration 208, loss = 0.47237825\n",
      "Iteration 209, loss = 0.47201581\n",
      "Iteration 210, loss = 0.47178689\n",
      "Iteration 211, loss = 0.47156867\n",
      "Iteration 212, loss = 0.47121000\n",
      "Iteration 213, loss = 0.47105134\n",
      "Iteration 214, loss = 0.47072497\n",
      "Iteration 215, loss = 0.47049920\n",
      "Iteration 216, loss = 0.47024830\n",
      "Iteration 217, loss = 0.46995982\n",
      "Iteration 218, loss = 0.46972438\n",
      "Iteration 219, loss = 0.46951449\n",
      "Iteration 220, loss = 0.46927340\n",
      "Iteration 221, loss = 0.46893224\n",
      "Iteration 222, loss = 0.46870795\n",
      "Iteration 223, loss = 0.46850633\n",
      "Iteration 224, loss = 0.46828825\n",
      "Iteration 225, loss = 0.46807274\n",
      "Iteration 226, loss = 0.46776570\n",
      "Iteration 227, loss = 0.46757761\n",
      "Iteration 228, loss = 0.46741860\n",
      "Iteration 229, loss = 0.46721768\n",
      "Iteration 230, loss = 0.46695258\n",
      "Iteration 231, loss = 0.46674017\n",
      "Iteration 232, loss = 0.46650727\n",
      "Iteration 233, loss = 0.46632979\n",
      "Iteration 234, loss = 0.46612612\n",
      "Iteration 235, loss = 0.46590651\n",
      "Iteration 236, loss = 0.46568005\n",
      "Iteration 237, loss = 0.46554857\n",
      "Iteration 238, loss = 0.46528436\n",
      "Iteration 239, loss = 0.46508731\n",
      "Iteration 240, loss = 0.46493201\n",
      "Iteration 241, loss = 0.46467899\n",
      "Iteration 242, loss = 0.46458453\n",
      "Iteration 243, loss = 0.46440475\n",
      "Iteration 244, loss = 0.46418087\n",
      "Iteration 245, loss = 0.46399285\n",
      "Iteration 246, loss = 0.46388171\n",
      "Iteration 247, loss = 0.46362655\n",
      "Iteration 248, loss = 0.46347504\n",
      "Iteration 249, loss = 0.46325745\n",
      "Iteration 250, loss = 0.46311516\n",
      "Iteration 251, loss = 0.46292258\n",
      "Iteration 252, loss = 0.46272046\n",
      "Iteration 253, loss = 0.46264938\n",
      "Iteration 254, loss = 0.46244493\n",
      "Iteration 255, loss = 0.46218715\n",
      "Iteration 256, loss = 0.46209257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.46193894\n",
      "Iteration 258, loss = 0.46171916\n",
      "Iteration 259, loss = 0.46158349\n",
      "Iteration 260, loss = 0.46149830\n",
      "Iteration 261, loss = 0.46127182\n",
      "Iteration 262, loss = 0.46112581\n",
      "Iteration 263, loss = 0.46098616\n",
      "Iteration 264, loss = 0.46075875\n",
      "Iteration 265, loss = 0.46070087\n",
      "Iteration 266, loss = 0.46047056\n",
      "Iteration 267, loss = 0.46040116\n",
      "Iteration 268, loss = 0.46019425\n",
      "Iteration 269, loss = 0.46007317\n",
      "Iteration 270, loss = 0.45991000\n",
      "Iteration 271, loss = 0.45981859\n",
      "Iteration 272, loss = 0.45963926\n",
      "Iteration 273, loss = 0.45953498\n",
      "Iteration 274, loss = 0.45931202\n",
      "Iteration 275, loss = 0.45921891\n",
      "Iteration 276, loss = 0.45921593\n",
      "Iteration 277, loss = 0.45898891\n",
      "Iteration 278, loss = 0.45881008\n",
      "Iteration 279, loss = 0.45872234\n",
      "Iteration 280, loss = 0.45854235\n",
      "Iteration 281, loss = 0.45841166\n",
      "Iteration 282, loss = 0.45828292\n",
      "Iteration 283, loss = 0.45811649\n",
      "Iteration 284, loss = 0.45800360\n",
      "Iteration 285, loss = 0.45785770\n",
      "Iteration 286, loss = 0.45784445\n",
      "Iteration 287, loss = 0.45770674\n",
      "Iteration 288, loss = 0.45753211\n",
      "Iteration 289, loss = 0.45739085\n",
      "Iteration 290, loss = 0.45732815\n",
      "Iteration 291, loss = 0.45724117\n",
      "Iteration 292, loss = 0.45714349\n",
      "Iteration 293, loss = 0.45695467\n",
      "Iteration 294, loss = 0.45683191\n",
      "Iteration 295, loss = 0.45673422\n",
      "Iteration 296, loss = 0.45658195\n",
      "Iteration 297, loss = 0.45649226\n",
      "Iteration 298, loss = 0.45642963\n",
      "Iteration 299, loss = 0.45639227\n",
      "Iteration 300, loss = 0.45606781\n",
      "Iteration 301, loss = 0.45598993\n",
      "Iteration 302, loss = 0.45587215\n",
      "Iteration 303, loss = 0.45587780\n",
      "Iteration 304, loss = 0.45571745\n",
      "Iteration 305, loss = 0.45553413\n",
      "Iteration 306, loss = 0.45544777\n",
      "Iteration 307, loss = 0.45535330\n",
      "Iteration 308, loss = 0.45530056\n",
      "Iteration 309, loss = 0.45511309\n",
      "Iteration 310, loss = 0.45510510\n",
      "Iteration 311, loss = 0.45495517\n",
      "Iteration 312, loss = 0.45486096\n",
      "Iteration 313, loss = 0.45472740\n",
      "Iteration 314, loss = 0.45461262\n",
      "Iteration 315, loss = 0.45449213\n",
      "Iteration 316, loss = 0.45439651\n",
      "Iteration 317, loss = 0.45431917\n",
      "Iteration 318, loss = 0.45420157\n",
      "Iteration 319, loss = 0.45409911\n",
      "Iteration 320, loss = 0.45406969\n",
      "Iteration 321, loss = 0.45400146\n",
      "Iteration 322, loss = 0.45377894\n",
      "Iteration 323, loss = 0.45378951\n",
      "Iteration 324, loss = 0.45364253\n",
      "Iteration 325, loss = 0.45369523\n",
      "Iteration 326, loss = 0.45349206\n",
      "Iteration 327, loss = 0.45345337\n",
      "Iteration 328, loss = 0.45329999\n",
      "Iteration 329, loss = 0.45320356\n",
      "Iteration 330, loss = 0.45315337\n",
      "Iteration 331, loss = 0.45303173\n",
      "Iteration 332, loss = 0.45304320\n",
      "Iteration 333, loss = 0.45286972\n",
      "Iteration 334, loss = 0.45275515\n",
      "Iteration 335, loss = 0.45268006\n",
      "Iteration 336, loss = 0.45254204\n",
      "Iteration 337, loss = 0.45246318\n",
      "Iteration 338, loss = 0.45244137\n",
      "Iteration 339, loss = 0.45236512\n",
      "Iteration 340, loss = 0.45220847\n",
      "Iteration 341, loss = 0.45216088\n",
      "Iteration 342, loss = 0.45206376\n",
      "Iteration 343, loss = 0.45191711\n",
      "Iteration 344, loss = 0.45189645\n",
      "Iteration 345, loss = 0.45184812\n",
      "Iteration 346, loss = 0.45168945\n",
      "Iteration 347, loss = 0.45163522\n",
      "Iteration 348, loss = 0.45163285\n",
      "Iteration 349, loss = 0.45139976\n",
      "Iteration 350, loss = 0.45142759\n",
      "Iteration 351, loss = 0.45135598\n",
      "Iteration 352, loss = 0.45128384\n",
      "Iteration 353, loss = 0.45115549\n",
      "Iteration 354, loss = 0.45110323\n",
      "Iteration 355, loss = 0.45103862\n",
      "Iteration 356, loss = 0.45084116\n",
      "Iteration 357, loss = 0.45082405\n",
      "Iteration 358, loss = 0.45073826\n",
      "Iteration 359, loss = 0.45071964\n",
      "Iteration 360, loss = 0.45058654\n",
      "Iteration 361, loss = 0.45058308\n",
      "Iteration 362, loss = 0.45060836\n",
      "Iteration 363, loss = 0.45039498\n",
      "Iteration 364, loss = 0.45026596\n",
      "Iteration 365, loss = 0.45018735\n",
      "Iteration 366, loss = 0.45031211\n",
      "Iteration 367, loss = 0.45009350\n",
      "Iteration 368, loss = 0.44998772\n",
      "Iteration 369, loss = 0.44992518\n",
      "Iteration 370, loss = 0.44989318\n",
      "Iteration 371, loss = 0.44984367\n",
      "Iteration 372, loss = 0.44982900\n",
      "Iteration 373, loss = 0.44976717\n",
      "Iteration 374, loss = 0.44967586\n",
      "Iteration 375, loss = 0.44953442\n",
      "Iteration 376, loss = 0.44939069\n",
      "Iteration 377, loss = 0.44940328\n",
      "Iteration 378, loss = 0.44929662\n",
      "Iteration 379, loss = 0.44919814\n",
      "Iteration 380, loss = 0.44915934\n",
      "Iteration 381, loss = 0.44912140\n",
      "Iteration 382, loss = 0.44907589\n",
      "Iteration 383, loss = 0.44909721\n",
      "Iteration 384, loss = 0.44885844\n",
      "Iteration 385, loss = 0.44879341\n",
      "Iteration 386, loss = 0.44879145\n",
      "Iteration 387, loss = 0.44866939\n",
      "Iteration 388, loss = 0.44864169\n",
      "Iteration 389, loss = 0.44856539\n",
      "Iteration 390, loss = 0.44851214\n",
      "Iteration 391, loss = 0.44843605\n",
      "Iteration 392, loss = 0.44836730\n",
      "Iteration 393, loss = 0.44833116\n",
      "Iteration 394, loss = 0.44820316\n",
      "Iteration 395, loss = 0.44826848\n",
      "Iteration 396, loss = 0.44812875\n",
      "Iteration 397, loss = 0.44812755\n",
      "Iteration 398, loss = 0.44799815\n",
      "Iteration 399, loss = 0.44801355\n",
      "Iteration 400, loss = 0.44788220\n",
      "Iteration 401, loss = 0.44781869\n",
      "Iteration 402, loss = 0.44775206\n",
      "Iteration 403, loss = 0.44766691\n",
      "Iteration 404, loss = 0.44764917\n",
      "Iteration 405, loss = 0.44758718\n",
      "Iteration 406, loss = 0.44743243\n",
      "Iteration 407, loss = 0.44744072\n",
      "Iteration 408, loss = 0.44744094\n",
      "Iteration 409, loss = 0.44734121\n",
      "Iteration 410, loss = 0.44737324\n",
      "Iteration 411, loss = 0.44720617\n",
      "Iteration 412, loss = 0.44716427\n",
      "Iteration 413, loss = 0.44717766\n",
      "Iteration 414, loss = 0.44698465\n",
      "Iteration 415, loss = 0.44703270\n",
      "Iteration 416, loss = 0.44704743\n",
      "Iteration 417, loss = 0.44687770\n",
      "Iteration 418, loss = 0.44682501\n",
      "Iteration 419, loss = 0.44681847\n",
      "Iteration 420, loss = 0.44670420\n",
      "Iteration 421, loss = 0.44657015\n",
      "Iteration 422, loss = 0.44654697\n",
      "Iteration 423, loss = 0.44653116\n",
      "Iteration 424, loss = 0.44649015\n",
      "Iteration 425, loss = 0.44646913\n",
      "Iteration 426, loss = 0.44631769\n",
      "Iteration 427, loss = 0.44630964\n",
      "Iteration 428, loss = 0.44628151\n",
      "Iteration 429, loss = 0.44620643\n",
      "Iteration 430, loss = 0.44615389\n",
      "Iteration 431, loss = 0.44608148\n",
      "Iteration 432, loss = 0.44606374\n",
      "Iteration 433, loss = 0.44597563\n",
      "Iteration 434, loss = 0.44601995\n",
      "Iteration 435, loss = 0.44588684\n",
      "Iteration 436, loss = 0.44576998\n",
      "Iteration 437, loss = 0.44596105\n",
      "Iteration 438, loss = 0.44573365\n",
      "Iteration 439, loss = 0.44559014\n",
      "Iteration 440, loss = 0.44563327\n",
      "Iteration 441, loss = 0.44560101\n",
      "Iteration 442, loss = 0.44554437\n",
      "Iteration 443, loss = 0.44548156\n",
      "Iteration 444, loss = 0.44543130\n",
      "Iteration 445, loss = 0.44543552\n",
      "Iteration 446, loss = 0.44531624\n",
      "Iteration 447, loss = 0.44532531\n",
      "Iteration 448, loss = 0.44526776\n",
      "Iteration 449, loss = 0.44514649\n",
      "Iteration 450, loss = 0.44518526\n",
      "Iteration 451, loss = 0.44505343\n",
      "Iteration 452, loss = 0.44494396\n",
      "Iteration 453, loss = 0.44494989\n",
      "Iteration 454, loss = 0.44492448\n",
      "Iteration 455, loss = 0.44490391\n",
      "Iteration 456, loss = 0.44474962\n",
      "Iteration 457, loss = 0.44473632\n",
      "Iteration 458, loss = 0.44469566\n",
      "Iteration 459, loss = 0.44464718\n",
      "Iteration 460, loss = 0.44457836\n",
      "Iteration 461, loss = 0.44455070\n",
      "Iteration 462, loss = 0.44444014\n",
      "Iteration 463, loss = 0.44441829\n",
      "Iteration 464, loss = 0.44442881\n",
      "Iteration 465, loss = 0.44443352\n",
      "Iteration 466, loss = 0.44435455\n",
      "Iteration 467, loss = 0.44430938\n",
      "Iteration 468, loss = 0.44427721\n",
      "Iteration 469, loss = 0.44415489\n",
      "Iteration 470, loss = 0.44413366\n",
      "Iteration 471, loss = 0.44407309\n",
      "Iteration 472, loss = 0.44403768\n",
      "Iteration 473, loss = 0.44406453\n",
      "Iteration 474, loss = 0.44394699\n",
      "Iteration 475, loss = 0.44394655\n",
      "Iteration 476, loss = 0.44381173\n",
      "Iteration 477, loss = 0.44381927\n",
      "Iteration 478, loss = 0.44376102\n",
      "Iteration 479, loss = 0.44377686\n",
      "Iteration 480, loss = 0.44366614\n",
      "Iteration 481, loss = 0.44357468\n",
      "Iteration 482, loss = 0.44364166\n",
      "Iteration 483, loss = 0.44347375\n",
      "Iteration 484, loss = 0.44356050\n",
      "Iteration 485, loss = 0.44346238\n",
      "Iteration 486, loss = 0.44339048\n",
      "Iteration 487, loss = 0.44336992\n",
      "Iteration 488, loss = 0.44332970\n",
      "Iteration 489, loss = 0.44327984\n",
      "Iteration 490, loss = 0.44320180\n",
      "Iteration 491, loss = 0.44323651\n",
      "Iteration 492, loss = 0.44308082\n",
      "Iteration 493, loss = 0.44308755\n",
      "Iteration 494, loss = 0.44302263\n",
      "Iteration 495, loss = 0.44303455\n",
      "Iteration 496, loss = 0.44293673\n",
      "Iteration 497, loss = 0.44293557\n",
      "Iteration 498, loss = 0.44292298\n",
      "Iteration 499, loss = 0.44277280\n",
      "Iteration 500, loss = 0.44271444\n",
      "Iteration 1, loss = 2.11300134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.95006857\n",
      "Iteration 3, loss = 1.83378505\n",
      "Iteration 4, loss = 1.74508299\n",
      "Iteration 5, loss = 1.67254669\n",
      "Iteration 6, loss = 1.60953332\n",
      "Iteration 7, loss = 1.55237615\n",
      "Iteration 8, loss = 1.49895045\n",
      "Iteration 9, loss = 1.44820581\n",
      "Iteration 10, loss = 1.39970266\n",
      "Iteration 11, loss = 1.35355883\n",
      "Iteration 12, loss = 1.30989566\n",
      "Iteration 13, loss = 1.26900825\n",
      "Iteration 14, loss = 1.23089961\n",
      "Iteration 15, loss = 1.19566752\n",
      "Iteration 16, loss = 1.16309220\n",
      "Iteration 17, loss = 1.13299632\n",
      "Iteration 18, loss = 1.10521400\n",
      "Iteration 19, loss = 1.07948860\n",
      "Iteration 20, loss = 1.05560981\n",
      "Iteration 21, loss = 1.03335181\n",
      "Iteration 22, loss = 1.01251490\n",
      "Iteration 23, loss = 0.99290187\n",
      "Iteration 24, loss = 0.97439426\n",
      "Iteration 25, loss = 0.95684505\n",
      "Iteration 26, loss = 0.94014892\n",
      "Iteration 27, loss = 0.92420869\n",
      "Iteration 28, loss = 0.90903053\n",
      "Iteration 29, loss = 0.89454426\n",
      "Iteration 30, loss = 0.88067442\n",
      "Iteration 31, loss = 0.86738270\n",
      "Iteration 32, loss = 0.85465728\n",
      "Iteration 33, loss = 0.84243210\n",
      "Iteration 34, loss = 0.83070873\n",
      "Iteration 35, loss = 0.81944918\n",
      "Iteration 36, loss = 0.80862852\n",
      "Iteration 37, loss = 0.79821424\n",
      "Iteration 38, loss = 0.78821942\n",
      "Iteration 39, loss = 0.77860319\n",
      "Iteration 40, loss = 0.76935358\n",
      "Iteration 41, loss = 0.76043621\n",
      "Iteration 42, loss = 0.75189744\n",
      "Iteration 43, loss = 0.74368856\n",
      "Iteration 44, loss = 0.73580605\n",
      "Iteration 45, loss = 0.72823495\n",
      "Iteration 46, loss = 0.72098069\n",
      "Iteration 47, loss = 0.71397646\n",
      "Iteration 48, loss = 0.70727778\n",
      "Iteration 49, loss = 0.70084153\n",
      "Iteration 50, loss = 0.69466328\n",
      "Iteration 51, loss = 0.68870912\n",
      "Iteration 52, loss = 0.68297425\n",
      "Iteration 53, loss = 0.67748198\n",
      "Iteration 54, loss = 0.67217784\n",
      "Iteration 55, loss = 0.66704729\n",
      "Iteration 56, loss = 0.66215334\n",
      "Iteration 57, loss = 0.65739041\n",
      "Iteration 58, loss = 0.65281350\n",
      "Iteration 59, loss = 0.64837041\n",
      "Iteration 60, loss = 0.64410338\n",
      "Iteration 61, loss = 0.63997881\n",
      "Iteration 62, loss = 0.63597465\n",
      "Iteration 63, loss = 0.63214001\n",
      "Iteration 64, loss = 0.62839177\n",
      "Iteration 65, loss = 0.62477572\n",
      "Iteration 66, loss = 0.62124252\n",
      "Iteration 67, loss = 0.61781309\n",
      "Iteration 68, loss = 0.61453368\n",
      "Iteration 69, loss = 0.61130948\n",
      "Iteration 70, loss = 0.60820172\n",
      "Iteration 71, loss = 0.60516120\n",
      "Iteration 72, loss = 0.60217707\n",
      "Iteration 73, loss = 0.59933804\n",
      "Iteration 74, loss = 0.59657061\n",
      "Iteration 75, loss = 0.59386561\n",
      "Iteration 76, loss = 0.59123906\n",
      "Iteration 77, loss = 0.58863225\n",
      "Iteration 78, loss = 0.58611701\n",
      "Iteration 79, loss = 0.58371988\n",
      "Iteration 80, loss = 0.58134575\n",
      "Iteration 81, loss = 0.57899250\n",
      "Iteration 82, loss = 0.57674557\n",
      "Iteration 83, loss = 0.57450936\n",
      "Iteration 84, loss = 0.57237908\n",
      "Iteration 85, loss = 0.57027024\n",
      "Iteration 86, loss = 0.56822842\n",
      "Iteration 87, loss = 0.56616757\n",
      "Iteration 88, loss = 0.56419115\n",
      "Iteration 89, loss = 0.56222401\n",
      "Iteration 90, loss = 0.56038879\n",
      "Iteration 91, loss = 0.55852816\n",
      "Iteration 92, loss = 0.55675178\n",
      "Iteration 93, loss = 0.55496884\n",
      "Iteration 94, loss = 0.55324613\n",
      "Iteration 95, loss = 0.55157249\n",
      "Iteration 96, loss = 0.54990778\n",
      "Iteration 97, loss = 0.54823829\n",
      "Iteration 98, loss = 0.54667010\n",
      "Iteration 99, loss = 0.54509690\n",
      "Iteration 100, loss = 0.54359620\n",
      "Iteration 101, loss = 0.54203047\n",
      "Iteration 102, loss = 0.54056731\n",
      "Iteration 103, loss = 0.53914292\n",
      "Iteration 104, loss = 0.53772386\n",
      "Iteration 105, loss = 0.53632507\n",
      "Iteration 106, loss = 0.53496165\n",
      "Iteration 107, loss = 0.53366688\n",
      "Iteration 108, loss = 0.53234972\n",
      "Iteration 109, loss = 0.53103449\n",
      "Iteration 110, loss = 0.52975516\n",
      "Iteration 111, loss = 0.52851044\n",
      "Iteration 112, loss = 0.52727875\n",
      "Iteration 113, loss = 0.52607698\n",
      "Iteration 114, loss = 0.52492002\n",
      "Iteration 115, loss = 0.52370492\n",
      "Iteration 116, loss = 0.52255928\n",
      "Iteration 117, loss = 0.52144685\n",
      "Iteration 118, loss = 0.52032755\n",
      "Iteration 119, loss = 0.51925351\n",
      "Iteration 120, loss = 0.51818268\n",
      "Iteration 121, loss = 0.51715930\n",
      "Iteration 122, loss = 0.51613787\n",
      "Iteration 123, loss = 0.51508153\n",
      "Iteration 124, loss = 0.51408570\n",
      "Iteration 125, loss = 0.51316741\n",
      "Iteration 126, loss = 0.51216554\n",
      "Iteration 127, loss = 0.51124707\n",
      "Iteration 128, loss = 0.51029467\n",
      "Iteration 129, loss = 0.50938874\n",
      "Iteration 130, loss = 0.50852454\n",
      "Iteration 131, loss = 0.50763798\n",
      "Iteration 132, loss = 0.50673451\n",
      "Iteration 133, loss = 0.50587975\n",
      "Iteration 134, loss = 0.50504203\n",
      "Iteration 135, loss = 0.50423082\n",
      "Iteration 136, loss = 0.50341168\n",
      "Iteration 137, loss = 0.50264551\n",
      "Iteration 138, loss = 0.50177858\n",
      "Iteration 139, loss = 0.50106463\n",
      "Iteration 140, loss = 0.50027327\n",
      "Iteration 141, loss = 0.49950527\n",
      "Iteration 142, loss = 0.49884662\n",
      "Iteration 143, loss = 0.49804320\n",
      "Iteration 144, loss = 0.49737622\n",
      "Iteration 145, loss = 0.49661990\n",
      "Iteration 146, loss = 0.49597631\n",
      "Iteration 147, loss = 0.49521081\n",
      "Iteration 148, loss = 0.49463055\n",
      "Iteration 149, loss = 0.49395123\n",
      "Iteration 150, loss = 0.49337822\n",
      "Iteration 151, loss = 0.49267007\n",
      "Iteration 152, loss = 0.49203168\n",
      "Iteration 153, loss = 0.49139298\n",
      "Iteration 154, loss = 0.49084241\n",
      "Iteration 155, loss = 0.49016562\n",
      "Iteration 156, loss = 0.48966288\n",
      "Iteration 157, loss = 0.48900187\n",
      "Iteration 158, loss = 0.48852152\n",
      "Iteration 159, loss = 0.48790162\n",
      "Iteration 160, loss = 0.48736837\n",
      "Iteration 161, loss = 0.48677890\n",
      "Iteration 162, loss = 0.48631863\n",
      "Iteration 163, loss = 0.48573318\n",
      "Iteration 164, loss = 0.48520893\n",
      "Iteration 165, loss = 0.48469242\n",
      "Iteration 166, loss = 0.48416926\n",
      "Iteration 167, loss = 0.48365936\n",
      "Iteration 168, loss = 0.48315158\n",
      "Iteration 169, loss = 0.48279724\n",
      "Iteration 170, loss = 0.48224102\n",
      "Iteration 171, loss = 0.48173612\n",
      "Iteration 172, loss = 0.48135252\n",
      "Iteration 173, loss = 0.48082632\n",
      "Iteration 174, loss = 0.48039696\n",
      "Iteration 175, loss = 0.47997239\n",
      "Iteration 176, loss = 0.47947359\n",
      "Iteration 177, loss = 0.47907307\n",
      "Iteration 178, loss = 0.47861419\n",
      "Iteration 179, loss = 0.47818892\n",
      "Iteration 180, loss = 0.47777219\n",
      "Iteration 181, loss = 0.47741026\n",
      "Iteration 182, loss = 0.47705002\n",
      "Iteration 183, loss = 0.47666015\n",
      "Iteration 184, loss = 0.47621590\n",
      "Iteration 185, loss = 0.47580923\n",
      "Iteration 186, loss = 0.47546473\n",
      "Iteration 187, loss = 0.47507889\n",
      "Iteration 188, loss = 0.47470728\n",
      "Iteration 189, loss = 0.47436985\n",
      "Iteration 190, loss = 0.47401516\n",
      "Iteration 191, loss = 0.47365575\n",
      "Iteration 192, loss = 0.47325240\n",
      "Iteration 193, loss = 0.47295709\n",
      "Iteration 194, loss = 0.47259481\n",
      "Iteration 195, loss = 0.47223014\n",
      "Iteration 196, loss = 0.47194941\n",
      "Iteration 197, loss = 0.47161882\n",
      "Iteration 198, loss = 0.47123265\n",
      "Iteration 199, loss = 0.47092933\n",
      "Iteration 200, loss = 0.47060660\n",
      "Iteration 201, loss = 0.47036240\n",
      "Iteration 202, loss = 0.47002523\n",
      "Iteration 203, loss = 0.46975369\n",
      "Iteration 204, loss = 0.46945226\n",
      "Iteration 205, loss = 0.46914255\n",
      "Iteration 206, loss = 0.46889447\n",
      "Iteration 207, loss = 0.46850171\n",
      "Iteration 208, loss = 0.46823875\n",
      "Iteration 209, loss = 0.46805397\n",
      "Iteration 210, loss = 0.46773497\n",
      "Iteration 211, loss = 0.46744554\n",
      "Iteration 212, loss = 0.46717261\n",
      "Iteration 213, loss = 0.46695578\n",
      "Iteration 214, loss = 0.46668311\n",
      "Iteration 215, loss = 0.46639740\n",
      "Iteration 216, loss = 0.46609135\n",
      "Iteration 217, loss = 0.46588549\n",
      "Iteration 218, loss = 0.46561607\n",
      "Iteration 219, loss = 0.46541263\n",
      "Iteration 220, loss = 0.46513839\n",
      "Iteration 221, loss = 0.46487343\n",
      "Iteration 222, loss = 0.46462418\n",
      "Iteration 223, loss = 0.46440720\n",
      "Iteration 224, loss = 0.46417456\n",
      "Iteration 225, loss = 0.46395659\n",
      "Iteration 226, loss = 0.46370340\n",
      "Iteration 227, loss = 0.46353095\n",
      "Iteration 228, loss = 0.46321602\n",
      "Iteration 229, loss = 0.46303576\n",
      "Iteration 230, loss = 0.46282331\n",
      "Iteration 231, loss = 0.46262747\n",
      "Iteration 232, loss = 0.46236698\n",
      "Iteration 233, loss = 0.46213121\n",
      "Iteration 234, loss = 0.46195012\n",
      "Iteration 235, loss = 0.46172983\n",
      "Iteration 236, loss = 0.46160043\n",
      "Iteration 237, loss = 0.46129567\n",
      "Iteration 238, loss = 0.46117883\n",
      "Iteration 239, loss = 0.46093579\n",
      "Iteration 240, loss = 0.46073409\n",
      "Iteration 241, loss = 0.46054277\n",
      "Iteration 242, loss = 0.46038196\n",
      "Iteration 243, loss = 0.46020637\n",
      "Iteration 244, loss = 0.45998456\n",
      "Iteration 245, loss = 0.45978907\n",
      "Iteration 246, loss = 0.45962741\n",
      "Iteration 247, loss = 0.45945157\n",
      "Iteration 248, loss = 0.45933613\n",
      "Iteration 249, loss = 0.45909818\n",
      "Iteration 250, loss = 0.45895413\n",
      "Iteration 251, loss = 0.45875774\n",
      "Iteration 252, loss = 0.45858277\n",
      "Iteration 253, loss = 0.45845245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.45816518\n",
      "Iteration 255, loss = 0.45803780\n",
      "Iteration 256, loss = 0.45782124\n",
      "Iteration 257, loss = 0.45774416\n",
      "Iteration 258, loss = 0.45758455\n",
      "Iteration 259, loss = 0.45741096\n",
      "Iteration 260, loss = 0.45717654\n",
      "Iteration 261, loss = 0.45706090\n",
      "Iteration 262, loss = 0.45686186\n",
      "Iteration 263, loss = 0.45677152\n",
      "Iteration 264, loss = 0.45669629\n",
      "Iteration 265, loss = 0.45642748\n",
      "Iteration 266, loss = 0.45624040\n",
      "Iteration 267, loss = 0.45613128\n",
      "Iteration 268, loss = 0.45594653\n",
      "Iteration 269, loss = 0.45586396\n",
      "Iteration 270, loss = 0.45568349\n",
      "Iteration 271, loss = 0.45554434\n",
      "Iteration 272, loss = 0.45544107\n",
      "Iteration 273, loss = 0.45532430\n",
      "Iteration 274, loss = 0.45510229\n",
      "Iteration 275, loss = 0.45500555\n",
      "Iteration 276, loss = 0.45482797\n",
      "Iteration 277, loss = 0.45471847\n",
      "Iteration 278, loss = 0.45459166\n",
      "Iteration 279, loss = 0.45442694\n",
      "Iteration 280, loss = 0.45434967\n",
      "Iteration 281, loss = 0.45424241\n",
      "Iteration 282, loss = 0.45398818\n",
      "Iteration 283, loss = 0.45387675\n",
      "Iteration 284, loss = 0.45381511\n",
      "Iteration 285, loss = 0.45370572\n",
      "Iteration 286, loss = 0.45348576\n",
      "Iteration 287, loss = 0.45334386\n",
      "Iteration 288, loss = 0.45324696\n",
      "Iteration 289, loss = 0.45311592\n",
      "Iteration 290, loss = 0.45299805\n",
      "Iteration 291, loss = 0.45297931\n",
      "Iteration 292, loss = 0.45279156\n",
      "Iteration 293, loss = 0.45264464\n",
      "Iteration 294, loss = 0.45248195\n",
      "Iteration 295, loss = 0.45238746\n",
      "Iteration 296, loss = 0.45227108\n",
      "Iteration 297, loss = 0.45222824\n",
      "Iteration 298, loss = 0.45209952\n",
      "Iteration 299, loss = 0.45192156\n",
      "Iteration 300, loss = 0.45187445\n",
      "Iteration 301, loss = 0.45177529\n",
      "Iteration 302, loss = 0.45167053\n",
      "Iteration 303, loss = 0.45154625\n",
      "Iteration 304, loss = 0.45130177\n",
      "Iteration 305, loss = 0.45119721\n",
      "Iteration 306, loss = 0.45119625\n",
      "Iteration 307, loss = 0.45103641\n",
      "Iteration 308, loss = 0.45092882\n",
      "Iteration 309, loss = 0.45091635\n",
      "Iteration 310, loss = 0.45069717\n",
      "Iteration 311, loss = 0.45061548\n",
      "Iteration 312, loss = 0.45052579\n",
      "Iteration 313, loss = 0.45037704\n",
      "Iteration 314, loss = 0.45026576\n",
      "Iteration 315, loss = 0.45020086\n",
      "Iteration 316, loss = 0.45008009\n",
      "Iteration 317, loss = 0.45000647\n",
      "Iteration 318, loss = 0.44982204\n",
      "Iteration 319, loss = 0.44979102\n",
      "Iteration 320, loss = 0.44973717\n",
      "Iteration 321, loss = 0.44959625\n",
      "Iteration 322, loss = 0.44953268\n",
      "Iteration 323, loss = 0.44939359\n",
      "Iteration 324, loss = 0.44930543\n",
      "Iteration 325, loss = 0.44921815\n",
      "Iteration 326, loss = 0.44908155\n",
      "Iteration 327, loss = 0.44899621\n",
      "Iteration 328, loss = 0.44893612\n",
      "Iteration 329, loss = 0.44882701\n",
      "Iteration 330, loss = 0.44873213\n",
      "Iteration 331, loss = 0.44871373\n",
      "Iteration 332, loss = 0.44860762\n",
      "Iteration 333, loss = 0.44840372\n",
      "Iteration 334, loss = 0.44829181\n",
      "Iteration 335, loss = 0.44823427\n",
      "Iteration 336, loss = 0.44820799\n",
      "Iteration 337, loss = 0.44812996\n",
      "Iteration 338, loss = 0.44807656\n",
      "Iteration 339, loss = 0.44790726\n",
      "Iteration 340, loss = 0.44781669\n",
      "Iteration 341, loss = 0.44773596\n",
      "Iteration 342, loss = 0.44764972\n",
      "Iteration 343, loss = 0.44765850\n",
      "Iteration 344, loss = 0.44755535\n",
      "Iteration 345, loss = 0.44732466\n",
      "Iteration 346, loss = 0.44734966\n",
      "Iteration 347, loss = 0.44727672\n",
      "Iteration 348, loss = 0.44721856\n",
      "Iteration 349, loss = 0.44707386\n",
      "Iteration 350, loss = 0.44697160\n",
      "Iteration 351, loss = 0.44685854\n",
      "Iteration 352, loss = 0.44682151\n",
      "Iteration 353, loss = 0.44678530\n",
      "Iteration 354, loss = 0.44664383\n",
      "Iteration 355, loss = 0.44670759\n",
      "Iteration 356, loss = 0.44650370\n",
      "Iteration 357, loss = 0.44648409\n",
      "Iteration 358, loss = 0.44628909\n",
      "Iteration 359, loss = 0.44623673\n",
      "Iteration 360, loss = 0.44617178\n",
      "Iteration 361, loss = 0.44611463\n",
      "Iteration 362, loss = 0.44607568\n",
      "Iteration 363, loss = 0.44592612\n",
      "Iteration 364, loss = 0.44585038\n",
      "Iteration 365, loss = 0.44586719\n",
      "Iteration 366, loss = 0.44579775\n",
      "Iteration 367, loss = 0.44560853\n",
      "Iteration 368, loss = 0.44555933\n",
      "Iteration 369, loss = 0.44560669\n",
      "Iteration 370, loss = 0.44547186\n",
      "Iteration 371, loss = 0.44546966\n",
      "Iteration 372, loss = 0.44530006\n",
      "Iteration 373, loss = 0.44518287\n",
      "Iteration 374, loss = 0.44515851\n",
      "Iteration 375, loss = 0.44508889\n",
      "Iteration 376, loss = 0.44492077\n",
      "Iteration 377, loss = 0.44500104\n",
      "Iteration 378, loss = 0.44486382\n",
      "Iteration 379, loss = 0.44478645\n",
      "Iteration 380, loss = 0.44466025\n",
      "Iteration 381, loss = 0.44471114\n",
      "Iteration 382, loss = 0.44463736\n",
      "Iteration 383, loss = 0.44452714\n",
      "Iteration 384, loss = 0.44451500\n",
      "Iteration 385, loss = 0.44435396\n",
      "Iteration 386, loss = 0.44427676\n",
      "Iteration 387, loss = 0.44424316\n",
      "Iteration 388, loss = 0.44418346\n",
      "Iteration 389, loss = 0.44417391\n",
      "Iteration 390, loss = 0.44406783\n",
      "Iteration 391, loss = 0.44403583\n",
      "Iteration 392, loss = 0.44395523\n",
      "Iteration 393, loss = 0.44388510\n",
      "Iteration 394, loss = 0.44375716\n",
      "Iteration 395, loss = 0.44381912\n",
      "Iteration 396, loss = 0.44363424\n",
      "Iteration 397, loss = 0.44364758\n",
      "Iteration 398, loss = 0.44359638\n",
      "Iteration 399, loss = 0.44351494\n",
      "Iteration 400, loss = 0.44340576\n",
      "Iteration 401, loss = 0.44330005\n",
      "Iteration 402, loss = 0.44327252\n",
      "Iteration 403, loss = 0.44321592\n",
      "Iteration 404, loss = 0.44316366\n",
      "Iteration 405, loss = 0.44307336\n",
      "Iteration 406, loss = 0.44306559\n",
      "Iteration 407, loss = 0.44293903\n",
      "Iteration 408, loss = 0.44288514\n",
      "Iteration 409, loss = 0.44281785\n",
      "Iteration 410, loss = 0.44279051\n",
      "Iteration 411, loss = 0.44270861\n",
      "Iteration 412, loss = 0.44265712\n",
      "Iteration 413, loss = 0.44259240\n",
      "Iteration 414, loss = 0.44251897\n",
      "Iteration 415, loss = 0.44256881\n",
      "Iteration 416, loss = 0.44246004\n",
      "Iteration 417, loss = 0.44228836\n",
      "Iteration 418, loss = 0.44233028\n",
      "Iteration 419, loss = 0.44228601\n",
      "Iteration 420, loss = 0.44219179\n",
      "Iteration 421, loss = 0.44210496\n",
      "Iteration 422, loss = 0.44212527\n",
      "Iteration 423, loss = 0.44211673\n",
      "Iteration 424, loss = 0.44202617\n",
      "Iteration 425, loss = 0.44197328\n",
      "Iteration 426, loss = 0.44186928\n",
      "Iteration 427, loss = 0.44183434\n",
      "Iteration 428, loss = 0.44181007\n",
      "Iteration 429, loss = 0.44171691\n",
      "Iteration 430, loss = 0.44164483\n",
      "Iteration 431, loss = 0.44160021\n",
      "Iteration 432, loss = 0.44161171\n",
      "Iteration 433, loss = 0.44142531\n",
      "Iteration 434, loss = 0.44137075\n",
      "Iteration 435, loss = 0.44133972\n",
      "Iteration 436, loss = 0.44134421\n",
      "Iteration 437, loss = 0.44125552\n",
      "Iteration 438, loss = 0.44118978\n",
      "Iteration 439, loss = 0.44118450\n",
      "Iteration 440, loss = 0.44109942\n",
      "Iteration 441, loss = 0.44108802\n",
      "Iteration 442, loss = 0.44102917\n",
      "Iteration 443, loss = 0.44100293\n",
      "Iteration 444, loss = 0.44089642\n",
      "Iteration 445, loss = 0.44090962\n",
      "Iteration 446, loss = 0.44076060\n",
      "Iteration 447, loss = 0.44069825\n",
      "Iteration 448, loss = 0.44072136\n",
      "Iteration 449, loss = 0.44064096\n",
      "Iteration 450, loss = 0.44057150\n",
      "Iteration 451, loss = 0.44057969\n",
      "Iteration 452, loss = 0.44050610\n",
      "Iteration 453, loss = 0.44042569\n",
      "Iteration 454, loss = 0.44042256\n",
      "Iteration 455, loss = 0.44035212\n",
      "Iteration 456, loss = 0.44027664\n",
      "Iteration 457, loss = 0.44020724\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85345826\n",
      "Iteration 2, loss = 1.60612961\n",
      "Iteration 3, loss = 1.40936386\n",
      "Iteration 4, loss = 1.25078254\n",
      "Iteration 5, loss = 1.12396573\n",
      "Iteration 6, loss = 1.02138813\n",
      "Iteration 7, loss = 0.93822814\n",
      "Iteration 8, loss = 0.87033837\n",
      "Iteration 9, loss = 0.81440507\n",
      "Iteration 10, loss = 0.76808907\n",
      "Iteration 11, loss = 0.72931365\n",
      "Iteration 12, loss = 0.69667258\n",
      "Iteration 13, loss = 0.66873673\n",
      "Iteration 14, loss = 0.64504372\n",
      "Iteration 15, loss = 0.62444172\n",
      "Iteration 16, loss = 0.60634412\n",
      "Iteration 17, loss = 0.59036394\n",
      "Iteration 18, loss = 0.57629976\n",
      "Iteration 19, loss = 0.56299942\n",
      "Iteration 20, loss = 0.55038001\n",
      "Iteration 21, loss = 0.54011605\n",
      "Iteration 22, loss = 0.53101988\n",
      "Iteration 23, loss = 0.52285042\n",
      "Iteration 24, loss = 0.51546084\n",
      "Iteration 25, loss = 0.50856294\n",
      "Iteration 26, loss = 0.50265318\n",
      "Iteration 27, loss = 0.49718654\n",
      "Iteration 28, loss = 0.49256237\n",
      "Iteration 29, loss = 0.48770670\n",
      "Iteration 30, loss = 0.48347213\n",
      "Iteration 31, loss = 0.47982939\n",
      "Iteration 32, loss = 0.47640451\n",
      "Iteration 33, loss = 0.47321923\n",
      "Iteration 34, loss = 0.47013340\n",
      "Iteration 35, loss = 0.46757958\n",
      "Iteration 36, loss = 0.46513982\n",
      "Iteration 37, loss = 0.46288376\n",
      "Iteration 38, loss = 0.46067552\n",
      "Iteration 39, loss = 0.45842790\n",
      "Iteration 40, loss = 0.45658848\n",
      "Iteration 41, loss = 0.45453537\n",
      "Iteration 42, loss = 0.45288424\n",
      "Iteration 43, loss = 0.45089701\n",
      "Iteration 44, loss = 0.44953032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.44778471\n",
      "Iteration 46, loss = 0.44656195\n",
      "Iteration 47, loss = 0.44515381\n",
      "Iteration 48, loss = 0.44373441\n",
      "Iteration 49, loss = 0.44239261\n",
      "Iteration 50, loss = 0.44098282\n",
      "Iteration 51, loss = 0.43971384\n",
      "Iteration 52, loss = 0.43878529\n",
      "Iteration 53, loss = 0.43738780\n",
      "Iteration 54, loss = 0.43656511\n",
      "Iteration 55, loss = 0.43523715\n",
      "Iteration 56, loss = 0.43408488\n",
      "Iteration 57, loss = 0.43275770\n",
      "Iteration 58, loss = 0.43186462\n",
      "Iteration 59, loss = 0.43079555\n",
      "Iteration 60, loss = 0.43009916\n",
      "Iteration 61, loss = 0.42907129\n",
      "Iteration 62, loss = 0.42804282\n",
      "Iteration 63, loss = 0.42724972\n",
      "Iteration 64, loss = 0.42648055\n",
      "Iteration 65, loss = 0.42545563\n",
      "Iteration 66, loss = 0.42467710\n",
      "Iteration 67, loss = 0.42393758\n",
      "Iteration 68, loss = 0.42303157\n",
      "Iteration 69, loss = 0.42233632\n",
      "Iteration 70, loss = 0.42140701\n",
      "Iteration 71, loss = 0.42074535\n",
      "Iteration 72, loss = 0.42009790\n",
      "Iteration 73, loss = 0.41931177\n",
      "Iteration 74, loss = 0.41847061\n",
      "Iteration 75, loss = 0.41778551\n",
      "Iteration 76, loss = 0.41722099\n",
      "Iteration 77, loss = 0.41652072\n",
      "Iteration 78, loss = 0.41598715\n",
      "Iteration 79, loss = 0.41558398\n",
      "Iteration 80, loss = 0.41489276\n",
      "Iteration 81, loss = 0.41412239\n",
      "Iteration 82, loss = 0.41388339\n",
      "Iteration 83, loss = 0.41318316\n",
      "Iteration 84, loss = 0.41265006\n",
      "Iteration 85, loss = 0.41193306\n",
      "Iteration 86, loss = 0.41157993\n",
      "Iteration 87, loss = 0.41118937\n",
      "Iteration 88, loss = 0.41061960\n",
      "Iteration 89, loss = 0.41002786\n",
      "Iteration 90, loss = 0.40954027\n",
      "Iteration 91, loss = 0.40917595\n",
      "Iteration 92, loss = 0.40874805\n",
      "Iteration 93, loss = 0.40838492\n",
      "Iteration 94, loss = 0.40771707\n",
      "Iteration 95, loss = 0.40766344\n",
      "Iteration 96, loss = 0.40699395\n",
      "Iteration 97, loss = 0.40681097\n",
      "Iteration 98, loss = 0.40637446\n",
      "Iteration 99, loss = 0.40611911\n",
      "Iteration 100, loss = 0.40566598\n",
      "Iteration 101, loss = 0.40547371\n",
      "Iteration 102, loss = 0.40504910\n",
      "Iteration 103, loss = 0.40467079\n",
      "Iteration 104, loss = 0.40429031\n",
      "Iteration 105, loss = 0.40402183\n",
      "Iteration 106, loss = 0.40371289\n",
      "Iteration 107, loss = 0.40352828\n",
      "Iteration 108, loss = 0.40317228\n",
      "Iteration 109, loss = 0.40294979\n",
      "Iteration 110, loss = 0.40247201\n",
      "Iteration 111, loss = 0.40239067\n",
      "Iteration 112, loss = 0.40217993\n",
      "Iteration 113, loss = 0.40174004\n",
      "Iteration 114, loss = 0.40139357\n",
      "Iteration 115, loss = 0.40132915\n",
      "Iteration 116, loss = 0.40097216\n",
      "Iteration 117, loss = 0.40091556\n",
      "Iteration 118, loss = 0.40025316\n",
      "Iteration 119, loss = 0.40023839\n",
      "Iteration 120, loss = 0.39987022\n",
      "Iteration 121, loss = 0.39960827\n",
      "Iteration 122, loss = 0.39963087\n",
      "Iteration 123, loss = 0.39940740\n",
      "Iteration 124, loss = 0.39918816\n",
      "Iteration 125, loss = 0.39908057\n",
      "Iteration 126, loss = 0.39856846\n",
      "Iteration 127, loss = 0.39858573\n",
      "Iteration 128, loss = 0.39821773\n",
      "Iteration 129, loss = 0.39801250\n",
      "Iteration 130, loss = 0.39801257\n",
      "Iteration 131, loss = 0.39757586\n",
      "Iteration 132, loss = 0.39736661\n",
      "Iteration 133, loss = 0.39741286\n",
      "Iteration 134, loss = 0.39695448\n",
      "Iteration 135, loss = 0.39703417\n",
      "Iteration 136, loss = 0.39661799\n",
      "Iteration 137, loss = 0.39642856\n",
      "Iteration 138, loss = 0.39641787\n",
      "Iteration 139, loss = 0.39623653\n",
      "Iteration 140, loss = 0.39586322\n",
      "Iteration 141, loss = 0.39575909\n",
      "Iteration 142, loss = 0.39557240\n",
      "Iteration 143, loss = 0.39536622\n",
      "Iteration 144, loss = 0.39532328\n",
      "Iteration 145, loss = 0.39517584\n",
      "Iteration 146, loss = 0.39478748\n",
      "Iteration 147, loss = 0.39464092\n",
      "Iteration 148, loss = 0.39434737\n",
      "Iteration 149, loss = 0.39463024\n",
      "Iteration 150, loss = 0.39421561\n",
      "Iteration 151, loss = 0.39415464\n",
      "Iteration 152, loss = 0.39386577\n",
      "Iteration 153, loss = 0.39369618\n",
      "Iteration 154, loss = 0.39363036\n",
      "Iteration 155, loss = 0.39358190\n",
      "Iteration 156, loss = 0.39337843\n",
      "Iteration 157, loss = 0.39327875\n",
      "Iteration 158, loss = 0.39309680\n",
      "Iteration 159, loss = 0.39309493\n",
      "Iteration 160, loss = 0.39303447\n",
      "Iteration 161, loss = 0.39251376\n",
      "Iteration 162, loss = 0.39245740\n",
      "Iteration 163, loss = 0.39255172\n",
      "Iteration 164, loss = 0.39222137\n",
      "Iteration 165, loss = 0.39228355\n",
      "Iteration 166, loss = 0.39200364\n",
      "Iteration 167, loss = 0.39218096\n",
      "Iteration 168, loss = 0.39152934\n",
      "Iteration 169, loss = 0.39178193\n",
      "Iteration 170, loss = 0.39136872\n",
      "Iteration 171, loss = 0.39125889\n",
      "Iteration 172, loss = 0.39120445\n",
      "Iteration 173, loss = 0.39118446\n",
      "Iteration 174, loss = 0.39108683\n",
      "Iteration 175, loss = 0.39080161\n",
      "Iteration 176, loss = 0.39071601\n",
      "Iteration 177, loss = 0.39070904\n",
      "Iteration 178, loss = 0.39065131\n",
      "Iteration 179, loss = 0.39068253\n",
      "Iteration 180, loss = 0.39051840\n",
      "Iteration 181, loss = 0.39025498\n",
      "Iteration 182, loss = 0.38998998\n",
      "Iteration 183, loss = 0.39012259\n",
      "Iteration 184, loss = 0.39008068\n",
      "Iteration 185, loss = 0.38961075\n",
      "Iteration 186, loss = 0.38978099\n",
      "Iteration 187, loss = 0.38956561\n",
      "Iteration 188, loss = 0.38961251\n",
      "Iteration 189, loss = 0.38947615\n",
      "Iteration 190, loss = 0.38961198\n",
      "Iteration 191, loss = 0.38962217\n",
      "Iteration 192, loss = 0.38914319\n",
      "Iteration 193, loss = 0.38896440\n",
      "Iteration 194, loss = 0.38899335\n",
      "Iteration 195, loss = 0.38883357\n",
      "Iteration 196, loss = 0.38888828\n",
      "Iteration 197, loss = 0.38875096\n",
      "Iteration 198, loss = 0.38848101\n",
      "Iteration 199, loss = 0.38830031\n",
      "Iteration 200, loss = 0.38844385\n",
      "Iteration 201, loss = 0.38843924\n",
      "Iteration 202, loss = 0.38831748\n",
      "Iteration 203, loss = 0.38794191\n",
      "Iteration 204, loss = 0.38800546\n",
      "Iteration 205, loss = 0.38786402\n",
      "Iteration 206, loss = 0.38786645\n",
      "Iteration 207, loss = 0.38781501\n",
      "Iteration 208, loss = 0.38752710\n",
      "Iteration 209, loss = 0.38781239\n",
      "Iteration 210, loss = 0.38754017\n",
      "Iteration 211, loss = 0.38730827\n",
      "Iteration 212, loss = 0.38749260\n",
      "Iteration 213, loss = 0.38721620\n",
      "Iteration 214, loss = 0.38729806\n",
      "Iteration 215, loss = 0.38715351\n",
      "Iteration 216, loss = 0.38737956\n",
      "Iteration 217, loss = 0.38710698\n",
      "Iteration 218, loss = 0.38704262\n",
      "Iteration 219, loss = 0.38701636\n",
      "Iteration 220, loss = 0.38687659\n",
      "Iteration 221, loss = 0.38682010\n",
      "Iteration 222, loss = 0.38660660\n",
      "Iteration 223, loss = 0.38665435\n",
      "Iteration 224, loss = 0.38624672\n",
      "Iteration 225, loss = 0.38642465\n",
      "Iteration 226, loss = 0.38634689\n",
      "Iteration 227, loss = 0.38622137\n",
      "Iteration 228, loss = 0.38623194\n",
      "Iteration 229, loss = 0.38604099\n",
      "Iteration 230, loss = 0.38610426\n",
      "Iteration 231, loss = 0.38601736\n",
      "Iteration 232, loss = 0.38604050\n",
      "Iteration 233, loss = 0.38565097\n",
      "Iteration 234, loss = 0.38562436\n",
      "Iteration 235, loss = 0.38569188\n",
      "Iteration 236, loss = 0.38569657\n",
      "Iteration 237, loss = 0.38561573\n",
      "Iteration 238, loss = 0.38539569\n",
      "Iteration 239, loss = 0.38542820\n",
      "Iteration 240, loss = 0.38551405\n",
      "Iteration 241, loss = 0.38527771\n",
      "Iteration 242, loss = 0.38534679\n",
      "Iteration 243, loss = 0.38518054\n",
      "Iteration 244, loss = 0.38533886\n",
      "Iteration 245, loss = 0.38518059\n",
      "Iteration 246, loss = 0.38506092\n",
      "Iteration 247, loss = 0.38491894\n",
      "Iteration 248, loss = 0.38498477\n",
      "Iteration 249, loss = 0.38480125\n",
      "Iteration 250, loss = 0.38521039\n",
      "Iteration 251, loss = 0.38489072\n",
      "Iteration 252, loss = 0.38444771\n",
      "Iteration 253, loss = 0.38454131\n",
      "Iteration 254, loss = 0.38450719\n",
      "Iteration 255, loss = 0.38436647\n",
      "Iteration 256, loss = 0.38433375\n",
      "Iteration 257, loss = 0.38452651\n",
      "Iteration 258, loss = 0.38442151\n",
      "Iteration 259, loss = 0.38444065\n",
      "Iteration 260, loss = 0.38388444\n",
      "Iteration 261, loss = 0.38424441\n",
      "Iteration 262, loss = 0.38396388\n",
      "Iteration 263, loss = 0.38402721\n",
      "Iteration 264, loss = 0.38411305\n",
      "Iteration 265, loss = 0.38385365\n",
      "Iteration 266, loss = 0.38379032\n",
      "Iteration 267, loss = 0.38366993\n",
      "Iteration 268, loss = 0.38354986\n",
      "Iteration 269, loss = 0.38368927\n",
      "Iteration 270, loss = 0.38337581\n",
      "Iteration 271, loss = 0.38372201\n",
      "Iteration 272, loss = 0.38331241\n",
      "Iteration 273, loss = 0.38353926\n",
      "Iteration 274, loss = 0.38337063\n",
      "Iteration 275, loss = 0.38328678\n",
      "Iteration 276, loss = 0.38330079\n",
      "Iteration 277, loss = 0.38313091\n",
      "Iteration 278, loss = 0.38318937\n",
      "Iteration 279, loss = 0.38304204\n",
      "Iteration 280, loss = 0.38315519\n",
      "Iteration 281, loss = 0.38300994\n",
      "Iteration 282, loss = 0.38311423\n",
      "Iteration 283, loss = 0.38278949\n",
      "Iteration 284, loss = 0.38295883\n",
      "Iteration 285, loss = 0.38264374\n",
      "Iteration 286, loss = 0.38290536\n",
      "Iteration 287, loss = 0.38270307\n",
      "Iteration 288, loss = 0.38266111\n",
      "Iteration 289, loss = 0.38271755\n",
      "Iteration 290, loss = 0.38261891\n",
      "Iteration 291, loss = 0.38242383\n",
      "Iteration 292, loss = 0.38247570\n",
      "Iteration 293, loss = 0.38268236\n",
      "Iteration 294, loss = 0.38250810\n",
      "Iteration 295, loss = 0.38235859\n",
      "Iteration 296, loss = 0.38245857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 297, loss = 0.38213867\n",
      "Iteration 298, loss = 0.38243640\n",
      "Iteration 299, loss = 0.38210254\n",
      "Iteration 300, loss = 0.38210201\n",
      "Iteration 301, loss = 0.38217050\n",
      "Iteration 302, loss = 0.38208629\n",
      "Iteration 303, loss = 0.38191938\n",
      "Iteration 304, loss = 0.38191998\n",
      "Iteration 305, loss = 0.38194195\n",
      "Iteration 306, loss = 0.38191306\n",
      "Iteration 307, loss = 0.38207505\n",
      "Iteration 308, loss = 0.38163907\n",
      "Iteration 309, loss = 0.38192096\n",
      "Iteration 310, loss = 0.38179644\n",
      "Iteration 311, loss = 0.38191031\n",
      "Iteration 312, loss = 0.38167224\n",
      "Iteration 313, loss = 0.38165842\n",
      "Iteration 314, loss = 0.38142619\n",
      "Iteration 315, loss = 0.38162467\n",
      "Iteration 316, loss = 0.38156453\n",
      "Iteration 317, loss = 0.38147785\n",
      "Iteration 318, loss = 0.38133271\n",
      "Iteration 319, loss = 0.38149659\n",
      "Iteration 320, loss = 0.38135195\n",
      "Iteration 321, loss = 0.38144877\n",
      "Iteration 322, loss = 0.38101997\n",
      "Iteration 323, loss = 0.38130960\n",
      "Iteration 324, loss = 0.38124987\n",
      "Iteration 325, loss = 0.38126010\n",
      "Iteration 326, loss = 0.38103934\n",
      "Iteration 327, loss = 0.38116600\n",
      "Iteration 328, loss = 0.38125553\n",
      "Iteration 329, loss = 0.38120792\n",
      "Iteration 330, loss = 0.38087538\n",
      "Iteration 331, loss = 0.38097793\n",
      "Iteration 332, loss = 0.38111878\n",
      "Iteration 333, loss = 0.38091350\n",
      "Iteration 334, loss = 0.38101288\n",
      "Iteration 335, loss = 0.38090990\n",
      "Iteration 336, loss = 0.38080443\n",
      "Iteration 337, loss = 0.38077590\n",
      "Iteration 338, loss = 0.38076939\n",
      "Iteration 339, loss = 0.38069617\n",
      "Iteration 340, loss = 0.38072090\n",
      "Iteration 341, loss = 0.38049187\n",
      "Iteration 342, loss = 0.38041893\n",
      "Iteration 343, loss = 0.38051767\n",
      "Iteration 344, loss = 0.38047903\n",
      "Iteration 345, loss = 0.38049661\n",
      "Iteration 346, loss = 0.38051976\n",
      "Iteration 347, loss = 0.38055919\n",
      "Iteration 348, loss = 0.38045541\n",
      "Iteration 349, loss = 0.38061243\n",
      "Iteration 350, loss = 0.38025124\n",
      "Iteration 351, loss = 0.38021592\n",
      "Iteration 352, loss = 0.38032391\n",
      "Iteration 353, loss = 0.38021573\n",
      "Iteration 354, loss = 0.38012448\n",
      "Iteration 355, loss = 0.38012298\n",
      "Iteration 356, loss = 0.38005748\n",
      "Iteration 357, loss = 0.38002759\n",
      "Iteration 358, loss = 0.37972432\n",
      "Iteration 359, loss = 0.37992963\n",
      "Iteration 360, loss = 0.37997158\n",
      "Iteration 361, loss = 0.37999814\n",
      "Iteration 362, loss = 0.37995365\n",
      "Iteration 363, loss = 0.37975896\n",
      "Iteration 364, loss = 0.37974024\n",
      "Iteration 365, loss = 0.37979513\n",
      "Iteration 366, loss = 0.38000570\n",
      "Iteration 367, loss = 0.37977688\n",
      "Iteration 368, loss = 0.37998316\n",
      "Iteration 369, loss = 0.37988579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85038783\n",
      "Iteration 2, loss = 1.60289761\n",
      "Iteration 3, loss = 1.40547647\n",
      "Iteration 4, loss = 1.24705471\n",
      "Iteration 5, loss = 1.12050302\n",
      "Iteration 6, loss = 1.01789062\n",
      "Iteration 7, loss = 0.93450434\n",
      "Iteration 8, loss = 0.86670252\n",
      "Iteration 9, loss = 0.81089439\n",
      "Iteration 10, loss = 0.76463686\n",
      "Iteration 11, loss = 0.72584710\n",
      "Iteration 12, loss = 0.69335072\n",
      "Iteration 13, loss = 0.66596115\n",
      "Iteration 14, loss = 0.64218579\n",
      "Iteration 15, loss = 0.62168559\n",
      "Iteration 16, loss = 0.60364309\n",
      "Iteration 17, loss = 0.58785705\n",
      "Iteration 18, loss = 0.57344429\n",
      "Iteration 19, loss = 0.56011571\n",
      "Iteration 20, loss = 0.54834386\n",
      "Iteration 21, loss = 0.53812122\n",
      "Iteration 22, loss = 0.52904632\n",
      "Iteration 23, loss = 0.52113766\n",
      "Iteration 24, loss = 0.51384124\n",
      "Iteration 25, loss = 0.50703817\n",
      "Iteration 26, loss = 0.50131670\n",
      "Iteration 27, loss = 0.49611287\n",
      "Iteration 28, loss = 0.49087511\n",
      "Iteration 29, loss = 0.48656770\n",
      "Iteration 30, loss = 0.48265309\n",
      "Iteration 31, loss = 0.47896740\n",
      "Iteration 32, loss = 0.47539154\n",
      "Iteration 33, loss = 0.47237688\n",
      "Iteration 34, loss = 0.46963978\n",
      "Iteration 35, loss = 0.46686410\n",
      "Iteration 36, loss = 0.46432410\n",
      "Iteration 37, loss = 0.46178751\n",
      "Iteration 38, loss = 0.45974542\n",
      "Iteration 39, loss = 0.45762861\n",
      "Iteration 40, loss = 0.45576763\n",
      "Iteration 41, loss = 0.45375399\n",
      "Iteration 42, loss = 0.45209319\n",
      "Iteration 43, loss = 0.45049008\n",
      "Iteration 44, loss = 0.44894661\n",
      "Iteration 45, loss = 0.44734313\n",
      "Iteration 46, loss = 0.44588891\n",
      "Iteration 47, loss = 0.44453878\n",
      "Iteration 48, loss = 0.44304731\n",
      "Iteration 49, loss = 0.44174496\n",
      "Iteration 50, loss = 0.44041319\n",
      "Iteration 51, loss = 0.43923692\n",
      "Iteration 52, loss = 0.43815375\n",
      "Iteration 53, loss = 0.43666631\n",
      "Iteration 54, loss = 0.43583002\n",
      "Iteration 55, loss = 0.43463211\n",
      "Iteration 56, loss = 0.43359223\n",
      "Iteration 57, loss = 0.43256917\n",
      "Iteration 58, loss = 0.43153315\n",
      "Iteration 59, loss = 0.43033682\n",
      "Iteration 60, loss = 0.42950301\n",
      "Iteration 61, loss = 0.42843459\n",
      "Iteration 62, loss = 0.42780997\n",
      "Iteration 63, loss = 0.42672737\n",
      "Iteration 64, loss = 0.42582908\n",
      "Iteration 65, loss = 0.42516092\n",
      "Iteration 66, loss = 0.42440046\n",
      "Iteration 67, loss = 0.42342457\n",
      "Iteration 68, loss = 0.42263236\n",
      "Iteration 69, loss = 0.42191816\n",
      "Iteration 70, loss = 0.42102076\n",
      "Iteration 71, loss = 0.42034905\n",
      "Iteration 72, loss = 0.41966310\n",
      "Iteration 73, loss = 0.41927056\n",
      "Iteration 74, loss = 0.41853583\n",
      "Iteration 75, loss = 0.41751508\n",
      "Iteration 76, loss = 0.41677542\n",
      "Iteration 77, loss = 0.41632387\n",
      "Iteration 78, loss = 0.41574850\n",
      "Iteration 79, loss = 0.41513096\n",
      "Iteration 80, loss = 0.41456969\n",
      "Iteration 81, loss = 0.41400186\n",
      "Iteration 82, loss = 0.41330213\n",
      "Iteration 83, loss = 0.41294143\n",
      "Iteration 84, loss = 0.41243603\n",
      "Iteration 85, loss = 0.41167211\n",
      "Iteration 86, loss = 0.41118649\n",
      "Iteration 87, loss = 0.41090318\n",
      "Iteration 88, loss = 0.41018544\n",
      "Iteration 89, loss = 0.40995573\n",
      "Iteration 90, loss = 0.40942575\n",
      "Iteration 91, loss = 0.40921832\n",
      "Iteration 92, loss = 0.40838898\n",
      "Iteration 93, loss = 0.40805072\n",
      "Iteration 94, loss = 0.40775711\n",
      "Iteration 95, loss = 0.40711037\n",
      "Iteration 96, loss = 0.40703372\n",
      "Iteration 97, loss = 0.40636530\n",
      "Iteration 98, loss = 0.40604208\n",
      "Iteration 99, loss = 0.40590297\n",
      "Iteration 100, loss = 0.40548623\n",
      "Iteration 101, loss = 0.40506810\n",
      "Iteration 102, loss = 0.40483620\n",
      "Iteration 103, loss = 0.40432670\n",
      "Iteration 104, loss = 0.40421836\n",
      "Iteration 105, loss = 0.40388004\n",
      "Iteration 106, loss = 0.40375730\n",
      "Iteration 107, loss = 0.40336500\n",
      "Iteration 108, loss = 0.40289363\n",
      "Iteration 109, loss = 0.40291589\n",
      "Iteration 110, loss = 0.40220755\n",
      "Iteration 111, loss = 0.40235510\n",
      "Iteration 112, loss = 0.40200259\n",
      "Iteration 113, loss = 0.40167485\n",
      "Iteration 114, loss = 0.40144288\n",
      "Iteration 115, loss = 0.40114281\n",
      "Iteration 116, loss = 0.40068075\n",
      "Iteration 117, loss = 0.40072341\n",
      "Iteration 118, loss = 0.40048617\n",
      "Iteration 119, loss = 0.40010817\n",
      "Iteration 120, loss = 0.39976768\n",
      "Iteration 121, loss = 0.39955400\n",
      "Iteration 122, loss = 0.39934152\n",
      "Iteration 123, loss = 0.39917858\n",
      "Iteration 124, loss = 0.39885555\n",
      "Iteration 125, loss = 0.39879038\n",
      "Iteration 126, loss = 0.39856332\n",
      "Iteration 127, loss = 0.39801559\n",
      "Iteration 128, loss = 0.39808613\n",
      "Iteration 129, loss = 0.39779619\n",
      "Iteration 130, loss = 0.39745250\n",
      "Iteration 131, loss = 0.39736806\n",
      "Iteration 132, loss = 0.39727592\n",
      "Iteration 133, loss = 0.39679350\n",
      "Iteration 134, loss = 0.39696680\n",
      "Iteration 135, loss = 0.39659763\n",
      "Iteration 136, loss = 0.39643906\n",
      "Iteration 137, loss = 0.39616708\n",
      "Iteration 138, loss = 0.39609243\n",
      "Iteration 139, loss = 0.39574881\n",
      "Iteration 140, loss = 0.39580793\n",
      "Iteration 141, loss = 0.39549665\n",
      "Iteration 142, loss = 0.39524510\n",
      "Iteration 143, loss = 0.39500529\n",
      "Iteration 144, loss = 0.39494741\n",
      "Iteration 145, loss = 0.39483987\n",
      "Iteration 146, loss = 0.39482666\n",
      "Iteration 147, loss = 0.39450648\n",
      "Iteration 148, loss = 0.39442488\n",
      "Iteration 149, loss = 0.39391483\n",
      "Iteration 150, loss = 0.39373573\n",
      "Iteration 151, loss = 0.39374239\n",
      "Iteration 152, loss = 0.39347787\n",
      "Iteration 153, loss = 0.39330398\n",
      "Iteration 154, loss = 0.39311945\n",
      "Iteration 155, loss = 0.39314885\n",
      "Iteration 156, loss = 0.39287274\n",
      "Iteration 157, loss = 0.39272342\n",
      "Iteration 158, loss = 0.39278309\n",
      "Iteration 159, loss = 0.39258577\n",
      "Iteration 160, loss = 0.39258652\n",
      "Iteration 161, loss = 0.39228446\n",
      "Iteration 162, loss = 0.39198636\n",
      "Iteration 163, loss = 0.39207802\n",
      "Iteration 164, loss = 0.39186689\n",
      "Iteration 165, loss = 0.39167604\n",
      "Iteration 166, loss = 0.39174625\n",
      "Iteration 167, loss = 0.39126866\n",
      "Iteration 168, loss = 0.39109856\n",
      "Iteration 169, loss = 0.39100013\n",
      "Iteration 170, loss = 0.39102632\n",
      "Iteration 171, loss = 0.39115753\n",
      "Iteration 172, loss = 0.39070332\n",
      "Iteration 173, loss = 0.39058991\n",
      "Iteration 174, loss = 0.39071036\n",
      "Iteration 175, loss = 0.39041448\n",
      "Iteration 176, loss = 0.39003069\n",
      "Iteration 177, loss = 0.39025012\n",
      "Iteration 178, loss = 0.38990065\n",
      "Iteration 179, loss = 0.38983513\n",
      "Iteration 180, loss = 0.38946593\n",
      "Iteration 181, loss = 0.38963511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 182, loss = 0.38953089\n",
      "Iteration 183, loss = 0.38928255\n",
      "Iteration 184, loss = 0.38919311\n",
      "Iteration 185, loss = 0.38913054\n",
      "Iteration 186, loss = 0.38891240\n",
      "Iteration 187, loss = 0.38884030\n",
      "Iteration 188, loss = 0.38894192\n",
      "Iteration 189, loss = 0.38853581\n",
      "Iteration 190, loss = 0.38881018\n",
      "Iteration 191, loss = 0.38844740\n",
      "Iteration 192, loss = 0.38843788\n",
      "Iteration 193, loss = 0.38828696\n",
      "Iteration 194, loss = 0.38834851\n",
      "Iteration 195, loss = 0.38816801\n",
      "Iteration 196, loss = 0.38807454\n",
      "Iteration 197, loss = 0.38792473\n",
      "Iteration 198, loss = 0.38768936\n",
      "Iteration 199, loss = 0.38765185\n",
      "Iteration 200, loss = 0.38755824\n",
      "Iteration 201, loss = 0.38771421\n",
      "Iteration 202, loss = 0.38723468\n",
      "Iteration 203, loss = 0.38750072\n",
      "Iteration 204, loss = 0.38723325\n",
      "Iteration 205, loss = 0.38733931\n",
      "Iteration 206, loss = 0.38732219\n",
      "Iteration 207, loss = 0.38704459\n",
      "Iteration 208, loss = 0.38696142\n",
      "Iteration 209, loss = 0.38693479\n",
      "Iteration 210, loss = 0.38662798\n",
      "Iteration 211, loss = 0.38654922\n",
      "Iteration 212, loss = 0.38665962\n",
      "Iteration 213, loss = 0.38655669\n",
      "Iteration 214, loss = 0.38642329\n",
      "Iteration 215, loss = 0.38670252\n",
      "Iteration 216, loss = 0.38628115\n",
      "Iteration 217, loss = 0.38615421\n",
      "Iteration 218, loss = 0.38585562\n",
      "Iteration 219, loss = 0.38621598\n",
      "Iteration 220, loss = 0.38604521\n",
      "Iteration 221, loss = 0.38604517\n",
      "Iteration 222, loss = 0.38581574\n",
      "Iteration 223, loss = 0.38553385\n",
      "Iteration 224, loss = 0.38588124\n",
      "Iteration 225, loss = 0.38565703\n",
      "Iteration 226, loss = 0.38548634\n",
      "Iteration 227, loss = 0.38577424\n",
      "Iteration 228, loss = 0.38543858\n",
      "Iteration 229, loss = 0.38558292\n",
      "Iteration 230, loss = 0.38493696\n",
      "Iteration 231, loss = 0.38506865\n",
      "Iteration 232, loss = 0.38479558\n",
      "Iteration 233, loss = 0.38524747\n",
      "Iteration 234, loss = 0.38519971\n",
      "Iteration 235, loss = 0.38497492\n",
      "Iteration 236, loss = 0.38500626\n",
      "Iteration 237, loss = 0.38458251\n",
      "Iteration 238, loss = 0.38465990\n",
      "Iteration 239, loss = 0.38454645\n",
      "Iteration 240, loss = 0.38460531\n",
      "Iteration 241, loss = 0.38447514\n",
      "Iteration 242, loss = 0.38441444\n",
      "Iteration 243, loss = 0.38457758\n",
      "Iteration 244, loss = 0.38447629\n",
      "Iteration 245, loss = 0.38416286\n",
      "Iteration 246, loss = 0.38426326\n",
      "Iteration 247, loss = 0.38405741\n",
      "Iteration 248, loss = 0.38401443\n",
      "Iteration 249, loss = 0.38383103\n",
      "Iteration 250, loss = 0.38374233\n",
      "Iteration 251, loss = 0.38374301\n",
      "Iteration 252, loss = 0.38401353\n",
      "Iteration 253, loss = 0.38402462\n",
      "Iteration 254, loss = 0.38400723\n",
      "Iteration 255, loss = 0.38351569\n",
      "Iteration 256, loss = 0.38368417\n",
      "Iteration 257, loss = 0.38353047\n",
      "Iteration 258, loss = 0.38346692\n",
      "Iteration 259, loss = 0.38376566\n",
      "Iteration 260, loss = 0.38346565\n",
      "Iteration 261, loss = 0.38332463\n",
      "Iteration 262, loss = 0.38341569\n",
      "Iteration 263, loss = 0.38288409\n",
      "Iteration 264, loss = 0.38300184\n",
      "Iteration 265, loss = 0.38327448\n",
      "Iteration 266, loss = 0.38300939\n",
      "Iteration 267, loss = 0.38308051\n",
      "Iteration 268, loss = 0.38291787\n",
      "Iteration 269, loss = 0.38294423\n",
      "Iteration 270, loss = 0.38273044\n",
      "Iteration 271, loss = 0.38283104\n",
      "Iteration 272, loss = 0.38282603\n",
      "Iteration 273, loss = 0.38265287\n",
      "Iteration 274, loss = 0.38276717\n",
      "Iteration 275, loss = 0.38264026\n",
      "Iteration 276, loss = 0.38258090\n",
      "Iteration 277, loss = 0.38261638\n",
      "Iteration 278, loss = 0.38238464\n",
      "Iteration 279, loss = 0.38245587\n",
      "Iteration 280, loss = 0.38223989\n",
      "Iteration 281, loss = 0.38241215\n",
      "Iteration 282, loss = 0.38245342\n",
      "Iteration 283, loss = 0.38224495\n",
      "Iteration 284, loss = 0.38210025\n",
      "Iteration 285, loss = 0.38228552\n",
      "Iteration 286, loss = 0.38203782\n",
      "Iteration 287, loss = 0.38212762\n",
      "Iteration 288, loss = 0.38221901\n",
      "Iteration 289, loss = 0.38185771\n",
      "Iteration 290, loss = 0.38203144\n",
      "Iteration 291, loss = 0.38220663\n",
      "Iteration 292, loss = 0.38200564\n",
      "Iteration 293, loss = 0.38206570\n",
      "Iteration 294, loss = 0.38175570\n",
      "Iteration 295, loss = 0.38180647\n",
      "Iteration 296, loss = 0.38191369\n",
      "Iteration 297, loss = 0.38144701\n",
      "Iteration 298, loss = 0.38163918\n",
      "Iteration 299, loss = 0.38162517\n",
      "Iteration 300, loss = 0.38159682\n",
      "Iteration 301, loss = 0.38142056\n",
      "Iteration 302, loss = 0.38139053\n",
      "Iteration 303, loss = 0.38156720\n",
      "Iteration 304, loss = 0.38114261\n",
      "Iteration 305, loss = 0.38126150\n",
      "Iteration 306, loss = 0.38132317\n",
      "Iteration 307, loss = 0.38139568\n",
      "Iteration 308, loss = 0.38107498\n",
      "Iteration 309, loss = 0.38124964\n",
      "Iteration 310, loss = 0.38107739\n",
      "Iteration 311, loss = 0.38094566\n",
      "Iteration 312, loss = 0.38127475\n",
      "Iteration 313, loss = 0.38092248\n",
      "Iteration 314, loss = 0.38076929\n",
      "Iteration 315, loss = 0.38088074\n",
      "Iteration 316, loss = 0.38078459\n",
      "Iteration 317, loss = 0.38075284\n",
      "Iteration 318, loss = 0.38055303\n",
      "Iteration 319, loss = 0.38085928\n",
      "Iteration 320, loss = 0.38079438\n",
      "Iteration 321, loss = 0.38052349\n",
      "Iteration 322, loss = 0.38044275\n",
      "Iteration 323, loss = 0.38051098\n",
      "Iteration 324, loss = 0.38052722\n",
      "Iteration 325, loss = 0.38044088\n",
      "Iteration 326, loss = 0.38072506\n",
      "Iteration 327, loss = 0.38050339\n",
      "Iteration 328, loss = 0.38018837\n",
      "Iteration 329, loss = 0.38049284\n",
      "Iteration 330, loss = 0.38058443\n",
      "Iteration 331, loss = 0.38048813\n",
      "Iteration 332, loss = 0.38020663\n",
      "Iteration 333, loss = 0.38034068\n",
      "Iteration 334, loss = 0.38017368\n",
      "Iteration 335, loss = 0.38029044\n",
      "Iteration 336, loss = 0.38017018\n",
      "Iteration 337, loss = 0.38009261\n",
      "Iteration 338, loss = 0.38023234\n",
      "Iteration 339, loss = 0.38017220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85197900\n",
      "Iteration 2, loss = 1.60524959\n",
      "Iteration 3, loss = 1.40755936\n",
      "Iteration 4, loss = 1.24887305\n",
      "Iteration 5, loss = 1.12190543\n",
      "Iteration 6, loss = 1.01884589\n",
      "Iteration 7, loss = 0.93527964\n",
      "Iteration 8, loss = 0.86731817\n",
      "Iteration 9, loss = 0.81170876\n",
      "Iteration 10, loss = 0.76536597\n",
      "Iteration 11, loss = 0.72670565\n",
      "Iteration 12, loss = 0.69386158\n",
      "Iteration 13, loss = 0.66628686\n",
      "Iteration 14, loss = 0.64218492\n",
      "Iteration 15, loss = 0.62168993\n",
      "Iteration 16, loss = 0.60394100\n",
      "Iteration 17, loss = 0.58781802\n",
      "Iteration 18, loss = 0.57338493\n",
      "Iteration 19, loss = 0.55992178\n",
      "Iteration 20, loss = 0.54781853\n",
      "Iteration 21, loss = 0.53803301\n",
      "Iteration 22, loss = 0.52887672\n",
      "Iteration 23, loss = 0.52067222\n",
      "Iteration 24, loss = 0.51323577\n",
      "Iteration 25, loss = 0.50681412\n",
      "Iteration 26, loss = 0.50070865\n",
      "Iteration 27, loss = 0.49533734\n",
      "Iteration 28, loss = 0.49065952\n",
      "Iteration 29, loss = 0.48611524\n",
      "Iteration 30, loss = 0.48187839\n",
      "Iteration 31, loss = 0.47815683\n",
      "Iteration 32, loss = 0.47458658\n",
      "Iteration 33, loss = 0.47167575\n",
      "Iteration 34, loss = 0.46865771\n",
      "Iteration 35, loss = 0.46598086\n",
      "Iteration 36, loss = 0.46347282\n",
      "Iteration 37, loss = 0.46086312\n",
      "Iteration 38, loss = 0.45870770\n",
      "Iteration 39, loss = 0.45675851\n",
      "Iteration 40, loss = 0.45486459\n",
      "Iteration 41, loss = 0.45273982\n",
      "Iteration 42, loss = 0.45113402\n",
      "Iteration 43, loss = 0.44951905\n",
      "Iteration 44, loss = 0.44774711\n",
      "Iteration 45, loss = 0.44619149\n",
      "Iteration 46, loss = 0.44463657\n",
      "Iteration 47, loss = 0.44312903\n",
      "Iteration 48, loss = 0.44188310\n",
      "Iteration 49, loss = 0.44046861\n",
      "Iteration 50, loss = 0.43917947\n",
      "Iteration 51, loss = 0.43794963\n",
      "Iteration 52, loss = 0.43670754\n",
      "Iteration 53, loss = 0.43562067\n",
      "Iteration 54, loss = 0.43444204\n",
      "Iteration 55, loss = 0.43347974\n",
      "Iteration 56, loss = 0.43203547\n",
      "Iteration 57, loss = 0.43134937\n",
      "Iteration 58, loss = 0.43009995\n",
      "Iteration 59, loss = 0.42935057\n",
      "Iteration 60, loss = 0.42801426\n",
      "Iteration 61, loss = 0.42728742\n",
      "Iteration 62, loss = 0.42644120\n",
      "Iteration 63, loss = 0.42549645\n",
      "Iteration 64, loss = 0.42448801\n",
      "Iteration 65, loss = 0.42351012\n",
      "Iteration 66, loss = 0.42281797\n",
      "Iteration 67, loss = 0.42234754\n",
      "Iteration 68, loss = 0.42136507\n",
      "Iteration 69, loss = 0.42067819\n",
      "Iteration 70, loss = 0.41964343\n",
      "Iteration 71, loss = 0.41928960\n",
      "Iteration 72, loss = 0.41859829\n",
      "Iteration 73, loss = 0.41759467\n",
      "Iteration 74, loss = 0.41722928\n",
      "Iteration 75, loss = 0.41653211\n",
      "Iteration 76, loss = 0.41591029\n",
      "Iteration 77, loss = 0.41532121\n",
      "Iteration 78, loss = 0.41456505\n",
      "Iteration 79, loss = 0.41398025\n",
      "Iteration 80, loss = 0.41365386\n",
      "Iteration 81, loss = 0.41290058\n",
      "Iteration 82, loss = 0.41244310\n",
      "Iteration 83, loss = 0.41200265\n",
      "Iteration 84, loss = 0.41150765\n",
      "Iteration 85, loss = 0.41108384\n",
      "Iteration 86, loss = 0.41020871\n",
      "Iteration 87, loss = 0.41008185\n",
      "Iteration 88, loss = 0.40961414\n",
      "Iteration 89, loss = 0.40914177\n",
      "Iteration 90, loss = 0.40870874\n",
      "Iteration 91, loss = 0.40865069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92, loss = 0.40778608\n",
      "Iteration 93, loss = 0.40753266\n",
      "Iteration 94, loss = 0.40699874\n",
      "Iteration 95, loss = 0.40687733\n",
      "Iteration 96, loss = 0.40627073\n",
      "Iteration 97, loss = 0.40610139\n",
      "Iteration 98, loss = 0.40543675\n",
      "Iteration 99, loss = 0.40529639\n",
      "Iteration 100, loss = 0.40497553\n",
      "Iteration 101, loss = 0.40466418\n",
      "Iteration 102, loss = 0.40429081\n",
      "Iteration 103, loss = 0.40395204\n",
      "Iteration 104, loss = 0.40370797\n",
      "Iteration 105, loss = 0.40347702\n",
      "Iteration 106, loss = 0.40305243\n",
      "Iteration 107, loss = 0.40295640\n",
      "Iteration 108, loss = 0.40248234\n",
      "Iteration 109, loss = 0.40212673\n",
      "Iteration 110, loss = 0.40175048\n",
      "Iteration 111, loss = 0.40151143\n",
      "Iteration 112, loss = 0.40117516\n",
      "Iteration 113, loss = 0.40136934\n",
      "Iteration 114, loss = 0.40092786\n",
      "Iteration 115, loss = 0.40075624\n",
      "Iteration 116, loss = 0.40038726\n",
      "Iteration 117, loss = 0.40032117\n",
      "Iteration 118, loss = 0.39992472\n",
      "Iteration 119, loss = 0.39977583\n",
      "Iteration 120, loss = 0.39932432\n",
      "Iteration 121, loss = 0.39914470\n",
      "Iteration 122, loss = 0.39924261\n",
      "Iteration 123, loss = 0.39870796\n",
      "Iteration 124, loss = 0.39859118\n",
      "Iteration 125, loss = 0.39806359\n",
      "Iteration 126, loss = 0.39792316\n",
      "Iteration 127, loss = 0.39802279\n",
      "Iteration 128, loss = 0.39782433\n",
      "Iteration 129, loss = 0.39732195\n",
      "Iteration 130, loss = 0.39733355\n",
      "Iteration 131, loss = 0.39686639\n",
      "Iteration 132, loss = 0.39680509\n",
      "Iteration 133, loss = 0.39651609\n",
      "Iteration 134, loss = 0.39642787\n",
      "Iteration 135, loss = 0.39599275\n",
      "Iteration 136, loss = 0.39616641\n",
      "Iteration 137, loss = 0.39585598\n",
      "Iteration 138, loss = 0.39573447\n",
      "Iteration 139, loss = 0.39570325\n",
      "Iteration 140, loss = 0.39528356\n",
      "Iteration 141, loss = 0.39536258\n",
      "Iteration 142, loss = 0.39505687\n",
      "Iteration 143, loss = 0.39487683\n",
      "Iteration 144, loss = 0.39454493\n",
      "Iteration 145, loss = 0.39433655\n",
      "Iteration 146, loss = 0.39420389\n",
      "Iteration 147, loss = 0.39422924\n",
      "Iteration 148, loss = 0.39382412\n",
      "Iteration 149, loss = 0.39369292\n",
      "Iteration 150, loss = 0.39382669\n",
      "Iteration 151, loss = 0.39344810\n",
      "Iteration 152, loss = 0.39321357\n",
      "Iteration 153, loss = 0.39326548\n",
      "Iteration 154, loss = 0.39286481\n",
      "Iteration 155, loss = 0.39274741\n",
      "Iteration 156, loss = 0.39265895\n",
      "Iteration 157, loss = 0.39285242\n",
      "Iteration 158, loss = 0.39246918\n",
      "Iteration 159, loss = 0.39243992\n",
      "Iteration 160, loss = 0.39222397\n",
      "Iteration 161, loss = 0.39205836\n",
      "Iteration 162, loss = 0.39196325\n",
      "Iteration 163, loss = 0.39164552\n",
      "Iteration 164, loss = 0.39177147\n",
      "Iteration 165, loss = 0.39146797\n",
      "Iteration 166, loss = 0.39138715\n",
      "Iteration 167, loss = 0.39123108\n",
      "Iteration 168, loss = 0.39109183\n",
      "Iteration 169, loss = 0.39086299\n",
      "Iteration 170, loss = 0.39076032\n",
      "Iteration 171, loss = 0.39097199\n",
      "Iteration 172, loss = 0.39081224\n",
      "Iteration 173, loss = 0.39053236\n",
      "Iteration 174, loss = 0.39038198\n",
      "Iteration 175, loss = 0.39021933\n",
      "Iteration 176, loss = 0.39009612\n",
      "Iteration 177, loss = 0.39021580\n",
      "Iteration 178, loss = 0.38997431\n",
      "Iteration 179, loss = 0.38989302\n",
      "Iteration 180, loss = 0.38960769\n",
      "Iteration 181, loss = 0.38960912\n",
      "Iteration 182, loss = 0.38938570\n",
      "Iteration 183, loss = 0.38931820\n",
      "Iteration 184, loss = 0.38947785\n",
      "Iteration 185, loss = 0.38944482\n",
      "Iteration 186, loss = 0.38900314\n",
      "Iteration 187, loss = 0.38893817\n",
      "Iteration 188, loss = 0.38879124\n",
      "Iteration 189, loss = 0.38857809\n",
      "Iteration 190, loss = 0.38867332\n",
      "Iteration 191, loss = 0.38864855\n",
      "Iteration 192, loss = 0.38867815\n",
      "Iteration 193, loss = 0.38824746\n",
      "Iteration 194, loss = 0.38818636\n",
      "Iteration 195, loss = 0.38805448\n",
      "Iteration 196, loss = 0.38781628\n",
      "Iteration 197, loss = 0.38820833\n",
      "Iteration 198, loss = 0.38766648\n",
      "Iteration 199, loss = 0.38763664\n",
      "Iteration 200, loss = 0.38772487\n",
      "Iteration 201, loss = 0.38725450\n",
      "Iteration 202, loss = 0.38736928\n",
      "Iteration 203, loss = 0.38739154\n",
      "Iteration 204, loss = 0.38734910\n",
      "Iteration 205, loss = 0.38715651\n",
      "Iteration 206, loss = 0.38695286\n",
      "Iteration 207, loss = 0.38707370\n",
      "Iteration 208, loss = 0.38665202\n",
      "Iteration 209, loss = 0.38671965\n",
      "Iteration 210, loss = 0.38662149\n",
      "Iteration 211, loss = 0.38656220\n",
      "Iteration 212, loss = 0.38650117\n",
      "Iteration 213, loss = 0.38625884\n",
      "Iteration 214, loss = 0.38634913\n",
      "Iteration 215, loss = 0.38614045\n",
      "Iteration 216, loss = 0.38608845\n",
      "Iteration 217, loss = 0.38598055\n",
      "Iteration 218, loss = 0.38580109\n",
      "Iteration 219, loss = 0.38576871\n",
      "Iteration 220, loss = 0.38560186\n",
      "Iteration 221, loss = 0.38537410\n",
      "Iteration 222, loss = 0.38540120\n",
      "Iteration 223, loss = 0.38523390\n",
      "Iteration 224, loss = 0.38513965\n",
      "Iteration 225, loss = 0.38511437\n",
      "Iteration 226, loss = 0.38495114\n",
      "Iteration 227, loss = 0.38508243\n",
      "Iteration 228, loss = 0.38496148\n",
      "Iteration 229, loss = 0.38478739\n",
      "Iteration 230, loss = 0.38476667\n",
      "Iteration 231, loss = 0.38472463\n",
      "Iteration 232, loss = 0.38445852\n",
      "Iteration 233, loss = 0.38492927\n",
      "Iteration 234, loss = 0.38445376\n",
      "Iteration 235, loss = 0.38429461\n",
      "Iteration 236, loss = 0.38435308\n",
      "Iteration 237, loss = 0.38422694\n",
      "Iteration 238, loss = 0.38410222\n",
      "Iteration 239, loss = 0.38398837\n",
      "Iteration 240, loss = 0.38363251\n",
      "Iteration 241, loss = 0.38381741\n",
      "Iteration 242, loss = 0.38386111\n",
      "Iteration 243, loss = 0.38376570\n",
      "Iteration 244, loss = 0.38363800\n",
      "Iteration 245, loss = 0.38334588\n",
      "Iteration 246, loss = 0.38349556\n",
      "Iteration 247, loss = 0.38358933\n",
      "Iteration 248, loss = 0.38317587\n",
      "Iteration 249, loss = 0.38311703\n",
      "Iteration 250, loss = 0.38325804\n",
      "Iteration 251, loss = 0.38303420\n",
      "Iteration 252, loss = 0.38315010\n",
      "Iteration 253, loss = 0.38305354\n",
      "Iteration 254, loss = 0.38318046\n",
      "Iteration 255, loss = 0.38271818\n",
      "Iteration 256, loss = 0.38286383\n",
      "Iteration 257, loss = 0.38250287\n",
      "Iteration 258, loss = 0.38249708\n",
      "Iteration 259, loss = 0.38257905\n",
      "Iteration 260, loss = 0.38254397\n",
      "Iteration 261, loss = 0.38231041\n",
      "Iteration 262, loss = 0.38221039\n",
      "Iteration 263, loss = 0.38212278\n",
      "Iteration 264, loss = 0.38216161\n",
      "Iteration 265, loss = 0.38230238\n",
      "Iteration 266, loss = 0.38215118\n",
      "Iteration 267, loss = 0.38202541\n",
      "Iteration 268, loss = 0.38166619\n",
      "Iteration 269, loss = 0.38188720\n",
      "Iteration 270, loss = 0.38164918\n",
      "Iteration 271, loss = 0.38173036\n",
      "Iteration 272, loss = 0.38156635\n",
      "Iteration 273, loss = 0.38145092\n",
      "Iteration 274, loss = 0.38157871\n",
      "Iteration 275, loss = 0.38151874\n",
      "Iteration 276, loss = 0.38127075\n",
      "Iteration 277, loss = 0.38155713\n",
      "Iteration 278, loss = 0.38111198\n",
      "Iteration 279, loss = 0.38124447\n",
      "Iteration 280, loss = 0.38125671\n",
      "Iteration 281, loss = 0.38115841\n",
      "Iteration 282, loss = 0.38096204\n",
      "Iteration 283, loss = 0.38089722\n",
      "Iteration 284, loss = 0.38101300\n",
      "Iteration 285, loss = 0.38059636\n",
      "Iteration 286, loss = 0.38075047\n",
      "Iteration 287, loss = 0.38067302\n",
      "Iteration 288, loss = 0.38074291\n",
      "Iteration 289, loss = 0.38071273\n",
      "Iteration 290, loss = 0.38045075\n",
      "Iteration 291, loss = 0.38022366\n",
      "Iteration 292, loss = 0.38032389\n",
      "Iteration 293, loss = 0.38018570\n",
      "Iteration 294, loss = 0.38037350\n",
      "Iteration 295, loss = 0.38032705\n",
      "Iteration 296, loss = 0.38036193\n",
      "Iteration 297, loss = 0.37999284\n",
      "Iteration 298, loss = 0.38029125\n",
      "Iteration 299, loss = 0.37992452\n",
      "Iteration 300, loss = 0.38012568\n",
      "Iteration 301, loss = 0.38017317\n",
      "Iteration 302, loss = 0.37977671\n",
      "Iteration 303, loss = 0.37986952\n",
      "Iteration 304, loss = 0.37979288\n",
      "Iteration 305, loss = 0.37996355\n",
      "Iteration 306, loss = 0.37960938\n",
      "Iteration 307, loss = 0.37950928\n",
      "Iteration 308, loss = 0.37958930\n",
      "Iteration 309, loss = 0.37950173\n",
      "Iteration 310, loss = 0.37934684\n",
      "Iteration 311, loss = 0.37925401\n",
      "Iteration 312, loss = 0.37913814\n",
      "Iteration 313, loss = 0.37937787\n",
      "Iteration 314, loss = 0.37927010\n",
      "Iteration 315, loss = 0.37917920\n",
      "Iteration 316, loss = 0.37943800\n",
      "Iteration 317, loss = 0.37929255\n",
      "Iteration 318, loss = 0.37926728\n",
      "Iteration 319, loss = 0.37895179\n",
      "Iteration 320, loss = 0.37917870\n",
      "Iteration 321, loss = 0.37893047\n",
      "Iteration 322, loss = 0.37870210\n",
      "Iteration 323, loss = 0.37864278\n",
      "Iteration 324, loss = 0.37885569\n",
      "Iteration 325, loss = 0.37855138\n",
      "Iteration 326, loss = 0.37855566\n",
      "Iteration 327, loss = 0.37857850\n",
      "Iteration 328, loss = 0.37868692\n",
      "Iteration 329, loss = 0.37871411\n",
      "Iteration 330, loss = 0.37840174\n",
      "Iteration 331, loss = 0.37806851\n",
      "Iteration 332, loss = 0.37835524\n",
      "Iteration 333, loss = 0.37833876\n",
      "Iteration 334, loss = 0.37841536\n",
      "Iteration 335, loss = 0.37809528\n",
      "Iteration 336, loss = 0.37821340\n",
      "Iteration 337, loss = 0.37832547\n",
      "Iteration 338, loss = 0.37813234\n",
      "Iteration 339, loss = 0.37785998\n",
      "Iteration 340, loss = 0.37793928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 341, loss = 0.37811621\n",
      "Iteration 342, loss = 0.37786175\n",
      "Iteration 343, loss = 0.37778863\n",
      "Iteration 344, loss = 0.37781938\n",
      "Iteration 345, loss = 0.37778419\n",
      "Iteration 346, loss = 0.37773585\n",
      "Iteration 347, loss = 0.37773698\n",
      "Iteration 348, loss = 0.37774604\n",
      "Iteration 349, loss = 0.37759724\n",
      "Iteration 350, loss = 0.37757282\n",
      "Iteration 351, loss = 0.37761877\n",
      "Iteration 352, loss = 0.37754281\n",
      "Iteration 353, loss = 0.37749892\n",
      "Iteration 354, loss = 0.37746269\n",
      "Iteration 355, loss = 0.37733392\n",
      "Iteration 356, loss = 0.37736761\n",
      "Iteration 357, loss = 0.37713020\n",
      "Iteration 358, loss = 0.37719467\n",
      "Iteration 359, loss = 0.37723557\n",
      "Iteration 360, loss = 0.37720166\n",
      "Iteration 361, loss = 0.37709979\n",
      "Iteration 362, loss = 0.37701480\n",
      "Iteration 363, loss = 0.37685848\n",
      "Iteration 364, loss = 0.37695605\n",
      "Iteration 365, loss = 0.37682418\n",
      "Iteration 366, loss = 0.37738876\n",
      "Iteration 367, loss = 0.37703396\n",
      "Iteration 368, loss = 0.37671918\n",
      "Iteration 369, loss = 0.37669537\n",
      "Iteration 370, loss = 0.37665153\n",
      "Iteration 371, loss = 0.37668830\n",
      "Iteration 372, loss = 0.37698542\n",
      "Iteration 373, loss = 0.37644785\n",
      "Iteration 374, loss = 0.37668725\n",
      "Iteration 375, loss = 0.37653295\n",
      "Iteration 376, loss = 0.37664049\n",
      "Iteration 377, loss = 0.37663176\n",
      "Iteration 378, loss = 0.37621793\n",
      "Iteration 379, loss = 0.37637804\n",
      "Iteration 380, loss = 0.37642028\n",
      "Iteration 381, loss = 0.37614279\n",
      "Iteration 382, loss = 0.37637468\n",
      "Iteration 383, loss = 0.37635533\n",
      "Iteration 384, loss = 0.37656489\n",
      "Iteration 385, loss = 0.37606828\n",
      "Iteration 386, loss = 0.37611263\n",
      "Iteration 387, loss = 0.37631619\n",
      "Iteration 388, loss = 0.37607046\n",
      "Iteration 389, loss = 0.37605525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85361849\n",
      "Iteration 2, loss = 1.60683770\n",
      "Iteration 3, loss = 1.40838605\n",
      "Iteration 4, loss = 1.24937180\n",
      "Iteration 5, loss = 1.12251886\n",
      "Iteration 6, loss = 1.01972195\n",
      "Iteration 7, loss = 0.93636316\n",
      "Iteration 8, loss = 0.86869707\n",
      "Iteration 9, loss = 0.81257239\n",
      "Iteration 10, loss = 0.76613028\n",
      "Iteration 11, loss = 0.72714739\n",
      "Iteration 12, loss = 0.69464607\n",
      "Iteration 13, loss = 0.66679676\n",
      "Iteration 14, loss = 0.64306313\n",
      "Iteration 15, loss = 0.62233083\n",
      "Iteration 16, loss = 0.60437958\n",
      "Iteration 17, loss = 0.58839214\n",
      "Iteration 18, loss = 0.57423976\n",
      "Iteration 19, loss = 0.56158899\n",
      "Iteration 20, loss = 0.55006401\n",
      "Iteration 21, loss = 0.53907659\n",
      "Iteration 22, loss = 0.52925996\n",
      "Iteration 23, loss = 0.52087782\n",
      "Iteration 24, loss = 0.51332341\n",
      "Iteration 25, loss = 0.50669994\n",
      "Iteration 26, loss = 0.50065761\n",
      "Iteration 27, loss = 0.49517243\n",
      "Iteration 28, loss = 0.49019112\n",
      "Iteration 29, loss = 0.48569472\n",
      "Iteration 30, loss = 0.48166786\n",
      "Iteration 31, loss = 0.47768489\n",
      "Iteration 32, loss = 0.47441749\n",
      "Iteration 33, loss = 0.47137782\n",
      "Iteration 34, loss = 0.46833732\n",
      "Iteration 35, loss = 0.46580456\n",
      "Iteration 36, loss = 0.46325989\n",
      "Iteration 37, loss = 0.46084952\n",
      "Iteration 38, loss = 0.45881888\n",
      "Iteration 39, loss = 0.45654338\n",
      "Iteration 40, loss = 0.45458712\n",
      "Iteration 41, loss = 0.45260419\n",
      "Iteration 42, loss = 0.45118042\n",
      "Iteration 43, loss = 0.44934664\n",
      "Iteration 44, loss = 0.44748734\n",
      "Iteration 45, loss = 0.44623751\n",
      "Iteration 46, loss = 0.44481129\n",
      "Iteration 47, loss = 0.44307992\n",
      "Iteration 48, loss = 0.44202697\n",
      "Iteration 49, loss = 0.44042548\n",
      "Iteration 50, loss = 0.43933710\n",
      "Iteration 51, loss = 0.43830156\n",
      "Iteration 52, loss = 0.43684200\n",
      "Iteration 53, loss = 0.43549091\n",
      "Iteration 54, loss = 0.43458582\n",
      "Iteration 55, loss = 0.43325046\n",
      "Iteration 56, loss = 0.43274475\n",
      "Iteration 57, loss = 0.43148450\n",
      "Iteration 58, loss = 0.43043376\n",
      "Iteration 59, loss = 0.42943700\n",
      "Iteration 60, loss = 0.42829854\n",
      "Iteration 61, loss = 0.42760676\n",
      "Iteration 62, loss = 0.42663705\n",
      "Iteration 63, loss = 0.42575210\n",
      "Iteration 64, loss = 0.42491669\n",
      "Iteration 65, loss = 0.42407600\n",
      "Iteration 66, loss = 0.42330291\n",
      "Iteration 67, loss = 0.42237698\n",
      "Iteration 68, loss = 0.42148096\n",
      "Iteration 69, loss = 0.42080322\n",
      "Iteration 70, loss = 0.42018008\n",
      "Iteration 71, loss = 0.41949989\n",
      "Iteration 72, loss = 0.41844950\n",
      "Iteration 73, loss = 0.41798481\n",
      "Iteration 74, loss = 0.41738120\n",
      "Iteration 75, loss = 0.41668244\n",
      "Iteration 76, loss = 0.41593819\n",
      "Iteration 77, loss = 0.41539991\n",
      "Iteration 78, loss = 0.41463174\n",
      "Iteration 79, loss = 0.41405632\n",
      "Iteration 80, loss = 0.41386551\n",
      "Iteration 81, loss = 0.41298550\n",
      "Iteration 82, loss = 0.41212841\n",
      "Iteration 83, loss = 0.41214874\n",
      "Iteration 84, loss = 0.41140251\n",
      "Iteration 85, loss = 0.41075430\n",
      "Iteration 86, loss = 0.41043665\n",
      "Iteration 87, loss = 0.40992167\n",
      "Iteration 88, loss = 0.40945870\n",
      "Iteration 89, loss = 0.40900126\n",
      "Iteration 90, loss = 0.40848020\n",
      "Iteration 91, loss = 0.40792171\n",
      "Iteration 92, loss = 0.40749694\n",
      "Iteration 93, loss = 0.40732837\n",
      "Iteration 94, loss = 0.40650630\n",
      "Iteration 95, loss = 0.40643687\n",
      "Iteration 96, loss = 0.40607070\n",
      "Iteration 97, loss = 0.40552880\n",
      "Iteration 98, loss = 0.40534596\n",
      "Iteration 99, loss = 0.40494251\n",
      "Iteration 100, loss = 0.40455853\n",
      "Iteration 101, loss = 0.40433581\n",
      "Iteration 102, loss = 0.40364199\n",
      "Iteration 103, loss = 0.40354692\n",
      "Iteration 104, loss = 0.40311716\n",
      "Iteration 105, loss = 0.40310191\n",
      "Iteration 106, loss = 0.40270807\n",
      "Iteration 107, loss = 0.40232469\n",
      "Iteration 108, loss = 0.40174461\n",
      "Iteration 109, loss = 0.40159108\n",
      "Iteration 110, loss = 0.40123553\n",
      "Iteration 111, loss = 0.40103307\n",
      "Iteration 112, loss = 0.40079263\n",
      "Iteration 113, loss = 0.40061667\n",
      "Iteration 114, loss = 0.40024870\n",
      "Iteration 115, loss = 0.40019765\n",
      "Iteration 116, loss = 0.39967617\n",
      "Iteration 117, loss = 0.39945220\n",
      "Iteration 118, loss = 0.39926038\n",
      "Iteration 119, loss = 0.39891723\n",
      "Iteration 120, loss = 0.39865755\n",
      "Iteration 121, loss = 0.39835338\n",
      "Iteration 122, loss = 0.39797175\n",
      "Iteration 123, loss = 0.39803701\n",
      "Iteration 124, loss = 0.39764031\n",
      "Iteration 125, loss = 0.39757987\n",
      "Iteration 126, loss = 0.39733016\n",
      "Iteration 127, loss = 0.39722077\n",
      "Iteration 128, loss = 0.39681464\n",
      "Iteration 129, loss = 0.39650975\n",
      "Iteration 130, loss = 0.39635182\n",
      "Iteration 131, loss = 0.39622381\n",
      "Iteration 132, loss = 0.39596030\n",
      "Iteration 133, loss = 0.39577722\n",
      "Iteration 134, loss = 0.39574263\n",
      "Iteration 135, loss = 0.39540343\n",
      "Iteration 136, loss = 0.39510173\n",
      "Iteration 137, loss = 0.39490521\n",
      "Iteration 138, loss = 0.39483315\n",
      "Iteration 139, loss = 0.39460106\n",
      "Iteration 140, loss = 0.39454236\n",
      "Iteration 141, loss = 0.39439447\n",
      "Iteration 142, loss = 0.39418940\n",
      "Iteration 143, loss = 0.39394637\n",
      "Iteration 144, loss = 0.39392228\n",
      "Iteration 145, loss = 0.39354537\n",
      "Iteration 146, loss = 0.39350220\n",
      "Iteration 147, loss = 0.39348165\n",
      "Iteration 148, loss = 0.39334687\n",
      "Iteration 149, loss = 0.39301746\n",
      "Iteration 150, loss = 0.39289169\n",
      "Iteration 151, loss = 0.39279763\n",
      "Iteration 152, loss = 0.39266063\n",
      "Iteration 153, loss = 0.39240132\n",
      "Iteration 154, loss = 0.39220160\n",
      "Iteration 155, loss = 0.39213624\n",
      "Iteration 156, loss = 0.39199065\n",
      "Iteration 157, loss = 0.39179746\n",
      "Iteration 158, loss = 0.39159634\n",
      "Iteration 159, loss = 0.39151464\n",
      "Iteration 160, loss = 0.39158298\n",
      "Iteration 161, loss = 0.39138813\n",
      "Iteration 162, loss = 0.39122289\n",
      "Iteration 163, loss = 0.39110014\n",
      "Iteration 164, loss = 0.39080569\n",
      "Iteration 165, loss = 0.39077501\n",
      "Iteration 166, loss = 0.39048945\n",
      "Iteration 167, loss = 0.39047736\n",
      "Iteration 168, loss = 0.39040303\n",
      "Iteration 169, loss = 0.39030519\n",
      "Iteration 170, loss = 0.39012237\n",
      "Iteration 171, loss = 0.39015643\n",
      "Iteration 172, loss = 0.38977126\n",
      "Iteration 173, loss = 0.38973477\n",
      "Iteration 174, loss = 0.38965313\n",
      "Iteration 175, loss = 0.38968840\n",
      "Iteration 176, loss = 0.38943352\n",
      "Iteration 177, loss = 0.38942393\n",
      "Iteration 178, loss = 0.38923725\n",
      "Iteration 179, loss = 0.38926934\n",
      "Iteration 180, loss = 0.38895769\n",
      "Iteration 181, loss = 0.38890092\n",
      "Iteration 182, loss = 0.38873449\n",
      "Iteration 183, loss = 0.38845718\n",
      "Iteration 184, loss = 0.38821576\n",
      "Iteration 185, loss = 0.38857788\n",
      "Iteration 186, loss = 0.38827926\n",
      "Iteration 187, loss = 0.38822840\n",
      "Iteration 188, loss = 0.38824565\n",
      "Iteration 189, loss = 0.38788588\n",
      "Iteration 190, loss = 0.38817918\n",
      "Iteration 191, loss = 0.38803948\n",
      "Iteration 192, loss = 0.38794262\n",
      "Iteration 193, loss = 0.38784383\n",
      "Iteration 194, loss = 0.38771055\n",
      "Iteration 195, loss = 0.38760664\n",
      "Iteration 196, loss = 0.38749691\n",
      "Iteration 197, loss = 0.38724741\n",
      "Iteration 198, loss = 0.38726826\n",
      "Iteration 199, loss = 0.38710816\n",
      "Iteration 200, loss = 0.38708384\n",
      "Iteration 201, loss = 0.38698967\n",
      "Iteration 202, loss = 0.38696483\n",
      "Iteration 203, loss = 0.38700289\n",
      "Iteration 204, loss = 0.38678069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 205, loss = 0.38668288\n",
      "Iteration 206, loss = 0.38680850\n",
      "Iteration 207, loss = 0.38636475\n",
      "Iteration 208, loss = 0.38658848\n",
      "Iteration 209, loss = 0.38641074\n",
      "Iteration 210, loss = 0.38629175\n",
      "Iteration 211, loss = 0.38629251\n",
      "Iteration 212, loss = 0.38608368\n",
      "Iteration 213, loss = 0.38602711\n",
      "Iteration 214, loss = 0.38587190\n",
      "Iteration 215, loss = 0.38576741\n",
      "Iteration 216, loss = 0.38574897\n",
      "Iteration 217, loss = 0.38602253\n",
      "Iteration 218, loss = 0.38567664\n",
      "Iteration 219, loss = 0.38564133\n",
      "Iteration 220, loss = 0.38581450\n",
      "Iteration 221, loss = 0.38544385\n",
      "Iteration 222, loss = 0.38544119\n",
      "Iteration 223, loss = 0.38515215\n",
      "Iteration 224, loss = 0.38535596\n",
      "Iteration 225, loss = 0.38494849\n",
      "Iteration 226, loss = 0.38543934\n",
      "Iteration 227, loss = 0.38527947\n",
      "Iteration 228, loss = 0.38526439\n",
      "Iteration 229, loss = 0.38485418\n",
      "Iteration 230, loss = 0.38493168\n",
      "Iteration 231, loss = 0.38497992\n",
      "Iteration 232, loss = 0.38455196\n",
      "Iteration 233, loss = 0.38505151\n",
      "Iteration 234, loss = 0.38458899\n",
      "Iteration 235, loss = 0.38433684\n",
      "Iteration 236, loss = 0.38446752\n",
      "Iteration 237, loss = 0.38435666\n",
      "Iteration 238, loss = 0.38443944\n",
      "Iteration 239, loss = 0.38421745\n",
      "Iteration 240, loss = 0.38424585\n",
      "Iteration 241, loss = 0.38411142\n",
      "Iteration 242, loss = 0.38405610\n",
      "Iteration 243, loss = 0.38411472\n",
      "Iteration 244, loss = 0.38398311\n",
      "Iteration 245, loss = 0.38405581\n",
      "Iteration 246, loss = 0.38381642\n",
      "Iteration 247, loss = 0.38370737\n",
      "Iteration 248, loss = 0.38358189\n",
      "Iteration 249, loss = 0.38389638\n",
      "Iteration 250, loss = 0.38353970\n",
      "Iteration 251, loss = 0.38367248\n",
      "Iteration 252, loss = 0.38352021\n",
      "Iteration 253, loss = 0.38372817\n",
      "Iteration 254, loss = 0.38357694\n",
      "Iteration 255, loss = 0.38327369\n",
      "Iteration 256, loss = 0.38319027\n",
      "Iteration 257, loss = 0.38311865\n",
      "Iteration 258, loss = 0.38307223\n",
      "Iteration 259, loss = 0.38341483\n",
      "Iteration 260, loss = 0.38324869\n",
      "Iteration 261, loss = 0.38296930\n",
      "Iteration 262, loss = 0.38300584\n",
      "Iteration 263, loss = 0.38284060\n",
      "Iteration 264, loss = 0.38289252\n",
      "Iteration 265, loss = 0.38267768\n",
      "Iteration 266, loss = 0.38281146\n",
      "Iteration 267, loss = 0.38243193\n",
      "Iteration 268, loss = 0.38260657\n",
      "Iteration 269, loss = 0.38246203\n",
      "Iteration 270, loss = 0.38258288\n",
      "Iteration 271, loss = 0.38263320\n",
      "Iteration 272, loss = 0.38264424\n",
      "Iteration 273, loss = 0.38214842\n",
      "Iteration 274, loss = 0.38251319\n",
      "Iteration 275, loss = 0.38215160\n",
      "Iteration 276, loss = 0.38241699\n",
      "Iteration 277, loss = 0.38237183\n",
      "Iteration 278, loss = 0.38219606\n",
      "Iteration 279, loss = 0.38221095\n",
      "Iteration 280, loss = 0.38210575\n",
      "Iteration 281, loss = 0.38210136\n",
      "Iteration 282, loss = 0.38189254\n",
      "Iteration 283, loss = 0.38206148\n",
      "Iteration 284, loss = 0.38180553\n",
      "Iteration 285, loss = 0.38177368\n",
      "Iteration 286, loss = 0.38176751\n",
      "Iteration 287, loss = 0.38188989\n",
      "Iteration 288, loss = 0.38166239\n",
      "Iteration 289, loss = 0.38168485\n",
      "Iteration 290, loss = 0.38167927\n",
      "Iteration 291, loss = 0.38170310\n",
      "Iteration 292, loss = 0.38141074\n",
      "Iteration 293, loss = 0.38132517\n",
      "Iteration 294, loss = 0.38136260\n",
      "Iteration 295, loss = 0.38150676\n",
      "Iteration 296, loss = 0.38122216\n",
      "Iteration 297, loss = 0.38123216\n",
      "Iteration 298, loss = 0.38120062\n",
      "Iteration 299, loss = 0.38110424\n",
      "Iteration 300, loss = 0.38105177\n",
      "Iteration 301, loss = 0.38090884\n",
      "Iteration 302, loss = 0.38132638\n",
      "Iteration 303, loss = 0.38098767\n",
      "Iteration 304, loss = 0.38093445\n",
      "Iteration 305, loss = 0.38117565\n",
      "Iteration 306, loss = 0.38079822\n",
      "Iteration 307, loss = 0.38107933\n",
      "Iteration 308, loss = 0.38091836\n",
      "Iteration 309, loss = 0.38062494\n",
      "Iteration 310, loss = 0.38075909\n",
      "Iteration 311, loss = 0.38087026\n",
      "Iteration 312, loss = 0.38052486\n",
      "Iteration 313, loss = 0.38045855\n",
      "Iteration 314, loss = 0.38050336\n",
      "Iteration 315, loss = 0.38061593\n",
      "Iteration 316, loss = 0.38048778\n",
      "Iteration 317, loss = 0.38034783\n",
      "Iteration 318, loss = 0.38050412\n",
      "Iteration 319, loss = 0.38016287\n",
      "Iteration 320, loss = 0.38037268\n",
      "Iteration 321, loss = 0.38031473\n",
      "Iteration 322, loss = 0.38046986\n",
      "Iteration 323, loss = 0.38021546\n",
      "Iteration 324, loss = 0.38031553\n",
      "Iteration 325, loss = 0.38000175\n",
      "Iteration 326, loss = 0.38010846\n",
      "Iteration 327, loss = 0.38011997\n",
      "Iteration 328, loss = 0.38003344\n",
      "Iteration 329, loss = 0.38037396\n",
      "Iteration 330, loss = 0.37995941\n",
      "Iteration 331, loss = 0.38003578\n",
      "Iteration 332, loss = 0.37978362\n",
      "Iteration 333, loss = 0.37971098\n",
      "Iteration 334, loss = 0.37950388\n",
      "Iteration 335, loss = 0.37985391\n",
      "Iteration 336, loss = 0.37976578\n",
      "Iteration 337, loss = 0.37948974\n",
      "Iteration 338, loss = 0.37982327\n",
      "Iteration 339, loss = 0.37953086\n",
      "Iteration 340, loss = 0.37955350\n",
      "Iteration 341, loss = 0.37949138\n",
      "Iteration 342, loss = 0.37950633\n",
      "Iteration 343, loss = 0.37958221\n",
      "Iteration 344, loss = 0.37962852\n",
      "Iteration 345, loss = 0.37920760\n",
      "Iteration 346, loss = 0.37945027\n",
      "Iteration 347, loss = 0.37916997\n",
      "Iteration 348, loss = 0.37922177\n",
      "Iteration 349, loss = 0.37953028\n",
      "Iteration 350, loss = 0.37890796\n",
      "Iteration 351, loss = 0.37911882\n",
      "Iteration 352, loss = 0.37928433\n",
      "Iteration 353, loss = 0.37904578\n",
      "Iteration 354, loss = 0.37875463\n",
      "Iteration 355, loss = 0.37918216\n",
      "Iteration 356, loss = 0.37895156\n",
      "Iteration 357, loss = 0.37894934\n",
      "Iteration 358, loss = 0.37880499\n",
      "Iteration 359, loss = 0.37894078\n",
      "Iteration 360, loss = 0.37897244\n",
      "Iteration 361, loss = 0.37876842\n",
      "Iteration 362, loss = 0.37877896\n",
      "Iteration 363, loss = 0.37868537\n",
      "Iteration 364, loss = 0.37859886\n",
      "Iteration 365, loss = 0.37858261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85150499\n",
      "Iteration 2, loss = 1.60316130\n",
      "Iteration 3, loss = 1.40499734\n",
      "Iteration 4, loss = 1.24682679\n",
      "Iteration 5, loss = 1.12095934\n",
      "Iteration 6, loss = 1.01856108\n",
      "Iteration 7, loss = 0.93515720\n",
      "Iteration 8, loss = 0.86701845\n",
      "Iteration 9, loss = 0.81113724\n",
      "Iteration 10, loss = 0.76491130\n",
      "Iteration 11, loss = 0.72602815\n",
      "Iteration 12, loss = 0.69344516\n",
      "Iteration 13, loss = 0.66552975\n",
      "Iteration 14, loss = 0.64168953\n",
      "Iteration 15, loss = 0.62083323\n",
      "Iteration 16, loss = 0.60263647\n",
      "Iteration 17, loss = 0.58683087\n",
      "Iteration 18, loss = 0.57227735\n",
      "Iteration 19, loss = 0.55874970\n",
      "Iteration 20, loss = 0.54664621\n",
      "Iteration 21, loss = 0.53624260\n",
      "Iteration 22, loss = 0.52718290\n",
      "Iteration 23, loss = 0.51879237\n",
      "Iteration 24, loss = 0.51169738\n",
      "Iteration 25, loss = 0.50465151\n",
      "Iteration 26, loss = 0.49887075\n",
      "Iteration 27, loss = 0.49320930\n",
      "Iteration 28, loss = 0.48835848\n",
      "Iteration 29, loss = 0.48404336\n",
      "Iteration 30, loss = 0.48004182\n",
      "Iteration 31, loss = 0.47626868\n",
      "Iteration 32, loss = 0.47264362\n",
      "Iteration 33, loss = 0.46977627\n",
      "Iteration 34, loss = 0.46668863\n",
      "Iteration 35, loss = 0.46409789\n",
      "Iteration 36, loss = 0.46161693\n",
      "Iteration 37, loss = 0.45893983\n",
      "Iteration 38, loss = 0.45697942\n",
      "Iteration 39, loss = 0.45527571\n",
      "Iteration 40, loss = 0.45295075\n",
      "Iteration 41, loss = 0.45113580\n",
      "Iteration 42, loss = 0.44927318\n",
      "Iteration 43, loss = 0.44786177\n",
      "Iteration 44, loss = 0.44618109\n",
      "Iteration 45, loss = 0.44475240\n",
      "Iteration 46, loss = 0.44306388\n",
      "Iteration 47, loss = 0.44188183\n",
      "Iteration 48, loss = 0.44049259\n",
      "Iteration 49, loss = 0.43919316\n",
      "Iteration 50, loss = 0.43793550\n",
      "Iteration 51, loss = 0.43671511\n",
      "Iteration 52, loss = 0.43555268\n",
      "Iteration 53, loss = 0.43424474\n",
      "Iteration 54, loss = 0.43298108\n",
      "Iteration 55, loss = 0.43203922\n",
      "Iteration 56, loss = 0.43099556\n",
      "Iteration 57, loss = 0.43016702\n",
      "Iteration 58, loss = 0.42892082\n",
      "Iteration 59, loss = 0.42801073\n",
      "Iteration 60, loss = 0.42696827\n",
      "Iteration 61, loss = 0.42600778\n",
      "Iteration 62, loss = 0.42525512\n",
      "Iteration 63, loss = 0.42423017\n",
      "Iteration 64, loss = 0.42339875\n",
      "Iteration 65, loss = 0.42251838\n",
      "Iteration 66, loss = 0.42156820\n",
      "Iteration 67, loss = 0.42114366\n",
      "Iteration 68, loss = 0.41995227\n",
      "Iteration 69, loss = 0.41913557\n",
      "Iteration 70, loss = 0.41884911\n",
      "Iteration 71, loss = 0.41783935\n",
      "Iteration 72, loss = 0.41710555\n",
      "Iteration 73, loss = 0.41632456\n",
      "Iteration 74, loss = 0.41549598\n",
      "Iteration 75, loss = 0.41483821\n",
      "Iteration 76, loss = 0.41437921\n",
      "Iteration 77, loss = 0.41362499\n",
      "Iteration 78, loss = 0.41297496\n",
      "Iteration 79, loss = 0.41244590\n",
      "Iteration 80, loss = 0.41190472\n",
      "Iteration 81, loss = 0.41131948\n",
      "Iteration 82, loss = 0.41085243\n",
      "Iteration 83, loss = 0.41008682\n",
      "Iteration 84, loss = 0.40991518\n",
      "Iteration 85, loss = 0.40923095\n",
      "Iteration 86, loss = 0.40879066\n",
      "Iteration 87, loss = 0.40815327\n",
      "Iteration 88, loss = 0.40768409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.40712012\n",
      "Iteration 90, loss = 0.40638223\n",
      "Iteration 91, loss = 0.40627640\n",
      "Iteration 92, loss = 0.40564920\n",
      "Iteration 93, loss = 0.40543897\n",
      "Iteration 94, loss = 0.40497868\n",
      "Iteration 95, loss = 0.40484112\n",
      "Iteration 96, loss = 0.40432203\n",
      "Iteration 97, loss = 0.40380939\n",
      "Iteration 98, loss = 0.40337270\n",
      "Iteration 99, loss = 0.40284017\n",
      "Iteration 100, loss = 0.40267990\n",
      "Iteration 101, loss = 0.40250095\n",
      "Iteration 102, loss = 0.40163157\n",
      "Iteration 103, loss = 0.40149581\n",
      "Iteration 104, loss = 0.40150018\n",
      "Iteration 105, loss = 0.40097396\n",
      "Iteration 106, loss = 0.40084093\n",
      "Iteration 107, loss = 0.40024360\n",
      "Iteration 108, loss = 0.40010816\n",
      "Iteration 109, loss = 0.39992434\n",
      "Iteration 110, loss = 0.39931470\n",
      "Iteration 111, loss = 0.39935576\n",
      "Iteration 112, loss = 0.39878605\n",
      "Iteration 113, loss = 0.39854848\n",
      "Iteration 114, loss = 0.39828744\n",
      "Iteration 115, loss = 0.39818038\n",
      "Iteration 116, loss = 0.39763769\n",
      "Iteration 117, loss = 0.39719241\n",
      "Iteration 118, loss = 0.39745264\n",
      "Iteration 119, loss = 0.39708908\n",
      "Iteration 120, loss = 0.39656116\n",
      "Iteration 121, loss = 0.39644628\n",
      "Iteration 122, loss = 0.39638052\n",
      "Iteration 123, loss = 0.39614045\n",
      "Iteration 124, loss = 0.39575905\n",
      "Iteration 125, loss = 0.39556982\n",
      "Iteration 126, loss = 0.39536672\n",
      "Iteration 127, loss = 0.39521269\n",
      "Iteration 128, loss = 0.39473731\n",
      "Iteration 129, loss = 0.39472257\n",
      "Iteration 130, loss = 0.39453328\n",
      "Iteration 131, loss = 0.39413500\n",
      "Iteration 132, loss = 0.39433377\n",
      "Iteration 133, loss = 0.39385156\n",
      "Iteration 134, loss = 0.39375586\n",
      "Iteration 135, loss = 0.39369796\n",
      "Iteration 136, loss = 0.39334391\n",
      "Iteration 137, loss = 0.39312015\n",
      "Iteration 138, loss = 0.39297767\n",
      "Iteration 139, loss = 0.39287018\n",
      "Iteration 140, loss = 0.39260400\n",
      "Iteration 141, loss = 0.39261502\n",
      "Iteration 142, loss = 0.39246156\n",
      "Iteration 143, loss = 0.39256178\n",
      "Iteration 144, loss = 0.39189616\n",
      "Iteration 145, loss = 0.39194787\n",
      "Iteration 146, loss = 0.39156546\n",
      "Iteration 147, loss = 0.39158361\n",
      "Iteration 148, loss = 0.39130886\n",
      "Iteration 149, loss = 0.39127041\n",
      "Iteration 150, loss = 0.39108282\n",
      "Iteration 151, loss = 0.39093828\n",
      "Iteration 152, loss = 0.39077057\n",
      "Iteration 153, loss = 0.39069355\n",
      "Iteration 154, loss = 0.39027551\n",
      "Iteration 155, loss = 0.39009031\n",
      "Iteration 156, loss = 0.39012062\n",
      "Iteration 157, loss = 0.39017857\n",
      "Iteration 158, loss = 0.39003331\n",
      "Iteration 159, loss = 0.38976830\n",
      "Iteration 160, loss = 0.38983890\n",
      "Iteration 161, loss = 0.38946316\n",
      "Iteration 162, loss = 0.38933009\n",
      "Iteration 163, loss = 0.38934565\n",
      "Iteration 164, loss = 0.38907402\n",
      "Iteration 165, loss = 0.38885600\n",
      "Iteration 166, loss = 0.38900450\n",
      "Iteration 167, loss = 0.38860514\n",
      "Iteration 168, loss = 0.38850282\n",
      "Iteration 169, loss = 0.38850747\n",
      "Iteration 170, loss = 0.38841665\n",
      "Iteration 171, loss = 0.38842674\n",
      "Iteration 172, loss = 0.38820941\n",
      "Iteration 173, loss = 0.38783902\n",
      "Iteration 174, loss = 0.38809991\n",
      "Iteration 175, loss = 0.38770435\n",
      "Iteration 176, loss = 0.38759866\n",
      "Iteration 177, loss = 0.38767008\n",
      "Iteration 178, loss = 0.38737779\n",
      "Iteration 179, loss = 0.38751032\n",
      "Iteration 180, loss = 0.38731404\n",
      "Iteration 181, loss = 0.38699126\n",
      "Iteration 182, loss = 0.38698611\n",
      "Iteration 183, loss = 0.38701349\n",
      "Iteration 184, loss = 0.38670086\n",
      "Iteration 185, loss = 0.38656846\n",
      "Iteration 186, loss = 0.38673937\n",
      "Iteration 187, loss = 0.38648665\n",
      "Iteration 188, loss = 0.38646382\n",
      "Iteration 189, loss = 0.38627313\n",
      "Iteration 190, loss = 0.38635751\n",
      "Iteration 191, loss = 0.38617378\n",
      "Iteration 192, loss = 0.38608838\n",
      "Iteration 193, loss = 0.38587721\n",
      "Iteration 194, loss = 0.38609985\n",
      "Iteration 195, loss = 0.38570435\n",
      "Iteration 196, loss = 0.38569914\n",
      "Iteration 197, loss = 0.38562513\n",
      "Iteration 198, loss = 0.38546639\n",
      "Iteration 199, loss = 0.38561518\n",
      "Iteration 200, loss = 0.38541381\n",
      "Iteration 201, loss = 0.38523977\n",
      "Iteration 202, loss = 0.38544597\n",
      "Iteration 203, loss = 0.38502838\n",
      "Iteration 204, loss = 0.38510972\n",
      "Iteration 205, loss = 0.38484022\n",
      "Iteration 206, loss = 0.38490375\n",
      "Iteration 207, loss = 0.38482132\n",
      "Iteration 208, loss = 0.38479400\n",
      "Iteration 209, loss = 0.38482707\n",
      "Iteration 210, loss = 0.38468790\n",
      "Iteration 211, loss = 0.38468147\n",
      "Iteration 212, loss = 0.38437553\n",
      "Iteration 213, loss = 0.38436905\n",
      "Iteration 214, loss = 0.38411458\n",
      "Iteration 215, loss = 0.38426080\n",
      "Iteration 216, loss = 0.38419689\n",
      "Iteration 217, loss = 0.38410576\n",
      "Iteration 218, loss = 0.38385976\n",
      "Iteration 219, loss = 0.38394210\n",
      "Iteration 220, loss = 0.38410664\n",
      "Iteration 221, loss = 0.38393225\n",
      "Iteration 222, loss = 0.38371973\n",
      "Iteration 223, loss = 0.38359681\n",
      "Iteration 224, loss = 0.38349717\n",
      "Iteration 225, loss = 0.38323591\n",
      "Iteration 226, loss = 0.38355331\n",
      "Iteration 227, loss = 0.38327952\n",
      "Iteration 228, loss = 0.38321717\n",
      "Iteration 229, loss = 0.38325289\n",
      "Iteration 230, loss = 0.38333380\n",
      "Iteration 231, loss = 0.38303893\n",
      "Iteration 232, loss = 0.38294236\n",
      "Iteration 233, loss = 0.38296100\n",
      "Iteration 234, loss = 0.38326476\n",
      "Iteration 235, loss = 0.38282016\n",
      "Iteration 236, loss = 0.38280478\n",
      "Iteration 237, loss = 0.38295924\n",
      "Iteration 238, loss = 0.38264805\n",
      "Iteration 239, loss = 0.38237265\n",
      "Iteration 240, loss = 0.38246638\n",
      "Iteration 241, loss = 0.38252236\n",
      "Iteration 242, loss = 0.38227688\n",
      "Iteration 243, loss = 0.38226922\n",
      "Iteration 244, loss = 0.38211038\n",
      "Iteration 245, loss = 0.38226731\n",
      "Iteration 246, loss = 0.38223188\n",
      "Iteration 247, loss = 0.38213348\n",
      "Iteration 248, loss = 0.38184677\n",
      "Iteration 249, loss = 0.38201124\n",
      "Iteration 250, loss = 0.38188990\n",
      "Iteration 251, loss = 0.38166952\n",
      "Iteration 252, loss = 0.38178060\n",
      "Iteration 253, loss = 0.38180685\n",
      "Iteration 254, loss = 0.38186021\n",
      "Iteration 255, loss = 0.38166342\n",
      "Iteration 256, loss = 0.38131351\n",
      "Iteration 257, loss = 0.38143256\n",
      "Iteration 258, loss = 0.38141443\n",
      "Iteration 259, loss = 0.38108982\n",
      "Iteration 260, loss = 0.38122795\n",
      "Iteration 261, loss = 0.38104148\n",
      "Iteration 262, loss = 0.38107949\n",
      "Iteration 263, loss = 0.38115718\n",
      "Iteration 264, loss = 0.38090841\n",
      "Iteration 265, loss = 0.38090850\n",
      "Iteration 266, loss = 0.38088686\n",
      "Iteration 267, loss = 0.38075780\n",
      "Iteration 268, loss = 0.38089748\n",
      "Iteration 269, loss = 0.38089152\n",
      "Iteration 270, loss = 0.38079264\n",
      "Iteration 271, loss = 0.38044090\n",
      "Iteration 272, loss = 0.38076028\n",
      "Iteration 273, loss = 0.38034602\n",
      "Iteration 274, loss = 0.38071711\n",
      "Iteration 275, loss = 0.38056460\n",
      "Iteration 276, loss = 0.38047030\n",
      "Iteration 277, loss = 0.38050224\n",
      "Iteration 278, loss = 0.38019740\n",
      "Iteration 279, loss = 0.38025922\n",
      "Iteration 280, loss = 0.38018150\n",
      "Iteration 281, loss = 0.38027971\n",
      "Iteration 282, loss = 0.37997679\n",
      "Iteration 283, loss = 0.37992808\n",
      "Iteration 284, loss = 0.38030202\n",
      "Iteration 285, loss = 0.37991239\n",
      "Iteration 286, loss = 0.37983541\n",
      "Iteration 287, loss = 0.37969751\n",
      "Iteration 288, loss = 0.37958632\n",
      "Iteration 289, loss = 0.37953739\n",
      "Iteration 290, loss = 0.37941435\n",
      "Iteration 291, loss = 0.37963610\n",
      "Iteration 292, loss = 0.37938858\n",
      "Iteration 293, loss = 0.37955568\n",
      "Iteration 294, loss = 0.37931287\n",
      "Iteration 295, loss = 0.37927480\n",
      "Iteration 296, loss = 0.37962196\n",
      "Iteration 297, loss = 0.37943206\n",
      "Iteration 298, loss = 0.37927575\n",
      "Iteration 299, loss = 0.37922957\n",
      "Iteration 300, loss = 0.37905880\n",
      "Iteration 301, loss = 0.37930160\n",
      "Iteration 302, loss = 0.37912202\n",
      "Iteration 303, loss = 0.37915041\n",
      "Iteration 304, loss = 0.37891263\n",
      "Iteration 305, loss = 0.37904162\n",
      "Iteration 306, loss = 0.37907671\n",
      "Iteration 307, loss = 0.37873574\n",
      "Iteration 308, loss = 0.37865623\n",
      "Iteration 309, loss = 0.37888409\n",
      "Iteration 310, loss = 0.37881341\n",
      "Iteration 311, loss = 0.37839453\n",
      "Iteration 312, loss = 0.37880977\n",
      "Iteration 313, loss = 0.37864892\n",
      "Iteration 314, loss = 0.37855914\n",
      "Iteration 315, loss = 0.37870846\n",
      "Iteration 316, loss = 0.37843406\n",
      "Iteration 317, loss = 0.37869155\n",
      "Iteration 318, loss = 0.37836360\n",
      "Iteration 319, loss = 0.37837988\n",
      "Iteration 320, loss = 0.37831120\n",
      "Iteration 321, loss = 0.37835722\n",
      "Iteration 322, loss = 0.37826112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85181737\n",
      "Iteration 2, loss = 1.60414778\n",
      "Iteration 3, loss = 1.40611521\n",
      "Iteration 4, loss = 1.24746815\n",
      "Iteration 5, loss = 1.12098569\n",
      "Iteration 6, loss = 1.01824650\n",
      "Iteration 7, loss = 0.93516761\n",
      "Iteration 8, loss = 0.86744286\n",
      "Iteration 9, loss = 0.81189998\n",
      "Iteration 10, loss = 0.76552429\n",
      "Iteration 11, loss = 0.72676777\n",
      "Iteration 12, loss = 0.69400487\n",
      "Iteration 13, loss = 0.66638926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.64255420\n",
      "Iteration 15, loss = 0.62214720\n",
      "Iteration 16, loss = 0.60410197\n",
      "Iteration 17, loss = 0.58783796\n",
      "Iteration 18, loss = 0.57391803\n",
      "Iteration 19, loss = 0.56063731\n",
      "Iteration 20, loss = 0.54822391\n",
      "Iteration 21, loss = 0.53767796\n",
      "Iteration 22, loss = 0.52845995\n",
      "Iteration 23, loss = 0.52012502\n",
      "Iteration 24, loss = 0.51295688\n",
      "Iteration 25, loss = 0.50622662\n",
      "Iteration 26, loss = 0.50023832\n",
      "Iteration 27, loss = 0.49479850\n",
      "Iteration 28, loss = 0.48993814\n",
      "Iteration 29, loss = 0.48539876\n",
      "Iteration 30, loss = 0.48119643\n",
      "Iteration 31, loss = 0.47748018\n",
      "Iteration 32, loss = 0.47410410\n",
      "Iteration 33, loss = 0.47122865\n",
      "Iteration 34, loss = 0.46810314\n",
      "Iteration 35, loss = 0.46520866\n",
      "Iteration 36, loss = 0.46285864\n",
      "Iteration 37, loss = 0.46037422\n",
      "Iteration 38, loss = 0.45818792\n",
      "Iteration 39, loss = 0.45606715\n",
      "Iteration 40, loss = 0.45414235\n",
      "Iteration 41, loss = 0.45235471\n",
      "Iteration 42, loss = 0.45054552\n",
      "Iteration 43, loss = 0.44877957\n",
      "Iteration 44, loss = 0.44706924\n",
      "Iteration 45, loss = 0.44561316\n",
      "Iteration 46, loss = 0.44406833\n",
      "Iteration 47, loss = 0.44262878\n",
      "Iteration 48, loss = 0.44153823\n",
      "Iteration 49, loss = 0.44007458\n",
      "Iteration 50, loss = 0.43895929\n",
      "Iteration 51, loss = 0.43732297\n",
      "Iteration 52, loss = 0.43640493\n",
      "Iteration 53, loss = 0.43507259\n",
      "Iteration 54, loss = 0.43410219\n",
      "Iteration 55, loss = 0.43317166\n",
      "Iteration 56, loss = 0.43179033\n",
      "Iteration 57, loss = 0.43103501\n",
      "Iteration 58, loss = 0.42972913\n",
      "Iteration 59, loss = 0.42862859\n",
      "Iteration 60, loss = 0.42785022\n",
      "Iteration 61, loss = 0.42692761\n",
      "Iteration 62, loss = 0.42605509\n",
      "Iteration 63, loss = 0.42509359\n",
      "Iteration 64, loss = 0.42431286\n",
      "Iteration 65, loss = 0.42344249\n",
      "Iteration 66, loss = 0.42274777\n",
      "Iteration 67, loss = 0.42187779\n",
      "Iteration 68, loss = 0.42099328\n",
      "Iteration 69, loss = 0.42031394\n",
      "Iteration 70, loss = 0.41946290\n",
      "Iteration 71, loss = 0.41892369\n",
      "Iteration 72, loss = 0.41818143\n",
      "Iteration 73, loss = 0.41728754\n",
      "Iteration 74, loss = 0.41668664\n",
      "Iteration 75, loss = 0.41613106\n",
      "Iteration 76, loss = 0.41537905\n",
      "Iteration 77, loss = 0.41486871\n",
      "Iteration 78, loss = 0.41408900\n",
      "Iteration 79, loss = 0.41371310\n",
      "Iteration 80, loss = 0.41300706\n",
      "Iteration 81, loss = 0.41256939\n",
      "Iteration 82, loss = 0.41190172\n",
      "Iteration 83, loss = 0.41134396\n",
      "Iteration 84, loss = 0.41095733\n",
      "Iteration 85, loss = 0.41044557\n",
      "Iteration 86, loss = 0.40972402\n",
      "Iteration 87, loss = 0.40913430\n",
      "Iteration 88, loss = 0.40874093\n",
      "Iteration 89, loss = 0.40847491\n",
      "Iteration 90, loss = 0.40786920\n",
      "Iteration 91, loss = 0.40761829\n",
      "Iteration 92, loss = 0.40689820\n",
      "Iteration 93, loss = 0.40675562\n",
      "Iteration 94, loss = 0.40645295\n",
      "Iteration 95, loss = 0.40580831\n",
      "Iteration 96, loss = 0.40577135\n",
      "Iteration 97, loss = 0.40511927\n",
      "Iteration 98, loss = 0.40457418\n",
      "Iteration 99, loss = 0.40454024\n",
      "Iteration 100, loss = 0.40399372\n",
      "Iteration 101, loss = 0.40369447\n",
      "Iteration 102, loss = 0.40329599\n",
      "Iteration 103, loss = 0.40321400\n",
      "Iteration 104, loss = 0.40275901\n",
      "Iteration 105, loss = 0.40234789\n",
      "Iteration 106, loss = 0.40227464\n",
      "Iteration 107, loss = 0.40192343\n",
      "Iteration 108, loss = 0.40154688\n",
      "Iteration 109, loss = 0.40124235\n",
      "Iteration 110, loss = 0.40112856\n",
      "Iteration 111, loss = 0.40107527\n",
      "Iteration 112, loss = 0.40046852\n",
      "Iteration 113, loss = 0.40034051\n",
      "Iteration 114, loss = 0.39991639\n",
      "Iteration 115, loss = 0.39977729\n",
      "Iteration 116, loss = 0.39927239\n",
      "Iteration 117, loss = 0.39910961\n",
      "Iteration 118, loss = 0.39870733\n",
      "Iteration 119, loss = 0.39879920\n",
      "Iteration 120, loss = 0.39827804\n",
      "Iteration 121, loss = 0.39824399\n",
      "Iteration 122, loss = 0.39787064\n",
      "Iteration 123, loss = 0.39788813\n",
      "Iteration 124, loss = 0.39754272\n",
      "Iteration 125, loss = 0.39700480\n",
      "Iteration 126, loss = 0.39737587\n",
      "Iteration 127, loss = 0.39673485\n",
      "Iteration 128, loss = 0.39671793\n",
      "Iteration 129, loss = 0.39656265\n",
      "Iteration 130, loss = 0.39615439\n",
      "Iteration 131, loss = 0.39582466\n",
      "Iteration 132, loss = 0.39572822\n",
      "Iteration 133, loss = 0.39541537\n",
      "Iteration 134, loss = 0.39549586\n",
      "Iteration 135, loss = 0.39521516\n",
      "Iteration 136, loss = 0.39517412\n",
      "Iteration 137, loss = 0.39474299\n",
      "Iteration 138, loss = 0.39493006\n",
      "Iteration 139, loss = 0.39434570\n",
      "Iteration 140, loss = 0.39423987\n",
      "Iteration 141, loss = 0.39435919\n",
      "Iteration 142, loss = 0.39398615\n",
      "Iteration 143, loss = 0.39403572\n",
      "Iteration 144, loss = 0.39392684\n",
      "Iteration 145, loss = 0.39368072\n",
      "Iteration 146, loss = 0.39342606\n",
      "Iteration 147, loss = 0.39318664\n",
      "Iteration 148, loss = 0.39319923\n",
      "Iteration 149, loss = 0.39295414\n",
      "Iteration 150, loss = 0.39284069\n",
      "Iteration 151, loss = 0.39262651\n",
      "Iteration 152, loss = 0.39249896\n",
      "Iteration 153, loss = 0.39247290\n",
      "Iteration 154, loss = 0.39230334\n",
      "Iteration 155, loss = 0.39210691\n",
      "Iteration 156, loss = 0.39187631\n",
      "Iteration 157, loss = 0.39166761\n",
      "Iteration 158, loss = 0.39150346\n",
      "Iteration 159, loss = 0.39145147\n",
      "Iteration 160, loss = 0.39148310\n",
      "Iteration 161, loss = 0.39132948\n",
      "Iteration 162, loss = 0.39122709\n",
      "Iteration 163, loss = 0.39088020\n",
      "Iteration 164, loss = 0.39102910\n",
      "Iteration 165, loss = 0.39098215\n",
      "Iteration 166, loss = 0.39083367\n",
      "Iteration 167, loss = 0.39062689\n",
      "Iteration 168, loss = 0.39044833\n",
      "Iteration 169, loss = 0.39025126\n",
      "Iteration 170, loss = 0.39022779\n",
      "Iteration 171, loss = 0.38999150\n",
      "Iteration 172, loss = 0.38994408\n",
      "Iteration 173, loss = 0.38973943\n",
      "Iteration 174, loss = 0.38976532\n",
      "Iteration 175, loss = 0.38961062\n",
      "Iteration 176, loss = 0.38942045\n",
      "Iteration 177, loss = 0.38937887\n",
      "Iteration 178, loss = 0.38934239\n",
      "Iteration 179, loss = 0.38917004\n",
      "Iteration 180, loss = 0.38913929\n",
      "Iteration 181, loss = 0.38919505\n",
      "Iteration 182, loss = 0.38870980\n",
      "Iteration 183, loss = 0.38891920\n",
      "Iteration 184, loss = 0.38858333\n",
      "Iteration 185, loss = 0.38840202\n",
      "Iteration 186, loss = 0.38834172\n",
      "Iteration 187, loss = 0.38822002\n",
      "Iteration 188, loss = 0.38803886\n",
      "Iteration 189, loss = 0.38819166\n",
      "Iteration 190, loss = 0.38830064\n",
      "Iteration 191, loss = 0.38785602\n",
      "Iteration 192, loss = 0.38781311\n",
      "Iteration 193, loss = 0.38766899\n",
      "Iteration 194, loss = 0.38764799\n",
      "Iteration 195, loss = 0.38745281\n",
      "Iteration 196, loss = 0.38743031\n",
      "Iteration 197, loss = 0.38738866\n",
      "Iteration 198, loss = 0.38728548\n",
      "Iteration 199, loss = 0.38733976\n",
      "Iteration 200, loss = 0.38691163\n",
      "Iteration 201, loss = 0.38690170\n",
      "Iteration 202, loss = 0.38703471\n",
      "Iteration 203, loss = 0.38710567\n",
      "Iteration 204, loss = 0.38690261\n",
      "Iteration 205, loss = 0.38678397\n",
      "Iteration 206, loss = 0.38660739\n",
      "Iteration 207, loss = 0.38632735\n",
      "Iteration 208, loss = 0.38659238\n",
      "Iteration 209, loss = 0.38633189\n",
      "Iteration 210, loss = 0.38614131\n",
      "Iteration 211, loss = 0.38613038\n",
      "Iteration 212, loss = 0.38603575\n",
      "Iteration 213, loss = 0.38598917\n",
      "Iteration 214, loss = 0.38599586\n",
      "Iteration 215, loss = 0.38602431\n",
      "Iteration 216, loss = 0.38589764\n",
      "Iteration 217, loss = 0.38553597\n",
      "Iteration 218, loss = 0.38550295\n",
      "Iteration 219, loss = 0.38552727\n",
      "Iteration 220, loss = 0.38536912\n",
      "Iteration 221, loss = 0.38532374\n",
      "Iteration 222, loss = 0.38515143\n",
      "Iteration 223, loss = 0.38525737\n",
      "Iteration 224, loss = 0.38472536\n",
      "Iteration 225, loss = 0.38499422\n",
      "Iteration 226, loss = 0.38470732\n",
      "Iteration 227, loss = 0.38473735\n",
      "Iteration 228, loss = 0.38474605\n",
      "Iteration 229, loss = 0.38468940\n",
      "Iteration 230, loss = 0.38457386\n",
      "Iteration 231, loss = 0.38447134\n",
      "Iteration 232, loss = 0.38446780\n",
      "Iteration 233, loss = 0.38428041\n",
      "Iteration 234, loss = 0.38430629\n",
      "Iteration 235, loss = 0.38412549\n",
      "Iteration 236, loss = 0.38406055\n",
      "Iteration 237, loss = 0.38418743\n",
      "Iteration 238, loss = 0.38367607\n",
      "Iteration 239, loss = 0.38364647\n",
      "Iteration 240, loss = 0.38367488\n",
      "Iteration 241, loss = 0.38360774\n",
      "Iteration 242, loss = 0.38351488\n",
      "Iteration 243, loss = 0.38377904\n",
      "Iteration 244, loss = 0.38348166\n",
      "Iteration 245, loss = 0.38357853\n",
      "Iteration 246, loss = 0.38338203\n",
      "Iteration 247, loss = 0.38322553\n",
      "Iteration 248, loss = 0.38318586\n",
      "Iteration 249, loss = 0.38320648\n",
      "Iteration 250, loss = 0.38307047\n",
      "Iteration 251, loss = 0.38297201\n",
      "Iteration 252, loss = 0.38282595\n",
      "Iteration 253, loss = 0.38262154\n",
      "Iteration 254, loss = 0.38284331\n",
      "Iteration 255, loss = 0.38261990\n",
      "Iteration 256, loss = 0.38273422\n",
      "Iteration 257, loss = 0.38263710\n",
      "Iteration 258, loss = 0.38272302\n",
      "Iteration 259, loss = 0.38235800\n",
      "Iteration 260, loss = 0.38242658\n",
      "Iteration 261, loss = 0.38251884\n",
      "Iteration 262, loss = 0.38236700\n",
      "Iteration 263, loss = 0.38221729\n",
      "Iteration 264, loss = 0.38219806\n",
      "Iteration 265, loss = 0.38220606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 266, loss = 0.38217871\n",
      "Iteration 267, loss = 0.38205998\n",
      "Iteration 268, loss = 0.38190631\n",
      "Iteration 269, loss = 0.38195826\n",
      "Iteration 270, loss = 0.38190480\n",
      "Iteration 271, loss = 0.38168722\n",
      "Iteration 272, loss = 0.38159361\n",
      "Iteration 273, loss = 0.38168041\n",
      "Iteration 274, loss = 0.38168333\n",
      "Iteration 275, loss = 0.38170307\n",
      "Iteration 276, loss = 0.38155833\n",
      "Iteration 277, loss = 0.38159814\n",
      "Iteration 278, loss = 0.38124698\n",
      "Iteration 279, loss = 0.38147576\n",
      "Iteration 280, loss = 0.38128924\n",
      "Iteration 281, loss = 0.38136298\n",
      "Iteration 282, loss = 0.38108473\n",
      "Iteration 283, loss = 0.38107111\n",
      "Iteration 284, loss = 0.38101573\n",
      "Iteration 285, loss = 0.38099927\n",
      "Iteration 286, loss = 0.38114762\n",
      "Iteration 287, loss = 0.38079351\n",
      "Iteration 288, loss = 0.38090151\n",
      "Iteration 289, loss = 0.38091587\n",
      "Iteration 290, loss = 0.38076547\n",
      "Iteration 291, loss = 0.38063922\n",
      "Iteration 292, loss = 0.38077870\n",
      "Iteration 293, loss = 0.38058597\n",
      "Iteration 294, loss = 0.38051140\n",
      "Iteration 295, loss = 0.38053209\n",
      "Iteration 296, loss = 0.38044199\n",
      "Iteration 297, loss = 0.38046119\n",
      "Iteration 298, loss = 0.38033717\n",
      "Iteration 299, loss = 0.38027181\n",
      "Iteration 300, loss = 0.38007035\n",
      "Iteration 301, loss = 0.38013539\n",
      "Iteration 302, loss = 0.38022785\n",
      "Iteration 303, loss = 0.38015161\n",
      "Iteration 304, loss = 0.37991686\n",
      "Iteration 305, loss = 0.38014099\n",
      "Iteration 306, loss = 0.38012368\n",
      "Iteration 307, loss = 0.37999554\n",
      "Iteration 308, loss = 0.37985822\n",
      "Iteration 309, loss = 0.37987056\n",
      "Iteration 310, loss = 0.37980986\n",
      "Iteration 311, loss = 0.37984646\n",
      "Iteration 312, loss = 0.37978559\n",
      "Iteration 313, loss = 0.37959900\n",
      "Iteration 314, loss = 0.37963663\n",
      "Iteration 315, loss = 0.37945227\n",
      "Iteration 316, loss = 0.37945767\n",
      "Iteration 317, loss = 0.37923510\n",
      "Iteration 318, loss = 0.37936282\n",
      "Iteration 319, loss = 0.37933764\n",
      "Iteration 320, loss = 0.37939156\n",
      "Iteration 321, loss = 0.37956997\n",
      "Iteration 322, loss = 0.37953179\n",
      "Iteration 323, loss = 0.37912187\n",
      "Iteration 324, loss = 0.37929503\n",
      "Iteration 325, loss = 0.37910658\n",
      "Iteration 326, loss = 0.37903418\n",
      "Iteration 327, loss = 0.37904058\n",
      "Iteration 328, loss = 0.37888527\n",
      "Iteration 329, loss = 0.37920516\n",
      "Iteration 330, loss = 0.37892594\n",
      "Iteration 331, loss = 0.37872652\n",
      "Iteration 332, loss = 0.37860775\n",
      "Iteration 333, loss = 0.37877831\n",
      "Iteration 334, loss = 0.37867215\n",
      "Iteration 335, loss = 0.37868811\n",
      "Iteration 336, loss = 0.37880048\n",
      "Iteration 337, loss = 0.37852164\n",
      "Iteration 338, loss = 0.37840593\n",
      "Iteration 339, loss = 0.37830700\n",
      "Iteration 340, loss = 0.37856594\n",
      "Iteration 341, loss = 0.37829417\n",
      "Iteration 342, loss = 0.37834065\n",
      "Iteration 343, loss = 0.37838676\n",
      "Iteration 344, loss = 0.37834569\n",
      "Iteration 345, loss = 0.37828211\n",
      "Iteration 346, loss = 0.37804975\n",
      "Iteration 347, loss = 0.37806498\n",
      "Iteration 348, loss = 0.37827770\n",
      "Iteration 349, loss = 0.37812628\n",
      "Iteration 350, loss = 0.37817709\n",
      "Iteration 351, loss = 0.37777226\n",
      "Iteration 352, loss = 0.37809938\n",
      "Iteration 353, loss = 0.37782206\n",
      "Iteration 354, loss = 0.37783385\n",
      "Iteration 355, loss = 0.37774686\n",
      "Iteration 356, loss = 0.37770795\n",
      "Iteration 357, loss = 0.37767643\n",
      "Iteration 358, loss = 0.37779229\n",
      "Iteration 359, loss = 0.37778053\n",
      "Iteration 360, loss = 0.37765809\n",
      "Iteration 361, loss = 0.37777276\n",
      "Iteration 362, loss = 0.37738421\n",
      "Iteration 363, loss = 0.37742095\n",
      "Iteration 364, loss = 0.37746582\n",
      "Iteration 365, loss = 0.37735443\n",
      "Iteration 366, loss = 0.37716759\n",
      "Iteration 367, loss = 0.37736429\n",
      "Iteration 368, loss = 0.37751388\n",
      "Iteration 369, loss = 0.37750668\n",
      "Iteration 370, loss = 0.37703202\n",
      "Iteration 371, loss = 0.37729180\n",
      "Iteration 372, loss = 0.37728797\n",
      "Iteration 373, loss = 0.37685682\n",
      "Iteration 374, loss = 0.37709731\n",
      "Iteration 375, loss = 0.37716657\n",
      "Iteration 376, loss = 0.37697475\n",
      "Iteration 377, loss = 0.37729860\n",
      "Iteration 378, loss = 0.37691884\n",
      "Iteration 379, loss = 0.37707800\n",
      "Iteration 380, loss = 0.37690773\n",
      "Iteration 381, loss = 0.37676740\n",
      "Iteration 382, loss = 0.37687060\n",
      "Iteration 383, loss = 0.37703230\n",
      "Iteration 384, loss = 0.37666834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85075919\n",
      "Iteration 2, loss = 1.60488584\n",
      "Iteration 3, loss = 1.40835766\n",
      "Iteration 4, loss = 1.25010308\n",
      "Iteration 5, loss = 1.12299965\n",
      "Iteration 6, loss = 1.01923097\n",
      "Iteration 7, loss = 0.93521023\n",
      "Iteration 8, loss = 0.86725084\n",
      "Iteration 9, loss = 0.81126491\n",
      "Iteration 10, loss = 0.76499859\n",
      "Iteration 11, loss = 0.72632279\n",
      "Iteration 12, loss = 0.69361360\n",
      "Iteration 13, loss = 0.66609144\n",
      "Iteration 14, loss = 0.64216868\n",
      "Iteration 15, loss = 0.62139589\n",
      "Iteration 16, loss = 0.60328745\n",
      "Iteration 17, loss = 0.58727346\n",
      "Iteration 18, loss = 0.57320766\n",
      "Iteration 19, loss = 0.55998874\n",
      "Iteration 20, loss = 0.54745673\n",
      "Iteration 21, loss = 0.53696789\n",
      "Iteration 22, loss = 0.52782340\n",
      "Iteration 23, loss = 0.51932701\n",
      "Iteration 24, loss = 0.51218639\n",
      "Iteration 25, loss = 0.50529593\n",
      "Iteration 26, loss = 0.49943079\n",
      "Iteration 27, loss = 0.49378333\n",
      "Iteration 28, loss = 0.48891398\n",
      "Iteration 29, loss = 0.48451427\n",
      "Iteration 30, loss = 0.48036909\n",
      "Iteration 31, loss = 0.47650088\n",
      "Iteration 32, loss = 0.47291439\n",
      "Iteration 33, loss = 0.46974183\n",
      "Iteration 34, loss = 0.46671644\n",
      "Iteration 35, loss = 0.46394157\n",
      "Iteration 36, loss = 0.46126487\n",
      "Iteration 37, loss = 0.45887569\n",
      "Iteration 38, loss = 0.45644213\n",
      "Iteration 39, loss = 0.45393631\n",
      "Iteration 40, loss = 0.45249231\n",
      "Iteration 41, loss = 0.45069582\n",
      "Iteration 42, loss = 0.44855907\n",
      "Iteration 43, loss = 0.44683998\n",
      "Iteration 44, loss = 0.44526083\n",
      "Iteration 45, loss = 0.44376087\n",
      "Iteration 46, loss = 0.44196330\n",
      "Iteration 47, loss = 0.44060873\n",
      "Iteration 48, loss = 0.43917294\n",
      "Iteration 49, loss = 0.43790822\n",
      "Iteration 50, loss = 0.43642734\n",
      "Iteration 51, loss = 0.43528759\n",
      "Iteration 52, loss = 0.43415629\n",
      "Iteration 53, loss = 0.43311266\n",
      "Iteration 54, loss = 0.43196252\n",
      "Iteration 55, loss = 0.43063125\n",
      "Iteration 56, loss = 0.42976626\n",
      "Iteration 57, loss = 0.42863065\n",
      "Iteration 58, loss = 0.42770079\n",
      "Iteration 59, loss = 0.42677669\n",
      "Iteration 60, loss = 0.42589268\n",
      "Iteration 61, loss = 0.42490919\n",
      "Iteration 62, loss = 0.42406141\n",
      "Iteration 63, loss = 0.42315530\n",
      "Iteration 64, loss = 0.42242866\n",
      "Iteration 65, loss = 0.42173437\n",
      "Iteration 66, loss = 0.42076709\n",
      "Iteration 67, loss = 0.41987226\n",
      "Iteration 68, loss = 0.41929662\n",
      "Iteration 69, loss = 0.41843408\n",
      "Iteration 70, loss = 0.41789551\n",
      "Iteration 71, loss = 0.41712793\n",
      "Iteration 72, loss = 0.41643312\n",
      "Iteration 73, loss = 0.41554970\n",
      "Iteration 74, loss = 0.41506946\n",
      "Iteration 75, loss = 0.41442700\n",
      "Iteration 76, loss = 0.41405788\n",
      "Iteration 77, loss = 0.41315057\n",
      "Iteration 78, loss = 0.41298400\n",
      "Iteration 79, loss = 0.41218595\n",
      "Iteration 80, loss = 0.41147389\n",
      "Iteration 81, loss = 0.41099078\n",
      "Iteration 82, loss = 0.41065211\n",
      "Iteration 83, loss = 0.40976493\n",
      "Iteration 84, loss = 0.41005060\n",
      "Iteration 85, loss = 0.40941847\n",
      "Iteration 86, loss = 0.40885138\n",
      "Iteration 87, loss = 0.40812573\n",
      "Iteration 88, loss = 0.40776009\n",
      "Iteration 89, loss = 0.40748054\n",
      "Iteration 90, loss = 0.40744653\n",
      "Iteration 91, loss = 0.40658960\n",
      "Iteration 92, loss = 0.40603753\n",
      "Iteration 93, loss = 0.40607700\n",
      "Iteration 94, loss = 0.40571911\n",
      "Iteration 95, loss = 0.40522478\n",
      "Iteration 96, loss = 0.40497305\n",
      "Iteration 97, loss = 0.40451378\n",
      "Iteration 98, loss = 0.40424864\n",
      "Iteration 99, loss = 0.40383693\n",
      "Iteration 100, loss = 0.40336495\n",
      "Iteration 101, loss = 0.40315954\n",
      "Iteration 102, loss = 0.40292522\n",
      "Iteration 103, loss = 0.40270746\n",
      "Iteration 104, loss = 0.40218653\n",
      "Iteration 105, loss = 0.40213308\n",
      "Iteration 106, loss = 0.40188115\n",
      "Iteration 107, loss = 0.40175458\n",
      "Iteration 108, loss = 0.40103619\n",
      "Iteration 109, loss = 0.40103465\n",
      "Iteration 110, loss = 0.40073003\n",
      "Iteration 111, loss = 0.40043771\n",
      "Iteration 112, loss = 0.40034163\n",
      "Iteration 113, loss = 0.39999893\n",
      "Iteration 114, loss = 0.40001564\n",
      "Iteration 115, loss = 0.39953311\n",
      "Iteration 116, loss = 0.39922425\n",
      "Iteration 117, loss = 0.39915776\n",
      "Iteration 118, loss = 0.39893394\n",
      "Iteration 119, loss = 0.39893404\n",
      "Iteration 120, loss = 0.39852859\n",
      "Iteration 121, loss = 0.39816773\n",
      "Iteration 122, loss = 0.39812249\n",
      "Iteration 123, loss = 0.39786708\n",
      "Iteration 124, loss = 0.39777959\n",
      "Iteration 125, loss = 0.39744036\n",
      "Iteration 126, loss = 0.39718504\n",
      "Iteration 127, loss = 0.39716433\n",
      "Iteration 128, loss = 0.39691703\n",
      "Iteration 129, loss = 0.39655482\n",
      "Iteration 130, loss = 0.39659999\n",
      "Iteration 131, loss = 0.39637860\n",
      "Iteration 132, loss = 0.39639444\n",
      "Iteration 133, loss = 0.39606369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 134, loss = 0.39606966\n",
      "Iteration 135, loss = 0.39566438\n",
      "Iteration 136, loss = 0.39575540\n",
      "Iteration 137, loss = 0.39535194\n",
      "Iteration 138, loss = 0.39528007\n",
      "Iteration 139, loss = 0.39516895\n",
      "Iteration 140, loss = 0.39496325\n",
      "Iteration 141, loss = 0.39465087\n",
      "Iteration 142, loss = 0.39484253\n",
      "Iteration 143, loss = 0.39452186\n",
      "Iteration 144, loss = 0.39441206\n",
      "Iteration 145, loss = 0.39413936\n",
      "Iteration 146, loss = 0.39392392\n",
      "Iteration 147, loss = 0.39400851\n",
      "Iteration 148, loss = 0.39362551\n",
      "Iteration 149, loss = 0.39344385\n",
      "Iteration 150, loss = 0.39357978\n",
      "Iteration 151, loss = 0.39355771\n",
      "Iteration 152, loss = 0.39317164\n",
      "Iteration 153, loss = 0.39309814\n",
      "Iteration 154, loss = 0.39305995\n",
      "Iteration 155, loss = 0.39290937\n",
      "Iteration 156, loss = 0.39266958\n",
      "Iteration 157, loss = 0.39236062\n",
      "Iteration 158, loss = 0.39243337\n",
      "Iteration 159, loss = 0.39222570\n",
      "Iteration 160, loss = 0.39226474\n",
      "Iteration 161, loss = 0.39192881\n",
      "Iteration 162, loss = 0.39178028\n",
      "Iteration 163, loss = 0.39194197\n",
      "Iteration 164, loss = 0.39185810\n",
      "Iteration 165, loss = 0.39159855\n",
      "Iteration 166, loss = 0.39153356\n",
      "Iteration 167, loss = 0.39129270\n",
      "Iteration 168, loss = 0.39129822\n",
      "Iteration 169, loss = 0.39097004\n",
      "Iteration 170, loss = 0.39091518\n",
      "Iteration 171, loss = 0.39078980\n",
      "Iteration 172, loss = 0.39073152\n",
      "Iteration 173, loss = 0.39071262\n",
      "Iteration 174, loss = 0.39054334\n",
      "Iteration 175, loss = 0.39030555\n",
      "Iteration 176, loss = 0.39017734\n",
      "Iteration 177, loss = 0.39010332\n",
      "Iteration 178, loss = 0.39001809\n",
      "Iteration 179, loss = 0.39001434\n",
      "Iteration 180, loss = 0.39006219\n",
      "Iteration 181, loss = 0.38990149\n",
      "Iteration 182, loss = 0.38962650\n",
      "Iteration 183, loss = 0.38977331\n",
      "Iteration 184, loss = 0.38954108\n",
      "Iteration 185, loss = 0.38930881\n",
      "Iteration 186, loss = 0.38918258\n",
      "Iteration 187, loss = 0.38921969\n",
      "Iteration 188, loss = 0.38911030\n",
      "Iteration 189, loss = 0.38919280\n",
      "Iteration 190, loss = 0.38883494\n",
      "Iteration 191, loss = 0.38862040\n",
      "Iteration 192, loss = 0.38871370\n",
      "Iteration 193, loss = 0.38861056\n",
      "Iteration 194, loss = 0.38843841\n",
      "Iteration 195, loss = 0.38843421\n",
      "Iteration 196, loss = 0.38846708\n",
      "Iteration 197, loss = 0.38827159\n",
      "Iteration 198, loss = 0.38802719\n",
      "Iteration 199, loss = 0.38801318\n",
      "Iteration 200, loss = 0.38816031\n",
      "Iteration 201, loss = 0.38771041\n",
      "Iteration 202, loss = 0.38779225\n",
      "Iteration 203, loss = 0.38769391\n",
      "Iteration 204, loss = 0.38746815\n",
      "Iteration 205, loss = 0.38739047\n",
      "Iteration 206, loss = 0.38744435\n",
      "Iteration 207, loss = 0.38748569\n",
      "Iteration 208, loss = 0.38755494\n",
      "Iteration 209, loss = 0.38726919\n",
      "Iteration 210, loss = 0.38723134\n",
      "Iteration 211, loss = 0.38709119\n",
      "Iteration 212, loss = 0.38712231\n",
      "Iteration 213, loss = 0.38708486\n",
      "Iteration 214, loss = 0.38691584\n",
      "Iteration 215, loss = 0.38666336\n",
      "Iteration 216, loss = 0.38652502\n",
      "Iteration 217, loss = 0.38678934\n",
      "Iteration 218, loss = 0.38661917\n",
      "Iteration 219, loss = 0.38624496\n",
      "Iteration 220, loss = 0.38661007\n",
      "Iteration 221, loss = 0.38614331\n",
      "Iteration 222, loss = 0.38621096\n",
      "Iteration 223, loss = 0.38592657\n",
      "Iteration 224, loss = 0.38592683\n",
      "Iteration 225, loss = 0.38576043\n",
      "Iteration 226, loss = 0.38596450\n",
      "Iteration 227, loss = 0.38582897\n",
      "Iteration 228, loss = 0.38588876\n",
      "Iteration 229, loss = 0.38580298\n",
      "Iteration 230, loss = 0.38568262\n",
      "Iteration 231, loss = 0.38552831\n",
      "Iteration 232, loss = 0.38551843\n",
      "Iteration 233, loss = 0.38521497\n",
      "Iteration 234, loss = 0.38525360\n",
      "Iteration 235, loss = 0.38539898\n",
      "Iteration 236, loss = 0.38507211\n",
      "Iteration 237, loss = 0.38513876\n",
      "Iteration 238, loss = 0.38495170\n",
      "Iteration 239, loss = 0.38511042\n",
      "Iteration 240, loss = 0.38499669\n",
      "Iteration 241, loss = 0.38506293\n",
      "Iteration 242, loss = 0.38485035\n",
      "Iteration 243, loss = 0.38479924\n",
      "Iteration 244, loss = 0.38472070\n",
      "Iteration 245, loss = 0.38455072\n",
      "Iteration 246, loss = 0.38478170\n",
      "Iteration 247, loss = 0.38477000\n",
      "Iteration 248, loss = 0.38447674\n",
      "Iteration 249, loss = 0.38429964\n",
      "Iteration 250, loss = 0.38433200\n",
      "Iteration 251, loss = 0.38433988\n",
      "Iteration 252, loss = 0.38407979\n",
      "Iteration 253, loss = 0.38422946\n",
      "Iteration 254, loss = 0.38423736\n",
      "Iteration 255, loss = 0.38407553\n",
      "Iteration 256, loss = 0.38388335\n",
      "Iteration 257, loss = 0.38368308\n",
      "Iteration 258, loss = 0.38374201\n",
      "Iteration 259, loss = 0.38368552\n",
      "Iteration 260, loss = 0.38415635\n",
      "Iteration 261, loss = 0.38364519\n",
      "Iteration 262, loss = 0.38379625\n",
      "Iteration 263, loss = 0.38332040\n",
      "Iteration 264, loss = 0.38365084\n",
      "Iteration 265, loss = 0.38347176\n",
      "Iteration 266, loss = 0.38321859\n",
      "Iteration 267, loss = 0.38339134\n",
      "Iteration 268, loss = 0.38326218\n",
      "Iteration 269, loss = 0.38324965\n",
      "Iteration 270, loss = 0.38299308\n",
      "Iteration 271, loss = 0.38309281\n",
      "Iteration 272, loss = 0.38303027\n",
      "Iteration 273, loss = 0.38295047\n",
      "Iteration 274, loss = 0.38275006\n",
      "Iteration 275, loss = 0.38316368\n",
      "Iteration 276, loss = 0.38283809\n",
      "Iteration 277, loss = 0.38269174\n",
      "Iteration 278, loss = 0.38271006\n",
      "Iteration 279, loss = 0.38284418\n",
      "Iteration 280, loss = 0.38278780\n",
      "Iteration 281, loss = 0.38273524\n",
      "Iteration 282, loss = 0.38251362\n",
      "Iteration 283, loss = 0.38241977\n",
      "Iteration 284, loss = 0.38230193\n",
      "Iteration 285, loss = 0.38245461\n",
      "Iteration 286, loss = 0.38217617\n",
      "Iteration 287, loss = 0.38226134\n",
      "Iteration 288, loss = 0.38218218\n",
      "Iteration 289, loss = 0.38186501\n",
      "Iteration 290, loss = 0.38213999\n",
      "Iteration 291, loss = 0.38211001\n",
      "Iteration 292, loss = 0.38197630\n",
      "Iteration 293, loss = 0.38201637\n",
      "Iteration 294, loss = 0.38175717\n",
      "Iteration 295, loss = 0.38189476\n",
      "Iteration 296, loss = 0.38171820\n",
      "Iteration 297, loss = 0.38170342\n",
      "Iteration 298, loss = 0.38161903\n",
      "Iteration 299, loss = 0.38172834\n",
      "Iteration 300, loss = 0.38163990\n",
      "Iteration 301, loss = 0.38144849\n",
      "Iteration 302, loss = 0.38178633\n",
      "Iteration 303, loss = 0.38146538\n",
      "Iteration 304, loss = 0.38133730\n",
      "Iteration 305, loss = 0.38139307\n",
      "Iteration 306, loss = 0.38149473\n",
      "Iteration 307, loss = 0.38138497\n",
      "Iteration 308, loss = 0.38132902\n",
      "Iteration 309, loss = 0.38107941\n",
      "Iteration 310, loss = 0.38122552\n",
      "Iteration 311, loss = 0.38138188\n",
      "Iteration 312, loss = 0.38112689\n",
      "Iteration 313, loss = 0.38089012\n",
      "Iteration 314, loss = 0.38092156\n",
      "Iteration 315, loss = 0.38096879\n",
      "Iteration 316, loss = 0.38113474\n",
      "Iteration 317, loss = 0.38064060\n",
      "Iteration 318, loss = 0.38084583\n",
      "Iteration 319, loss = 0.38073504\n",
      "Iteration 320, loss = 0.38066974\n",
      "Iteration 321, loss = 0.38082499\n",
      "Iteration 322, loss = 0.38074253\n",
      "Iteration 323, loss = 0.38035946\n",
      "Iteration 324, loss = 0.38052812\n",
      "Iteration 325, loss = 0.38044745\n",
      "Iteration 326, loss = 0.38032757\n",
      "Iteration 327, loss = 0.38055159\n",
      "Iteration 328, loss = 0.38048099\n",
      "Iteration 329, loss = 0.38061140\n",
      "Iteration 330, loss = 0.38037382\n",
      "Iteration 331, loss = 0.38017732\n",
      "Iteration 332, loss = 0.38025551\n",
      "Iteration 333, loss = 0.38063781\n",
      "Iteration 334, loss = 0.38009837\n",
      "Iteration 335, loss = 0.38011662\n",
      "Iteration 336, loss = 0.38030584\n",
      "Iteration 337, loss = 0.37998205\n",
      "Iteration 338, loss = 0.38023385\n",
      "Iteration 339, loss = 0.37983551\n",
      "Iteration 340, loss = 0.38003711\n",
      "Iteration 341, loss = 0.38005376\n",
      "Iteration 342, loss = 0.37987749\n",
      "Iteration 343, loss = 0.38008492\n",
      "Iteration 344, loss = 0.37983818\n",
      "Iteration 345, loss = 0.37982585\n",
      "Iteration 346, loss = 0.37960920\n",
      "Iteration 347, loss = 0.37966149\n",
      "Iteration 348, loss = 0.37951999\n",
      "Iteration 349, loss = 0.37941289\n",
      "Iteration 350, loss = 0.37964759\n",
      "Iteration 351, loss = 0.37946797\n",
      "Iteration 352, loss = 0.37942721\n",
      "Iteration 353, loss = 0.37941213\n",
      "Iteration 354, loss = 0.37936313\n",
      "Iteration 355, loss = 0.37923126\n",
      "Iteration 356, loss = 0.37932339\n",
      "Iteration 357, loss = 0.37923958\n",
      "Iteration 358, loss = 0.37927691\n",
      "Iteration 359, loss = 0.37922828\n",
      "Iteration 360, loss = 0.37933117\n",
      "Iteration 361, loss = 0.37906634\n",
      "Iteration 362, loss = 0.37934379\n",
      "Iteration 363, loss = 0.37880739\n",
      "Iteration 364, loss = 0.37922736\n",
      "Iteration 365, loss = 0.37887833\n",
      "Iteration 366, loss = 0.37900845\n",
      "Iteration 367, loss = 0.37914947\n",
      "Iteration 368, loss = 0.37883627\n",
      "Iteration 369, loss = 0.37879674\n",
      "Iteration 370, loss = 0.37875566\n",
      "Iteration 371, loss = 0.37863342\n",
      "Iteration 372, loss = 0.37862899\n",
      "Iteration 373, loss = 0.37870777\n",
      "Iteration 374, loss = 0.37868213\n",
      "Iteration 375, loss = 0.37865729\n",
      "Iteration 376, loss = 0.37862760\n",
      "Iteration 377, loss = 0.37856881\n",
      "Iteration 378, loss = 0.37836568\n",
      "Iteration 379, loss = 0.37857196\n",
      "Iteration 380, loss = 0.37837591\n",
      "Iteration 381, loss = 0.37813956\n",
      "Iteration 382, loss = 0.37838655\n",
      "Iteration 383, loss = 0.37846866\n",
      "Iteration 384, loss = 0.37825752\n",
      "Iteration 385, loss = 0.37871283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 386, loss = 0.37827426\n",
      "Iteration 387, loss = 0.37832809\n",
      "Iteration 388, loss = 0.37811257\n",
      "Iteration 389, loss = 0.37840783\n",
      "Iteration 390, loss = 0.37790674\n",
      "Iteration 391, loss = 0.37800043\n",
      "Iteration 392, loss = 0.37809938\n",
      "Iteration 393, loss = 0.37782894\n",
      "Iteration 394, loss = 0.37793716\n",
      "Iteration 395, loss = 0.37828465\n",
      "Iteration 396, loss = 0.37788701\n",
      "Iteration 397, loss = 0.37770174\n",
      "Iteration 398, loss = 0.37781654\n",
      "Iteration 399, loss = 0.37792911\n",
      "Iteration 400, loss = 0.37792369\n",
      "Iteration 401, loss = 0.37795539\n",
      "Iteration 402, loss = 0.37774866\n",
      "Iteration 403, loss = 0.37787508\n",
      "Iteration 404, loss = 0.37754568\n",
      "Iteration 405, loss = 0.37778217\n",
      "Iteration 406, loss = 0.37760086\n",
      "Iteration 407, loss = 0.37754915\n",
      "Iteration 408, loss = 0.37746892\n",
      "Iteration 409, loss = 0.37753201\n",
      "Iteration 410, loss = 0.37756799\n",
      "Iteration 411, loss = 0.37764825\n",
      "Iteration 412, loss = 0.37755558\n",
      "Iteration 413, loss = 0.37751040\n",
      "Iteration 414, loss = 0.37725324\n",
      "Iteration 415, loss = 0.37728290\n",
      "Iteration 416, loss = 0.37714978\n",
      "Iteration 417, loss = 0.37713314\n",
      "Iteration 418, loss = 0.37706213\n",
      "Iteration 419, loss = 0.37736137\n",
      "Iteration 420, loss = 0.37731942\n",
      "Iteration 421, loss = 0.37725607\n",
      "Iteration 422, loss = 0.37711687\n",
      "Iteration 423, loss = 0.37689046\n",
      "Iteration 424, loss = 0.37706234\n",
      "Iteration 425, loss = 0.37706427\n",
      "Iteration 426, loss = 0.37725255\n",
      "Iteration 427, loss = 0.37692774\n",
      "Iteration 428, loss = 0.37705657\n",
      "Iteration 429, loss = 0.37703497\n",
      "Iteration 430, loss = 0.37693776\n",
      "Iteration 431, loss = 0.37666943\n",
      "Iteration 432, loss = 0.37658401\n",
      "Iteration 433, loss = 0.37703899\n",
      "Iteration 434, loss = 0.37701020\n",
      "Iteration 435, loss = 0.37683195\n",
      "Iteration 436, loss = 0.37690004\n",
      "Iteration 437, loss = 0.37680804\n",
      "Iteration 438, loss = 0.37661809\n",
      "Iteration 439, loss = 0.37675986\n",
      "Iteration 440, loss = 0.37666256\n",
      "Iteration 441, loss = 0.37660685\n",
      "Iteration 442, loss = 0.37651497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85214065\n",
      "Iteration 2, loss = 1.60408562\n",
      "Iteration 3, loss = 1.40602474\n",
      "Iteration 4, loss = 1.24755248\n",
      "Iteration 5, loss = 1.12066635\n",
      "Iteration 6, loss = 1.01772171\n",
      "Iteration 7, loss = 0.93454973\n",
      "Iteration 8, loss = 0.86669503\n",
      "Iteration 9, loss = 0.81095544\n",
      "Iteration 10, loss = 0.76486325\n",
      "Iteration 11, loss = 0.72625962\n",
      "Iteration 12, loss = 0.69382511\n",
      "Iteration 13, loss = 0.66587083\n",
      "Iteration 14, loss = 0.64201919\n",
      "Iteration 15, loss = 0.62163497\n",
      "Iteration 16, loss = 0.60367114\n",
      "Iteration 17, loss = 0.58758734\n",
      "Iteration 18, loss = 0.57344764\n",
      "Iteration 19, loss = 0.56025823\n",
      "Iteration 20, loss = 0.54756068\n",
      "Iteration 21, loss = 0.53752764\n",
      "Iteration 22, loss = 0.52824083\n",
      "Iteration 23, loss = 0.52021082\n",
      "Iteration 24, loss = 0.51289510\n",
      "Iteration 25, loss = 0.50625217\n",
      "Iteration 26, loss = 0.50033661\n",
      "Iteration 27, loss = 0.49515720\n",
      "Iteration 28, loss = 0.49007900\n",
      "Iteration 29, loss = 0.48599607\n",
      "Iteration 30, loss = 0.48203838\n",
      "Iteration 31, loss = 0.47823329\n",
      "Iteration 32, loss = 0.47500024\n",
      "Iteration 33, loss = 0.47164430\n",
      "Iteration 34, loss = 0.46887240\n",
      "Iteration 35, loss = 0.46660159\n",
      "Iteration 36, loss = 0.46392627\n",
      "Iteration 37, loss = 0.46167753\n",
      "Iteration 38, loss = 0.45933616\n",
      "Iteration 39, loss = 0.45738462\n",
      "Iteration 40, loss = 0.45552423\n",
      "Iteration 41, loss = 0.45356231\n",
      "Iteration 42, loss = 0.45204248\n",
      "Iteration 43, loss = 0.45030991\n",
      "Iteration 44, loss = 0.44859584\n",
      "Iteration 45, loss = 0.44717275\n",
      "Iteration 46, loss = 0.44572270\n",
      "Iteration 47, loss = 0.44435104\n",
      "Iteration 48, loss = 0.44291565\n",
      "Iteration 49, loss = 0.44163533\n",
      "Iteration 50, loss = 0.44021985\n",
      "Iteration 51, loss = 0.43921957\n",
      "Iteration 52, loss = 0.43787156\n",
      "Iteration 53, loss = 0.43666365\n",
      "Iteration 54, loss = 0.43576360\n",
      "Iteration 55, loss = 0.43468407\n",
      "Iteration 56, loss = 0.43334248\n",
      "Iteration 57, loss = 0.43225289\n",
      "Iteration 58, loss = 0.43139037\n",
      "Iteration 59, loss = 0.43031193\n",
      "Iteration 60, loss = 0.42936210\n",
      "Iteration 61, loss = 0.42847740\n",
      "Iteration 62, loss = 0.42767605\n",
      "Iteration 63, loss = 0.42676270\n",
      "Iteration 64, loss = 0.42573262\n",
      "Iteration 65, loss = 0.42481040\n",
      "Iteration 66, loss = 0.42400651\n",
      "Iteration 67, loss = 0.42337740\n",
      "Iteration 68, loss = 0.42268985\n",
      "Iteration 69, loss = 0.42145485\n",
      "Iteration 70, loss = 0.42085449\n",
      "Iteration 71, loss = 0.42024818\n",
      "Iteration 72, loss = 0.41926529\n",
      "Iteration 73, loss = 0.41861171\n",
      "Iteration 74, loss = 0.41799742\n",
      "Iteration 75, loss = 0.41721751\n",
      "Iteration 76, loss = 0.41685760\n",
      "Iteration 77, loss = 0.41612195\n",
      "Iteration 78, loss = 0.41554543\n",
      "Iteration 79, loss = 0.41492955\n",
      "Iteration 80, loss = 0.41413701\n",
      "Iteration 81, loss = 0.41343270\n",
      "Iteration 82, loss = 0.41284517\n",
      "Iteration 83, loss = 0.41223868\n",
      "Iteration 84, loss = 0.41209786\n",
      "Iteration 85, loss = 0.41123066\n",
      "Iteration 86, loss = 0.41069975\n",
      "Iteration 87, loss = 0.41023852\n",
      "Iteration 88, loss = 0.40964982\n",
      "Iteration 89, loss = 0.40927788\n",
      "Iteration 90, loss = 0.40879430\n",
      "Iteration 91, loss = 0.40816481\n",
      "Iteration 92, loss = 0.40792156\n",
      "Iteration 93, loss = 0.40752880\n",
      "Iteration 94, loss = 0.40707920\n",
      "Iteration 95, loss = 0.40638380\n",
      "Iteration 96, loss = 0.40618844\n",
      "Iteration 97, loss = 0.40596982\n",
      "Iteration 98, loss = 0.40550375\n",
      "Iteration 99, loss = 0.40484115\n",
      "Iteration 100, loss = 0.40477168\n",
      "Iteration 101, loss = 0.40414725\n",
      "Iteration 102, loss = 0.40396831\n",
      "Iteration 103, loss = 0.40360079\n",
      "Iteration 104, loss = 0.40313237\n",
      "Iteration 105, loss = 0.40295048\n",
      "Iteration 106, loss = 0.40237717\n",
      "Iteration 107, loss = 0.40181521\n",
      "Iteration 108, loss = 0.40168634\n",
      "Iteration 109, loss = 0.40152882\n",
      "Iteration 110, loss = 0.40115282\n",
      "Iteration 111, loss = 0.40091893\n",
      "Iteration 112, loss = 0.40045441\n",
      "Iteration 113, loss = 0.40029309\n",
      "Iteration 114, loss = 0.40004424\n",
      "Iteration 115, loss = 0.39985337\n",
      "Iteration 116, loss = 0.39946259\n",
      "Iteration 117, loss = 0.39908901\n",
      "Iteration 118, loss = 0.39908688\n",
      "Iteration 119, loss = 0.39871479\n",
      "Iteration 120, loss = 0.39852768\n",
      "Iteration 121, loss = 0.39853109\n",
      "Iteration 122, loss = 0.39800822\n",
      "Iteration 123, loss = 0.39771071\n",
      "Iteration 124, loss = 0.39742569\n",
      "Iteration 125, loss = 0.39741100\n",
      "Iteration 126, loss = 0.39712526\n",
      "Iteration 127, loss = 0.39690904\n",
      "Iteration 128, loss = 0.39676337\n",
      "Iteration 129, loss = 0.39634803\n",
      "Iteration 130, loss = 0.39643800\n",
      "Iteration 131, loss = 0.39601368\n",
      "Iteration 132, loss = 0.39575941\n",
      "Iteration 133, loss = 0.39549745\n",
      "Iteration 134, loss = 0.39564370\n",
      "Iteration 135, loss = 0.39528069\n",
      "Iteration 136, loss = 0.39531595\n",
      "Iteration 137, loss = 0.39488107\n",
      "Iteration 138, loss = 0.39471565\n",
      "Iteration 139, loss = 0.39432735\n",
      "Iteration 140, loss = 0.39449521\n",
      "Iteration 141, loss = 0.39434823\n",
      "Iteration 142, loss = 0.39414073\n",
      "Iteration 143, loss = 0.39399188\n",
      "Iteration 144, loss = 0.39366001\n",
      "Iteration 145, loss = 0.39390293\n",
      "Iteration 146, loss = 0.39342474\n",
      "Iteration 147, loss = 0.39302443\n",
      "Iteration 148, loss = 0.39311853\n",
      "Iteration 149, loss = 0.39294501\n",
      "Iteration 150, loss = 0.39297633\n",
      "Iteration 151, loss = 0.39246031\n",
      "Iteration 152, loss = 0.39256768\n",
      "Iteration 153, loss = 0.39223879\n",
      "Iteration 154, loss = 0.39215507\n",
      "Iteration 155, loss = 0.39221068\n",
      "Iteration 156, loss = 0.39211038\n",
      "Iteration 157, loss = 0.39160498\n",
      "Iteration 158, loss = 0.39155068\n",
      "Iteration 159, loss = 0.39157172\n",
      "Iteration 160, loss = 0.39116661\n",
      "Iteration 161, loss = 0.39144801\n",
      "Iteration 162, loss = 0.39088007\n",
      "Iteration 163, loss = 0.39073756\n",
      "Iteration 164, loss = 0.39058362\n",
      "Iteration 165, loss = 0.39058470\n",
      "Iteration 166, loss = 0.39050589\n",
      "Iteration 167, loss = 0.39042393\n",
      "Iteration 168, loss = 0.39052434\n",
      "Iteration 169, loss = 0.39042383\n",
      "Iteration 170, loss = 0.39012505\n",
      "Iteration 171, loss = 0.38978839\n",
      "Iteration 172, loss = 0.38989124\n",
      "Iteration 173, loss = 0.38960746\n",
      "Iteration 174, loss = 0.38962454\n",
      "Iteration 175, loss = 0.38947971\n",
      "Iteration 176, loss = 0.38932405\n",
      "Iteration 177, loss = 0.38939702\n",
      "Iteration 178, loss = 0.38934211\n",
      "Iteration 179, loss = 0.38892080\n",
      "Iteration 180, loss = 0.38903525\n",
      "Iteration 181, loss = 0.38878427\n",
      "Iteration 182, loss = 0.38866213\n",
      "Iteration 183, loss = 0.38856468\n",
      "Iteration 184, loss = 0.38857950\n",
      "Iteration 185, loss = 0.38826020\n",
      "Iteration 186, loss = 0.38837612\n",
      "Iteration 187, loss = 0.38820205\n",
      "Iteration 188, loss = 0.38820582\n",
      "Iteration 189, loss = 0.38799820\n",
      "Iteration 190, loss = 0.38802111\n",
      "Iteration 191, loss = 0.38811933\n",
      "Iteration 192, loss = 0.38766650\n",
      "Iteration 193, loss = 0.38763575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 194, loss = 0.38752463\n",
      "Iteration 195, loss = 0.38738178\n",
      "Iteration 196, loss = 0.38728160\n",
      "Iteration 197, loss = 0.38734954\n",
      "Iteration 198, loss = 0.38732175\n",
      "Iteration 199, loss = 0.38708638\n",
      "Iteration 200, loss = 0.38705664\n",
      "Iteration 201, loss = 0.38719383\n",
      "Iteration 202, loss = 0.38685133\n",
      "Iteration 203, loss = 0.38659968\n",
      "Iteration 204, loss = 0.38675966\n",
      "Iteration 205, loss = 0.38634655\n",
      "Iteration 206, loss = 0.38639166\n",
      "Iteration 207, loss = 0.38644958\n",
      "Iteration 208, loss = 0.38642491\n",
      "Iteration 209, loss = 0.38629015\n",
      "Iteration 210, loss = 0.38612267\n",
      "Iteration 211, loss = 0.38604110\n",
      "Iteration 212, loss = 0.38615063\n",
      "Iteration 213, loss = 0.38618085\n",
      "Iteration 214, loss = 0.38597207\n",
      "Iteration 215, loss = 0.38600362\n",
      "Iteration 216, loss = 0.38603031\n",
      "Iteration 217, loss = 0.38578793\n",
      "Iteration 218, loss = 0.38563257\n",
      "Iteration 219, loss = 0.38555044\n",
      "Iteration 220, loss = 0.38554088\n",
      "Iteration 221, loss = 0.38531919\n",
      "Iteration 222, loss = 0.38520966\n",
      "Iteration 223, loss = 0.38530117\n",
      "Iteration 224, loss = 0.38511865\n",
      "Iteration 225, loss = 0.38506057\n",
      "Iteration 226, loss = 0.38505147\n",
      "Iteration 227, loss = 0.38500105\n",
      "Iteration 228, loss = 0.38474523\n",
      "Iteration 229, loss = 0.38485594\n",
      "Iteration 230, loss = 0.38479314\n",
      "Iteration 231, loss = 0.38455949\n",
      "Iteration 232, loss = 0.38478518\n",
      "Iteration 233, loss = 0.38460586\n",
      "Iteration 234, loss = 0.38435951\n",
      "Iteration 235, loss = 0.38436655\n",
      "Iteration 236, loss = 0.38427325\n",
      "Iteration 237, loss = 0.38456084\n",
      "Iteration 238, loss = 0.38421125\n",
      "Iteration 239, loss = 0.38413754\n",
      "Iteration 240, loss = 0.38409348\n",
      "Iteration 241, loss = 0.38425215\n",
      "Iteration 242, loss = 0.38393471\n",
      "Iteration 243, loss = 0.38416169\n",
      "Iteration 244, loss = 0.38389235\n",
      "Iteration 245, loss = 0.38368556\n",
      "Iteration 246, loss = 0.38377585\n",
      "Iteration 247, loss = 0.38384800\n",
      "Iteration 248, loss = 0.38364666\n",
      "Iteration 249, loss = 0.38368294\n",
      "Iteration 250, loss = 0.38329067\n",
      "Iteration 251, loss = 0.38353323\n",
      "Iteration 252, loss = 0.38339697\n",
      "Iteration 253, loss = 0.38340024\n",
      "Iteration 254, loss = 0.38336283\n",
      "Iteration 255, loss = 0.38307557\n",
      "Iteration 256, loss = 0.38311672\n",
      "Iteration 257, loss = 0.38333099\n",
      "Iteration 258, loss = 0.38305450\n",
      "Iteration 259, loss = 0.38284149\n",
      "Iteration 260, loss = 0.38292486\n",
      "Iteration 261, loss = 0.38276565\n",
      "Iteration 262, loss = 0.38285504\n",
      "Iteration 263, loss = 0.38290286\n",
      "Iteration 264, loss = 0.38269154\n",
      "Iteration 265, loss = 0.38262505\n",
      "Iteration 266, loss = 0.38264561\n",
      "Iteration 267, loss = 0.38230939\n",
      "Iteration 268, loss = 0.38259523\n",
      "Iteration 269, loss = 0.38250187\n",
      "Iteration 270, loss = 0.38223423\n",
      "Iteration 271, loss = 0.38245134\n",
      "Iteration 272, loss = 0.38245798\n",
      "Iteration 273, loss = 0.38247899\n",
      "Iteration 274, loss = 0.38228505\n",
      "Iteration 275, loss = 0.38213666\n",
      "Iteration 276, loss = 0.38228530\n",
      "Iteration 277, loss = 0.38228246\n",
      "Iteration 278, loss = 0.38240845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.84818796\n",
      "Iteration 2, loss = 1.60078913\n",
      "Iteration 3, loss = 1.40323259\n",
      "Iteration 4, loss = 1.24535320\n",
      "Iteration 5, loss = 1.11924934\n",
      "Iteration 6, loss = 1.01712660\n",
      "Iteration 7, loss = 0.93406705\n",
      "Iteration 8, loss = 0.86670378\n",
      "Iteration 9, loss = 0.81120399\n",
      "Iteration 10, loss = 0.76537231\n",
      "Iteration 11, loss = 0.72679218\n",
      "Iteration 12, loss = 0.69449589\n",
      "Iteration 13, loss = 0.66690200\n",
      "Iteration 14, loss = 0.64329329\n",
      "Iteration 15, loss = 0.62282295\n",
      "Iteration 16, loss = 0.60517098\n",
      "Iteration 17, loss = 0.58907570\n",
      "Iteration 18, loss = 0.57503712\n",
      "Iteration 19, loss = 0.56152451\n",
      "Iteration 20, loss = 0.54976553\n",
      "Iteration 21, loss = 0.53907539\n",
      "Iteration 22, loss = 0.53004981\n",
      "Iteration 23, loss = 0.52193595\n",
      "Iteration 24, loss = 0.51476041\n",
      "Iteration 25, loss = 0.50795116\n",
      "Iteration 26, loss = 0.50222874\n",
      "Iteration 27, loss = 0.49671071\n",
      "Iteration 28, loss = 0.49209762\n",
      "Iteration 29, loss = 0.48759111\n",
      "Iteration 30, loss = 0.48358173\n",
      "Iteration 31, loss = 0.47966970\n",
      "Iteration 32, loss = 0.47647317\n",
      "Iteration 33, loss = 0.47347836\n",
      "Iteration 34, loss = 0.47067840\n",
      "Iteration 35, loss = 0.46809942\n",
      "Iteration 36, loss = 0.46550113\n",
      "Iteration 37, loss = 0.46302476\n",
      "Iteration 38, loss = 0.46104596\n",
      "Iteration 39, loss = 0.45884970\n",
      "Iteration 40, loss = 0.45736959\n",
      "Iteration 41, loss = 0.45517117\n",
      "Iteration 42, loss = 0.45345984\n",
      "Iteration 43, loss = 0.45197550\n",
      "Iteration 44, loss = 0.45031417\n",
      "Iteration 45, loss = 0.44868741\n",
      "Iteration 46, loss = 0.44721367\n",
      "Iteration 47, loss = 0.44593539\n",
      "Iteration 48, loss = 0.44433439\n",
      "Iteration 49, loss = 0.44339629\n",
      "Iteration 50, loss = 0.44186741\n",
      "Iteration 51, loss = 0.44067112\n",
      "Iteration 52, loss = 0.43917511\n",
      "Iteration 53, loss = 0.43832566\n",
      "Iteration 54, loss = 0.43709354\n",
      "Iteration 55, loss = 0.43600276\n",
      "Iteration 56, loss = 0.43500532\n",
      "Iteration 57, loss = 0.43379301\n",
      "Iteration 58, loss = 0.43317605\n",
      "Iteration 59, loss = 0.43225974\n",
      "Iteration 60, loss = 0.43113401\n",
      "Iteration 61, loss = 0.43026801\n",
      "Iteration 62, loss = 0.42928860\n",
      "Iteration 63, loss = 0.42864072\n",
      "Iteration 64, loss = 0.42771046\n",
      "Iteration 65, loss = 0.42681780\n",
      "Iteration 66, loss = 0.42576429\n",
      "Iteration 67, loss = 0.42544911\n",
      "Iteration 68, loss = 0.42421713\n",
      "Iteration 69, loss = 0.42372714\n",
      "Iteration 70, loss = 0.42297625\n",
      "Iteration 71, loss = 0.42208189\n",
      "Iteration 72, loss = 0.42150976\n",
      "Iteration 73, loss = 0.42064024\n",
      "Iteration 74, loss = 0.42019054\n",
      "Iteration 75, loss = 0.41943959\n",
      "Iteration 76, loss = 0.41878598\n",
      "Iteration 77, loss = 0.41810759\n",
      "Iteration 78, loss = 0.41759895\n",
      "Iteration 79, loss = 0.41714119\n",
      "Iteration 80, loss = 0.41639215\n",
      "Iteration 81, loss = 0.41616570\n",
      "Iteration 82, loss = 0.41548342\n",
      "Iteration 83, loss = 0.41498883\n",
      "Iteration 84, loss = 0.41448098\n",
      "Iteration 85, loss = 0.41405721\n",
      "Iteration 86, loss = 0.41340942\n",
      "Iteration 87, loss = 0.41296225\n",
      "Iteration 88, loss = 0.41271790\n",
      "Iteration 89, loss = 0.41195300\n",
      "Iteration 90, loss = 0.41165844\n",
      "Iteration 91, loss = 0.41118067\n",
      "Iteration 92, loss = 0.41059493\n",
      "Iteration 93, loss = 0.41031054\n",
      "Iteration 94, loss = 0.40995682\n",
      "Iteration 95, loss = 0.40955196\n",
      "Iteration 96, loss = 0.40913844\n",
      "Iteration 97, loss = 0.40857428\n",
      "Iteration 98, loss = 0.40832650\n",
      "Iteration 99, loss = 0.40822920\n",
      "Iteration 100, loss = 0.40774144\n",
      "Iteration 101, loss = 0.40727689\n",
      "Iteration 102, loss = 0.40685839\n",
      "Iteration 103, loss = 0.40657295\n",
      "Iteration 104, loss = 0.40644563\n",
      "Iteration 105, loss = 0.40613427\n",
      "Iteration 106, loss = 0.40591519\n",
      "Iteration 107, loss = 0.40568598\n",
      "Iteration 108, loss = 0.40540823\n",
      "Iteration 109, loss = 0.40484566\n",
      "Iteration 110, loss = 0.40484399\n",
      "Iteration 111, loss = 0.40435446\n",
      "Iteration 112, loss = 0.40386333\n",
      "Iteration 113, loss = 0.40384142\n",
      "Iteration 114, loss = 0.40372888\n",
      "Iteration 115, loss = 0.40329649\n",
      "Iteration 116, loss = 0.40324619\n",
      "Iteration 117, loss = 0.40281329\n",
      "Iteration 118, loss = 0.40256382\n",
      "Iteration 119, loss = 0.40232624\n",
      "Iteration 120, loss = 0.40219462\n",
      "Iteration 121, loss = 0.40162367\n",
      "Iteration 122, loss = 0.40142273\n",
      "Iteration 123, loss = 0.40126748\n",
      "Iteration 124, loss = 0.40114339\n",
      "Iteration 125, loss = 0.40091436\n",
      "Iteration 126, loss = 0.40089903\n",
      "Iteration 127, loss = 0.40047190\n",
      "Iteration 128, loss = 0.40027733\n",
      "Iteration 129, loss = 0.40006014\n",
      "Iteration 130, loss = 0.39990281\n",
      "Iteration 131, loss = 0.39982456\n",
      "Iteration 132, loss = 0.39938759\n",
      "Iteration 133, loss = 0.39906707\n",
      "Iteration 134, loss = 0.39899229\n",
      "Iteration 135, loss = 0.39875218\n",
      "Iteration 136, loss = 0.39888926\n",
      "Iteration 137, loss = 0.39836078\n",
      "Iteration 138, loss = 0.39822917\n",
      "Iteration 139, loss = 0.39790389\n",
      "Iteration 140, loss = 0.39772722\n",
      "Iteration 141, loss = 0.39764687\n",
      "Iteration 142, loss = 0.39772569\n",
      "Iteration 143, loss = 0.39729198\n",
      "Iteration 144, loss = 0.39706868\n",
      "Iteration 145, loss = 0.39703673\n",
      "Iteration 146, loss = 0.39696565\n",
      "Iteration 147, loss = 0.39674672\n",
      "Iteration 148, loss = 0.39657229\n",
      "Iteration 149, loss = 0.39634229\n",
      "Iteration 150, loss = 0.39602771\n",
      "Iteration 151, loss = 0.39600444\n",
      "Iteration 152, loss = 0.39606990\n",
      "Iteration 153, loss = 0.39573108\n",
      "Iteration 154, loss = 0.39557597\n",
      "Iteration 155, loss = 0.39533151\n",
      "Iteration 156, loss = 0.39534014\n",
      "Iteration 157, loss = 0.39499449\n",
      "Iteration 158, loss = 0.39509987\n",
      "Iteration 159, loss = 0.39486926\n",
      "Iteration 160, loss = 0.39478932\n",
      "Iteration 161, loss = 0.39447817\n",
      "Iteration 162, loss = 0.39454317\n",
      "Iteration 163, loss = 0.39445878\n",
      "Iteration 164, loss = 0.39409361\n",
      "Iteration 165, loss = 0.39411607\n",
      "Iteration 166, loss = 0.39413782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 167, loss = 0.39388392\n",
      "Iteration 168, loss = 0.39368024\n",
      "Iteration 169, loss = 0.39377561\n",
      "Iteration 170, loss = 0.39349844\n",
      "Iteration 171, loss = 0.39362252\n",
      "Iteration 172, loss = 0.39320153\n",
      "Iteration 173, loss = 0.39287950\n",
      "Iteration 174, loss = 0.39288434\n",
      "Iteration 175, loss = 0.39294899\n",
      "Iteration 176, loss = 0.39276817\n",
      "Iteration 177, loss = 0.39266175\n",
      "Iteration 178, loss = 0.39257526\n",
      "Iteration 179, loss = 0.39254424\n",
      "Iteration 180, loss = 0.39223786\n",
      "Iteration 181, loss = 0.39224865\n",
      "Iteration 182, loss = 0.39214469\n",
      "Iteration 183, loss = 0.39217589\n",
      "Iteration 184, loss = 0.39227392\n",
      "Iteration 185, loss = 0.39193892\n",
      "Iteration 186, loss = 0.39163913\n",
      "Iteration 187, loss = 0.39183071\n",
      "Iteration 188, loss = 0.39151373\n",
      "Iteration 189, loss = 0.39145640\n",
      "Iteration 190, loss = 0.39139904\n",
      "Iteration 191, loss = 0.39120177\n",
      "Iteration 192, loss = 0.39133138\n",
      "Iteration 193, loss = 0.39092947\n",
      "Iteration 194, loss = 0.39105429\n",
      "Iteration 195, loss = 0.39090466\n",
      "Iteration 196, loss = 0.39082952\n",
      "Iteration 197, loss = 0.39066008\n",
      "Iteration 198, loss = 0.39043926\n",
      "Iteration 199, loss = 0.39044448\n",
      "Iteration 200, loss = 0.39060529\n",
      "Iteration 201, loss = 0.39055030\n",
      "Iteration 202, loss = 0.39029103\n",
      "Iteration 203, loss = 0.38991731\n",
      "Iteration 204, loss = 0.38989502\n",
      "Iteration 205, loss = 0.39010051\n",
      "Iteration 206, loss = 0.38955301\n",
      "Iteration 207, loss = 0.38979572\n",
      "Iteration 208, loss = 0.38984849\n",
      "Iteration 209, loss = 0.38948033\n",
      "Iteration 210, loss = 0.38964405\n",
      "Iteration 211, loss = 0.38938973\n",
      "Iteration 212, loss = 0.38928293\n",
      "Iteration 213, loss = 0.38929074\n",
      "Iteration 214, loss = 0.38926747\n",
      "Iteration 215, loss = 0.38899275\n",
      "Iteration 216, loss = 0.38905604\n",
      "Iteration 217, loss = 0.38898645\n",
      "Iteration 218, loss = 0.38892510\n",
      "Iteration 219, loss = 0.38886062\n",
      "Iteration 220, loss = 0.38854615\n",
      "Iteration 221, loss = 0.38862705\n",
      "Iteration 222, loss = 0.38852961\n",
      "Iteration 223, loss = 0.38837742\n",
      "Iteration 224, loss = 0.38840737\n",
      "Iteration 225, loss = 0.38822875\n",
      "Iteration 226, loss = 0.38839738\n",
      "Iteration 227, loss = 0.38830135\n",
      "Iteration 228, loss = 0.38804297\n",
      "Iteration 229, loss = 0.38807400\n",
      "Iteration 230, loss = 0.38777974\n",
      "Iteration 231, loss = 0.38813266\n",
      "Iteration 232, loss = 0.38790917\n",
      "Iteration 233, loss = 0.38779868\n",
      "Iteration 234, loss = 0.38794282\n",
      "Iteration 235, loss = 0.38763722\n",
      "Iteration 236, loss = 0.38757874\n",
      "Iteration 237, loss = 0.38766237\n",
      "Iteration 238, loss = 0.38757719\n",
      "Iteration 239, loss = 0.38728343\n",
      "Iteration 240, loss = 0.38740536\n",
      "Iteration 241, loss = 0.38716943\n",
      "Iteration 242, loss = 0.38718548\n",
      "Iteration 243, loss = 0.38707147\n",
      "Iteration 244, loss = 0.38691236\n",
      "Iteration 245, loss = 0.38704007\n",
      "Iteration 246, loss = 0.38682592\n",
      "Iteration 247, loss = 0.38669348\n",
      "Iteration 248, loss = 0.38673442\n",
      "Iteration 249, loss = 0.38666366\n",
      "Iteration 250, loss = 0.38666677\n",
      "Iteration 251, loss = 0.38661759\n",
      "Iteration 252, loss = 0.38648551\n",
      "Iteration 253, loss = 0.38660815\n",
      "Iteration 254, loss = 0.38635440\n",
      "Iteration 255, loss = 0.38651238\n",
      "Iteration 256, loss = 0.38633393\n",
      "Iteration 257, loss = 0.38599325\n",
      "Iteration 258, loss = 0.38644505\n",
      "Iteration 259, loss = 0.38616957\n",
      "Iteration 260, loss = 0.38643809\n",
      "Iteration 261, loss = 0.38590167\n",
      "Iteration 262, loss = 0.38583767\n",
      "Iteration 263, loss = 0.38592042\n",
      "Iteration 264, loss = 0.38590598\n",
      "Iteration 265, loss = 0.38591945\n",
      "Iteration 266, loss = 0.38599209\n",
      "Iteration 267, loss = 0.38571166\n",
      "Iteration 268, loss = 0.38550659\n",
      "Iteration 269, loss = 0.38550854\n",
      "Iteration 270, loss = 0.38556084\n",
      "Iteration 271, loss = 0.38541159\n",
      "Iteration 272, loss = 0.38566533\n",
      "Iteration 273, loss = 0.38536903\n",
      "Iteration 274, loss = 0.38537591\n",
      "Iteration 275, loss = 0.38506562\n",
      "Iteration 276, loss = 0.38508897\n",
      "Iteration 277, loss = 0.38517501\n",
      "Iteration 278, loss = 0.38514665\n",
      "Iteration 279, loss = 0.38519704\n",
      "Iteration 280, loss = 0.38497753\n",
      "Iteration 281, loss = 0.38506729\n",
      "Iteration 282, loss = 0.38491939\n",
      "Iteration 283, loss = 0.38473437\n",
      "Iteration 284, loss = 0.38480452\n",
      "Iteration 285, loss = 0.38491498\n",
      "Iteration 286, loss = 0.38472635\n",
      "Iteration 287, loss = 0.38466551\n",
      "Iteration 288, loss = 0.38473482\n",
      "Iteration 289, loss = 0.38450379\n",
      "Iteration 290, loss = 0.38425164\n",
      "Iteration 291, loss = 0.38460119\n",
      "Iteration 292, loss = 0.38467666\n",
      "Iteration 293, loss = 0.38460347\n",
      "Iteration 294, loss = 0.38413993\n",
      "Iteration 295, loss = 0.38439873\n",
      "Iteration 296, loss = 0.38465259\n",
      "Iteration 297, loss = 0.38407345\n",
      "Iteration 298, loss = 0.38425347\n",
      "Iteration 299, loss = 0.38411729\n",
      "Iteration 300, loss = 0.38401848\n",
      "Iteration 301, loss = 0.38409889\n",
      "Iteration 302, loss = 0.38405663\n",
      "Iteration 303, loss = 0.38394257\n",
      "Iteration 304, loss = 0.38407814\n",
      "Iteration 305, loss = 0.38404970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.85144336\n",
      "Iteration 2, loss = 1.60484076\n",
      "Iteration 3, loss = 1.40808016\n",
      "Iteration 4, loss = 1.25039554\n",
      "Iteration 5, loss = 1.12422401\n",
      "Iteration 6, loss = 1.02119641\n",
      "Iteration 7, loss = 0.93736426\n",
      "Iteration 8, loss = 0.86913112\n",
      "Iteration 9, loss = 0.81302439\n",
      "Iteration 10, loss = 0.76644543\n",
      "Iteration 11, loss = 0.72767946\n",
      "Iteration 12, loss = 0.69464478\n",
      "Iteration 13, loss = 0.66683312\n",
      "Iteration 14, loss = 0.64292613\n",
      "Iteration 15, loss = 0.62226406\n",
      "Iteration 16, loss = 0.60422236\n",
      "Iteration 17, loss = 0.58840260\n",
      "Iteration 18, loss = 0.57402235\n",
      "Iteration 19, loss = 0.56115082\n",
      "Iteration 20, loss = 0.54871389\n",
      "Iteration 21, loss = 0.53813875\n",
      "Iteration 22, loss = 0.52909420\n",
      "Iteration 23, loss = 0.52060152\n",
      "Iteration 24, loss = 0.51332652\n",
      "Iteration 25, loss = 0.50703368\n",
      "Iteration 26, loss = 0.50070966\n",
      "Iteration 27, loss = 0.49517427\n",
      "Iteration 28, loss = 0.49040713\n",
      "Iteration 29, loss = 0.48610162\n",
      "Iteration 30, loss = 0.48200112\n",
      "Iteration 31, loss = 0.47817963\n",
      "Iteration 32, loss = 0.47452295\n",
      "Iteration 33, loss = 0.47177412\n",
      "Iteration 34, loss = 0.46887483\n",
      "Iteration 35, loss = 0.46602182\n",
      "Iteration 36, loss = 0.46334143\n",
      "Iteration 37, loss = 0.46120626\n",
      "Iteration 38, loss = 0.45893417\n",
      "Iteration 39, loss = 0.45677541\n",
      "Iteration 40, loss = 0.45514686\n",
      "Iteration 41, loss = 0.45303275\n",
      "Iteration 42, loss = 0.45120134\n",
      "Iteration 43, loss = 0.44972100\n",
      "Iteration 44, loss = 0.44801211\n",
      "Iteration 45, loss = 0.44645544\n",
      "Iteration 46, loss = 0.44483997\n",
      "Iteration 47, loss = 0.44331796\n",
      "Iteration 48, loss = 0.44200881\n",
      "Iteration 49, loss = 0.44077165\n",
      "Iteration 50, loss = 0.43947623\n",
      "Iteration 51, loss = 0.43813274\n",
      "Iteration 52, loss = 0.43702322\n",
      "Iteration 53, loss = 0.43580334\n",
      "Iteration 54, loss = 0.43504542\n",
      "Iteration 55, loss = 0.43363986\n",
      "Iteration 56, loss = 0.43256918\n",
      "Iteration 57, loss = 0.43148819\n",
      "Iteration 58, loss = 0.43049590\n",
      "Iteration 59, loss = 0.42962189\n",
      "Iteration 60, loss = 0.42886688\n",
      "Iteration 61, loss = 0.42767314\n",
      "Iteration 62, loss = 0.42693833\n",
      "Iteration 63, loss = 0.42598165\n",
      "Iteration 64, loss = 0.42505207\n",
      "Iteration 65, loss = 0.42407195\n",
      "Iteration 66, loss = 0.42314910\n",
      "Iteration 67, loss = 0.42227331\n",
      "Iteration 68, loss = 0.42179512\n",
      "Iteration 69, loss = 0.42113734\n",
      "Iteration 70, loss = 0.42009146\n",
      "Iteration 71, loss = 0.41952415\n",
      "Iteration 72, loss = 0.41876395\n",
      "Iteration 73, loss = 0.41820874\n",
      "Iteration 74, loss = 0.41730381\n",
      "Iteration 75, loss = 0.41661386\n",
      "Iteration 76, loss = 0.41576950\n",
      "Iteration 77, loss = 0.41540728\n",
      "Iteration 78, loss = 0.41456637\n",
      "Iteration 79, loss = 0.41422268\n",
      "Iteration 80, loss = 0.41361461\n",
      "Iteration 81, loss = 0.41307541\n",
      "Iteration 82, loss = 0.41234521\n",
      "Iteration 83, loss = 0.41221793\n",
      "Iteration 84, loss = 0.41138958\n",
      "Iteration 85, loss = 0.41087484\n",
      "Iteration 86, loss = 0.41033170\n",
      "Iteration 87, loss = 0.41008425\n",
      "Iteration 88, loss = 0.40945926\n",
      "Iteration 89, loss = 0.40908610\n",
      "Iteration 90, loss = 0.40847650\n",
      "Iteration 91, loss = 0.40807074\n",
      "Iteration 92, loss = 0.40761269\n",
      "Iteration 93, loss = 0.40717155\n",
      "Iteration 94, loss = 0.40712561\n",
      "Iteration 95, loss = 0.40651739\n",
      "Iteration 96, loss = 0.40620639\n",
      "Iteration 97, loss = 0.40599401\n",
      "Iteration 98, loss = 0.40502870\n",
      "Iteration 99, loss = 0.40506191\n",
      "Iteration 100, loss = 0.40454785\n",
      "Iteration 101, loss = 0.40407245\n",
      "Iteration 102, loss = 0.40380081\n",
      "Iteration 103, loss = 0.40352808\n",
      "Iteration 104, loss = 0.40343535\n",
      "Iteration 105, loss = 0.40272271\n",
      "Iteration 106, loss = 0.40266803\n",
      "Iteration 107, loss = 0.40244290\n",
      "Iteration 108, loss = 0.40217279\n",
      "Iteration 109, loss = 0.40187273\n",
      "Iteration 110, loss = 0.40151745\n",
      "Iteration 111, loss = 0.40160896\n",
      "Iteration 112, loss = 0.40101970\n",
      "Iteration 113, loss = 0.40089154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 114, loss = 0.40047509\n",
      "Iteration 115, loss = 0.40018069\n",
      "Iteration 116, loss = 0.39998638\n",
      "Iteration 117, loss = 0.39962965\n",
      "Iteration 118, loss = 0.39965830\n",
      "Iteration 119, loss = 0.39913078\n",
      "Iteration 120, loss = 0.39900467\n",
      "Iteration 121, loss = 0.39868996\n",
      "Iteration 122, loss = 0.39848132\n",
      "Iteration 123, loss = 0.39842170\n",
      "Iteration 124, loss = 0.39812727\n",
      "Iteration 125, loss = 0.39799004\n",
      "Iteration 126, loss = 0.39792331\n",
      "Iteration 127, loss = 0.39749410\n",
      "Iteration 128, loss = 0.39732242\n",
      "Iteration 129, loss = 0.39711194\n",
      "Iteration 130, loss = 0.39682029\n",
      "Iteration 131, loss = 0.39661060\n",
      "Iteration 132, loss = 0.39651604\n",
      "Iteration 133, loss = 0.39639236\n",
      "Iteration 134, loss = 0.39621000\n",
      "Iteration 135, loss = 0.39587333\n",
      "Iteration 136, loss = 0.39590003\n",
      "Iteration 137, loss = 0.39560772\n",
      "Iteration 138, loss = 0.39537586\n",
      "Iteration 139, loss = 0.39522816\n",
      "Iteration 140, loss = 0.39521763\n",
      "Iteration 141, loss = 0.39479913\n",
      "Iteration 142, loss = 0.39487093\n",
      "Iteration 143, loss = 0.39475263\n",
      "Iteration 144, loss = 0.39448811\n",
      "Iteration 145, loss = 0.39428128\n",
      "Iteration 146, loss = 0.39428365\n",
      "Iteration 147, loss = 0.39401610\n",
      "Iteration 148, loss = 0.39390143\n",
      "Iteration 149, loss = 0.39350951\n",
      "Iteration 150, loss = 0.39324366\n",
      "Iteration 151, loss = 0.39313510\n",
      "Iteration 152, loss = 0.39328165\n",
      "Iteration 153, loss = 0.39305598\n",
      "Iteration 154, loss = 0.39292560\n",
      "Iteration 155, loss = 0.39271225\n",
      "Iteration 156, loss = 0.39266807\n",
      "Iteration 157, loss = 0.39256914\n",
      "Iteration 158, loss = 0.39267229\n",
      "Iteration 159, loss = 0.39211520\n",
      "Iteration 160, loss = 0.39202482\n",
      "Iteration 161, loss = 0.39207047\n",
      "Iteration 162, loss = 0.39166823\n",
      "Iteration 163, loss = 0.39218355\n",
      "Iteration 164, loss = 0.39140424\n",
      "Iteration 165, loss = 0.39130765\n",
      "Iteration 166, loss = 0.39134719\n",
      "Iteration 167, loss = 0.39118778\n",
      "Iteration 168, loss = 0.39101154\n",
      "Iteration 169, loss = 0.39087306\n",
      "Iteration 170, loss = 0.39084217\n",
      "Iteration 171, loss = 0.39071552\n",
      "Iteration 172, loss = 0.39047383\n",
      "Iteration 173, loss = 0.39033568\n",
      "Iteration 174, loss = 0.39035101\n",
      "Iteration 175, loss = 0.39018123\n",
      "Iteration 176, loss = 0.39019888\n",
      "Iteration 177, loss = 0.38991615\n",
      "Iteration 178, loss = 0.38994220\n",
      "Iteration 179, loss = 0.38991962\n",
      "Iteration 180, loss = 0.38940856\n",
      "Iteration 181, loss = 0.38963800\n",
      "Iteration 182, loss = 0.38925754\n",
      "Iteration 183, loss = 0.38923814\n",
      "Iteration 184, loss = 0.38901379\n",
      "Iteration 185, loss = 0.38921853\n",
      "Iteration 186, loss = 0.38916944\n",
      "Iteration 187, loss = 0.38885425\n",
      "Iteration 188, loss = 0.38871860\n",
      "Iteration 189, loss = 0.38855055\n",
      "Iteration 190, loss = 0.38848788\n",
      "Iteration 191, loss = 0.38833337\n",
      "Iteration 192, loss = 0.38816660\n",
      "Iteration 193, loss = 0.38818015\n",
      "Iteration 194, loss = 0.38803997\n",
      "Iteration 195, loss = 0.38796173\n",
      "Iteration 196, loss = 0.38810987\n",
      "Iteration 197, loss = 0.38763545\n",
      "Iteration 198, loss = 0.38779177\n",
      "Iteration 199, loss = 0.38763153\n",
      "Iteration 200, loss = 0.38757853\n",
      "Iteration 201, loss = 0.38750425\n",
      "Iteration 202, loss = 0.38748514\n",
      "Iteration 203, loss = 0.38721503\n",
      "Iteration 204, loss = 0.38729789\n",
      "Iteration 205, loss = 0.38679920\n",
      "Iteration 206, loss = 0.38702309\n",
      "Iteration 207, loss = 0.38700436\n",
      "Iteration 208, loss = 0.38708938\n",
      "Iteration 209, loss = 0.38680030\n",
      "Iteration 210, loss = 0.38661776\n",
      "Iteration 211, loss = 0.38673532\n",
      "Iteration 212, loss = 0.38645444\n",
      "Iteration 213, loss = 0.38659210\n",
      "Iteration 214, loss = 0.38635925\n",
      "Iteration 215, loss = 0.38612124\n",
      "Iteration 216, loss = 0.38597753\n",
      "Iteration 217, loss = 0.38625090\n",
      "Iteration 218, loss = 0.38593693\n",
      "Iteration 219, loss = 0.38626884\n",
      "Iteration 220, loss = 0.38582497\n",
      "Iteration 221, loss = 0.38573101\n",
      "Iteration 222, loss = 0.38578307\n",
      "Iteration 223, loss = 0.38576475\n",
      "Iteration 224, loss = 0.38557132\n",
      "Iteration 225, loss = 0.38562547\n",
      "Iteration 226, loss = 0.38535564\n",
      "Iteration 227, loss = 0.38530039\n",
      "Iteration 228, loss = 0.38525820\n",
      "Iteration 229, loss = 0.38530424\n",
      "Iteration 230, loss = 0.38548921\n",
      "Iteration 231, loss = 0.38493190\n",
      "Iteration 232, loss = 0.38537900\n",
      "Iteration 233, loss = 0.38506425\n",
      "Iteration 234, loss = 0.38470496\n",
      "Iteration 235, loss = 0.38501863\n",
      "Iteration 236, loss = 0.38461463\n",
      "Iteration 237, loss = 0.38481066\n",
      "Iteration 238, loss = 0.38476889\n",
      "Iteration 239, loss = 0.38463108\n",
      "Iteration 240, loss = 0.38459713\n",
      "Iteration 241, loss = 0.38437732\n",
      "Iteration 242, loss = 0.38449299\n",
      "Iteration 243, loss = 0.38439783\n",
      "Iteration 244, loss = 0.38415948\n",
      "Iteration 245, loss = 0.38405782\n",
      "Iteration 246, loss = 0.38409790\n",
      "Iteration 247, loss = 0.38389797\n",
      "Iteration 248, loss = 0.38394186\n",
      "Iteration 249, loss = 0.38381132\n",
      "Iteration 250, loss = 0.38388040\n",
      "Iteration 251, loss = 0.38376049\n",
      "Iteration 252, loss = 0.38375680\n",
      "Iteration 253, loss = 0.38384285\n",
      "Iteration 254, loss = 0.38336082\n",
      "Iteration 255, loss = 0.38362839\n",
      "Iteration 256, loss = 0.38356552\n",
      "Iteration 257, loss = 0.38344135\n",
      "Iteration 258, loss = 0.38357584\n",
      "Iteration 259, loss = 0.38326075\n",
      "Iteration 260, loss = 0.38321238\n",
      "Iteration 261, loss = 0.38305180\n",
      "Iteration 262, loss = 0.38305681\n",
      "Iteration 263, loss = 0.38314359\n",
      "Iteration 264, loss = 0.38304274\n",
      "Iteration 265, loss = 0.38279341\n",
      "Iteration 266, loss = 0.38290739\n",
      "Iteration 267, loss = 0.38302538\n",
      "Iteration 268, loss = 0.38277152\n",
      "Iteration 269, loss = 0.38249146\n",
      "Iteration 270, loss = 0.38289188\n",
      "Iteration 271, loss = 0.38277349\n",
      "Iteration 272, loss = 0.38247477\n",
      "Iteration 273, loss = 0.38237789\n",
      "Iteration 274, loss = 0.38274920\n",
      "Iteration 275, loss = 0.38234139\n",
      "Iteration 276, loss = 0.38232820\n",
      "Iteration 277, loss = 0.38232838\n",
      "Iteration 278, loss = 0.38226885\n",
      "Iteration 279, loss = 0.38233218\n",
      "Iteration 280, loss = 0.38230041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93866662\n",
      "Iteration 2, loss = 1.80401092\n",
      "Iteration 3, loss = 1.69591749\n",
      "Iteration 4, loss = 1.60576432\n",
      "Iteration 5, loss = 1.52760447\n",
      "Iteration 6, loss = 1.45857467\n",
      "Iteration 7, loss = 1.39713949\n",
      "Iteration 8, loss = 1.34235630\n",
      "Iteration 9, loss = 1.29328032\n",
      "Iteration 10, loss = 1.24912748\n",
      "Iteration 11, loss = 1.20922191\n",
      "Iteration 12, loss = 1.17291326\n",
      "Iteration 13, loss = 1.13967314\n",
      "Iteration 14, loss = 1.10907988\n",
      "Iteration 15, loss = 1.08077186\n",
      "Iteration 16, loss = 1.05452455\n",
      "Iteration 17, loss = 1.03004001\n",
      "Iteration 18, loss = 1.00716726\n",
      "Iteration 19, loss = 0.98574938\n",
      "Iteration 20, loss = 0.96559265\n",
      "Iteration 21, loss = 0.94663581\n",
      "Iteration 22, loss = 0.92865180\n",
      "Iteration 23, loss = 0.91161849\n",
      "Iteration 24, loss = 0.89542221\n",
      "Iteration 25, loss = 0.88003808\n",
      "Iteration 26, loss = 0.86546567\n",
      "Iteration 27, loss = 0.85175850\n",
      "Iteration 28, loss = 0.83884272\n",
      "Iteration 29, loss = 0.82663343\n",
      "Iteration 30, loss = 0.81512640\n",
      "Iteration 31, loss = 0.80422239\n",
      "Iteration 32, loss = 0.79392977\n",
      "Iteration 33, loss = 0.78417120\n",
      "Iteration 34, loss = 0.77490158\n",
      "Iteration 35, loss = 0.76614571\n",
      "Iteration 36, loss = 0.75774039\n",
      "Iteration 37, loss = 0.74977400\n",
      "Iteration 38, loss = 0.74215873\n",
      "Iteration 39, loss = 0.73486840\n",
      "Iteration 40, loss = 0.72789907\n",
      "Iteration 41, loss = 0.72121884\n",
      "Iteration 42, loss = 0.71481321\n",
      "Iteration 43, loss = 0.70865845\n",
      "Iteration 44, loss = 0.70275000\n",
      "Iteration 45, loss = 0.69703939\n",
      "Iteration 46, loss = 0.69160148\n",
      "Iteration 47, loss = 0.68633001\n",
      "Iteration 48, loss = 0.68124508\n",
      "Iteration 49, loss = 0.67633513\n",
      "Iteration 50, loss = 0.67158589\n",
      "Iteration 51, loss = 0.66699269\n",
      "Iteration 52, loss = 0.66254813\n",
      "Iteration 53, loss = 0.65823025\n",
      "Iteration 54, loss = 0.65404453\n",
      "Iteration 55, loss = 0.64996977\n",
      "Iteration 56, loss = 0.64597215\n",
      "Iteration 57, loss = 0.64210210\n",
      "Iteration 58, loss = 0.63834402\n",
      "Iteration 59, loss = 0.63467462\n",
      "Iteration 60, loss = 0.63110335\n",
      "Iteration 61, loss = 0.62761256\n",
      "Iteration 62, loss = 0.62415776\n",
      "Iteration 63, loss = 0.62082827\n",
      "Iteration 64, loss = 0.61754994\n",
      "Iteration 65, loss = 0.61432652\n",
      "Iteration 66, loss = 0.61119414\n",
      "Iteration 67, loss = 0.60812296\n",
      "Iteration 68, loss = 0.60510794\n",
      "Iteration 69, loss = 0.60211190\n",
      "Iteration 70, loss = 0.59917904\n",
      "Iteration 71, loss = 0.59629820\n",
      "Iteration 72, loss = 0.59349134\n",
      "Iteration 73, loss = 0.59068982\n",
      "Iteration 74, loss = 0.58796186\n",
      "Iteration 75, loss = 0.58527080\n",
      "Iteration 76, loss = 0.58265600\n",
      "Iteration 77, loss = 0.58004331\n",
      "Iteration 78, loss = 0.57749231\n",
      "Iteration 79, loss = 0.57497345\n",
      "Iteration 80, loss = 0.57249851\n",
      "Iteration 81, loss = 0.57002870\n",
      "Iteration 82, loss = 0.56767664\n",
      "Iteration 83, loss = 0.56527478\n",
      "Iteration 84, loss = 0.56292667\n",
      "Iteration 85, loss = 0.56062915\n",
      "Iteration 86, loss = 0.55839000\n",
      "Iteration 87, loss = 0.55617884\n",
      "Iteration 88, loss = 0.55391421\n",
      "Iteration 89, loss = 0.55176837\n",
      "Iteration 90, loss = 0.54966750\n",
      "Iteration 91, loss = 0.54757813\n",
      "Iteration 92, loss = 0.54552971\n",
      "Iteration 93, loss = 0.54353737\n",
      "Iteration 94, loss = 0.54152967\n",
      "Iteration 95, loss = 0.53959892\n",
      "Iteration 96, loss = 0.53767354\n",
      "Iteration 97, loss = 0.53579096\n",
      "Iteration 98, loss = 0.53394918\n",
      "Iteration 99, loss = 0.53216492\n",
      "Iteration 100, loss = 0.53038785\n",
      "Iteration 101, loss = 0.52861240\n",
      "Iteration 102, loss = 0.52687898\n",
      "Iteration 103, loss = 0.52522497\n",
      "Iteration 104, loss = 0.52357197\n",
      "Iteration 105, loss = 0.52195713\n",
      "Iteration 106, loss = 0.52037166\n",
      "Iteration 107, loss = 0.51884399\n",
      "Iteration 108, loss = 0.51730579\n",
      "Iteration 109, loss = 0.51581516\n",
      "Iteration 110, loss = 0.51437016\n",
      "Iteration 111, loss = 0.51291415\n",
      "Iteration 112, loss = 0.51151152\n",
      "Iteration 113, loss = 0.51014146\n",
      "Iteration 114, loss = 0.50874473\n",
      "Iteration 115, loss = 0.50746098\n",
      "Iteration 116, loss = 0.50614198\n",
      "Iteration 117, loss = 0.50487478\n",
      "Iteration 118, loss = 0.50359864\n",
      "Iteration 119, loss = 0.50239470\n",
      "Iteration 120, loss = 0.50117094\n",
      "Iteration 121, loss = 0.50001135\n",
      "Iteration 122, loss = 0.49887280\n",
      "Iteration 123, loss = 0.49773645\n",
      "Iteration 124, loss = 0.49663153\n",
      "Iteration 125, loss = 0.49560439\n",
      "Iteration 126, loss = 0.49451594\n",
      "Iteration 127, loss = 0.49342570\n",
      "Iteration 128, loss = 0.49240765\n",
      "Iteration 129, loss = 0.49139506\n",
      "Iteration 130, loss = 0.49042407\n",
      "Iteration 131, loss = 0.48945201\n",
      "Iteration 132, loss = 0.48851307\n",
      "Iteration 133, loss = 0.48757236\n",
      "Iteration 134, loss = 0.48664665\n",
      "Iteration 135, loss = 0.48579230\n",
      "Iteration 136, loss = 0.48482400\n",
      "Iteration 137, loss = 0.48401098\n",
      "Iteration 138, loss = 0.48316294\n",
      "Iteration 139, loss = 0.48236479\n",
      "Iteration 140, loss = 0.48148647\n",
      "Iteration 141, loss = 0.48075781\n",
      "Iteration 142, loss = 0.47992885\n",
      "Iteration 143, loss = 0.47914748\n",
      "Iteration 144, loss = 0.47841062\n",
      "Iteration 145, loss = 0.47767102\n",
      "Iteration 146, loss = 0.47687489\n",
      "Iteration 147, loss = 0.47619637\n",
      "Iteration 148, loss = 0.47548191\n",
      "Iteration 149, loss = 0.47485395\n",
      "Iteration 150, loss = 0.47413756\n",
      "Iteration 151, loss = 0.47344391\n",
      "Iteration 152, loss = 0.47279976\n",
      "Iteration 153, loss = 0.47213527\n",
      "Iteration 154, loss = 0.47158018\n",
      "Iteration 155, loss = 0.47086916\n",
      "Iteration 156, loss = 0.47030055\n",
      "Iteration 157, loss = 0.46968844\n",
      "Iteration 158, loss = 0.46913632\n",
      "Iteration 159, loss = 0.46856132\n",
      "Iteration 160, loss = 0.46793738\n",
      "Iteration 161, loss = 0.46736166\n",
      "Iteration 162, loss = 0.46681070\n",
      "Iteration 163, loss = 0.46632741\n",
      "Iteration 164, loss = 0.46568168\n",
      "Iteration 165, loss = 0.46521497\n",
      "Iteration 166, loss = 0.46465449\n",
      "Iteration 167, loss = 0.46417171\n",
      "Iteration 168, loss = 0.46355507\n",
      "Iteration 169, loss = 0.46317671\n",
      "Iteration 170, loss = 0.46266534\n",
      "Iteration 171, loss = 0.46216765\n",
      "Iteration 172, loss = 0.46165602\n",
      "Iteration 173, loss = 0.46121801\n",
      "Iteration 174, loss = 0.46080157\n",
      "Iteration 175, loss = 0.46025275\n",
      "Iteration 176, loss = 0.45986603\n",
      "Iteration 177, loss = 0.45937600\n",
      "Iteration 178, loss = 0.45897001\n",
      "Iteration 179, loss = 0.45849715\n",
      "Iteration 180, loss = 0.45807513\n",
      "Iteration 181, loss = 0.45764467\n",
      "Iteration 182, loss = 0.45724164\n",
      "Iteration 183, loss = 0.45680989\n",
      "Iteration 184, loss = 0.45644521\n",
      "Iteration 185, loss = 0.45599676\n",
      "Iteration 186, loss = 0.45561773\n",
      "Iteration 187, loss = 0.45521926\n",
      "Iteration 188, loss = 0.45489918\n",
      "Iteration 189, loss = 0.45446184\n",
      "Iteration 190, loss = 0.45406991\n",
      "Iteration 191, loss = 0.45374585\n",
      "Iteration 192, loss = 0.45334696\n",
      "Iteration 193, loss = 0.45290874\n",
      "Iteration 194, loss = 0.45262099\n",
      "Iteration 195, loss = 0.45228138\n",
      "Iteration 196, loss = 0.45187770\n",
      "Iteration 197, loss = 0.45156356\n",
      "Iteration 198, loss = 0.45116294\n",
      "Iteration 199, loss = 0.45084121\n",
      "Iteration 200, loss = 0.45050947\n",
      "Iteration 201, loss = 0.45019480\n",
      "Iteration 202, loss = 0.44989189\n",
      "Iteration 203, loss = 0.44948400\n",
      "Iteration 204, loss = 0.44922452\n",
      "Iteration 205, loss = 0.44886898\n",
      "Iteration 206, loss = 0.44860894\n",
      "Iteration 207, loss = 0.44823886\n",
      "Iteration 208, loss = 0.44793050\n",
      "Iteration 209, loss = 0.44768179\n",
      "Iteration 210, loss = 0.44736111\n",
      "Iteration 211, loss = 0.44703826\n",
      "Iteration 212, loss = 0.44675472\n",
      "Iteration 213, loss = 0.44640948\n",
      "Iteration 214, loss = 0.44617148\n",
      "Iteration 215, loss = 0.44588109\n",
      "Iteration 216, loss = 0.44567526\n",
      "Iteration 217, loss = 0.44536594\n",
      "Iteration 218, loss = 0.44505536\n",
      "Iteration 219, loss = 0.44473972\n",
      "Iteration 220, loss = 0.44448655\n",
      "Iteration 221, loss = 0.44421060\n",
      "Iteration 222, loss = 0.44387406\n",
      "Iteration 223, loss = 0.44370815\n",
      "Iteration 224, loss = 0.44347313\n",
      "Iteration 225, loss = 0.44318884\n",
      "Iteration 226, loss = 0.44292108\n",
      "Iteration 227, loss = 0.44268929\n",
      "Iteration 228, loss = 0.44239853\n",
      "Iteration 229, loss = 0.44215214\n",
      "Iteration 230, loss = 0.44188094\n",
      "Iteration 231, loss = 0.44165545\n",
      "Iteration 232, loss = 0.44145511\n",
      "Iteration 233, loss = 0.44117022\n",
      "Iteration 234, loss = 0.44090912\n",
      "Iteration 235, loss = 0.44070713\n",
      "Iteration 236, loss = 0.44049863\n",
      "Iteration 237, loss = 0.44025002\n",
      "Iteration 238, loss = 0.43998378\n",
      "Iteration 239, loss = 0.43982236\n",
      "Iteration 240, loss = 0.43961729\n",
      "Iteration 241, loss = 0.43934560\n",
      "Iteration 242, loss = 0.43919876\n",
      "Iteration 243, loss = 0.43889907\n",
      "Iteration 244, loss = 0.43879846\n",
      "Iteration 245, loss = 0.43851940\n",
      "Iteration 246, loss = 0.43826082\n",
      "Iteration 247, loss = 0.43800756\n",
      "Iteration 248, loss = 0.43788433\n",
      "Iteration 249, loss = 0.43769689\n",
      "Iteration 250, loss = 0.43749301\n",
      "Iteration 251, loss = 0.43723688\n",
      "Iteration 252, loss = 0.43704004\n",
      "Iteration 253, loss = 0.43681270\n",
      "Iteration 254, loss = 0.43662074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.43641375\n",
      "Iteration 256, loss = 0.43625611\n",
      "Iteration 257, loss = 0.43604295\n",
      "Iteration 258, loss = 0.43591637\n",
      "Iteration 259, loss = 0.43573411\n",
      "Iteration 260, loss = 0.43544341\n",
      "Iteration 261, loss = 0.43537649\n",
      "Iteration 262, loss = 0.43512456\n",
      "Iteration 263, loss = 0.43495325\n",
      "Iteration 264, loss = 0.43478970\n",
      "Iteration 265, loss = 0.43455071\n",
      "Iteration 266, loss = 0.43434184\n",
      "Iteration 267, loss = 0.43417229\n",
      "Iteration 268, loss = 0.43404365\n",
      "Iteration 269, loss = 0.43387917\n",
      "Iteration 270, loss = 0.43364857\n",
      "Iteration 271, loss = 0.43355927\n",
      "Iteration 272, loss = 0.43336183\n",
      "Iteration 273, loss = 0.43317343\n",
      "Iteration 274, loss = 0.43299132\n",
      "Iteration 275, loss = 0.43287576\n",
      "Iteration 276, loss = 0.43266779\n",
      "Iteration 277, loss = 0.43253928\n",
      "Iteration 278, loss = 0.43236086\n",
      "Iteration 279, loss = 0.43214999\n",
      "Iteration 280, loss = 0.43198964\n",
      "Iteration 281, loss = 0.43183725\n",
      "Iteration 282, loss = 0.43169737\n",
      "Iteration 283, loss = 0.43146562\n",
      "Iteration 284, loss = 0.43134849\n",
      "Iteration 285, loss = 0.43119093\n",
      "Iteration 286, loss = 0.43110848\n",
      "Iteration 287, loss = 0.43085684\n",
      "Iteration 288, loss = 0.43076782\n",
      "Iteration 289, loss = 0.43054133\n",
      "Iteration 290, loss = 0.43046391\n",
      "Iteration 291, loss = 0.43021481\n",
      "Iteration 292, loss = 0.43011165\n",
      "Iteration 293, loss = 0.43004035\n",
      "Iteration 294, loss = 0.42981664\n",
      "Iteration 295, loss = 0.42962919\n",
      "Iteration 296, loss = 0.42958302\n",
      "Iteration 297, loss = 0.42934356\n",
      "Iteration 298, loss = 0.42926991\n",
      "Iteration 299, loss = 0.42906079\n",
      "Iteration 300, loss = 0.42898578\n",
      "Iteration 301, loss = 0.42882548\n",
      "Iteration 302, loss = 0.42869250\n",
      "Iteration 303, loss = 0.42849457\n",
      "Iteration 304, loss = 0.42838759\n",
      "Iteration 305, loss = 0.42820588\n",
      "Iteration 306, loss = 0.42815786\n",
      "Iteration 307, loss = 0.42803819\n",
      "Iteration 308, loss = 0.42775512\n",
      "Iteration 309, loss = 0.42774956\n",
      "Iteration 310, loss = 0.42762511\n",
      "Iteration 311, loss = 0.42745976\n",
      "Iteration 312, loss = 0.42724662\n",
      "Iteration 313, loss = 0.42718773\n",
      "Iteration 314, loss = 0.42700888\n",
      "Iteration 315, loss = 0.42690987\n",
      "Iteration 316, loss = 0.42682436\n",
      "Iteration 317, loss = 0.42665756\n",
      "Iteration 318, loss = 0.42645990\n",
      "Iteration 319, loss = 0.42636787\n",
      "Iteration 320, loss = 0.42623206\n",
      "Iteration 321, loss = 0.42618767\n",
      "Iteration 322, loss = 0.42594223\n",
      "Iteration 323, loss = 0.42590334\n",
      "Iteration 324, loss = 0.42573180\n",
      "Iteration 325, loss = 0.42562380\n",
      "Iteration 326, loss = 0.42556071\n",
      "Iteration 327, loss = 0.42539197\n",
      "Iteration 328, loss = 0.42532837\n",
      "Iteration 329, loss = 0.42516196\n",
      "Iteration 330, loss = 0.42503947\n",
      "Iteration 331, loss = 0.42489722\n",
      "Iteration 332, loss = 0.42480904\n",
      "Iteration 333, loss = 0.42471365\n",
      "Iteration 334, loss = 0.42454999\n",
      "Iteration 335, loss = 0.42442719\n",
      "Iteration 336, loss = 0.42432230\n",
      "Iteration 337, loss = 0.42424471\n",
      "Iteration 338, loss = 0.42413459\n",
      "Iteration 339, loss = 0.42396653\n",
      "Iteration 340, loss = 0.42389998\n",
      "Iteration 341, loss = 0.42377562\n",
      "Iteration 342, loss = 0.42360534\n",
      "Iteration 343, loss = 0.42354066\n",
      "Iteration 344, loss = 0.42339527\n",
      "Iteration 345, loss = 0.42326232\n",
      "Iteration 346, loss = 0.42316612\n",
      "Iteration 347, loss = 0.42310133\n",
      "Iteration 348, loss = 0.42296100\n",
      "Iteration 349, loss = 0.42290741\n",
      "Iteration 350, loss = 0.42270906\n",
      "Iteration 351, loss = 0.42263831\n",
      "Iteration 352, loss = 0.42259113\n",
      "Iteration 353, loss = 0.42236821\n",
      "Iteration 354, loss = 0.42229191\n",
      "Iteration 355, loss = 0.42225245\n",
      "Iteration 356, loss = 0.42206701\n",
      "Iteration 357, loss = 0.42193986\n",
      "Iteration 358, loss = 0.42179357\n",
      "Iteration 359, loss = 0.42180842\n",
      "Iteration 360, loss = 0.42163079\n",
      "Iteration 361, loss = 0.42153014\n",
      "Iteration 362, loss = 0.42150689\n",
      "Iteration 363, loss = 0.42130578\n",
      "Iteration 364, loss = 0.42122020\n",
      "Iteration 365, loss = 0.42115657\n",
      "Iteration 366, loss = 0.42108923\n",
      "Iteration 367, loss = 0.42096022\n",
      "Iteration 368, loss = 0.42092862\n",
      "Iteration 369, loss = 0.42076918\n",
      "Iteration 370, loss = 0.42064721\n",
      "Iteration 371, loss = 0.42053038\n",
      "Iteration 372, loss = 0.42052214\n",
      "Iteration 373, loss = 0.42032107\n",
      "Iteration 374, loss = 0.42024741\n",
      "Iteration 375, loss = 0.42024506\n",
      "Iteration 376, loss = 0.41998697\n",
      "Iteration 377, loss = 0.41991071\n",
      "Iteration 378, loss = 0.41986439\n",
      "Iteration 379, loss = 0.41977914\n",
      "Iteration 380, loss = 0.41968508\n",
      "Iteration 381, loss = 0.41959274\n",
      "Iteration 382, loss = 0.41945017\n",
      "Iteration 383, loss = 0.41938525\n",
      "Iteration 384, loss = 0.41935330\n",
      "Iteration 385, loss = 0.41923503\n",
      "Iteration 386, loss = 0.41909038\n",
      "Iteration 387, loss = 0.41902479\n",
      "Iteration 388, loss = 0.41892547\n",
      "Iteration 389, loss = 0.41891727\n",
      "Iteration 390, loss = 0.41883003\n",
      "Iteration 391, loss = 0.41864517\n",
      "Iteration 392, loss = 0.41859657\n",
      "Iteration 393, loss = 0.41849306\n",
      "Iteration 394, loss = 0.41840457\n",
      "Iteration 395, loss = 0.41830956\n",
      "Iteration 396, loss = 0.41817223\n",
      "Iteration 397, loss = 0.41811328\n",
      "Iteration 398, loss = 0.41806186\n",
      "Iteration 399, loss = 0.41804194\n",
      "Iteration 400, loss = 0.41788351\n",
      "Iteration 401, loss = 0.41778810\n",
      "Iteration 402, loss = 0.41770162\n",
      "Iteration 403, loss = 0.41758799\n",
      "Iteration 404, loss = 0.41751428\n",
      "Iteration 405, loss = 0.41738010\n",
      "Iteration 406, loss = 0.41730600\n",
      "Iteration 407, loss = 0.41722189\n",
      "Iteration 408, loss = 0.41717488\n",
      "Iteration 409, loss = 0.41700857\n",
      "Iteration 410, loss = 0.41706509\n",
      "Iteration 411, loss = 0.41692618\n",
      "Iteration 412, loss = 0.41682405\n",
      "Iteration 413, loss = 0.41676211\n",
      "Iteration 414, loss = 0.41663802\n",
      "Iteration 415, loss = 0.41654864\n",
      "Iteration 416, loss = 0.41651426\n",
      "Iteration 417, loss = 0.41642398\n",
      "Iteration 418, loss = 0.41633208\n",
      "Iteration 419, loss = 0.41625353\n",
      "Iteration 420, loss = 0.41612101\n",
      "Iteration 421, loss = 0.41614239\n",
      "Iteration 422, loss = 0.41605410\n",
      "Iteration 423, loss = 0.41601214\n",
      "Iteration 424, loss = 0.41581698\n",
      "Iteration 425, loss = 0.41575324\n",
      "Iteration 426, loss = 0.41568571\n",
      "Iteration 427, loss = 0.41556794\n",
      "Iteration 428, loss = 0.41553627\n",
      "Iteration 429, loss = 0.41550096\n",
      "Iteration 430, loss = 0.41541543\n",
      "Iteration 431, loss = 0.41529655\n",
      "Iteration 432, loss = 0.41521256\n",
      "Iteration 433, loss = 0.41514643\n",
      "Iteration 434, loss = 0.41508051\n",
      "Iteration 435, loss = 0.41500623\n",
      "Iteration 436, loss = 0.41496234\n",
      "Iteration 437, loss = 0.41487786\n",
      "Iteration 438, loss = 0.41472073\n",
      "Iteration 439, loss = 0.41471334\n",
      "Iteration 440, loss = 0.41463040\n",
      "Iteration 441, loss = 0.41448416\n",
      "Iteration 442, loss = 0.41443601\n",
      "Iteration 443, loss = 0.41431281\n",
      "Iteration 444, loss = 0.41424485\n",
      "Iteration 445, loss = 0.41413479\n",
      "Iteration 446, loss = 0.41414057\n",
      "Iteration 447, loss = 0.41405080\n",
      "Iteration 448, loss = 0.41396960\n",
      "Iteration 449, loss = 0.41398143\n",
      "Iteration 450, loss = 0.41384021\n",
      "Iteration 451, loss = 0.41378562\n",
      "Iteration 452, loss = 0.41365671\n",
      "Iteration 453, loss = 0.41365976\n",
      "Iteration 454, loss = 0.41351073\n",
      "Iteration 455, loss = 0.41349532\n",
      "Iteration 456, loss = 0.41341844\n",
      "Iteration 457, loss = 0.41333684\n",
      "Iteration 458, loss = 0.41322631\n",
      "Iteration 459, loss = 0.41321835\n",
      "Iteration 460, loss = 0.41314481\n",
      "Iteration 461, loss = 0.41309567\n",
      "Iteration 462, loss = 0.41300074\n",
      "Iteration 463, loss = 0.41296347\n",
      "Iteration 464, loss = 0.41284039\n",
      "Iteration 465, loss = 0.41279473\n",
      "Iteration 466, loss = 0.41271416\n",
      "Iteration 467, loss = 0.41272182\n",
      "Iteration 468, loss = 0.41263540\n",
      "Iteration 469, loss = 0.41253848\n",
      "Iteration 470, loss = 0.41254744\n",
      "Iteration 471, loss = 0.41235215\n",
      "Iteration 472, loss = 0.41231712\n",
      "Iteration 473, loss = 0.41235311\n",
      "Iteration 474, loss = 0.41216256\n",
      "Iteration 475, loss = 0.41210707\n",
      "Iteration 476, loss = 0.41199908\n",
      "Iteration 477, loss = 0.41201723\n",
      "Iteration 478, loss = 0.41195752\n",
      "Iteration 479, loss = 0.41182661\n",
      "Iteration 480, loss = 0.41179407\n",
      "Iteration 481, loss = 0.41169223\n",
      "Iteration 482, loss = 0.41166191\n",
      "Iteration 483, loss = 0.41167050\n",
      "Iteration 484, loss = 0.41155702\n",
      "Iteration 485, loss = 0.41153335\n",
      "Iteration 486, loss = 0.41141819\n",
      "Iteration 487, loss = 0.41135979\n",
      "Iteration 488, loss = 0.41128745\n",
      "Iteration 489, loss = 0.41121387\n",
      "Iteration 490, loss = 0.41117794\n",
      "Iteration 491, loss = 0.41120428\n",
      "Iteration 492, loss = 0.41107872\n",
      "Iteration 493, loss = 0.41098365\n",
      "Iteration 494, loss = 0.41089659\n",
      "Iteration 495, loss = 0.41080365\n",
      "Iteration 496, loss = 0.41080246\n",
      "Iteration 497, loss = 0.41068331\n",
      "Iteration 498, loss = 0.41062712\n",
      "Iteration 499, loss = 0.41058692\n",
      "Iteration 500, loss = 0.41057529\n",
      "Iteration 1, loss = 1.93751249\n",
      "Iteration 2, loss = 1.80409357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.69657830\n",
      "Iteration 4, loss = 1.60679685\n",
      "Iteration 5, loss = 1.52882905\n",
      "Iteration 6, loss = 1.45986405\n",
      "Iteration 7, loss = 1.39840552\n",
      "Iteration 8, loss = 1.34362557\n",
      "Iteration 9, loss = 1.29460708\n",
      "Iteration 10, loss = 1.25060251\n",
      "Iteration 11, loss = 1.21075294\n",
      "Iteration 12, loss = 1.17449941\n",
      "Iteration 13, loss = 1.14124832\n",
      "Iteration 14, loss = 1.11062152\n",
      "Iteration 15, loss = 1.08221949\n",
      "Iteration 16, loss = 1.05582683\n",
      "Iteration 17, loss = 1.03125305\n",
      "Iteration 18, loss = 1.00826061\n",
      "Iteration 19, loss = 0.98674630\n",
      "Iteration 20, loss = 0.96650508\n",
      "Iteration 21, loss = 0.94743320\n",
      "Iteration 22, loss = 0.92946785\n",
      "Iteration 23, loss = 0.91243046\n",
      "Iteration 24, loss = 0.89630543\n",
      "Iteration 25, loss = 0.88093822\n",
      "Iteration 26, loss = 0.86637293\n",
      "Iteration 27, loss = 0.85254891\n",
      "Iteration 28, loss = 0.83949000\n",
      "Iteration 29, loss = 0.82720384\n",
      "Iteration 30, loss = 0.81563831\n",
      "Iteration 31, loss = 0.80471397\n",
      "Iteration 32, loss = 0.79438683\n",
      "Iteration 33, loss = 0.78466732\n",
      "Iteration 34, loss = 0.77541232\n",
      "Iteration 35, loss = 0.76664228\n",
      "Iteration 36, loss = 0.75830934\n",
      "Iteration 37, loss = 0.75033452\n",
      "Iteration 38, loss = 0.74276467\n",
      "Iteration 39, loss = 0.73550074\n",
      "Iteration 40, loss = 0.72854682\n",
      "Iteration 41, loss = 0.72190616\n",
      "Iteration 42, loss = 0.71548959\n",
      "Iteration 43, loss = 0.70938942\n",
      "Iteration 44, loss = 0.70347731\n",
      "Iteration 45, loss = 0.69781654\n",
      "Iteration 46, loss = 0.69234542\n",
      "Iteration 47, loss = 0.68711568\n",
      "Iteration 48, loss = 0.68201695\n",
      "Iteration 49, loss = 0.67713385\n",
      "Iteration 50, loss = 0.67237785\n",
      "Iteration 51, loss = 0.66779571\n",
      "Iteration 52, loss = 0.66336016\n",
      "Iteration 53, loss = 0.65904848\n",
      "Iteration 54, loss = 0.65487979\n",
      "Iteration 55, loss = 0.65079327\n",
      "Iteration 56, loss = 0.64686840\n",
      "Iteration 57, loss = 0.64300499\n",
      "Iteration 58, loss = 0.63925442\n",
      "Iteration 59, loss = 0.63557129\n",
      "Iteration 60, loss = 0.63198816\n",
      "Iteration 61, loss = 0.62849159\n",
      "Iteration 62, loss = 0.62508055\n",
      "Iteration 63, loss = 0.62173021\n",
      "Iteration 64, loss = 0.61844021\n",
      "Iteration 65, loss = 0.61520748\n",
      "Iteration 66, loss = 0.61208302\n",
      "Iteration 67, loss = 0.60896038\n",
      "Iteration 68, loss = 0.60592336\n",
      "Iteration 69, loss = 0.60292971\n",
      "Iteration 70, loss = 0.59996943\n",
      "Iteration 71, loss = 0.59708185\n",
      "Iteration 72, loss = 0.59426815\n",
      "Iteration 73, loss = 0.59143562\n",
      "Iteration 74, loss = 0.58868828\n",
      "Iteration 75, loss = 0.58597686\n",
      "Iteration 76, loss = 0.58328645\n",
      "Iteration 77, loss = 0.58065858\n",
      "Iteration 78, loss = 0.57805202\n",
      "Iteration 79, loss = 0.57551568\n",
      "Iteration 80, loss = 0.57303104\n",
      "Iteration 81, loss = 0.57050751\n",
      "Iteration 82, loss = 0.56812298\n",
      "Iteration 83, loss = 0.56567526\n",
      "Iteration 84, loss = 0.56332955\n",
      "Iteration 85, loss = 0.56099398\n",
      "Iteration 86, loss = 0.55870621\n",
      "Iteration 87, loss = 0.55646119\n",
      "Iteration 88, loss = 0.55423220\n",
      "Iteration 89, loss = 0.55208965\n",
      "Iteration 90, loss = 0.54993209\n",
      "Iteration 91, loss = 0.54782356\n",
      "Iteration 92, loss = 0.54576049\n",
      "Iteration 93, loss = 0.54370940\n",
      "Iteration 94, loss = 0.54172435\n",
      "Iteration 95, loss = 0.53978010\n",
      "Iteration 96, loss = 0.53785084\n",
      "Iteration 97, loss = 0.53595277\n",
      "Iteration 98, loss = 0.53411762\n",
      "Iteration 99, loss = 0.53231873\n",
      "Iteration 100, loss = 0.53050815\n",
      "Iteration 101, loss = 0.52876150\n",
      "Iteration 102, loss = 0.52701457\n",
      "Iteration 103, loss = 0.52534172\n",
      "Iteration 104, loss = 0.52370118\n",
      "Iteration 105, loss = 0.52206382\n",
      "Iteration 106, loss = 0.52049332\n",
      "Iteration 107, loss = 0.51894191\n",
      "Iteration 108, loss = 0.51740877\n",
      "Iteration 109, loss = 0.51591614\n",
      "Iteration 110, loss = 0.51441906\n",
      "Iteration 111, loss = 0.51302630\n",
      "Iteration 112, loss = 0.51162972\n",
      "Iteration 113, loss = 0.51024793\n",
      "Iteration 114, loss = 0.50888309\n",
      "Iteration 115, loss = 0.50756885\n",
      "Iteration 116, loss = 0.50626367\n",
      "Iteration 117, loss = 0.50499666\n",
      "Iteration 118, loss = 0.50377504\n",
      "Iteration 119, loss = 0.50251859\n",
      "Iteration 120, loss = 0.50131759\n",
      "Iteration 121, loss = 0.50013478\n",
      "Iteration 122, loss = 0.49904277\n",
      "Iteration 123, loss = 0.49787842\n",
      "Iteration 124, loss = 0.49680699\n",
      "Iteration 125, loss = 0.49573227\n",
      "Iteration 126, loss = 0.49465078\n",
      "Iteration 127, loss = 0.49357803\n",
      "Iteration 128, loss = 0.49259238\n",
      "Iteration 129, loss = 0.49159290\n",
      "Iteration 130, loss = 0.49060572\n",
      "Iteration 131, loss = 0.48967938\n",
      "Iteration 132, loss = 0.48869654\n",
      "Iteration 133, loss = 0.48773863\n",
      "Iteration 134, loss = 0.48688958\n",
      "Iteration 135, loss = 0.48599776\n",
      "Iteration 136, loss = 0.48507277\n",
      "Iteration 137, loss = 0.48425072\n",
      "Iteration 138, loss = 0.48343526\n",
      "Iteration 139, loss = 0.48259154\n",
      "Iteration 140, loss = 0.48185108\n",
      "Iteration 141, loss = 0.48095997\n",
      "Iteration 142, loss = 0.48018067\n",
      "Iteration 143, loss = 0.47941609\n",
      "Iteration 144, loss = 0.47865190\n",
      "Iteration 145, loss = 0.47791050\n",
      "Iteration 146, loss = 0.47725884\n",
      "Iteration 147, loss = 0.47652902\n",
      "Iteration 148, loss = 0.47580185\n",
      "Iteration 149, loss = 0.47503712\n",
      "Iteration 150, loss = 0.47440482\n",
      "Iteration 151, loss = 0.47371754\n",
      "Iteration 152, loss = 0.47305472\n",
      "Iteration 153, loss = 0.47242524\n",
      "Iteration 154, loss = 0.47180554\n",
      "Iteration 155, loss = 0.47119186\n",
      "Iteration 156, loss = 0.47051764\n",
      "Iteration 157, loss = 0.46990814\n",
      "Iteration 158, loss = 0.46936867\n",
      "Iteration 159, loss = 0.46871294\n",
      "Iteration 160, loss = 0.46824296\n",
      "Iteration 161, loss = 0.46763931\n",
      "Iteration 162, loss = 0.46703170\n",
      "Iteration 163, loss = 0.46651004\n",
      "Iteration 164, loss = 0.46591857\n",
      "Iteration 165, loss = 0.46542284\n",
      "Iteration 166, loss = 0.46492199\n",
      "Iteration 167, loss = 0.46431086\n",
      "Iteration 168, loss = 0.46383625\n",
      "Iteration 169, loss = 0.46337621\n",
      "Iteration 170, loss = 0.46284367\n",
      "Iteration 171, loss = 0.46243628\n",
      "Iteration 172, loss = 0.46187720\n",
      "Iteration 173, loss = 0.46137489\n",
      "Iteration 174, loss = 0.46094586\n",
      "Iteration 175, loss = 0.46044173\n",
      "Iteration 176, loss = 0.45995769\n",
      "Iteration 177, loss = 0.45961716\n",
      "Iteration 178, loss = 0.45908080\n",
      "Iteration 179, loss = 0.45863424\n",
      "Iteration 180, loss = 0.45815493\n",
      "Iteration 181, loss = 0.45781576\n",
      "Iteration 182, loss = 0.45735975\n",
      "Iteration 183, loss = 0.45693821\n",
      "Iteration 184, loss = 0.45652938\n",
      "Iteration 185, loss = 0.45617736\n",
      "Iteration 186, loss = 0.45569367\n",
      "Iteration 187, loss = 0.45533615\n",
      "Iteration 188, loss = 0.45493346\n",
      "Iteration 189, loss = 0.45455276\n",
      "Iteration 190, loss = 0.45417128\n",
      "Iteration 191, loss = 0.45375723\n",
      "Iteration 192, loss = 0.45344025\n",
      "Iteration 193, loss = 0.45307769\n",
      "Iteration 194, loss = 0.45270299\n",
      "Iteration 195, loss = 0.45235956\n",
      "Iteration 196, loss = 0.45196677\n",
      "Iteration 197, loss = 0.45162480\n",
      "Iteration 198, loss = 0.45123997\n",
      "Iteration 199, loss = 0.45092888\n",
      "Iteration 200, loss = 0.45057542\n",
      "Iteration 201, loss = 0.45022598\n",
      "Iteration 202, loss = 0.44992720\n",
      "Iteration 203, loss = 0.44958862\n",
      "Iteration 204, loss = 0.44927178\n",
      "Iteration 205, loss = 0.44901632\n",
      "Iteration 206, loss = 0.44867744\n",
      "Iteration 207, loss = 0.44836908\n",
      "Iteration 208, loss = 0.44799361\n",
      "Iteration 209, loss = 0.44774225\n",
      "Iteration 210, loss = 0.44734997\n",
      "Iteration 211, loss = 0.44709082\n",
      "Iteration 212, loss = 0.44677102\n",
      "Iteration 213, loss = 0.44656744\n",
      "Iteration 214, loss = 0.44620647\n",
      "Iteration 215, loss = 0.44596641\n",
      "Iteration 216, loss = 0.44567628\n",
      "Iteration 217, loss = 0.44539807\n",
      "Iteration 218, loss = 0.44509424\n",
      "Iteration 219, loss = 0.44485772\n",
      "Iteration 220, loss = 0.44457691\n",
      "Iteration 221, loss = 0.44429673\n",
      "Iteration 222, loss = 0.44397092\n",
      "Iteration 223, loss = 0.44374266\n",
      "Iteration 224, loss = 0.44348635\n",
      "Iteration 225, loss = 0.44326895\n",
      "Iteration 226, loss = 0.44296242\n",
      "Iteration 227, loss = 0.44277533\n",
      "Iteration 228, loss = 0.44249285\n",
      "Iteration 229, loss = 0.44221597\n",
      "Iteration 230, loss = 0.44190519\n",
      "Iteration 231, loss = 0.44168392\n",
      "Iteration 232, loss = 0.44143240\n",
      "Iteration 233, loss = 0.44131456\n",
      "Iteration 234, loss = 0.44100740\n",
      "Iteration 235, loss = 0.44078621\n",
      "Iteration 236, loss = 0.44054681\n",
      "Iteration 237, loss = 0.44031152\n",
      "Iteration 238, loss = 0.44005710\n",
      "Iteration 239, loss = 0.43983235\n",
      "Iteration 240, loss = 0.43960286\n",
      "Iteration 241, loss = 0.43940268\n",
      "Iteration 242, loss = 0.43911718\n",
      "Iteration 243, loss = 0.43897185\n",
      "Iteration 244, loss = 0.43871507\n",
      "Iteration 245, loss = 0.43851795\n",
      "Iteration 246, loss = 0.43829509\n",
      "Iteration 247, loss = 0.43801579\n",
      "Iteration 248, loss = 0.43786328\n",
      "Iteration 249, loss = 0.43758845\n",
      "Iteration 250, loss = 0.43741134\n",
      "Iteration 251, loss = 0.43723893\n",
      "Iteration 252, loss = 0.43712125\n",
      "Iteration 253, loss = 0.43689097\n",
      "Iteration 254, loss = 0.43671417\n",
      "Iteration 255, loss = 0.43641919\n",
      "Iteration 256, loss = 0.43627828\n",
      "Iteration 257, loss = 0.43601586\n",
      "Iteration 258, loss = 0.43588426\n",
      "Iteration 259, loss = 0.43571666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.43552327\n",
      "Iteration 261, loss = 0.43527633\n",
      "Iteration 262, loss = 0.43504324\n",
      "Iteration 263, loss = 0.43484239\n",
      "Iteration 264, loss = 0.43466054\n",
      "Iteration 265, loss = 0.43448935\n",
      "Iteration 266, loss = 0.43435579\n",
      "Iteration 267, loss = 0.43415424\n",
      "Iteration 268, loss = 0.43396720\n",
      "Iteration 269, loss = 0.43382835\n",
      "Iteration 270, loss = 0.43365969\n",
      "Iteration 271, loss = 0.43350103\n",
      "Iteration 272, loss = 0.43326872\n",
      "Iteration 273, loss = 0.43313097\n",
      "Iteration 274, loss = 0.43296679\n",
      "Iteration 275, loss = 0.43272899\n",
      "Iteration 276, loss = 0.43258142\n",
      "Iteration 277, loss = 0.43243285\n",
      "Iteration 278, loss = 0.43227132\n",
      "Iteration 279, loss = 0.43210657\n",
      "Iteration 280, loss = 0.43191821\n",
      "Iteration 281, loss = 0.43179946\n",
      "Iteration 282, loss = 0.43159261\n",
      "Iteration 283, loss = 0.43144684\n",
      "Iteration 284, loss = 0.43130269\n",
      "Iteration 285, loss = 0.43111868\n",
      "Iteration 286, loss = 0.43094389\n",
      "Iteration 287, loss = 0.43090939\n",
      "Iteration 288, loss = 0.43070450\n",
      "Iteration 289, loss = 0.43049540\n",
      "Iteration 290, loss = 0.43031158\n",
      "Iteration 291, loss = 0.43030669\n",
      "Iteration 292, loss = 0.43008785\n",
      "Iteration 293, loss = 0.42987161\n",
      "Iteration 294, loss = 0.42976978\n",
      "Iteration 295, loss = 0.42961434\n",
      "Iteration 296, loss = 0.42950115\n",
      "Iteration 297, loss = 0.42925818\n",
      "Iteration 298, loss = 0.42914941\n",
      "Iteration 299, loss = 0.42903521\n",
      "Iteration 300, loss = 0.42892027\n",
      "Iteration 301, loss = 0.42865918\n",
      "Iteration 302, loss = 0.42859990\n",
      "Iteration 303, loss = 0.42847521\n",
      "Iteration 304, loss = 0.42828903\n",
      "Iteration 305, loss = 0.42819095\n",
      "Iteration 306, loss = 0.42803178\n",
      "Iteration 307, loss = 0.42793249\n",
      "Iteration 308, loss = 0.42771497\n",
      "Iteration 309, loss = 0.42760520\n",
      "Iteration 310, loss = 0.42748523\n",
      "Iteration 311, loss = 0.42725985\n",
      "Iteration 312, loss = 0.42726284\n",
      "Iteration 313, loss = 0.42703166\n",
      "Iteration 314, loss = 0.42691294\n",
      "Iteration 315, loss = 0.42678820\n",
      "Iteration 316, loss = 0.42663174\n",
      "Iteration 317, loss = 0.42653666\n",
      "Iteration 318, loss = 0.42641819\n",
      "Iteration 319, loss = 0.42632673\n",
      "Iteration 320, loss = 0.42617739\n",
      "Iteration 321, loss = 0.42603228\n",
      "Iteration 322, loss = 0.42593100\n",
      "Iteration 323, loss = 0.42575366\n",
      "Iteration 324, loss = 0.42566614\n",
      "Iteration 325, loss = 0.42551509\n",
      "Iteration 326, loss = 0.42540795\n",
      "Iteration 327, loss = 0.42527060\n",
      "Iteration 328, loss = 0.42511091\n",
      "Iteration 329, loss = 0.42503295\n",
      "Iteration 330, loss = 0.42501910\n",
      "Iteration 331, loss = 0.42486414\n",
      "Iteration 332, loss = 0.42471430\n",
      "Iteration 333, loss = 0.42462074\n",
      "Iteration 334, loss = 0.42445756\n",
      "Iteration 335, loss = 0.42433508\n",
      "Iteration 336, loss = 0.42424365\n",
      "Iteration 337, loss = 0.42408561\n",
      "Iteration 338, loss = 0.42405008\n",
      "Iteration 339, loss = 0.42395489\n",
      "Iteration 340, loss = 0.42378179\n",
      "Iteration 341, loss = 0.42367806\n",
      "Iteration 342, loss = 0.42357181\n",
      "Iteration 343, loss = 0.42340835\n",
      "Iteration 344, loss = 0.42338362\n",
      "Iteration 345, loss = 0.42319779\n",
      "Iteration 346, loss = 0.42312016\n",
      "Iteration 347, loss = 0.42304012\n",
      "Iteration 348, loss = 0.42300251\n",
      "Iteration 349, loss = 0.42283394\n",
      "Iteration 350, loss = 0.42264863\n",
      "Iteration 351, loss = 0.42262721\n",
      "Iteration 352, loss = 0.42255727\n",
      "Iteration 353, loss = 0.42227678\n",
      "Iteration 354, loss = 0.42224745\n",
      "Iteration 355, loss = 0.42214536\n",
      "Iteration 356, loss = 0.42201465\n",
      "Iteration 357, loss = 0.42193864\n",
      "Iteration 358, loss = 0.42188352\n",
      "Iteration 359, loss = 0.42179781\n",
      "Iteration 360, loss = 0.42167214\n",
      "Iteration 361, loss = 0.42158706\n",
      "Iteration 362, loss = 0.42149270\n",
      "Iteration 363, loss = 0.42132240\n",
      "Iteration 364, loss = 0.42126685\n",
      "Iteration 365, loss = 0.42113435\n",
      "Iteration 366, loss = 0.42105608\n",
      "Iteration 367, loss = 0.42102193\n",
      "Iteration 368, loss = 0.42085391\n",
      "Iteration 369, loss = 0.42075635\n",
      "Iteration 370, loss = 0.42062902\n",
      "Iteration 371, loss = 0.42062676\n",
      "Iteration 372, loss = 0.42042262\n",
      "Iteration 373, loss = 0.42036887\n",
      "Iteration 374, loss = 0.42020618\n",
      "Iteration 375, loss = 0.42013019\n",
      "Iteration 376, loss = 0.42012110\n",
      "Iteration 377, loss = 0.42000511\n",
      "Iteration 378, loss = 0.41984818\n",
      "Iteration 379, loss = 0.41976775\n",
      "Iteration 380, loss = 0.41969912\n",
      "Iteration 381, loss = 0.41966539\n",
      "Iteration 382, loss = 0.41945901\n",
      "Iteration 383, loss = 0.41940955\n",
      "Iteration 384, loss = 0.41936771\n",
      "Iteration 385, loss = 0.41924542\n",
      "Iteration 386, loss = 0.41915348\n",
      "Iteration 387, loss = 0.41900055\n",
      "Iteration 388, loss = 0.41893565\n",
      "Iteration 389, loss = 0.41890142\n",
      "Iteration 390, loss = 0.41878209\n",
      "Iteration 391, loss = 0.41871178\n",
      "Iteration 392, loss = 0.41862339\n",
      "Iteration 393, loss = 0.41848889\n",
      "Iteration 394, loss = 0.41845562\n",
      "Iteration 395, loss = 0.41836259\n",
      "Iteration 396, loss = 0.41819746\n",
      "Iteration 397, loss = 0.41813169\n",
      "Iteration 398, loss = 0.41811920\n",
      "Iteration 399, loss = 0.41802254\n",
      "Iteration 400, loss = 0.41790599\n",
      "Iteration 401, loss = 0.41793245\n",
      "Iteration 402, loss = 0.41773094\n",
      "Iteration 403, loss = 0.41767865\n",
      "Iteration 404, loss = 0.41759532\n",
      "Iteration 405, loss = 0.41757086\n",
      "Iteration 406, loss = 0.41739170\n",
      "Iteration 407, loss = 0.41733894\n",
      "Iteration 408, loss = 0.41722590\n",
      "Iteration 409, loss = 0.41716959\n",
      "Iteration 410, loss = 0.41711733\n",
      "Iteration 411, loss = 0.41699185\n",
      "Iteration 412, loss = 0.41694269\n",
      "Iteration 413, loss = 0.41686137\n",
      "Iteration 414, loss = 0.41674036\n",
      "Iteration 415, loss = 0.41666121\n",
      "Iteration 416, loss = 0.41664533\n",
      "Iteration 417, loss = 0.41653915\n",
      "Iteration 418, loss = 0.41638142\n",
      "Iteration 419, loss = 0.41640956\n",
      "Iteration 420, loss = 0.41632382\n",
      "Iteration 421, loss = 0.41615459\n",
      "Iteration 422, loss = 0.41612821\n",
      "Iteration 423, loss = 0.41608476\n",
      "Iteration 424, loss = 0.41597339\n",
      "Iteration 425, loss = 0.41593817\n",
      "Iteration 426, loss = 0.41580643\n",
      "Iteration 427, loss = 0.41573446\n",
      "Iteration 428, loss = 0.41564409\n",
      "Iteration 429, loss = 0.41558704\n",
      "Iteration 430, loss = 0.41550316\n",
      "Iteration 431, loss = 0.41544386\n",
      "Iteration 432, loss = 0.41537233\n",
      "Iteration 433, loss = 0.41523483\n",
      "Iteration 434, loss = 0.41519820\n",
      "Iteration 435, loss = 0.41511744\n",
      "Iteration 436, loss = 0.41500377\n",
      "Iteration 437, loss = 0.41499367\n",
      "Iteration 438, loss = 0.41487118\n",
      "Iteration 439, loss = 0.41483970\n",
      "Iteration 440, loss = 0.41479356\n",
      "Iteration 441, loss = 0.41464844\n",
      "Iteration 442, loss = 0.41469744\n",
      "Iteration 443, loss = 0.41450974\n",
      "Iteration 444, loss = 0.41444511\n",
      "Iteration 445, loss = 0.41447954\n",
      "Iteration 446, loss = 0.41429205\n",
      "Iteration 447, loss = 0.41432578\n",
      "Iteration 448, loss = 0.41415993\n",
      "Iteration 449, loss = 0.41417215\n",
      "Iteration 450, loss = 0.41402382\n",
      "Iteration 451, loss = 0.41392464\n",
      "Iteration 452, loss = 0.41400109\n",
      "Iteration 453, loss = 0.41384123\n",
      "Iteration 454, loss = 0.41377627\n",
      "Iteration 455, loss = 0.41366730\n",
      "Iteration 456, loss = 0.41361484\n",
      "Iteration 457, loss = 0.41356097\n",
      "Iteration 458, loss = 0.41353142\n",
      "Iteration 459, loss = 0.41342584\n",
      "Iteration 460, loss = 0.41336524\n",
      "Iteration 461, loss = 0.41336063\n",
      "Iteration 462, loss = 0.41328013\n",
      "Iteration 463, loss = 0.41311935\n",
      "Iteration 464, loss = 0.41309575\n",
      "Iteration 465, loss = 0.41299241\n",
      "Iteration 466, loss = 0.41288655\n",
      "Iteration 467, loss = 0.41293032\n",
      "Iteration 468, loss = 0.41280720\n",
      "Iteration 469, loss = 0.41270613\n",
      "Iteration 470, loss = 0.41271255\n",
      "Iteration 471, loss = 0.41263981\n",
      "Iteration 472, loss = 0.41259761\n",
      "Iteration 473, loss = 0.41243810\n",
      "Iteration 474, loss = 0.41238123\n",
      "Iteration 475, loss = 0.41239280\n",
      "Iteration 476, loss = 0.41229489\n",
      "Iteration 477, loss = 0.41214228\n",
      "Iteration 478, loss = 0.41215759\n",
      "Iteration 479, loss = 0.41209058\n",
      "Iteration 480, loss = 0.41202559\n",
      "Iteration 481, loss = 0.41196921\n",
      "Iteration 482, loss = 0.41186025\n",
      "Iteration 483, loss = 0.41182613\n",
      "Iteration 484, loss = 0.41172172\n",
      "Iteration 485, loss = 0.41165838\n",
      "Iteration 486, loss = 0.41161093\n",
      "Iteration 487, loss = 0.41167669\n",
      "Iteration 488, loss = 0.41153234\n",
      "Iteration 489, loss = 0.41145333\n",
      "Iteration 490, loss = 0.41138023\n",
      "Iteration 491, loss = 0.41128860\n",
      "Iteration 492, loss = 0.41125931\n",
      "Iteration 493, loss = 0.41121809\n",
      "Iteration 494, loss = 0.41112396\n",
      "Iteration 495, loss = 0.41102394\n",
      "Iteration 496, loss = 0.41102773\n",
      "Iteration 497, loss = 0.41094699\n",
      "Iteration 498, loss = 0.41090678\n",
      "Iteration 499, loss = 0.41077177\n",
      "Iteration 500, loss = 0.41071925\n",
      "Iteration 1, loss = 1.93708242\n",
      "Iteration 2, loss = 1.80320627\n",
      "Iteration 3, loss = 1.69543890\n",
      "Iteration 4, loss = 1.60526606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.52708624\n",
      "Iteration 6, loss = 1.45796565\n",
      "Iteration 7, loss = 1.39648026\n",
      "Iteration 8, loss = 1.34176513\n",
      "Iteration 9, loss = 1.29283304\n",
      "Iteration 10, loss = 1.24890382\n",
      "Iteration 11, loss = 1.20914074\n",
      "Iteration 12, loss = 1.17300631\n",
      "Iteration 13, loss = 1.13983129\n",
      "Iteration 14, loss = 1.10930145\n",
      "Iteration 15, loss = 1.08097041\n",
      "Iteration 16, loss = 1.05463673\n",
      "Iteration 17, loss = 1.03014857\n",
      "Iteration 18, loss = 1.00716530\n",
      "Iteration 19, loss = 0.98568603\n",
      "Iteration 20, loss = 0.96550239\n",
      "Iteration 21, loss = 0.94646993\n",
      "Iteration 22, loss = 0.92849094\n",
      "Iteration 23, loss = 0.91148013\n",
      "Iteration 24, loss = 0.89526748\n",
      "Iteration 25, loss = 0.87992860\n",
      "Iteration 26, loss = 0.86537941\n",
      "Iteration 27, loss = 0.85162261\n",
      "Iteration 28, loss = 0.83868050\n",
      "Iteration 29, loss = 0.82644520\n",
      "Iteration 30, loss = 0.81488562\n",
      "Iteration 31, loss = 0.80400516\n",
      "Iteration 32, loss = 0.79369582\n",
      "Iteration 33, loss = 0.78394529\n",
      "Iteration 34, loss = 0.77471995\n",
      "Iteration 35, loss = 0.76593095\n",
      "Iteration 36, loss = 0.75753923\n",
      "Iteration 37, loss = 0.74957896\n",
      "Iteration 38, loss = 0.74193638\n",
      "Iteration 39, loss = 0.73467919\n",
      "Iteration 40, loss = 0.72768342\n",
      "Iteration 41, loss = 0.72099643\n",
      "Iteration 42, loss = 0.71456462\n",
      "Iteration 43, loss = 0.70842223\n",
      "Iteration 44, loss = 0.70249018\n",
      "Iteration 45, loss = 0.69677509\n",
      "Iteration 46, loss = 0.69127525\n",
      "Iteration 47, loss = 0.68598586\n",
      "Iteration 48, loss = 0.68085186\n",
      "Iteration 49, loss = 0.67590764\n",
      "Iteration 50, loss = 0.67115009\n",
      "Iteration 51, loss = 0.66651128\n",
      "Iteration 52, loss = 0.66199920\n",
      "Iteration 53, loss = 0.65767648\n",
      "Iteration 54, loss = 0.65346107\n",
      "Iteration 55, loss = 0.64935544\n",
      "Iteration 56, loss = 0.64531298\n",
      "Iteration 57, loss = 0.64143774\n",
      "Iteration 58, loss = 0.63761141\n",
      "Iteration 59, loss = 0.63390112\n",
      "Iteration 60, loss = 0.63030894\n",
      "Iteration 61, loss = 0.62676079\n",
      "Iteration 62, loss = 0.62328720\n",
      "Iteration 63, loss = 0.61988552\n",
      "Iteration 64, loss = 0.61654519\n",
      "Iteration 65, loss = 0.61325732\n",
      "Iteration 66, loss = 0.61006264\n",
      "Iteration 67, loss = 0.60695327\n",
      "Iteration 68, loss = 0.60388761\n",
      "Iteration 69, loss = 0.60085289\n",
      "Iteration 70, loss = 0.59786844\n",
      "Iteration 71, loss = 0.59499261\n",
      "Iteration 72, loss = 0.59206206\n",
      "Iteration 73, loss = 0.58925797\n",
      "Iteration 74, loss = 0.58648354\n",
      "Iteration 75, loss = 0.58376175\n",
      "Iteration 76, loss = 0.58108912\n",
      "Iteration 77, loss = 0.57844487\n",
      "Iteration 78, loss = 0.57581017\n",
      "Iteration 79, loss = 0.57325276\n",
      "Iteration 80, loss = 0.57075321\n",
      "Iteration 81, loss = 0.56827147\n",
      "Iteration 82, loss = 0.56584307\n",
      "Iteration 83, loss = 0.56344441\n",
      "Iteration 84, loss = 0.56104985\n",
      "Iteration 85, loss = 0.55876294\n",
      "Iteration 86, loss = 0.55645363\n",
      "Iteration 87, loss = 0.55420891\n",
      "Iteration 88, loss = 0.55198858\n",
      "Iteration 89, loss = 0.54985911\n",
      "Iteration 90, loss = 0.54772263\n",
      "Iteration 91, loss = 0.54566897\n",
      "Iteration 92, loss = 0.54360942\n",
      "Iteration 93, loss = 0.54158746\n",
      "Iteration 94, loss = 0.53961051\n",
      "Iteration 95, loss = 0.53766083\n",
      "Iteration 96, loss = 0.53575685\n",
      "Iteration 97, loss = 0.53389457\n",
      "Iteration 98, loss = 0.53205238\n",
      "Iteration 99, loss = 0.53027533\n",
      "Iteration 100, loss = 0.52851175\n",
      "Iteration 101, loss = 0.52680013\n",
      "Iteration 102, loss = 0.52509351\n",
      "Iteration 103, loss = 0.52343835\n",
      "Iteration 104, loss = 0.52179402\n",
      "Iteration 105, loss = 0.52021170\n",
      "Iteration 106, loss = 0.51867137\n",
      "Iteration 107, loss = 0.51717258\n",
      "Iteration 108, loss = 0.51570777\n",
      "Iteration 109, loss = 0.51415497\n",
      "Iteration 110, loss = 0.51274448\n",
      "Iteration 111, loss = 0.51130536\n",
      "Iteration 112, loss = 0.50992941\n",
      "Iteration 113, loss = 0.50863660\n",
      "Iteration 114, loss = 0.50726540\n",
      "Iteration 115, loss = 0.50596972\n",
      "Iteration 116, loss = 0.50471978\n",
      "Iteration 117, loss = 0.50343444\n",
      "Iteration 118, loss = 0.50225538\n",
      "Iteration 119, loss = 0.50101062\n",
      "Iteration 120, loss = 0.49985464\n",
      "Iteration 121, loss = 0.49869528\n",
      "Iteration 122, loss = 0.49759795\n",
      "Iteration 123, loss = 0.49649382\n",
      "Iteration 124, loss = 0.49539491\n",
      "Iteration 125, loss = 0.49427320\n",
      "Iteration 126, loss = 0.49323356\n",
      "Iteration 127, loss = 0.49224575\n",
      "Iteration 128, loss = 0.49126275\n",
      "Iteration 129, loss = 0.49024629\n",
      "Iteration 130, loss = 0.48932739\n",
      "Iteration 131, loss = 0.48832796\n",
      "Iteration 132, loss = 0.48739412\n",
      "Iteration 133, loss = 0.48649812\n",
      "Iteration 134, loss = 0.48556573\n",
      "Iteration 135, loss = 0.48471137\n",
      "Iteration 136, loss = 0.48384927\n",
      "Iteration 137, loss = 0.48300204\n",
      "Iteration 138, loss = 0.48217177\n",
      "Iteration 139, loss = 0.48139101\n",
      "Iteration 140, loss = 0.48059195\n",
      "Iteration 141, loss = 0.47979143\n",
      "Iteration 142, loss = 0.47902250\n",
      "Iteration 143, loss = 0.47824135\n",
      "Iteration 144, loss = 0.47750775\n",
      "Iteration 145, loss = 0.47675632\n",
      "Iteration 146, loss = 0.47604539\n",
      "Iteration 147, loss = 0.47534772\n",
      "Iteration 148, loss = 0.47462540\n",
      "Iteration 149, loss = 0.47396921\n",
      "Iteration 150, loss = 0.47333577\n",
      "Iteration 151, loss = 0.47267512\n",
      "Iteration 152, loss = 0.47202394\n",
      "Iteration 153, loss = 0.47143620\n",
      "Iteration 154, loss = 0.47075056\n",
      "Iteration 155, loss = 0.47014859\n",
      "Iteration 156, loss = 0.46954981\n",
      "Iteration 157, loss = 0.46901711\n",
      "Iteration 158, loss = 0.46834684\n",
      "Iteration 159, loss = 0.46779486\n",
      "Iteration 160, loss = 0.46718674\n",
      "Iteration 161, loss = 0.46666616\n",
      "Iteration 162, loss = 0.46611058\n",
      "Iteration 163, loss = 0.46557688\n",
      "Iteration 164, loss = 0.46510717\n",
      "Iteration 165, loss = 0.46451383\n",
      "Iteration 166, loss = 0.46401795\n",
      "Iteration 167, loss = 0.46351352\n",
      "Iteration 168, loss = 0.46296684\n",
      "Iteration 169, loss = 0.46248281\n",
      "Iteration 170, loss = 0.46204308\n",
      "Iteration 171, loss = 0.46152960\n",
      "Iteration 172, loss = 0.46107085\n",
      "Iteration 173, loss = 0.46058486\n",
      "Iteration 174, loss = 0.46012024\n",
      "Iteration 175, loss = 0.45963146\n",
      "Iteration 176, loss = 0.45922543\n",
      "Iteration 177, loss = 0.45887702\n",
      "Iteration 178, loss = 0.45832878\n",
      "Iteration 179, loss = 0.45793285\n",
      "Iteration 180, loss = 0.45742841\n",
      "Iteration 181, loss = 0.45710637\n",
      "Iteration 182, loss = 0.45667134\n",
      "Iteration 183, loss = 0.45628451\n",
      "Iteration 184, loss = 0.45586875\n",
      "Iteration 185, loss = 0.45551150\n",
      "Iteration 186, loss = 0.45511212\n",
      "Iteration 187, loss = 0.45465066\n",
      "Iteration 188, loss = 0.45429867\n",
      "Iteration 189, loss = 0.45384909\n",
      "Iteration 190, loss = 0.45353961\n",
      "Iteration 191, loss = 0.45317499\n",
      "Iteration 192, loss = 0.45279799\n",
      "Iteration 193, loss = 0.45243360\n",
      "Iteration 194, loss = 0.45209780\n",
      "Iteration 195, loss = 0.45175288\n",
      "Iteration 196, loss = 0.45138911\n",
      "Iteration 197, loss = 0.45108827\n",
      "Iteration 198, loss = 0.45072099\n",
      "Iteration 199, loss = 0.45035906\n",
      "Iteration 200, loss = 0.45010672\n",
      "Iteration 201, loss = 0.44968148\n",
      "Iteration 202, loss = 0.44940111\n",
      "Iteration 203, loss = 0.44906993\n",
      "Iteration 204, loss = 0.44877521\n",
      "Iteration 205, loss = 0.44844817\n",
      "Iteration 206, loss = 0.44813019\n",
      "Iteration 207, loss = 0.44788160\n",
      "Iteration 208, loss = 0.44748555\n",
      "Iteration 209, loss = 0.44723220\n",
      "Iteration 210, loss = 0.44691389\n",
      "Iteration 211, loss = 0.44670494\n",
      "Iteration 212, loss = 0.44643601\n",
      "Iteration 213, loss = 0.44607765\n",
      "Iteration 214, loss = 0.44581937\n",
      "Iteration 215, loss = 0.44555658\n",
      "Iteration 216, loss = 0.44526154\n",
      "Iteration 217, loss = 0.44496688\n",
      "Iteration 218, loss = 0.44472898\n",
      "Iteration 219, loss = 0.44443404\n",
      "Iteration 220, loss = 0.44415837\n",
      "Iteration 221, loss = 0.44391560\n",
      "Iteration 222, loss = 0.44363890\n",
      "Iteration 223, loss = 0.44341703\n",
      "Iteration 224, loss = 0.44313411\n",
      "Iteration 225, loss = 0.44285670\n",
      "Iteration 226, loss = 0.44261262\n",
      "Iteration 227, loss = 0.44236303\n",
      "Iteration 228, loss = 0.44213670\n",
      "Iteration 229, loss = 0.44189214\n",
      "Iteration 230, loss = 0.44162397\n",
      "Iteration 231, loss = 0.44137758\n",
      "Iteration 232, loss = 0.44115889\n",
      "Iteration 233, loss = 0.44101152\n",
      "Iteration 234, loss = 0.44069097\n",
      "Iteration 235, loss = 0.44047326\n",
      "Iteration 236, loss = 0.44025401\n",
      "Iteration 237, loss = 0.44003438\n",
      "Iteration 238, loss = 0.43977986\n",
      "Iteration 239, loss = 0.43953667\n",
      "Iteration 240, loss = 0.43937317\n",
      "Iteration 241, loss = 0.43912281\n",
      "Iteration 242, loss = 0.43892694\n",
      "Iteration 243, loss = 0.43869561\n",
      "Iteration 244, loss = 0.43849107\n",
      "Iteration 245, loss = 0.43821888\n",
      "Iteration 246, loss = 0.43807662\n",
      "Iteration 247, loss = 0.43789821\n",
      "Iteration 248, loss = 0.43762316\n",
      "Iteration 249, loss = 0.43745216\n",
      "Iteration 250, loss = 0.43733911\n",
      "Iteration 251, loss = 0.43706328\n",
      "Iteration 252, loss = 0.43687496\n",
      "Iteration 253, loss = 0.43669766\n",
      "Iteration 254, loss = 0.43647221\n",
      "Iteration 255, loss = 0.43624883\n",
      "Iteration 256, loss = 0.43604717\n",
      "Iteration 257, loss = 0.43587236\n",
      "Iteration 258, loss = 0.43568265\n",
      "Iteration 259, loss = 0.43554978\n",
      "Iteration 260, loss = 0.43533194\n",
      "Iteration 261, loss = 0.43509172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 262, loss = 0.43489193\n",
      "Iteration 263, loss = 0.43481664\n",
      "Iteration 264, loss = 0.43455124\n",
      "Iteration 265, loss = 0.43444516\n",
      "Iteration 266, loss = 0.43426233\n",
      "Iteration 267, loss = 0.43406267\n",
      "Iteration 268, loss = 0.43384035\n",
      "Iteration 269, loss = 0.43371730\n",
      "Iteration 270, loss = 0.43355557\n",
      "Iteration 271, loss = 0.43337235\n",
      "Iteration 272, loss = 0.43318895\n",
      "Iteration 273, loss = 0.43305626\n",
      "Iteration 274, loss = 0.43288980\n",
      "Iteration 275, loss = 0.43271758\n",
      "Iteration 276, loss = 0.43247224\n",
      "Iteration 277, loss = 0.43247708\n",
      "Iteration 278, loss = 0.43225349\n",
      "Iteration 279, loss = 0.43205470\n",
      "Iteration 280, loss = 0.43189350\n",
      "Iteration 281, loss = 0.43177743\n",
      "Iteration 282, loss = 0.43159452\n",
      "Iteration 283, loss = 0.43136448\n",
      "Iteration 284, loss = 0.43125743\n",
      "Iteration 285, loss = 0.43112563\n",
      "Iteration 286, loss = 0.43091665\n",
      "Iteration 287, loss = 0.43084377\n",
      "Iteration 288, loss = 0.43071120\n",
      "Iteration 289, loss = 0.43049492\n",
      "Iteration 290, loss = 0.43037009\n",
      "Iteration 291, loss = 0.43013987\n",
      "Iteration 292, loss = 0.43003036\n",
      "Iteration 293, loss = 0.42990038\n",
      "Iteration 294, loss = 0.42978762\n",
      "Iteration 295, loss = 0.42965749\n",
      "Iteration 296, loss = 0.42947392\n",
      "Iteration 297, loss = 0.42938794\n",
      "Iteration 298, loss = 0.42920473\n",
      "Iteration 299, loss = 0.42899480\n",
      "Iteration 300, loss = 0.42894952\n",
      "Iteration 301, loss = 0.42877954\n",
      "Iteration 302, loss = 0.42861134\n",
      "Iteration 303, loss = 0.42849357\n",
      "Iteration 304, loss = 0.42835693\n",
      "Iteration 305, loss = 0.42825258\n",
      "Iteration 306, loss = 0.42806536\n",
      "Iteration 307, loss = 0.42793693\n",
      "Iteration 308, loss = 0.42779899\n",
      "Iteration 309, loss = 0.42768497\n",
      "Iteration 310, loss = 0.42754651\n",
      "Iteration 311, loss = 0.42741508\n",
      "Iteration 312, loss = 0.42728845\n",
      "Iteration 313, loss = 0.42724588\n",
      "Iteration 314, loss = 0.42705496\n",
      "Iteration 315, loss = 0.42694757\n",
      "Iteration 316, loss = 0.42684809\n",
      "Iteration 317, loss = 0.42674239\n",
      "Iteration 318, loss = 0.42658525\n",
      "Iteration 319, loss = 0.42638771\n",
      "Iteration 320, loss = 0.42630282\n",
      "Iteration 321, loss = 0.42618364\n",
      "Iteration 322, loss = 0.42601125\n",
      "Iteration 323, loss = 0.42593215\n",
      "Iteration 324, loss = 0.42580501\n",
      "Iteration 325, loss = 0.42565945\n",
      "Iteration 326, loss = 0.42552285\n",
      "Iteration 327, loss = 0.42542025\n",
      "Iteration 328, loss = 0.42535179\n",
      "Iteration 329, loss = 0.42518114\n",
      "Iteration 330, loss = 0.42508686\n",
      "Iteration 331, loss = 0.42492632\n",
      "Iteration 332, loss = 0.42485029\n",
      "Iteration 333, loss = 0.42474297\n",
      "Iteration 334, loss = 0.42469622\n",
      "Iteration 335, loss = 0.42448515\n",
      "Iteration 336, loss = 0.42439157\n",
      "Iteration 337, loss = 0.42425568\n",
      "Iteration 338, loss = 0.42423314\n",
      "Iteration 339, loss = 0.42407404\n",
      "Iteration 340, loss = 0.42395284\n",
      "Iteration 341, loss = 0.42384988\n",
      "Iteration 342, loss = 0.42370708\n",
      "Iteration 343, loss = 0.42361399\n",
      "Iteration 344, loss = 0.42353532\n",
      "Iteration 345, loss = 0.42341972\n",
      "Iteration 346, loss = 0.42330630\n",
      "Iteration 347, loss = 0.42314724\n",
      "Iteration 348, loss = 0.42310258\n",
      "Iteration 349, loss = 0.42298206\n",
      "Iteration 350, loss = 0.42290757\n",
      "Iteration 351, loss = 0.42280664\n",
      "Iteration 352, loss = 0.42266582\n",
      "Iteration 353, loss = 0.42254375\n",
      "Iteration 354, loss = 0.42246748\n",
      "Iteration 355, loss = 0.42237864\n",
      "Iteration 356, loss = 0.42226507\n",
      "Iteration 357, loss = 0.42212688\n",
      "Iteration 358, loss = 0.42197683\n",
      "Iteration 359, loss = 0.42196833\n",
      "Iteration 360, loss = 0.42184415\n",
      "Iteration 361, loss = 0.42172723\n",
      "Iteration 362, loss = 0.42163898\n",
      "Iteration 363, loss = 0.42147659\n",
      "Iteration 364, loss = 0.42139179\n",
      "Iteration 365, loss = 0.42135630\n",
      "Iteration 366, loss = 0.42125808\n",
      "Iteration 367, loss = 0.42112224\n",
      "Iteration 368, loss = 0.42102646\n",
      "Iteration 369, loss = 0.42089452\n",
      "Iteration 370, loss = 0.42089234\n",
      "Iteration 371, loss = 0.42079434\n",
      "Iteration 372, loss = 0.42068924\n",
      "Iteration 373, loss = 0.42050198\n",
      "Iteration 374, loss = 0.42050863\n",
      "Iteration 375, loss = 0.42042860\n",
      "Iteration 376, loss = 0.42026374\n",
      "Iteration 377, loss = 0.42017577\n",
      "Iteration 378, loss = 0.42004654\n",
      "Iteration 379, loss = 0.41998339\n",
      "Iteration 380, loss = 0.41988911\n",
      "Iteration 381, loss = 0.41974838\n",
      "Iteration 382, loss = 0.41972208\n",
      "Iteration 383, loss = 0.41957024\n",
      "Iteration 384, loss = 0.41951315\n",
      "Iteration 385, loss = 0.41940421\n",
      "Iteration 386, loss = 0.41932421\n",
      "Iteration 387, loss = 0.41930633\n",
      "Iteration 388, loss = 0.41911134\n",
      "Iteration 389, loss = 0.41905192\n",
      "Iteration 390, loss = 0.41904596\n",
      "Iteration 391, loss = 0.41891715\n",
      "Iteration 392, loss = 0.41877351\n",
      "Iteration 393, loss = 0.41868585\n",
      "Iteration 394, loss = 0.41863971\n",
      "Iteration 395, loss = 0.41851044\n",
      "Iteration 396, loss = 0.41848503\n",
      "Iteration 397, loss = 0.41829482\n",
      "Iteration 398, loss = 0.41827234\n",
      "Iteration 399, loss = 0.41822194\n",
      "Iteration 400, loss = 0.41809644\n",
      "Iteration 401, loss = 0.41802782\n",
      "Iteration 402, loss = 0.41799718\n",
      "Iteration 403, loss = 0.41784265\n",
      "Iteration 404, loss = 0.41781417\n",
      "Iteration 405, loss = 0.41763358\n",
      "Iteration 406, loss = 0.41760125\n",
      "Iteration 407, loss = 0.41751188\n",
      "Iteration 408, loss = 0.41740326\n",
      "Iteration 409, loss = 0.41738901\n",
      "Iteration 410, loss = 0.41724991\n",
      "Iteration 411, loss = 0.41714275\n",
      "Iteration 412, loss = 0.41711429\n",
      "Iteration 413, loss = 0.41691342\n",
      "Iteration 414, loss = 0.41692056\n",
      "Iteration 415, loss = 0.41685426\n",
      "Iteration 416, loss = 0.41673168\n",
      "Iteration 417, loss = 0.41663824\n",
      "Iteration 418, loss = 0.41660772\n",
      "Iteration 419, loss = 0.41647360\n",
      "Iteration 420, loss = 0.41649050\n",
      "Iteration 421, loss = 0.41635504\n",
      "Iteration 422, loss = 0.41629464\n",
      "Iteration 423, loss = 0.41616450\n",
      "Iteration 424, loss = 0.41606692\n",
      "Iteration 425, loss = 0.41605891\n",
      "Iteration 426, loss = 0.41587079\n",
      "Iteration 427, loss = 0.41586423\n",
      "Iteration 428, loss = 0.41574417\n",
      "Iteration 429, loss = 0.41570486\n",
      "Iteration 430, loss = 0.41562900\n",
      "Iteration 431, loss = 0.41550453\n",
      "Iteration 432, loss = 0.41547404\n",
      "Iteration 433, loss = 0.41535665\n",
      "Iteration 434, loss = 0.41529950\n",
      "Iteration 435, loss = 0.41522002\n",
      "Iteration 436, loss = 0.41517806\n",
      "Iteration 437, loss = 0.41518884\n",
      "Iteration 438, loss = 0.41499354\n",
      "Iteration 439, loss = 0.41492704\n",
      "Iteration 440, loss = 0.41487911\n",
      "Iteration 441, loss = 0.41483278\n",
      "Iteration 442, loss = 0.41481718\n",
      "Iteration 443, loss = 0.41462851\n",
      "Iteration 444, loss = 0.41453241\n",
      "Iteration 445, loss = 0.41449078\n",
      "Iteration 446, loss = 0.41442960\n",
      "Iteration 447, loss = 0.41428591\n",
      "Iteration 448, loss = 0.41429131\n",
      "Iteration 449, loss = 0.41430511\n",
      "Iteration 450, loss = 0.41411216\n",
      "Iteration 451, loss = 0.41400230\n",
      "Iteration 452, loss = 0.41392025\n",
      "Iteration 453, loss = 0.41388600\n",
      "Iteration 454, loss = 0.41376892\n",
      "Iteration 455, loss = 0.41374296\n",
      "Iteration 456, loss = 0.41368092\n",
      "Iteration 457, loss = 0.41358221\n",
      "Iteration 458, loss = 0.41354301\n",
      "Iteration 459, loss = 0.41345103\n",
      "Iteration 460, loss = 0.41333280\n",
      "Iteration 461, loss = 0.41330369\n",
      "Iteration 462, loss = 0.41319904\n",
      "Iteration 463, loss = 0.41318354\n",
      "Iteration 464, loss = 0.41307856\n",
      "Iteration 465, loss = 0.41301199\n",
      "Iteration 466, loss = 0.41291058\n",
      "Iteration 467, loss = 0.41289133\n",
      "Iteration 468, loss = 0.41280329\n",
      "Iteration 469, loss = 0.41269590\n",
      "Iteration 470, loss = 0.41258918\n",
      "Iteration 471, loss = 0.41252821\n",
      "Iteration 472, loss = 0.41259570\n",
      "Iteration 473, loss = 0.41240590\n",
      "Iteration 474, loss = 0.41239150\n",
      "Iteration 475, loss = 0.41232530\n",
      "Iteration 476, loss = 0.41221778\n",
      "Iteration 477, loss = 0.41214405\n",
      "Iteration 478, loss = 0.41209581\n",
      "Iteration 479, loss = 0.41203087\n",
      "Iteration 480, loss = 0.41192388\n",
      "Iteration 481, loss = 0.41198754\n",
      "Iteration 482, loss = 0.41190329\n",
      "Iteration 483, loss = 0.41175354\n",
      "Iteration 484, loss = 0.41164897\n",
      "Iteration 485, loss = 0.41165415\n",
      "Iteration 486, loss = 0.41152579\n",
      "Iteration 487, loss = 0.41151963\n",
      "Iteration 488, loss = 0.41140127\n",
      "Iteration 489, loss = 0.41131304\n",
      "Iteration 490, loss = 0.41125606\n",
      "Iteration 491, loss = 0.41119577\n",
      "Iteration 492, loss = 0.41120176\n",
      "Iteration 493, loss = 0.41108692\n",
      "Iteration 494, loss = 0.41105183\n",
      "Iteration 495, loss = 0.41091798\n",
      "Iteration 496, loss = 0.41087017\n",
      "Iteration 497, loss = 0.41089864\n",
      "Iteration 498, loss = 0.41071053\n",
      "Iteration 499, loss = 0.41070292\n",
      "Iteration 500, loss = 0.41058544\n",
      "Iteration 1, loss = 1.93933642\n",
      "Iteration 2, loss = 1.80543244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.69763516\n",
      "Iteration 4, loss = 1.60736328\n",
      "Iteration 5, loss = 1.52896537\n",
      "Iteration 6, loss = 1.45960239\n",
      "Iteration 7, loss = 1.39784398\n",
      "Iteration 8, loss = 1.34283793\n",
      "Iteration 9, loss = 1.29367564\n",
      "Iteration 10, loss = 1.24951636\n",
      "Iteration 11, loss = 1.20959737\n",
      "Iteration 12, loss = 1.17325731\n",
      "Iteration 13, loss = 1.14001420\n",
      "Iteration 14, loss = 1.10938433\n",
      "Iteration 15, loss = 1.08105937\n",
      "Iteration 16, loss = 1.05468708\n",
      "Iteration 17, loss = 1.03016545\n",
      "Iteration 18, loss = 1.00723719\n",
      "Iteration 19, loss = 0.98577146\n",
      "Iteration 20, loss = 0.96563376\n",
      "Iteration 21, loss = 0.94666295\n",
      "Iteration 22, loss = 0.92874720\n",
      "Iteration 23, loss = 0.91181515\n",
      "Iteration 24, loss = 0.89571254\n",
      "Iteration 25, loss = 0.88044560\n",
      "Iteration 26, loss = 0.86589053\n",
      "Iteration 27, loss = 0.85205204\n",
      "Iteration 28, loss = 0.83894831\n",
      "Iteration 29, loss = 0.82661465\n",
      "Iteration 30, loss = 0.81498471\n",
      "Iteration 31, loss = 0.80400867\n",
      "Iteration 32, loss = 0.79368447\n",
      "Iteration 33, loss = 0.78386980\n",
      "Iteration 34, loss = 0.77461443\n",
      "Iteration 35, loss = 0.76579854\n",
      "Iteration 36, loss = 0.75742780\n",
      "Iteration 37, loss = 0.74941392\n",
      "Iteration 38, loss = 0.74178278\n",
      "Iteration 39, loss = 0.73447304\n",
      "Iteration 40, loss = 0.72750208\n",
      "Iteration 41, loss = 0.72079911\n",
      "Iteration 42, loss = 0.71440309\n",
      "Iteration 43, loss = 0.70826753\n",
      "Iteration 44, loss = 0.70234744\n",
      "Iteration 45, loss = 0.69670584\n",
      "Iteration 46, loss = 0.69125127\n",
      "Iteration 47, loss = 0.68599330\n",
      "Iteration 48, loss = 0.68094268\n",
      "Iteration 49, loss = 0.67604627\n",
      "Iteration 50, loss = 0.67130909\n",
      "Iteration 51, loss = 0.66674987\n",
      "Iteration 52, loss = 0.66229780\n",
      "Iteration 53, loss = 0.65798011\n",
      "Iteration 54, loss = 0.65383226\n",
      "Iteration 55, loss = 0.64975452\n",
      "Iteration 56, loss = 0.64582937\n",
      "Iteration 57, loss = 0.64198786\n",
      "Iteration 58, loss = 0.63822062\n",
      "Iteration 59, loss = 0.63458534\n",
      "Iteration 60, loss = 0.63100269\n",
      "Iteration 61, loss = 0.62753537\n",
      "Iteration 62, loss = 0.62413670\n",
      "Iteration 63, loss = 0.62079362\n",
      "Iteration 64, loss = 0.61751251\n",
      "Iteration 65, loss = 0.61435679\n",
      "Iteration 66, loss = 0.61121154\n",
      "Iteration 67, loss = 0.60814511\n",
      "Iteration 68, loss = 0.60512491\n",
      "Iteration 69, loss = 0.60218417\n",
      "Iteration 70, loss = 0.59930338\n",
      "Iteration 71, loss = 0.59646800\n",
      "Iteration 72, loss = 0.59365069\n",
      "Iteration 73, loss = 0.59089815\n",
      "Iteration 74, loss = 0.58820323\n",
      "Iteration 75, loss = 0.58554504\n",
      "Iteration 76, loss = 0.58293425\n",
      "Iteration 77, loss = 0.58039495\n",
      "Iteration 78, loss = 0.57782974\n",
      "Iteration 79, loss = 0.57534391\n",
      "Iteration 80, loss = 0.57291473\n",
      "Iteration 81, loss = 0.57047237\n",
      "Iteration 82, loss = 0.56808556\n",
      "Iteration 83, loss = 0.56577487\n",
      "Iteration 84, loss = 0.56343292\n",
      "Iteration 85, loss = 0.56117986\n",
      "Iteration 86, loss = 0.55895301\n",
      "Iteration 87, loss = 0.55677695\n",
      "Iteration 88, loss = 0.55459815\n",
      "Iteration 89, loss = 0.55245792\n",
      "Iteration 90, loss = 0.55032059\n",
      "Iteration 91, loss = 0.54825052\n",
      "Iteration 92, loss = 0.54620870\n",
      "Iteration 93, loss = 0.54420819\n",
      "Iteration 94, loss = 0.54225323\n",
      "Iteration 95, loss = 0.54028997\n",
      "Iteration 96, loss = 0.53838281\n",
      "Iteration 97, loss = 0.53651156\n",
      "Iteration 98, loss = 0.53467661\n",
      "Iteration 99, loss = 0.53286278\n",
      "Iteration 100, loss = 0.53105331\n",
      "Iteration 101, loss = 0.52931579\n",
      "Iteration 102, loss = 0.52756744\n",
      "Iteration 103, loss = 0.52587102\n",
      "Iteration 104, loss = 0.52424854\n",
      "Iteration 105, loss = 0.52261303\n",
      "Iteration 106, loss = 0.52099190\n",
      "Iteration 107, loss = 0.51942682\n",
      "Iteration 108, loss = 0.51783889\n",
      "Iteration 109, loss = 0.51636765\n",
      "Iteration 110, loss = 0.51488383\n",
      "Iteration 111, loss = 0.51338775\n",
      "Iteration 112, loss = 0.51199807\n",
      "Iteration 113, loss = 0.51058963\n",
      "Iteration 114, loss = 0.50918500\n",
      "Iteration 115, loss = 0.50787674\n",
      "Iteration 116, loss = 0.50648732\n",
      "Iteration 117, loss = 0.50524631\n",
      "Iteration 118, loss = 0.50391242\n",
      "Iteration 119, loss = 0.50264745\n",
      "Iteration 120, loss = 0.50146885\n",
      "Iteration 121, loss = 0.50021141\n",
      "Iteration 122, loss = 0.49905586\n",
      "Iteration 123, loss = 0.49791940\n",
      "Iteration 124, loss = 0.49676746\n",
      "Iteration 125, loss = 0.49562900\n",
      "Iteration 126, loss = 0.49455937\n",
      "Iteration 127, loss = 0.49349101\n",
      "Iteration 128, loss = 0.49242097\n",
      "Iteration 129, loss = 0.49137002\n",
      "Iteration 130, loss = 0.49037591\n",
      "Iteration 131, loss = 0.48936340\n",
      "Iteration 132, loss = 0.48840263\n",
      "Iteration 133, loss = 0.48747298\n",
      "Iteration 134, loss = 0.48651881\n",
      "Iteration 135, loss = 0.48562511\n",
      "Iteration 136, loss = 0.48467783\n",
      "Iteration 137, loss = 0.48381085\n",
      "Iteration 138, loss = 0.48293404\n",
      "Iteration 139, loss = 0.48207907\n",
      "Iteration 140, loss = 0.48126479\n",
      "Iteration 141, loss = 0.48045811\n",
      "Iteration 142, loss = 0.47966008\n",
      "Iteration 143, loss = 0.47883790\n",
      "Iteration 144, loss = 0.47812554\n",
      "Iteration 145, loss = 0.47730370\n",
      "Iteration 146, loss = 0.47656488\n",
      "Iteration 147, loss = 0.47582778\n",
      "Iteration 148, loss = 0.47512041\n",
      "Iteration 149, loss = 0.47435820\n",
      "Iteration 150, loss = 0.47368478\n",
      "Iteration 151, loss = 0.47298272\n",
      "Iteration 152, loss = 0.47235182\n",
      "Iteration 153, loss = 0.47164469\n",
      "Iteration 154, loss = 0.47103992\n",
      "Iteration 155, loss = 0.47035476\n",
      "Iteration 156, loss = 0.46974813\n",
      "Iteration 157, loss = 0.46918963\n",
      "Iteration 158, loss = 0.46852539\n",
      "Iteration 159, loss = 0.46791829\n",
      "Iteration 160, loss = 0.46733185\n",
      "Iteration 161, loss = 0.46677599\n",
      "Iteration 162, loss = 0.46618078\n",
      "Iteration 163, loss = 0.46565891\n",
      "Iteration 164, loss = 0.46505380\n",
      "Iteration 165, loss = 0.46450942\n",
      "Iteration 166, loss = 0.46395495\n",
      "Iteration 167, loss = 0.46346481\n",
      "Iteration 168, loss = 0.46294145\n",
      "Iteration 169, loss = 0.46239089\n",
      "Iteration 170, loss = 0.46197118\n",
      "Iteration 171, loss = 0.46148378\n",
      "Iteration 172, loss = 0.46094108\n",
      "Iteration 173, loss = 0.46044543\n",
      "Iteration 174, loss = 0.45997934\n",
      "Iteration 175, loss = 0.45955966\n",
      "Iteration 176, loss = 0.45902379\n",
      "Iteration 177, loss = 0.45861737\n",
      "Iteration 178, loss = 0.45814863\n",
      "Iteration 179, loss = 0.45770169\n",
      "Iteration 180, loss = 0.45727018\n",
      "Iteration 181, loss = 0.45687165\n",
      "Iteration 182, loss = 0.45639153\n",
      "Iteration 183, loss = 0.45592962\n",
      "Iteration 184, loss = 0.45557375\n",
      "Iteration 185, loss = 0.45512923\n",
      "Iteration 186, loss = 0.45473723\n",
      "Iteration 187, loss = 0.45437428\n",
      "Iteration 188, loss = 0.45391801\n",
      "Iteration 189, loss = 0.45354711\n",
      "Iteration 190, loss = 0.45322204\n",
      "Iteration 191, loss = 0.45284895\n",
      "Iteration 192, loss = 0.45241884\n",
      "Iteration 193, loss = 0.45209816\n",
      "Iteration 194, loss = 0.45172089\n",
      "Iteration 195, loss = 0.45132335\n",
      "Iteration 196, loss = 0.45098223\n",
      "Iteration 197, loss = 0.45057547\n",
      "Iteration 198, loss = 0.45025665\n",
      "Iteration 199, loss = 0.44990745\n",
      "Iteration 200, loss = 0.44953709\n",
      "Iteration 201, loss = 0.44922169\n",
      "Iteration 202, loss = 0.44889484\n",
      "Iteration 203, loss = 0.44859712\n",
      "Iteration 204, loss = 0.44825265\n",
      "Iteration 205, loss = 0.44790717\n",
      "Iteration 206, loss = 0.44763101\n",
      "Iteration 207, loss = 0.44724186\n",
      "Iteration 208, loss = 0.44698076\n",
      "Iteration 209, loss = 0.44665312\n",
      "Iteration 210, loss = 0.44639513\n",
      "Iteration 211, loss = 0.44609255\n",
      "Iteration 212, loss = 0.44577676\n",
      "Iteration 213, loss = 0.44547960\n",
      "Iteration 214, loss = 0.44519119\n",
      "Iteration 215, loss = 0.44489021\n",
      "Iteration 216, loss = 0.44460549\n",
      "Iteration 217, loss = 0.44442790\n",
      "Iteration 218, loss = 0.44405602\n",
      "Iteration 219, loss = 0.44377347\n",
      "Iteration 220, loss = 0.44361014\n",
      "Iteration 221, loss = 0.44321507\n",
      "Iteration 222, loss = 0.44297238\n",
      "Iteration 223, loss = 0.44269311\n",
      "Iteration 224, loss = 0.44244265\n",
      "Iteration 225, loss = 0.44217469\n",
      "Iteration 226, loss = 0.44195979\n",
      "Iteration 227, loss = 0.44172295\n",
      "Iteration 228, loss = 0.44149491\n",
      "Iteration 229, loss = 0.44114662\n",
      "Iteration 230, loss = 0.44093649\n",
      "Iteration 231, loss = 0.44069117\n",
      "Iteration 232, loss = 0.44041043\n",
      "Iteration 233, loss = 0.44019593\n",
      "Iteration 234, loss = 0.43996240\n",
      "Iteration 235, loss = 0.43967843\n",
      "Iteration 236, loss = 0.43950307\n",
      "Iteration 237, loss = 0.43921908\n",
      "Iteration 238, loss = 0.43904079\n",
      "Iteration 239, loss = 0.43879205\n",
      "Iteration 240, loss = 0.43860697\n",
      "Iteration 241, loss = 0.43831325\n",
      "Iteration 242, loss = 0.43813225\n",
      "Iteration 243, loss = 0.43797155\n",
      "Iteration 244, loss = 0.43768708\n",
      "Iteration 245, loss = 0.43750930\n",
      "Iteration 246, loss = 0.43725963\n",
      "Iteration 247, loss = 0.43705487\n",
      "Iteration 248, loss = 0.43686587\n",
      "Iteration 249, loss = 0.43672872\n",
      "Iteration 250, loss = 0.43647124\n",
      "Iteration 251, loss = 0.43624096\n",
      "Iteration 252, loss = 0.43598893\n",
      "Iteration 253, loss = 0.43588859\n",
      "Iteration 254, loss = 0.43567193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.43544771\n",
      "Iteration 256, loss = 0.43525085\n",
      "Iteration 257, loss = 0.43501436\n",
      "Iteration 258, loss = 0.43483758\n",
      "Iteration 259, loss = 0.43468471\n",
      "Iteration 260, loss = 0.43446698\n",
      "Iteration 261, loss = 0.43424532\n",
      "Iteration 262, loss = 0.43406215\n",
      "Iteration 263, loss = 0.43390261\n",
      "Iteration 264, loss = 0.43373578\n",
      "Iteration 265, loss = 0.43348297\n",
      "Iteration 266, loss = 0.43338985\n",
      "Iteration 267, loss = 0.43312350\n",
      "Iteration 268, loss = 0.43300192\n",
      "Iteration 269, loss = 0.43283766\n",
      "Iteration 270, loss = 0.43270777\n",
      "Iteration 271, loss = 0.43251547\n",
      "Iteration 272, loss = 0.43233070\n",
      "Iteration 273, loss = 0.43211498\n",
      "Iteration 274, loss = 0.43196091\n",
      "Iteration 275, loss = 0.43180563\n",
      "Iteration 276, loss = 0.43167496\n",
      "Iteration 277, loss = 0.43150534\n",
      "Iteration 278, loss = 0.43128909\n",
      "Iteration 279, loss = 0.43116823\n",
      "Iteration 280, loss = 0.43097872\n",
      "Iteration 281, loss = 0.43087706\n",
      "Iteration 282, loss = 0.43066206\n",
      "Iteration 283, loss = 0.43048912\n",
      "Iteration 284, loss = 0.43034660\n",
      "Iteration 285, loss = 0.43016131\n",
      "Iteration 286, loss = 0.42997535\n",
      "Iteration 287, loss = 0.42987866\n",
      "Iteration 288, loss = 0.42969174\n",
      "Iteration 289, loss = 0.42958159\n",
      "Iteration 290, loss = 0.42941531\n",
      "Iteration 291, loss = 0.42935513\n",
      "Iteration 292, loss = 0.42905854\n",
      "Iteration 293, loss = 0.42894619\n",
      "Iteration 294, loss = 0.42881675\n",
      "Iteration 295, loss = 0.42868278\n",
      "Iteration 296, loss = 0.42845673\n",
      "Iteration 297, loss = 0.42835473\n",
      "Iteration 298, loss = 0.42821899\n",
      "Iteration 299, loss = 0.42805833\n",
      "Iteration 300, loss = 0.42788956\n",
      "Iteration 301, loss = 0.42784124\n",
      "Iteration 302, loss = 0.42772427\n",
      "Iteration 303, loss = 0.42750994\n",
      "Iteration 304, loss = 0.42738980\n",
      "Iteration 305, loss = 0.42725477\n",
      "Iteration 306, loss = 0.42708318\n",
      "Iteration 307, loss = 0.42698801\n",
      "Iteration 308, loss = 0.42683702\n",
      "Iteration 309, loss = 0.42663961\n",
      "Iteration 310, loss = 0.42656345\n",
      "Iteration 311, loss = 0.42646354\n",
      "Iteration 312, loss = 0.42626062\n",
      "Iteration 313, loss = 0.42613451\n",
      "Iteration 314, loss = 0.42602075\n",
      "Iteration 315, loss = 0.42588394\n",
      "Iteration 316, loss = 0.42582714\n",
      "Iteration 317, loss = 0.42562923\n",
      "Iteration 318, loss = 0.42555035\n",
      "Iteration 319, loss = 0.42534558\n",
      "Iteration 320, loss = 0.42525297\n",
      "Iteration 321, loss = 0.42517263\n",
      "Iteration 322, loss = 0.42506761\n",
      "Iteration 323, loss = 0.42493181\n",
      "Iteration 324, loss = 0.42480050\n",
      "Iteration 325, loss = 0.42460370\n",
      "Iteration 326, loss = 0.42453071\n",
      "Iteration 327, loss = 0.42440884\n",
      "Iteration 328, loss = 0.42433472\n",
      "Iteration 329, loss = 0.42420515\n",
      "Iteration 330, loss = 0.42404003\n",
      "Iteration 331, loss = 0.42397735\n",
      "Iteration 332, loss = 0.42379048\n",
      "Iteration 333, loss = 0.42367735\n",
      "Iteration 334, loss = 0.42354234\n",
      "Iteration 335, loss = 0.42348588\n",
      "Iteration 336, loss = 0.42332627\n",
      "Iteration 337, loss = 0.42320638\n",
      "Iteration 338, loss = 0.42313204\n",
      "Iteration 339, loss = 0.42301810\n",
      "Iteration 340, loss = 0.42292890\n",
      "Iteration 341, loss = 0.42278178\n",
      "Iteration 342, loss = 0.42271941\n",
      "Iteration 343, loss = 0.42258858\n",
      "Iteration 344, loss = 0.42254500\n",
      "Iteration 345, loss = 0.42230555\n",
      "Iteration 346, loss = 0.42223528\n",
      "Iteration 347, loss = 0.42208983\n",
      "Iteration 348, loss = 0.42199249\n",
      "Iteration 349, loss = 0.42193874\n",
      "Iteration 350, loss = 0.42174205\n",
      "Iteration 351, loss = 0.42166678\n",
      "Iteration 352, loss = 0.42161518\n",
      "Iteration 353, loss = 0.42145358\n",
      "Iteration 354, loss = 0.42131889\n",
      "Iteration 355, loss = 0.42132149\n",
      "Iteration 356, loss = 0.42116577\n",
      "Iteration 357, loss = 0.42106919\n",
      "Iteration 358, loss = 0.42090856\n",
      "Iteration 359, loss = 0.42091453\n",
      "Iteration 360, loss = 0.42076960\n",
      "Iteration 361, loss = 0.42061376\n",
      "Iteration 362, loss = 0.42054359\n",
      "Iteration 363, loss = 0.42046955\n",
      "Iteration 364, loss = 0.42037102\n",
      "Iteration 365, loss = 0.42028603\n",
      "Iteration 366, loss = 0.42013765\n",
      "Iteration 367, loss = 0.42010551\n",
      "Iteration 368, loss = 0.41997580\n",
      "Iteration 369, loss = 0.41985454\n",
      "Iteration 370, loss = 0.41977850\n",
      "Iteration 371, loss = 0.41968397\n",
      "Iteration 372, loss = 0.41955332\n",
      "Iteration 373, loss = 0.41951703\n",
      "Iteration 374, loss = 0.41942550\n",
      "Iteration 375, loss = 0.41932902\n",
      "Iteration 376, loss = 0.41915072\n",
      "Iteration 377, loss = 0.41911469\n",
      "Iteration 378, loss = 0.41900515\n",
      "Iteration 379, loss = 0.41893683\n",
      "Iteration 380, loss = 0.41882290\n",
      "Iteration 381, loss = 0.41874039\n",
      "Iteration 382, loss = 0.41862397\n",
      "Iteration 383, loss = 0.41848249\n",
      "Iteration 384, loss = 0.41847098\n",
      "Iteration 385, loss = 0.41838684\n",
      "Iteration 386, loss = 0.41823341\n",
      "Iteration 387, loss = 0.41814671\n",
      "Iteration 388, loss = 0.41812946\n",
      "Iteration 389, loss = 0.41803835\n",
      "Iteration 390, loss = 0.41788468\n",
      "Iteration 391, loss = 0.41781032\n",
      "Iteration 392, loss = 0.41769420\n",
      "Iteration 393, loss = 0.41763954\n",
      "Iteration 394, loss = 0.41750580\n",
      "Iteration 395, loss = 0.41743869\n",
      "Iteration 396, loss = 0.41732826\n",
      "Iteration 397, loss = 0.41731228\n",
      "Iteration 398, loss = 0.41722463\n",
      "Iteration 399, loss = 0.41710290\n",
      "Iteration 400, loss = 0.41700524\n",
      "Iteration 401, loss = 0.41695679\n",
      "Iteration 402, loss = 0.41684403\n",
      "Iteration 403, loss = 0.41680714\n",
      "Iteration 404, loss = 0.41671035\n",
      "Iteration 405, loss = 0.41659249\n",
      "Iteration 406, loss = 0.41653610\n",
      "Iteration 407, loss = 0.41645922\n",
      "Iteration 408, loss = 0.41636160\n",
      "Iteration 409, loss = 0.41627377\n",
      "Iteration 410, loss = 0.41617167\n",
      "Iteration 411, loss = 0.41612706\n",
      "Iteration 412, loss = 0.41603086\n",
      "Iteration 413, loss = 0.41595780\n",
      "Iteration 414, loss = 0.41582251\n",
      "Iteration 415, loss = 0.41577548\n",
      "Iteration 416, loss = 0.41564721\n",
      "Iteration 417, loss = 0.41561013\n",
      "Iteration 418, loss = 0.41552519\n",
      "Iteration 419, loss = 0.41540552\n",
      "Iteration 420, loss = 0.41534058\n",
      "Iteration 421, loss = 0.41525245\n",
      "Iteration 422, loss = 0.41523122\n",
      "Iteration 423, loss = 0.41514303\n",
      "Iteration 424, loss = 0.41505515\n",
      "Iteration 425, loss = 0.41496657\n",
      "Iteration 426, loss = 0.41491712\n",
      "Iteration 427, loss = 0.41483150\n",
      "Iteration 428, loss = 0.41469686\n",
      "Iteration 429, loss = 0.41463161\n",
      "Iteration 430, loss = 0.41463133\n",
      "Iteration 431, loss = 0.41448191\n",
      "Iteration 432, loss = 0.41444662\n",
      "Iteration 433, loss = 0.41435944\n",
      "Iteration 434, loss = 0.41437351\n",
      "Iteration 435, loss = 0.41426065\n",
      "Iteration 436, loss = 0.41417728\n",
      "Iteration 437, loss = 0.41409539\n",
      "Iteration 438, loss = 0.41395229\n",
      "Iteration 439, loss = 0.41387292\n",
      "Iteration 440, loss = 0.41384544\n",
      "Iteration 441, loss = 0.41371168\n",
      "Iteration 442, loss = 0.41370171\n",
      "Iteration 443, loss = 0.41362000\n",
      "Iteration 444, loss = 0.41353338\n",
      "Iteration 445, loss = 0.41347861\n",
      "Iteration 446, loss = 0.41340365\n",
      "Iteration 447, loss = 0.41337730\n",
      "Iteration 448, loss = 0.41326130\n",
      "Iteration 449, loss = 0.41313395\n",
      "Iteration 450, loss = 0.41311772\n",
      "Iteration 451, loss = 0.41306418\n",
      "Iteration 452, loss = 0.41299481\n",
      "Iteration 453, loss = 0.41291458\n",
      "Iteration 454, loss = 0.41282067\n",
      "Iteration 455, loss = 0.41280921\n",
      "Iteration 456, loss = 0.41267188\n",
      "Iteration 457, loss = 0.41268721\n",
      "Iteration 458, loss = 0.41255854\n",
      "Iteration 459, loss = 0.41250311\n",
      "Iteration 460, loss = 0.41243839\n",
      "Iteration 461, loss = 0.41235364\n",
      "Iteration 462, loss = 0.41228064\n",
      "Iteration 463, loss = 0.41228933\n",
      "Iteration 464, loss = 0.41217676\n",
      "Iteration 465, loss = 0.41213776\n",
      "Iteration 466, loss = 0.41203143\n",
      "Iteration 467, loss = 0.41198762\n",
      "Iteration 468, loss = 0.41194419\n",
      "Iteration 469, loss = 0.41183528\n",
      "Iteration 470, loss = 0.41179194\n",
      "Iteration 471, loss = 0.41166244\n",
      "Iteration 472, loss = 0.41159034\n",
      "Iteration 473, loss = 0.41151342\n",
      "Iteration 474, loss = 0.41150864\n",
      "Iteration 475, loss = 0.41138275\n",
      "Iteration 476, loss = 0.41132806\n",
      "Iteration 477, loss = 0.41132587\n",
      "Iteration 478, loss = 0.41123335\n",
      "Iteration 479, loss = 0.41119984\n",
      "Iteration 480, loss = 0.41110806\n",
      "Iteration 481, loss = 0.41101108\n",
      "Iteration 482, loss = 0.41096437\n",
      "Iteration 483, loss = 0.41095511\n",
      "Iteration 484, loss = 0.41082536\n",
      "Iteration 485, loss = 0.41078602\n",
      "Iteration 486, loss = 0.41072483\n",
      "Iteration 487, loss = 0.41064656\n",
      "Iteration 488, loss = 0.41056387\n",
      "Iteration 489, loss = 0.41057789\n",
      "Iteration 490, loss = 0.41049780\n",
      "Iteration 491, loss = 0.41043306\n",
      "Iteration 492, loss = 0.41036559\n",
      "Iteration 493, loss = 0.41031805\n",
      "Iteration 494, loss = 0.41022237\n",
      "Iteration 495, loss = 0.41017487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93698813\n",
      "Iteration 2, loss = 1.80321194\n",
      "Iteration 3, loss = 1.69552990\n",
      "Iteration 4, loss = 1.60549488\n",
      "Iteration 5, loss = 1.52715648\n",
      "Iteration 6, loss = 1.45793684\n",
      "Iteration 7, loss = 1.39635602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.34146785\n",
      "Iteration 9, loss = 1.29243312\n",
      "Iteration 10, loss = 1.24835212\n",
      "Iteration 11, loss = 1.20849756\n",
      "Iteration 12, loss = 1.17223038\n",
      "Iteration 13, loss = 1.13896457\n",
      "Iteration 14, loss = 1.10834420\n",
      "Iteration 15, loss = 1.07997987\n",
      "Iteration 16, loss = 1.05360518\n",
      "Iteration 17, loss = 1.02904580\n",
      "Iteration 18, loss = 1.00611378\n",
      "Iteration 19, loss = 0.98460981\n",
      "Iteration 20, loss = 0.96440153\n",
      "Iteration 21, loss = 0.94535122\n",
      "Iteration 22, loss = 0.92738985\n",
      "Iteration 23, loss = 0.91037226\n",
      "Iteration 24, loss = 0.89425792\n",
      "Iteration 25, loss = 0.87889754\n",
      "Iteration 26, loss = 0.86434881\n",
      "Iteration 27, loss = 0.85054606\n",
      "Iteration 28, loss = 0.83751074\n",
      "Iteration 29, loss = 0.82524417\n",
      "Iteration 30, loss = 0.81367022\n",
      "Iteration 31, loss = 0.80271275\n",
      "Iteration 32, loss = 0.79235150\n",
      "Iteration 33, loss = 0.78255905\n",
      "Iteration 34, loss = 0.77328627\n",
      "Iteration 35, loss = 0.76446139\n",
      "Iteration 36, loss = 0.75608663\n",
      "Iteration 37, loss = 0.74808729\n",
      "Iteration 38, loss = 0.74046304\n",
      "Iteration 39, loss = 0.73318827\n",
      "Iteration 40, loss = 0.72619538\n",
      "Iteration 41, loss = 0.71951417\n",
      "Iteration 42, loss = 0.71310271\n",
      "Iteration 43, loss = 0.70698188\n",
      "Iteration 44, loss = 0.70103030\n",
      "Iteration 45, loss = 0.69537052\n",
      "Iteration 46, loss = 0.68984663\n",
      "Iteration 47, loss = 0.68455354\n",
      "Iteration 48, loss = 0.67944242\n",
      "Iteration 49, loss = 0.67454404\n",
      "Iteration 50, loss = 0.66972669\n",
      "Iteration 51, loss = 0.66513533\n",
      "Iteration 52, loss = 0.66069236\n",
      "Iteration 53, loss = 0.65635429\n",
      "Iteration 54, loss = 0.65210972\n",
      "Iteration 55, loss = 0.64805751\n",
      "Iteration 56, loss = 0.64405367\n",
      "Iteration 57, loss = 0.64019167\n",
      "Iteration 58, loss = 0.63643318\n",
      "Iteration 59, loss = 0.63272060\n",
      "Iteration 60, loss = 0.62913982\n",
      "Iteration 61, loss = 0.62564604\n",
      "Iteration 62, loss = 0.62221045\n",
      "Iteration 63, loss = 0.61884848\n",
      "Iteration 64, loss = 0.61558200\n",
      "Iteration 65, loss = 0.61238474\n",
      "Iteration 66, loss = 0.60920784\n",
      "Iteration 67, loss = 0.60612613\n",
      "Iteration 68, loss = 0.60305311\n",
      "Iteration 69, loss = 0.60006255\n",
      "Iteration 70, loss = 0.59719068\n",
      "Iteration 71, loss = 0.59426914\n",
      "Iteration 72, loss = 0.59146500\n",
      "Iteration 73, loss = 0.58867067\n",
      "Iteration 74, loss = 0.58593735\n",
      "Iteration 75, loss = 0.58324201\n",
      "Iteration 76, loss = 0.58058163\n",
      "Iteration 77, loss = 0.57796727\n",
      "Iteration 78, loss = 0.57540330\n",
      "Iteration 79, loss = 0.57287433\n",
      "Iteration 80, loss = 0.57037786\n",
      "Iteration 81, loss = 0.56792765\n",
      "Iteration 82, loss = 0.56548535\n",
      "Iteration 83, loss = 0.56310247\n",
      "Iteration 84, loss = 0.56077432\n",
      "Iteration 85, loss = 0.55843253\n",
      "Iteration 86, loss = 0.55616251\n",
      "Iteration 87, loss = 0.55390836\n",
      "Iteration 88, loss = 0.55169221\n",
      "Iteration 89, loss = 0.54953330\n",
      "Iteration 90, loss = 0.54736815\n",
      "Iteration 91, loss = 0.54528873\n",
      "Iteration 92, loss = 0.54318280\n",
      "Iteration 93, loss = 0.54121013\n",
      "Iteration 94, loss = 0.53916958\n",
      "Iteration 95, loss = 0.53723119\n",
      "Iteration 96, loss = 0.53529458\n",
      "Iteration 97, loss = 0.53337765\n",
      "Iteration 98, loss = 0.53150109\n",
      "Iteration 99, loss = 0.52964281\n",
      "Iteration 100, loss = 0.52786009\n",
      "Iteration 101, loss = 0.52614372\n",
      "Iteration 102, loss = 0.52434273\n",
      "Iteration 103, loss = 0.52265115\n",
      "Iteration 104, loss = 0.52101861\n",
      "Iteration 105, loss = 0.51939805\n",
      "Iteration 106, loss = 0.51780999\n",
      "Iteration 107, loss = 0.51616164\n",
      "Iteration 108, loss = 0.51464376\n",
      "Iteration 109, loss = 0.51316154\n",
      "Iteration 110, loss = 0.51168497\n",
      "Iteration 111, loss = 0.51024196\n",
      "Iteration 112, loss = 0.50880287\n",
      "Iteration 113, loss = 0.50738840\n",
      "Iteration 114, loss = 0.50604043\n",
      "Iteration 115, loss = 0.50471912\n",
      "Iteration 116, loss = 0.50334381\n",
      "Iteration 117, loss = 0.50206380\n",
      "Iteration 118, loss = 0.50086370\n",
      "Iteration 119, loss = 0.49961059\n",
      "Iteration 120, loss = 0.49840919\n",
      "Iteration 121, loss = 0.49719701\n",
      "Iteration 122, loss = 0.49605719\n",
      "Iteration 123, loss = 0.49492500\n",
      "Iteration 124, loss = 0.49377278\n",
      "Iteration 125, loss = 0.49270076\n",
      "Iteration 126, loss = 0.49161724\n",
      "Iteration 127, loss = 0.49056488\n",
      "Iteration 128, loss = 0.48948471\n",
      "Iteration 129, loss = 0.48850732\n",
      "Iteration 130, loss = 0.48751845\n",
      "Iteration 131, loss = 0.48652614\n",
      "Iteration 132, loss = 0.48560436\n",
      "Iteration 133, loss = 0.48463116\n",
      "Iteration 134, loss = 0.48367947\n",
      "Iteration 135, loss = 0.48282223\n",
      "Iteration 136, loss = 0.48192824\n",
      "Iteration 137, loss = 0.48099406\n",
      "Iteration 138, loss = 0.48017533\n",
      "Iteration 139, loss = 0.47933430\n",
      "Iteration 140, loss = 0.47849126\n",
      "Iteration 141, loss = 0.47768659\n",
      "Iteration 142, loss = 0.47687374\n",
      "Iteration 143, loss = 0.47613331\n",
      "Iteration 144, loss = 0.47531411\n",
      "Iteration 145, loss = 0.47459654\n",
      "Iteration 146, loss = 0.47384404\n",
      "Iteration 147, loss = 0.47312689\n",
      "Iteration 148, loss = 0.47240076\n",
      "Iteration 149, loss = 0.47173299\n",
      "Iteration 150, loss = 0.47104617\n",
      "Iteration 151, loss = 0.47037366\n",
      "Iteration 152, loss = 0.46963287\n",
      "Iteration 153, loss = 0.46899455\n",
      "Iteration 154, loss = 0.46830603\n",
      "Iteration 155, loss = 0.46770453\n",
      "Iteration 156, loss = 0.46708157\n",
      "Iteration 157, loss = 0.46649186\n",
      "Iteration 158, loss = 0.46588409\n",
      "Iteration 159, loss = 0.46529989\n",
      "Iteration 160, loss = 0.46473144\n",
      "Iteration 161, loss = 0.46410714\n",
      "Iteration 162, loss = 0.46354096\n",
      "Iteration 163, loss = 0.46304205\n",
      "Iteration 164, loss = 0.46244802\n",
      "Iteration 165, loss = 0.46185183\n",
      "Iteration 166, loss = 0.46134556\n",
      "Iteration 167, loss = 0.46079433\n",
      "Iteration 168, loss = 0.46032057\n",
      "Iteration 169, loss = 0.45983412\n",
      "Iteration 170, loss = 0.45928640\n",
      "Iteration 171, loss = 0.45885802\n",
      "Iteration 172, loss = 0.45831722\n",
      "Iteration 173, loss = 0.45784375\n",
      "Iteration 174, loss = 0.45741232\n",
      "Iteration 175, loss = 0.45688561\n",
      "Iteration 176, loss = 0.45638856\n",
      "Iteration 177, loss = 0.45602025\n",
      "Iteration 178, loss = 0.45556166\n",
      "Iteration 179, loss = 0.45507143\n",
      "Iteration 180, loss = 0.45468795\n",
      "Iteration 181, loss = 0.45423853\n",
      "Iteration 182, loss = 0.45374334\n",
      "Iteration 183, loss = 0.45339367\n",
      "Iteration 184, loss = 0.45295416\n",
      "Iteration 185, loss = 0.45250375\n",
      "Iteration 186, loss = 0.45211899\n",
      "Iteration 187, loss = 0.45173272\n",
      "Iteration 188, loss = 0.45135708\n",
      "Iteration 189, loss = 0.45097763\n",
      "Iteration 190, loss = 0.45056824\n",
      "Iteration 191, loss = 0.45021015\n",
      "Iteration 192, loss = 0.44985261\n",
      "Iteration 193, loss = 0.44943307\n",
      "Iteration 194, loss = 0.44913844\n",
      "Iteration 195, loss = 0.44872360\n",
      "Iteration 196, loss = 0.44834467\n",
      "Iteration 197, loss = 0.44799233\n",
      "Iteration 198, loss = 0.44767900\n",
      "Iteration 199, loss = 0.44733594\n",
      "Iteration 200, loss = 0.44699956\n",
      "Iteration 201, loss = 0.44663599\n",
      "Iteration 202, loss = 0.44632958\n",
      "Iteration 203, loss = 0.44597325\n",
      "Iteration 204, loss = 0.44564657\n",
      "Iteration 205, loss = 0.44530010\n",
      "Iteration 206, loss = 0.44501345\n",
      "Iteration 207, loss = 0.44467210\n",
      "Iteration 208, loss = 0.44440328\n",
      "Iteration 209, loss = 0.44413371\n",
      "Iteration 210, loss = 0.44379471\n",
      "Iteration 211, loss = 0.44351033\n",
      "Iteration 212, loss = 0.44317653\n",
      "Iteration 213, loss = 0.44280836\n",
      "Iteration 214, loss = 0.44259723\n",
      "Iteration 215, loss = 0.44230295\n",
      "Iteration 216, loss = 0.44209282\n",
      "Iteration 217, loss = 0.44178568\n",
      "Iteration 218, loss = 0.44143187\n",
      "Iteration 219, loss = 0.44122396\n",
      "Iteration 220, loss = 0.44096305\n",
      "Iteration 221, loss = 0.44068402\n",
      "Iteration 222, loss = 0.44046111\n",
      "Iteration 223, loss = 0.44011612\n",
      "Iteration 224, loss = 0.43988748\n",
      "Iteration 225, loss = 0.43959216\n",
      "Iteration 226, loss = 0.43938659\n",
      "Iteration 227, loss = 0.43910440\n",
      "Iteration 228, loss = 0.43888448\n",
      "Iteration 229, loss = 0.43865551\n",
      "Iteration 230, loss = 0.43842720\n",
      "Iteration 231, loss = 0.43808223\n",
      "Iteration 232, loss = 0.43784267\n",
      "Iteration 233, loss = 0.43762366\n",
      "Iteration 234, loss = 0.43747712\n",
      "Iteration 235, loss = 0.43718145\n",
      "Iteration 236, loss = 0.43696850\n",
      "Iteration 237, loss = 0.43681704\n",
      "Iteration 238, loss = 0.43651225\n",
      "Iteration 239, loss = 0.43626271\n",
      "Iteration 240, loss = 0.43609643\n",
      "Iteration 241, loss = 0.43590186\n",
      "Iteration 242, loss = 0.43562362\n",
      "Iteration 243, loss = 0.43542388\n",
      "Iteration 244, loss = 0.43520728\n",
      "Iteration 245, loss = 0.43502225\n",
      "Iteration 246, loss = 0.43479735\n",
      "Iteration 247, loss = 0.43456582\n",
      "Iteration 248, loss = 0.43434348\n",
      "Iteration 249, loss = 0.43415393\n",
      "Iteration 250, loss = 0.43398926\n",
      "Iteration 251, loss = 0.43372719\n",
      "Iteration 252, loss = 0.43353941\n",
      "Iteration 253, loss = 0.43335881\n",
      "Iteration 254, loss = 0.43323263\n",
      "Iteration 255, loss = 0.43294669\n",
      "Iteration 256, loss = 0.43272511\n",
      "Iteration 257, loss = 0.43257067\n",
      "Iteration 258, loss = 0.43238238\n",
      "Iteration 259, loss = 0.43212227\n",
      "Iteration 260, loss = 0.43200385\n",
      "Iteration 261, loss = 0.43182064\n",
      "Iteration 262, loss = 0.43161745\n",
      "Iteration 263, loss = 0.43148917\n",
      "Iteration 264, loss = 0.43123383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, loss = 0.43107843\n",
      "Iteration 266, loss = 0.43097590\n",
      "Iteration 267, loss = 0.43067744\n",
      "Iteration 268, loss = 0.43055784\n",
      "Iteration 269, loss = 0.43045001\n",
      "Iteration 270, loss = 0.43019967\n",
      "Iteration 271, loss = 0.43002311\n",
      "Iteration 272, loss = 0.42988959\n",
      "Iteration 273, loss = 0.42972116\n",
      "Iteration 274, loss = 0.42955261\n",
      "Iteration 275, loss = 0.42940428\n",
      "Iteration 276, loss = 0.42921846\n",
      "Iteration 277, loss = 0.42902830\n",
      "Iteration 278, loss = 0.42882333\n",
      "Iteration 279, loss = 0.42868703\n",
      "Iteration 280, loss = 0.42853544\n",
      "Iteration 281, loss = 0.42842656\n",
      "Iteration 282, loss = 0.42815257\n",
      "Iteration 283, loss = 0.42809484\n",
      "Iteration 284, loss = 0.42796942\n",
      "Iteration 285, loss = 0.42780447\n",
      "Iteration 286, loss = 0.42759836\n",
      "Iteration 287, loss = 0.42747326\n",
      "Iteration 288, loss = 0.42732547\n",
      "Iteration 289, loss = 0.42712164\n",
      "Iteration 290, loss = 0.42696155\n",
      "Iteration 291, loss = 0.42683671\n",
      "Iteration 292, loss = 0.42670319\n",
      "Iteration 293, loss = 0.42655714\n",
      "Iteration 294, loss = 0.42640216\n",
      "Iteration 295, loss = 0.42620260\n",
      "Iteration 296, loss = 0.42620616\n",
      "Iteration 297, loss = 0.42601189\n",
      "Iteration 298, loss = 0.42580396\n",
      "Iteration 299, loss = 0.42567700\n",
      "Iteration 300, loss = 0.42557092\n",
      "Iteration 301, loss = 0.42540731\n",
      "Iteration 302, loss = 0.42532689\n",
      "Iteration 303, loss = 0.42516559\n",
      "Iteration 304, loss = 0.42505190\n",
      "Iteration 305, loss = 0.42490890\n",
      "Iteration 306, loss = 0.42481144\n",
      "Iteration 307, loss = 0.42457264\n",
      "Iteration 308, loss = 0.42445433\n",
      "Iteration 309, loss = 0.42433918\n",
      "Iteration 310, loss = 0.42421744\n",
      "Iteration 311, loss = 0.42404745\n",
      "Iteration 312, loss = 0.42395826\n",
      "Iteration 313, loss = 0.42386331\n",
      "Iteration 314, loss = 0.42362814\n",
      "Iteration 315, loss = 0.42356095\n",
      "Iteration 316, loss = 0.42340818\n",
      "Iteration 317, loss = 0.42337765\n",
      "Iteration 318, loss = 0.42320399\n",
      "Iteration 319, loss = 0.42308451\n",
      "Iteration 320, loss = 0.42295587\n",
      "Iteration 321, loss = 0.42283430\n",
      "Iteration 322, loss = 0.42272226\n",
      "Iteration 323, loss = 0.42253519\n",
      "Iteration 324, loss = 0.42250807\n",
      "Iteration 325, loss = 0.42231659\n",
      "Iteration 326, loss = 0.42225030\n",
      "Iteration 327, loss = 0.42211338\n",
      "Iteration 328, loss = 0.42190218\n",
      "Iteration 329, loss = 0.42187953\n",
      "Iteration 330, loss = 0.42173391\n",
      "Iteration 331, loss = 0.42161594\n",
      "Iteration 332, loss = 0.42154604\n",
      "Iteration 333, loss = 0.42141226\n",
      "Iteration 334, loss = 0.42130542\n",
      "Iteration 335, loss = 0.42120839\n",
      "Iteration 336, loss = 0.42102649\n",
      "Iteration 337, loss = 0.42093936\n",
      "Iteration 338, loss = 0.42085822\n",
      "Iteration 339, loss = 0.42073167\n",
      "Iteration 340, loss = 0.42061056\n",
      "Iteration 341, loss = 0.42051041\n",
      "Iteration 342, loss = 0.42040366\n",
      "Iteration 343, loss = 0.42031255\n",
      "Iteration 344, loss = 0.42021117\n",
      "Iteration 345, loss = 0.42007207\n",
      "Iteration 346, loss = 0.41997118\n",
      "Iteration 347, loss = 0.41989561\n",
      "Iteration 348, loss = 0.41977896\n",
      "Iteration 349, loss = 0.41959274\n",
      "Iteration 350, loss = 0.41948287\n",
      "Iteration 351, loss = 0.41947127\n",
      "Iteration 352, loss = 0.41930187\n",
      "Iteration 353, loss = 0.41930578\n",
      "Iteration 354, loss = 0.41911645\n",
      "Iteration 355, loss = 0.41905367\n",
      "Iteration 356, loss = 0.41894333\n",
      "Iteration 357, loss = 0.41887370\n",
      "Iteration 358, loss = 0.41870703\n",
      "Iteration 359, loss = 0.41860343\n",
      "Iteration 360, loss = 0.41851845\n",
      "Iteration 361, loss = 0.41843216\n",
      "Iteration 362, loss = 0.41834887\n",
      "Iteration 363, loss = 0.41820064\n",
      "Iteration 364, loss = 0.41815646\n",
      "Iteration 365, loss = 0.41805012\n",
      "Iteration 366, loss = 0.41793830\n",
      "Iteration 367, loss = 0.41786729\n",
      "Iteration 368, loss = 0.41779683\n",
      "Iteration 369, loss = 0.41767250\n",
      "Iteration 370, loss = 0.41752585\n",
      "Iteration 371, loss = 0.41746504\n",
      "Iteration 372, loss = 0.41734701\n",
      "Iteration 373, loss = 0.41728384\n",
      "Iteration 374, loss = 0.41715876\n",
      "Iteration 375, loss = 0.41711545\n",
      "Iteration 376, loss = 0.41701218\n",
      "Iteration 377, loss = 0.41689692\n",
      "Iteration 378, loss = 0.41680116\n",
      "Iteration 379, loss = 0.41667236\n",
      "Iteration 380, loss = 0.41670188\n",
      "Iteration 381, loss = 0.41654862\n",
      "Iteration 382, loss = 0.41640838\n",
      "Iteration 383, loss = 0.41644521\n",
      "Iteration 384, loss = 0.41625778\n",
      "Iteration 385, loss = 0.41617342\n",
      "Iteration 386, loss = 0.41614863\n",
      "Iteration 387, loss = 0.41596833\n",
      "Iteration 388, loss = 0.41594444\n",
      "Iteration 389, loss = 0.41591857\n",
      "Iteration 390, loss = 0.41578655\n",
      "Iteration 391, loss = 0.41571163\n",
      "Iteration 392, loss = 0.41563367\n",
      "Iteration 393, loss = 0.41550282\n",
      "Iteration 394, loss = 0.41546213\n",
      "Iteration 395, loss = 0.41539274\n",
      "Iteration 396, loss = 0.41528106\n",
      "Iteration 397, loss = 0.41515616\n",
      "Iteration 398, loss = 0.41510197\n",
      "Iteration 399, loss = 0.41503797\n",
      "Iteration 400, loss = 0.41490622\n",
      "Iteration 401, loss = 0.41480870\n",
      "Iteration 402, loss = 0.41471115\n",
      "Iteration 403, loss = 0.41466596\n",
      "Iteration 404, loss = 0.41462029\n",
      "Iteration 405, loss = 0.41456340\n",
      "Iteration 406, loss = 0.41444732\n",
      "Iteration 407, loss = 0.41435051\n",
      "Iteration 408, loss = 0.41426534\n",
      "Iteration 409, loss = 0.41422083\n",
      "Iteration 410, loss = 0.41413025\n",
      "Iteration 411, loss = 0.41409634\n",
      "Iteration 412, loss = 0.41395222\n",
      "Iteration 413, loss = 0.41386366\n",
      "Iteration 414, loss = 0.41380470\n",
      "Iteration 415, loss = 0.41369300\n",
      "Iteration 416, loss = 0.41366090\n",
      "Iteration 417, loss = 0.41359326\n",
      "Iteration 418, loss = 0.41355700\n",
      "Iteration 419, loss = 0.41342773\n",
      "Iteration 420, loss = 0.41335787\n",
      "Iteration 421, loss = 0.41328087\n",
      "Iteration 422, loss = 0.41315167\n",
      "Iteration 423, loss = 0.41312915\n",
      "Iteration 424, loss = 0.41312680\n",
      "Iteration 425, loss = 0.41302969\n",
      "Iteration 426, loss = 0.41289226\n",
      "Iteration 427, loss = 0.41282159\n",
      "Iteration 428, loss = 0.41275741\n",
      "Iteration 429, loss = 0.41272454\n",
      "Iteration 430, loss = 0.41263901\n",
      "Iteration 431, loss = 0.41255737\n",
      "Iteration 432, loss = 0.41247599\n",
      "Iteration 433, loss = 0.41234184\n",
      "Iteration 434, loss = 0.41240696\n",
      "Iteration 435, loss = 0.41228716\n",
      "Iteration 436, loss = 0.41218378\n",
      "Iteration 437, loss = 0.41211081\n",
      "Iteration 438, loss = 0.41207536\n",
      "Iteration 439, loss = 0.41194351\n",
      "Iteration 440, loss = 0.41185794\n",
      "Iteration 441, loss = 0.41179318\n",
      "Iteration 442, loss = 0.41173449\n",
      "Iteration 443, loss = 0.41162401\n",
      "Iteration 444, loss = 0.41158139\n",
      "Iteration 445, loss = 0.41155135\n",
      "Iteration 446, loss = 0.41150128\n",
      "Iteration 447, loss = 0.41143376\n",
      "Iteration 448, loss = 0.41137883\n",
      "Iteration 449, loss = 0.41124751\n",
      "Iteration 450, loss = 0.41128532\n",
      "Iteration 451, loss = 0.41114562\n",
      "Iteration 452, loss = 0.41105998\n",
      "Iteration 453, loss = 0.41105023\n",
      "Iteration 454, loss = 0.41083720\n",
      "Iteration 455, loss = 0.41090241\n",
      "Iteration 456, loss = 0.41078341\n",
      "Iteration 457, loss = 0.41072596\n",
      "Iteration 458, loss = 0.41074953\n",
      "Iteration 459, loss = 0.41064911\n",
      "Iteration 460, loss = 0.41055964\n",
      "Iteration 461, loss = 0.41043799\n",
      "Iteration 462, loss = 0.41042787\n",
      "Iteration 463, loss = 0.41029796\n",
      "Iteration 464, loss = 0.41029035\n",
      "Iteration 465, loss = 0.41029528\n",
      "Iteration 466, loss = 0.41016914\n",
      "Iteration 467, loss = 0.41008337\n",
      "Iteration 468, loss = 0.41005262\n",
      "Iteration 469, loss = 0.40997885\n",
      "Iteration 470, loss = 0.40986467\n",
      "Iteration 471, loss = 0.40989608\n",
      "Iteration 472, loss = 0.40984011\n",
      "Iteration 473, loss = 0.40970798\n",
      "Iteration 474, loss = 0.40966730\n",
      "Iteration 475, loss = 0.40959164\n",
      "Iteration 476, loss = 0.40956705\n",
      "Iteration 477, loss = 0.40954041\n",
      "Iteration 478, loss = 0.40940003\n",
      "Iteration 479, loss = 0.40936632\n",
      "Iteration 480, loss = 0.40932248\n",
      "Iteration 481, loss = 0.40927531\n",
      "Iteration 482, loss = 0.40917154\n",
      "Iteration 483, loss = 0.40911301\n",
      "Iteration 484, loss = 0.40904463\n",
      "Iteration 485, loss = 0.40899821\n",
      "Iteration 486, loss = 0.40886508\n",
      "Iteration 487, loss = 0.40888125\n",
      "Iteration 488, loss = 0.40881297\n",
      "Iteration 489, loss = 0.40879976\n",
      "Iteration 490, loss = 0.40875085\n",
      "Iteration 491, loss = 0.40870100\n",
      "Iteration 492, loss = 0.40856229\n",
      "Iteration 493, loss = 0.40852012\n",
      "Iteration 494, loss = 0.40851993\n",
      "Iteration 495, loss = 0.40837486\n",
      "Iteration 496, loss = 0.40838694\n",
      "Iteration 497, loss = 0.40833701\n",
      "Iteration 498, loss = 0.40825671\n",
      "Iteration 499, loss = 0.40822126\n",
      "Iteration 500, loss = 0.40806952\n",
      "Iteration 1, loss = 1.93778438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.80407237\n",
      "Iteration 3, loss = 1.69644563\n",
      "Iteration 4, loss = 1.60645477\n",
      "Iteration 5, loss = 1.52836618\n",
      "Iteration 6, loss = 1.45921759\n",
      "Iteration 7, loss = 1.39771956\n",
      "Iteration 8, loss = 1.34285880\n",
      "Iteration 9, loss = 1.29380420\n",
      "Iteration 10, loss = 1.24972655\n",
      "Iteration 11, loss = 1.20980773\n",
      "Iteration 12, loss = 1.17349298\n",
      "Iteration 13, loss = 1.14026507\n",
      "Iteration 14, loss = 1.10961463\n",
      "Iteration 15, loss = 1.08128755\n",
      "Iteration 16, loss = 1.05492592\n",
      "Iteration 17, loss = 1.03037437\n",
      "Iteration 18, loss = 1.00745760\n",
      "Iteration 19, loss = 0.98598635\n",
      "Iteration 20, loss = 0.96582317\n",
      "Iteration 21, loss = 0.94684235\n",
      "Iteration 22, loss = 0.92889932\n",
      "Iteration 23, loss = 0.91188306\n",
      "Iteration 24, loss = 0.89576511\n",
      "Iteration 25, loss = 0.88044927\n",
      "Iteration 26, loss = 0.86591056\n",
      "Iteration 27, loss = 0.85218432\n",
      "Iteration 28, loss = 0.83927137\n",
      "Iteration 29, loss = 0.82707665\n",
      "Iteration 30, loss = 0.81556300\n",
      "Iteration 31, loss = 0.80467743\n",
      "Iteration 32, loss = 0.79438741\n",
      "Iteration 33, loss = 0.78463533\n",
      "Iteration 34, loss = 0.77539252\n",
      "Iteration 35, loss = 0.76662572\n",
      "Iteration 36, loss = 0.75831114\n",
      "Iteration 37, loss = 0.75035950\n",
      "Iteration 38, loss = 0.74276696\n",
      "Iteration 39, loss = 0.73551397\n",
      "Iteration 40, loss = 0.72855699\n",
      "Iteration 41, loss = 0.72191113\n",
      "Iteration 42, loss = 0.71551517\n",
      "Iteration 43, loss = 0.70938198\n",
      "Iteration 44, loss = 0.70346479\n",
      "Iteration 45, loss = 0.69778458\n",
      "Iteration 46, loss = 0.69232794\n",
      "Iteration 47, loss = 0.68704256\n",
      "Iteration 48, loss = 0.68195053\n",
      "Iteration 49, loss = 0.67703548\n",
      "Iteration 50, loss = 0.67227992\n",
      "Iteration 51, loss = 0.66766824\n",
      "Iteration 52, loss = 0.66321375\n",
      "Iteration 53, loss = 0.65888170\n",
      "Iteration 54, loss = 0.65468407\n",
      "Iteration 55, loss = 0.65059723\n",
      "Iteration 56, loss = 0.64660353\n",
      "Iteration 57, loss = 0.64276045\n",
      "Iteration 58, loss = 0.63896578\n",
      "Iteration 59, loss = 0.63525774\n",
      "Iteration 60, loss = 0.63166965\n",
      "Iteration 61, loss = 0.62814204\n",
      "Iteration 62, loss = 0.62469507\n",
      "Iteration 63, loss = 0.62134754\n",
      "Iteration 64, loss = 0.61804488\n",
      "Iteration 65, loss = 0.61482139\n",
      "Iteration 66, loss = 0.61163625\n",
      "Iteration 67, loss = 0.60854012\n",
      "Iteration 68, loss = 0.60546817\n",
      "Iteration 69, loss = 0.60251455\n",
      "Iteration 70, loss = 0.59954160\n",
      "Iteration 71, loss = 0.59667775\n",
      "Iteration 72, loss = 0.59381577\n",
      "Iteration 73, loss = 0.59103341\n",
      "Iteration 74, loss = 0.58829159\n",
      "Iteration 75, loss = 0.58560019\n",
      "Iteration 76, loss = 0.58291453\n",
      "Iteration 77, loss = 0.58030436\n",
      "Iteration 78, loss = 0.57770631\n",
      "Iteration 79, loss = 0.57516788\n",
      "Iteration 80, loss = 0.57268250\n",
      "Iteration 81, loss = 0.57022223\n",
      "Iteration 82, loss = 0.56777766\n",
      "Iteration 83, loss = 0.56535134\n",
      "Iteration 84, loss = 0.56304746\n",
      "Iteration 85, loss = 0.56070798\n",
      "Iteration 86, loss = 0.55842173\n",
      "Iteration 87, loss = 0.55614755\n",
      "Iteration 88, loss = 0.55395692\n",
      "Iteration 89, loss = 0.55175814\n",
      "Iteration 90, loss = 0.54964434\n",
      "Iteration 91, loss = 0.54751544\n",
      "Iteration 92, loss = 0.54544543\n",
      "Iteration 93, loss = 0.54341250\n",
      "Iteration 94, loss = 0.54146259\n",
      "Iteration 95, loss = 0.53946153\n",
      "Iteration 96, loss = 0.53753300\n",
      "Iteration 97, loss = 0.53561159\n",
      "Iteration 98, loss = 0.53372470\n",
      "Iteration 99, loss = 0.53192274\n",
      "Iteration 100, loss = 0.53012947\n",
      "Iteration 101, loss = 0.52834972\n",
      "Iteration 102, loss = 0.52660368\n",
      "Iteration 103, loss = 0.52494045\n",
      "Iteration 104, loss = 0.52327145\n",
      "Iteration 105, loss = 0.52163917\n",
      "Iteration 106, loss = 0.52008656\n",
      "Iteration 107, loss = 0.51844592\n",
      "Iteration 108, loss = 0.51692833\n",
      "Iteration 109, loss = 0.51538523\n",
      "Iteration 110, loss = 0.51397007\n",
      "Iteration 111, loss = 0.51250670\n",
      "Iteration 112, loss = 0.51109315\n",
      "Iteration 113, loss = 0.50969933\n",
      "Iteration 114, loss = 0.50831731\n",
      "Iteration 115, loss = 0.50698188\n",
      "Iteration 116, loss = 0.50567103\n",
      "Iteration 117, loss = 0.50438763\n",
      "Iteration 118, loss = 0.50311415\n",
      "Iteration 119, loss = 0.50190438\n",
      "Iteration 120, loss = 0.50071731\n",
      "Iteration 121, loss = 0.49952340\n",
      "Iteration 122, loss = 0.49836600\n",
      "Iteration 123, loss = 0.49725575\n",
      "Iteration 124, loss = 0.49610068\n",
      "Iteration 125, loss = 0.49501116\n",
      "Iteration 126, loss = 0.49398124\n",
      "Iteration 127, loss = 0.49287894\n",
      "Iteration 128, loss = 0.49185836\n",
      "Iteration 129, loss = 0.49088741\n",
      "Iteration 130, loss = 0.48983915\n",
      "Iteration 131, loss = 0.48887038\n",
      "Iteration 132, loss = 0.48791338\n",
      "Iteration 133, loss = 0.48695211\n",
      "Iteration 134, loss = 0.48607510\n",
      "Iteration 135, loss = 0.48514968\n",
      "Iteration 136, loss = 0.48428536\n",
      "Iteration 137, loss = 0.48338600\n",
      "Iteration 138, loss = 0.48254139\n",
      "Iteration 139, loss = 0.48168239\n",
      "Iteration 140, loss = 0.48087914\n",
      "Iteration 141, loss = 0.48012544\n",
      "Iteration 142, loss = 0.47930848\n",
      "Iteration 143, loss = 0.47852320\n",
      "Iteration 144, loss = 0.47779219\n",
      "Iteration 145, loss = 0.47703007\n",
      "Iteration 146, loss = 0.47627237\n",
      "Iteration 147, loss = 0.47559248\n",
      "Iteration 148, loss = 0.47487172\n",
      "Iteration 149, loss = 0.47414358\n",
      "Iteration 150, loss = 0.47345057\n",
      "Iteration 151, loss = 0.47282188\n",
      "Iteration 152, loss = 0.47216188\n",
      "Iteration 153, loss = 0.47150041\n",
      "Iteration 154, loss = 0.47085331\n",
      "Iteration 155, loss = 0.47020852\n",
      "Iteration 156, loss = 0.46960910\n",
      "Iteration 157, loss = 0.46894665\n",
      "Iteration 158, loss = 0.46835952\n",
      "Iteration 159, loss = 0.46775517\n",
      "Iteration 160, loss = 0.46722743\n",
      "Iteration 161, loss = 0.46665153\n",
      "Iteration 162, loss = 0.46607128\n",
      "Iteration 163, loss = 0.46550905\n",
      "Iteration 164, loss = 0.46499412\n",
      "Iteration 165, loss = 0.46453138\n",
      "Iteration 166, loss = 0.46390745\n",
      "Iteration 167, loss = 0.46340638\n",
      "Iteration 168, loss = 0.46290677\n",
      "Iteration 169, loss = 0.46237239\n",
      "Iteration 170, loss = 0.46188880\n",
      "Iteration 171, loss = 0.46140050\n",
      "Iteration 172, loss = 0.46089367\n",
      "Iteration 173, loss = 0.46045008\n",
      "Iteration 174, loss = 0.45997362\n",
      "Iteration 175, loss = 0.45950182\n",
      "Iteration 176, loss = 0.45908433\n",
      "Iteration 177, loss = 0.45857399\n",
      "Iteration 178, loss = 0.45815873\n",
      "Iteration 179, loss = 0.45774753\n",
      "Iteration 180, loss = 0.45732535\n",
      "Iteration 181, loss = 0.45691108\n",
      "Iteration 182, loss = 0.45637079\n",
      "Iteration 183, loss = 0.45604513\n",
      "Iteration 184, loss = 0.45560516\n",
      "Iteration 185, loss = 0.45519785\n",
      "Iteration 186, loss = 0.45473967\n",
      "Iteration 187, loss = 0.45438143\n",
      "Iteration 188, loss = 0.45404352\n",
      "Iteration 189, loss = 0.45362792\n",
      "Iteration 190, loss = 0.45324058\n",
      "Iteration 191, loss = 0.45289952\n",
      "Iteration 192, loss = 0.45251166\n",
      "Iteration 193, loss = 0.45210199\n",
      "Iteration 194, loss = 0.45177671\n",
      "Iteration 195, loss = 0.45140592\n",
      "Iteration 196, loss = 0.45108354\n",
      "Iteration 197, loss = 0.45068216\n",
      "Iteration 198, loss = 0.45033519\n",
      "Iteration 199, loss = 0.45006506\n",
      "Iteration 200, loss = 0.44967089\n",
      "Iteration 201, loss = 0.44931785\n",
      "Iteration 202, loss = 0.44901662\n",
      "Iteration 203, loss = 0.44868942\n",
      "Iteration 204, loss = 0.44843468\n",
      "Iteration 205, loss = 0.44807396\n",
      "Iteration 206, loss = 0.44777672\n",
      "Iteration 207, loss = 0.44742101\n",
      "Iteration 208, loss = 0.44717726\n",
      "Iteration 209, loss = 0.44685910\n",
      "Iteration 210, loss = 0.44658770\n",
      "Iteration 211, loss = 0.44628330\n",
      "Iteration 212, loss = 0.44597445\n",
      "Iteration 213, loss = 0.44565541\n",
      "Iteration 214, loss = 0.44541239\n",
      "Iteration 215, loss = 0.44513736\n",
      "Iteration 216, loss = 0.44482404\n",
      "Iteration 217, loss = 0.44449410\n",
      "Iteration 218, loss = 0.44425111\n",
      "Iteration 219, loss = 0.44397208\n",
      "Iteration 220, loss = 0.44371505\n",
      "Iteration 221, loss = 0.44346055\n",
      "Iteration 222, loss = 0.44316437\n",
      "Iteration 223, loss = 0.44293932\n",
      "Iteration 224, loss = 0.44262993\n",
      "Iteration 225, loss = 0.44245759\n",
      "Iteration 226, loss = 0.44215409\n",
      "Iteration 227, loss = 0.44192090\n",
      "Iteration 228, loss = 0.44172154\n",
      "Iteration 229, loss = 0.44150459\n",
      "Iteration 230, loss = 0.44122166\n",
      "Iteration 231, loss = 0.44091482\n",
      "Iteration 232, loss = 0.44069819\n",
      "Iteration 233, loss = 0.44053646\n",
      "Iteration 234, loss = 0.44026118\n",
      "Iteration 235, loss = 0.44002543\n",
      "Iteration 236, loss = 0.43977850\n",
      "Iteration 237, loss = 0.43954982\n",
      "Iteration 238, loss = 0.43925580\n",
      "Iteration 239, loss = 0.43908309\n",
      "Iteration 240, loss = 0.43885285\n",
      "Iteration 241, loss = 0.43859180\n",
      "Iteration 242, loss = 0.43843918\n",
      "Iteration 243, loss = 0.43823431\n",
      "Iteration 244, loss = 0.43801958\n",
      "Iteration 245, loss = 0.43780976\n",
      "Iteration 246, loss = 0.43756022\n",
      "Iteration 247, loss = 0.43738126\n",
      "Iteration 248, loss = 0.43716962\n",
      "Iteration 249, loss = 0.43697014\n",
      "Iteration 250, loss = 0.43674767\n",
      "Iteration 251, loss = 0.43656312\n",
      "Iteration 252, loss = 0.43637550\n",
      "Iteration 253, loss = 0.43612256\n",
      "Iteration 254, loss = 0.43599561\n",
      "Iteration 255, loss = 0.43576961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.43558202\n",
      "Iteration 257, loss = 0.43540878\n",
      "Iteration 258, loss = 0.43524213\n",
      "Iteration 259, loss = 0.43501018\n",
      "Iteration 260, loss = 0.43481840\n",
      "Iteration 261, loss = 0.43466836\n",
      "Iteration 262, loss = 0.43449045\n",
      "Iteration 263, loss = 0.43425589\n",
      "Iteration 264, loss = 0.43409568\n",
      "Iteration 265, loss = 0.43391379\n",
      "Iteration 266, loss = 0.43376815\n",
      "Iteration 267, loss = 0.43356933\n",
      "Iteration 268, loss = 0.43338067\n",
      "Iteration 269, loss = 0.43325585\n",
      "Iteration 270, loss = 0.43307240\n",
      "Iteration 271, loss = 0.43284603\n",
      "Iteration 272, loss = 0.43269632\n",
      "Iteration 273, loss = 0.43252119\n",
      "Iteration 274, loss = 0.43235234\n",
      "Iteration 275, loss = 0.43222710\n",
      "Iteration 276, loss = 0.43202939\n",
      "Iteration 277, loss = 0.43188779\n",
      "Iteration 278, loss = 0.43166002\n",
      "Iteration 279, loss = 0.43157501\n",
      "Iteration 280, loss = 0.43143390\n",
      "Iteration 281, loss = 0.43120684\n",
      "Iteration 282, loss = 0.43105621\n",
      "Iteration 283, loss = 0.43088329\n",
      "Iteration 284, loss = 0.43077129\n",
      "Iteration 285, loss = 0.43055516\n",
      "Iteration 286, loss = 0.43049674\n",
      "Iteration 287, loss = 0.43032503\n",
      "Iteration 288, loss = 0.43017811\n",
      "Iteration 289, loss = 0.43001279\n",
      "Iteration 290, loss = 0.42986940\n",
      "Iteration 291, loss = 0.42972723\n",
      "Iteration 292, loss = 0.42958348\n",
      "Iteration 293, loss = 0.42943005\n",
      "Iteration 294, loss = 0.42929168\n",
      "Iteration 295, loss = 0.42911647\n",
      "Iteration 296, loss = 0.42892101\n",
      "Iteration 297, loss = 0.42887029\n",
      "Iteration 298, loss = 0.42872158\n",
      "Iteration 299, loss = 0.42850886\n",
      "Iteration 300, loss = 0.42839459\n",
      "Iteration 301, loss = 0.42820746\n",
      "Iteration 302, loss = 0.42815517\n",
      "Iteration 303, loss = 0.42803804\n",
      "Iteration 304, loss = 0.42782434\n",
      "Iteration 305, loss = 0.42773779\n",
      "Iteration 306, loss = 0.42762952\n",
      "Iteration 307, loss = 0.42749623\n",
      "Iteration 308, loss = 0.42733041\n",
      "Iteration 309, loss = 0.42721254\n",
      "Iteration 310, loss = 0.42704545\n",
      "Iteration 311, loss = 0.42691645\n",
      "Iteration 312, loss = 0.42683793\n",
      "Iteration 313, loss = 0.42669773\n",
      "Iteration 314, loss = 0.42656636\n",
      "Iteration 315, loss = 0.42644281\n",
      "Iteration 316, loss = 0.42625752\n",
      "Iteration 317, loss = 0.42612805\n",
      "Iteration 318, loss = 0.42605675\n",
      "Iteration 319, loss = 0.42594997\n",
      "Iteration 320, loss = 0.42583394\n",
      "Iteration 321, loss = 0.42577793\n",
      "Iteration 322, loss = 0.42568011\n",
      "Iteration 323, loss = 0.42542141\n",
      "Iteration 324, loss = 0.42534262\n",
      "Iteration 325, loss = 0.42520838\n",
      "Iteration 326, loss = 0.42505357\n",
      "Iteration 327, loss = 0.42497272\n",
      "Iteration 328, loss = 0.42481854\n",
      "Iteration 329, loss = 0.42481929\n",
      "Iteration 330, loss = 0.42462566\n",
      "Iteration 331, loss = 0.42446318\n",
      "Iteration 332, loss = 0.42437185\n",
      "Iteration 333, loss = 0.42423924\n",
      "Iteration 334, loss = 0.42415583\n",
      "Iteration 335, loss = 0.42404944\n",
      "Iteration 336, loss = 0.42399081\n",
      "Iteration 337, loss = 0.42380603\n",
      "Iteration 338, loss = 0.42370142\n",
      "Iteration 339, loss = 0.42358594\n",
      "Iteration 340, loss = 0.42352500\n",
      "Iteration 341, loss = 0.42337018\n",
      "Iteration 342, loss = 0.42330503\n",
      "Iteration 343, loss = 0.42313330\n",
      "Iteration 344, loss = 0.42311346\n",
      "Iteration 345, loss = 0.42291135\n",
      "Iteration 346, loss = 0.42279522\n",
      "Iteration 347, loss = 0.42272293\n",
      "Iteration 348, loss = 0.42270115\n",
      "Iteration 349, loss = 0.42252095\n",
      "Iteration 350, loss = 0.42246213\n",
      "Iteration 351, loss = 0.42230354\n",
      "Iteration 352, loss = 0.42226057\n",
      "Iteration 353, loss = 0.42206632\n",
      "Iteration 354, loss = 0.42198850\n",
      "Iteration 355, loss = 0.42192030\n",
      "Iteration 356, loss = 0.42181443\n",
      "Iteration 357, loss = 0.42167892\n",
      "Iteration 358, loss = 0.42157485\n",
      "Iteration 359, loss = 0.42155504\n",
      "Iteration 360, loss = 0.42145173\n",
      "Iteration 361, loss = 0.42133625\n",
      "Iteration 362, loss = 0.42115430\n",
      "Iteration 363, loss = 0.42107504\n",
      "Iteration 364, loss = 0.42099353\n",
      "Iteration 365, loss = 0.42093191\n",
      "Iteration 366, loss = 0.42084872\n",
      "Iteration 367, loss = 0.42071511\n",
      "Iteration 368, loss = 0.42063565\n",
      "Iteration 369, loss = 0.42052467\n",
      "Iteration 370, loss = 0.42036545\n",
      "Iteration 371, loss = 0.42041742\n",
      "Iteration 372, loss = 0.42022822\n",
      "Iteration 373, loss = 0.42012317\n",
      "Iteration 374, loss = 0.42002921\n",
      "Iteration 375, loss = 0.41992015\n",
      "Iteration 376, loss = 0.41984612\n",
      "Iteration 377, loss = 0.41978111\n",
      "Iteration 378, loss = 0.41966552\n",
      "Iteration 379, loss = 0.41964216\n",
      "Iteration 380, loss = 0.41951105\n",
      "Iteration 381, loss = 0.41937792\n",
      "Iteration 382, loss = 0.41934129\n",
      "Iteration 383, loss = 0.41928525\n",
      "Iteration 384, loss = 0.41913235\n",
      "Iteration 385, loss = 0.41910149\n",
      "Iteration 386, loss = 0.41898125\n",
      "Iteration 387, loss = 0.41887743\n",
      "Iteration 388, loss = 0.41881513\n",
      "Iteration 389, loss = 0.41873095\n",
      "Iteration 390, loss = 0.41865545\n",
      "Iteration 391, loss = 0.41852626\n",
      "Iteration 392, loss = 0.41846919\n",
      "Iteration 393, loss = 0.41830986\n",
      "Iteration 394, loss = 0.41826408\n",
      "Iteration 395, loss = 0.41818723\n",
      "Iteration 396, loss = 0.41815696\n",
      "Iteration 397, loss = 0.41801775\n",
      "Iteration 398, loss = 0.41794519\n",
      "Iteration 399, loss = 0.41786792\n",
      "Iteration 400, loss = 0.41776945\n",
      "Iteration 401, loss = 0.41771702\n",
      "Iteration 402, loss = 0.41763073\n",
      "Iteration 403, loss = 0.41752532\n",
      "Iteration 404, loss = 0.41744521\n",
      "Iteration 405, loss = 0.41732129\n",
      "Iteration 406, loss = 0.41726549\n",
      "Iteration 407, loss = 0.41726940\n",
      "Iteration 408, loss = 0.41711247\n",
      "Iteration 409, loss = 0.41707908\n",
      "Iteration 410, loss = 0.41694162\n",
      "Iteration 411, loss = 0.41687368\n",
      "Iteration 412, loss = 0.41683057\n",
      "Iteration 413, loss = 0.41681879\n",
      "Iteration 414, loss = 0.41667267\n",
      "Iteration 415, loss = 0.41654426\n",
      "Iteration 416, loss = 0.41653498\n",
      "Iteration 417, loss = 0.41642020\n",
      "Iteration 418, loss = 0.41635651\n",
      "Iteration 419, loss = 0.41630917\n",
      "Iteration 420, loss = 0.41624837\n",
      "Iteration 421, loss = 0.41613153\n",
      "Iteration 422, loss = 0.41605808\n",
      "Iteration 423, loss = 0.41598246\n",
      "Iteration 424, loss = 0.41587591\n",
      "Iteration 425, loss = 0.41579418\n",
      "Iteration 426, loss = 0.41573431\n",
      "Iteration 427, loss = 0.41575954\n",
      "Iteration 428, loss = 0.41560609\n",
      "Iteration 429, loss = 0.41551168\n",
      "Iteration 430, loss = 0.41542954\n",
      "Iteration 431, loss = 0.41535970\n",
      "Iteration 432, loss = 0.41531718\n",
      "Iteration 433, loss = 0.41526243\n",
      "Iteration 434, loss = 0.41517186\n",
      "Iteration 435, loss = 0.41509065\n",
      "Iteration 436, loss = 0.41510875\n",
      "Iteration 437, loss = 0.41493633\n",
      "Iteration 438, loss = 0.41488393\n",
      "Iteration 439, loss = 0.41484357\n",
      "Iteration 440, loss = 0.41469407\n",
      "Iteration 441, loss = 0.41461365\n",
      "Iteration 442, loss = 0.41457208\n",
      "Iteration 443, loss = 0.41450680\n",
      "Iteration 444, loss = 0.41450147\n",
      "Iteration 445, loss = 0.41441260\n",
      "Iteration 446, loss = 0.41428443\n",
      "Iteration 447, loss = 0.41418047\n",
      "Iteration 448, loss = 0.41414154\n",
      "Iteration 449, loss = 0.41408464\n",
      "Iteration 450, loss = 0.41400870\n",
      "Iteration 451, loss = 0.41392069\n",
      "Iteration 452, loss = 0.41387884\n",
      "Iteration 453, loss = 0.41380564\n",
      "Iteration 454, loss = 0.41378159\n",
      "Iteration 455, loss = 0.41366163\n",
      "Iteration 456, loss = 0.41363855\n",
      "Iteration 457, loss = 0.41358304\n",
      "Iteration 458, loss = 0.41348893\n",
      "Iteration 459, loss = 0.41336381\n",
      "Iteration 460, loss = 0.41334469\n",
      "Iteration 461, loss = 0.41329884\n",
      "Iteration 462, loss = 0.41319892\n",
      "Iteration 463, loss = 0.41314379\n",
      "Iteration 464, loss = 0.41307541\n",
      "Iteration 465, loss = 0.41299485\n",
      "Iteration 466, loss = 0.41292584\n",
      "Iteration 467, loss = 0.41297001\n",
      "Iteration 468, loss = 0.41282665\n",
      "Iteration 469, loss = 0.41281165\n",
      "Iteration 470, loss = 0.41267108\n",
      "Iteration 471, loss = 0.41266296\n",
      "Iteration 472, loss = 0.41255766\n",
      "Iteration 473, loss = 0.41247067\n",
      "Iteration 474, loss = 0.41246523\n",
      "Iteration 475, loss = 0.41233845\n",
      "Iteration 476, loss = 0.41235181\n",
      "Iteration 477, loss = 0.41226686\n",
      "Iteration 478, loss = 0.41210241\n",
      "Iteration 479, loss = 0.41210994\n",
      "Iteration 480, loss = 0.41210015\n",
      "Iteration 481, loss = 0.41196782\n",
      "Iteration 482, loss = 0.41191409\n",
      "Iteration 483, loss = 0.41195638\n",
      "Iteration 484, loss = 0.41182912\n",
      "Iteration 485, loss = 0.41176756\n",
      "Iteration 486, loss = 0.41167305\n",
      "Iteration 487, loss = 0.41170699\n",
      "Iteration 488, loss = 0.41156749\n",
      "Iteration 489, loss = 0.41154269\n",
      "Iteration 490, loss = 0.41144292\n",
      "Iteration 491, loss = 0.41139548\n",
      "Iteration 492, loss = 0.41131931\n",
      "Iteration 493, loss = 0.41124996\n",
      "Iteration 494, loss = 0.41119599\n",
      "Iteration 495, loss = 0.41104988\n",
      "Iteration 496, loss = 0.41111436\n",
      "Iteration 497, loss = 0.41099387\n",
      "Iteration 498, loss = 0.41087216\n",
      "Iteration 499, loss = 0.41094946\n",
      "Iteration 500, loss = 0.41082557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93677520\n",
      "Iteration 2, loss = 1.80364128\n",
      "Iteration 3, loss = 1.69636445\n",
      "Iteration 4, loss = 1.60659189\n",
      "Iteration 5, loss = 1.52854737\n",
      "Iteration 6, loss = 1.45951517\n",
      "Iteration 7, loss = 1.39800400\n",
      "Iteration 8, loss = 1.34321891\n",
      "Iteration 9, loss = 1.29416607\n",
      "Iteration 10, loss = 1.25008487\n",
      "Iteration 11, loss = 1.21016547\n",
      "Iteration 12, loss = 1.17385672\n",
      "Iteration 13, loss = 1.14054266\n",
      "Iteration 14, loss = 1.10984153\n",
      "Iteration 15, loss = 1.08139786\n",
      "Iteration 16, loss = 1.05498642\n",
      "Iteration 17, loss = 1.03036209\n",
      "Iteration 18, loss = 1.00738049\n",
      "Iteration 19, loss = 0.98581616\n",
      "Iteration 20, loss = 0.96554641\n",
      "Iteration 21, loss = 0.94647430\n",
      "Iteration 22, loss = 0.92845674\n",
      "Iteration 23, loss = 0.91135164\n",
      "Iteration 24, loss = 0.89514070\n",
      "Iteration 25, loss = 0.87972325\n",
      "Iteration 26, loss = 0.86507662\n",
      "Iteration 27, loss = 0.85121818\n",
      "Iteration 28, loss = 0.83818435\n",
      "Iteration 29, loss = 0.82584806\n",
      "Iteration 30, loss = 0.81419906\n",
      "Iteration 31, loss = 0.80320426\n",
      "Iteration 32, loss = 0.79285660\n",
      "Iteration 33, loss = 0.78301372\n",
      "Iteration 34, loss = 0.77372729\n",
      "Iteration 35, loss = 0.76486281\n",
      "Iteration 36, loss = 0.75644296\n",
      "Iteration 37, loss = 0.74842426\n",
      "Iteration 38, loss = 0.74073653\n",
      "Iteration 39, loss = 0.73341484\n",
      "Iteration 40, loss = 0.72641209\n",
      "Iteration 41, loss = 0.71969191\n",
      "Iteration 42, loss = 0.71323960\n",
      "Iteration 43, loss = 0.70704635\n",
      "Iteration 44, loss = 0.70110663\n",
      "Iteration 45, loss = 0.69540513\n",
      "Iteration 46, loss = 0.68987127\n",
      "Iteration 47, loss = 0.68456715\n",
      "Iteration 48, loss = 0.67945731\n",
      "Iteration 49, loss = 0.67453185\n",
      "Iteration 50, loss = 0.66975593\n",
      "Iteration 51, loss = 0.66511854\n",
      "Iteration 52, loss = 0.66064998\n",
      "Iteration 53, loss = 0.65630268\n",
      "Iteration 54, loss = 0.65207945\n",
      "Iteration 55, loss = 0.64795368\n",
      "Iteration 56, loss = 0.64399687\n",
      "Iteration 57, loss = 0.64007353\n",
      "Iteration 58, loss = 0.63628959\n",
      "Iteration 59, loss = 0.63260221\n",
      "Iteration 60, loss = 0.62901033\n",
      "Iteration 61, loss = 0.62545407\n",
      "Iteration 62, loss = 0.62201245\n",
      "Iteration 63, loss = 0.61861256\n",
      "Iteration 64, loss = 0.61535267\n",
      "Iteration 65, loss = 0.61209926\n",
      "Iteration 66, loss = 0.60892856\n",
      "Iteration 67, loss = 0.60581559\n",
      "Iteration 68, loss = 0.60276598\n",
      "Iteration 69, loss = 0.59975864\n",
      "Iteration 70, loss = 0.59680663\n",
      "Iteration 71, loss = 0.59392781\n",
      "Iteration 72, loss = 0.59106437\n",
      "Iteration 73, loss = 0.58826081\n",
      "Iteration 74, loss = 0.58550809\n",
      "Iteration 75, loss = 0.58279969\n",
      "Iteration 76, loss = 0.58014685\n",
      "Iteration 77, loss = 0.57751629\n",
      "Iteration 78, loss = 0.57494891\n",
      "Iteration 79, loss = 0.57236189\n",
      "Iteration 80, loss = 0.56986069\n",
      "Iteration 81, loss = 0.56740500\n",
      "Iteration 82, loss = 0.56499884\n",
      "Iteration 83, loss = 0.56258891\n",
      "Iteration 84, loss = 0.56025696\n",
      "Iteration 85, loss = 0.55792458\n",
      "Iteration 86, loss = 0.55563279\n",
      "Iteration 87, loss = 0.55338567\n",
      "Iteration 88, loss = 0.55116228\n",
      "Iteration 89, loss = 0.54899277\n",
      "Iteration 90, loss = 0.54688504\n",
      "Iteration 91, loss = 0.54476819\n",
      "Iteration 92, loss = 0.54267062\n",
      "Iteration 93, loss = 0.54064122\n",
      "Iteration 94, loss = 0.53867117\n",
      "Iteration 95, loss = 0.53670785\n",
      "Iteration 96, loss = 0.53478277\n",
      "Iteration 97, loss = 0.53288823\n",
      "Iteration 98, loss = 0.53101692\n",
      "Iteration 99, loss = 0.52921079\n",
      "Iteration 100, loss = 0.52738187\n",
      "Iteration 101, loss = 0.52567274\n",
      "Iteration 102, loss = 0.52391646\n",
      "Iteration 103, loss = 0.52226111\n",
      "Iteration 104, loss = 0.52056404\n",
      "Iteration 105, loss = 0.51898562\n",
      "Iteration 106, loss = 0.51737098\n",
      "Iteration 107, loss = 0.51585011\n",
      "Iteration 108, loss = 0.51426745\n",
      "Iteration 109, loss = 0.51280190\n",
      "Iteration 110, loss = 0.51129881\n",
      "Iteration 111, loss = 0.50988002\n",
      "Iteration 112, loss = 0.50846052\n",
      "Iteration 113, loss = 0.50709475\n",
      "Iteration 114, loss = 0.50577497\n",
      "Iteration 115, loss = 0.50440082\n",
      "Iteration 116, loss = 0.50310857\n",
      "Iteration 117, loss = 0.50183580\n",
      "Iteration 118, loss = 0.50061307\n",
      "Iteration 119, loss = 0.49941853\n",
      "Iteration 120, loss = 0.49815842\n",
      "Iteration 121, loss = 0.49702243\n",
      "Iteration 122, loss = 0.49583382\n",
      "Iteration 123, loss = 0.49468914\n",
      "Iteration 124, loss = 0.49362605\n",
      "Iteration 125, loss = 0.49251226\n",
      "Iteration 126, loss = 0.49144326\n",
      "Iteration 127, loss = 0.49043655\n",
      "Iteration 128, loss = 0.48937869\n",
      "Iteration 129, loss = 0.48840579\n",
      "Iteration 130, loss = 0.48738462\n",
      "Iteration 131, loss = 0.48642472\n",
      "Iteration 132, loss = 0.48548441\n",
      "Iteration 133, loss = 0.48454461\n",
      "Iteration 134, loss = 0.48366366\n",
      "Iteration 135, loss = 0.48277807\n",
      "Iteration 136, loss = 0.48187270\n",
      "Iteration 137, loss = 0.48097956\n",
      "Iteration 138, loss = 0.48016587\n",
      "Iteration 139, loss = 0.47935644\n",
      "Iteration 140, loss = 0.47851309\n",
      "Iteration 141, loss = 0.47772329\n",
      "Iteration 142, loss = 0.47698832\n",
      "Iteration 143, loss = 0.47614056\n",
      "Iteration 144, loss = 0.47541255\n",
      "Iteration 145, loss = 0.47468789\n",
      "Iteration 146, loss = 0.47393731\n",
      "Iteration 147, loss = 0.47320617\n",
      "Iteration 148, loss = 0.47252702\n",
      "Iteration 149, loss = 0.47183832\n",
      "Iteration 150, loss = 0.47113250\n",
      "Iteration 151, loss = 0.47050477\n",
      "Iteration 152, loss = 0.46979879\n",
      "Iteration 153, loss = 0.46919531\n",
      "Iteration 154, loss = 0.46851831\n",
      "Iteration 155, loss = 0.46794297\n",
      "Iteration 156, loss = 0.46727014\n",
      "Iteration 157, loss = 0.46666119\n",
      "Iteration 158, loss = 0.46608807\n",
      "Iteration 159, loss = 0.46546597\n",
      "Iteration 160, loss = 0.46494807\n",
      "Iteration 161, loss = 0.46430126\n",
      "Iteration 162, loss = 0.46379056\n",
      "Iteration 163, loss = 0.46327452\n",
      "Iteration 164, loss = 0.46274734\n",
      "Iteration 165, loss = 0.46221559\n",
      "Iteration 166, loss = 0.46171076\n",
      "Iteration 167, loss = 0.46112910\n",
      "Iteration 168, loss = 0.46065940\n",
      "Iteration 169, loss = 0.46010072\n",
      "Iteration 170, loss = 0.45961001\n",
      "Iteration 171, loss = 0.45911003\n",
      "Iteration 172, loss = 0.45866272\n",
      "Iteration 173, loss = 0.45819426\n",
      "Iteration 174, loss = 0.45774346\n",
      "Iteration 175, loss = 0.45724433\n",
      "Iteration 176, loss = 0.45679261\n",
      "Iteration 177, loss = 0.45635041\n",
      "Iteration 178, loss = 0.45589331\n",
      "Iteration 179, loss = 0.45546650\n",
      "Iteration 180, loss = 0.45506153\n",
      "Iteration 181, loss = 0.45461475\n",
      "Iteration 182, loss = 0.45416788\n",
      "Iteration 183, loss = 0.45384374\n",
      "Iteration 184, loss = 0.45339084\n",
      "Iteration 185, loss = 0.45293107\n",
      "Iteration 186, loss = 0.45256951\n",
      "Iteration 187, loss = 0.45216496\n",
      "Iteration 188, loss = 0.45176664\n",
      "Iteration 189, loss = 0.45145052\n",
      "Iteration 190, loss = 0.45103721\n",
      "Iteration 191, loss = 0.45062125\n",
      "Iteration 192, loss = 0.45027540\n",
      "Iteration 193, loss = 0.44988520\n",
      "Iteration 194, loss = 0.44957189\n",
      "Iteration 195, loss = 0.44918582\n",
      "Iteration 196, loss = 0.44888132\n",
      "Iteration 197, loss = 0.44852786\n",
      "Iteration 198, loss = 0.44814422\n",
      "Iteration 199, loss = 0.44781294\n",
      "Iteration 200, loss = 0.44749240\n",
      "Iteration 201, loss = 0.44720354\n",
      "Iteration 202, loss = 0.44682308\n",
      "Iteration 203, loss = 0.44651040\n",
      "Iteration 204, loss = 0.44624027\n",
      "Iteration 205, loss = 0.44586234\n",
      "Iteration 206, loss = 0.44557026\n",
      "Iteration 207, loss = 0.44532149\n",
      "Iteration 208, loss = 0.44501631\n",
      "Iteration 209, loss = 0.44469629\n",
      "Iteration 210, loss = 0.44439248\n",
      "Iteration 211, loss = 0.44413711\n",
      "Iteration 212, loss = 0.44384866\n",
      "Iteration 213, loss = 0.44354861\n",
      "Iteration 214, loss = 0.44329082\n",
      "Iteration 215, loss = 0.44294698\n",
      "Iteration 216, loss = 0.44268825\n",
      "Iteration 217, loss = 0.44244406\n",
      "Iteration 218, loss = 0.44215758\n",
      "Iteration 219, loss = 0.44183391\n",
      "Iteration 220, loss = 0.44163430\n",
      "Iteration 221, loss = 0.44129481\n",
      "Iteration 222, loss = 0.44108480\n",
      "Iteration 223, loss = 0.44081891\n",
      "Iteration 224, loss = 0.44058111\n",
      "Iteration 225, loss = 0.44029999\n",
      "Iteration 226, loss = 0.44005712\n",
      "Iteration 227, loss = 0.43978016\n",
      "Iteration 228, loss = 0.43958339\n",
      "Iteration 229, loss = 0.43936145\n",
      "Iteration 230, loss = 0.43910679\n",
      "Iteration 231, loss = 0.43891724\n",
      "Iteration 232, loss = 0.43867325\n",
      "Iteration 233, loss = 0.43841010\n",
      "Iteration 234, loss = 0.43809584\n",
      "Iteration 235, loss = 0.43798903\n",
      "Iteration 236, loss = 0.43768232\n",
      "Iteration 237, loss = 0.43745473\n",
      "Iteration 238, loss = 0.43720674\n",
      "Iteration 239, loss = 0.43701967\n",
      "Iteration 240, loss = 0.43678287\n",
      "Iteration 241, loss = 0.43662506\n",
      "Iteration 242, loss = 0.43643716\n",
      "Iteration 243, loss = 0.43622054\n",
      "Iteration 244, loss = 0.43604101\n",
      "Iteration 245, loss = 0.43574166\n",
      "Iteration 246, loss = 0.43556870\n",
      "Iteration 247, loss = 0.43536001\n",
      "Iteration 248, loss = 0.43511269\n",
      "Iteration 249, loss = 0.43494363\n",
      "Iteration 250, loss = 0.43473712\n",
      "Iteration 251, loss = 0.43457754\n",
      "Iteration 252, loss = 0.43427961\n",
      "Iteration 253, loss = 0.43419610\n",
      "Iteration 254, loss = 0.43397738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.43379174\n",
      "Iteration 256, loss = 0.43351665\n",
      "Iteration 257, loss = 0.43334557\n",
      "Iteration 258, loss = 0.43316561\n",
      "Iteration 259, loss = 0.43301453\n",
      "Iteration 260, loss = 0.43288084\n",
      "Iteration 261, loss = 0.43265049\n",
      "Iteration 262, loss = 0.43248600\n",
      "Iteration 263, loss = 0.43226555\n",
      "Iteration 264, loss = 0.43212414\n",
      "Iteration 265, loss = 0.43194677\n",
      "Iteration 266, loss = 0.43168907\n",
      "Iteration 267, loss = 0.43156131\n",
      "Iteration 268, loss = 0.43139430\n",
      "Iteration 269, loss = 0.43122196\n",
      "Iteration 270, loss = 0.43102106\n",
      "Iteration 271, loss = 0.43084622\n",
      "Iteration 272, loss = 0.43073211\n",
      "Iteration 273, loss = 0.43050114\n",
      "Iteration 274, loss = 0.43034429\n",
      "Iteration 275, loss = 0.43026856\n",
      "Iteration 276, loss = 0.43007087\n",
      "Iteration 277, loss = 0.42985788\n",
      "Iteration 278, loss = 0.42973860\n",
      "Iteration 279, loss = 0.42957583\n",
      "Iteration 280, loss = 0.42944632\n",
      "Iteration 281, loss = 0.42922782\n",
      "Iteration 282, loss = 0.42911582\n",
      "Iteration 283, loss = 0.42891348\n",
      "Iteration 284, loss = 0.42883099\n",
      "Iteration 285, loss = 0.42867146\n",
      "Iteration 286, loss = 0.42850575\n",
      "Iteration 287, loss = 0.42834410\n",
      "Iteration 288, loss = 0.42824100\n",
      "Iteration 289, loss = 0.42801189\n",
      "Iteration 290, loss = 0.42784689\n",
      "Iteration 291, loss = 0.42776692\n",
      "Iteration 292, loss = 0.42758158\n",
      "Iteration 293, loss = 0.42745020\n",
      "Iteration 294, loss = 0.42730074\n",
      "Iteration 295, loss = 0.42720268\n",
      "Iteration 296, loss = 0.42700716\n",
      "Iteration 297, loss = 0.42685103\n",
      "Iteration 298, loss = 0.42672484\n",
      "Iteration 299, loss = 0.42662343\n",
      "Iteration 300, loss = 0.42645203\n",
      "Iteration 301, loss = 0.42631729\n",
      "Iteration 302, loss = 0.42626735\n",
      "Iteration 303, loss = 0.42605271\n",
      "Iteration 304, loss = 0.42591203\n",
      "Iteration 305, loss = 0.42579035\n",
      "Iteration 306, loss = 0.42574782\n",
      "Iteration 307, loss = 0.42552118\n",
      "Iteration 308, loss = 0.42541649\n",
      "Iteration 309, loss = 0.42525386\n",
      "Iteration 310, loss = 0.42517232\n",
      "Iteration 311, loss = 0.42505334\n",
      "Iteration 312, loss = 0.42487274\n",
      "Iteration 313, loss = 0.42471713\n",
      "Iteration 314, loss = 0.42463794\n",
      "Iteration 315, loss = 0.42454741\n",
      "Iteration 316, loss = 0.42436897\n",
      "Iteration 317, loss = 0.42422883\n",
      "Iteration 318, loss = 0.42411296\n",
      "Iteration 319, loss = 0.42403758\n",
      "Iteration 320, loss = 0.42384975\n",
      "Iteration 321, loss = 0.42385333\n",
      "Iteration 322, loss = 0.42368742\n",
      "Iteration 323, loss = 0.42352540\n",
      "Iteration 324, loss = 0.42341075\n",
      "Iteration 325, loss = 0.42329440\n",
      "Iteration 326, loss = 0.42317114\n",
      "Iteration 327, loss = 0.42308548\n",
      "Iteration 328, loss = 0.42293060\n",
      "Iteration 329, loss = 0.42292651\n",
      "Iteration 330, loss = 0.42267236\n",
      "Iteration 331, loss = 0.42255158\n",
      "Iteration 332, loss = 0.42248028\n",
      "Iteration 333, loss = 0.42242802\n",
      "Iteration 334, loss = 0.42227364\n",
      "Iteration 335, loss = 0.42213699\n",
      "Iteration 336, loss = 0.42211524\n",
      "Iteration 337, loss = 0.42195933\n",
      "Iteration 338, loss = 0.42186676\n",
      "Iteration 339, loss = 0.42174091\n",
      "Iteration 340, loss = 0.42162352\n",
      "Iteration 341, loss = 0.42157681\n",
      "Iteration 342, loss = 0.42136312\n",
      "Iteration 343, loss = 0.42137524\n",
      "Iteration 344, loss = 0.42118878\n",
      "Iteration 345, loss = 0.42114649\n",
      "Iteration 346, loss = 0.42100130\n",
      "Iteration 347, loss = 0.42087175\n",
      "Iteration 348, loss = 0.42077236\n",
      "Iteration 349, loss = 0.42060050\n",
      "Iteration 350, loss = 0.42057526\n",
      "Iteration 351, loss = 0.42041849\n",
      "Iteration 352, loss = 0.42036294\n",
      "Iteration 353, loss = 0.42031506\n",
      "Iteration 354, loss = 0.42026436\n",
      "Iteration 355, loss = 0.42008201\n",
      "Iteration 356, loss = 0.41995055\n",
      "Iteration 357, loss = 0.41994813\n",
      "Iteration 358, loss = 0.41978460\n",
      "Iteration 359, loss = 0.41969164\n",
      "Iteration 360, loss = 0.41958465\n",
      "Iteration 361, loss = 0.41947252\n",
      "Iteration 362, loss = 0.41942043\n",
      "Iteration 363, loss = 0.41927282\n",
      "Iteration 364, loss = 0.41925061\n",
      "Iteration 365, loss = 0.41903939\n",
      "Iteration 366, loss = 0.41904427\n",
      "Iteration 367, loss = 0.41890778\n",
      "Iteration 368, loss = 0.41882020\n",
      "Iteration 369, loss = 0.41869220\n",
      "Iteration 370, loss = 0.41879974\n",
      "Iteration 371, loss = 0.41847407\n",
      "Iteration 372, loss = 0.41843500\n",
      "Iteration 373, loss = 0.41839049\n",
      "Iteration 374, loss = 0.41828712\n",
      "Iteration 375, loss = 0.41816361\n",
      "Iteration 376, loss = 0.41810335\n",
      "Iteration 377, loss = 0.41801046\n",
      "Iteration 378, loss = 0.41788281\n",
      "Iteration 379, loss = 0.41782767\n",
      "Iteration 380, loss = 0.41769281\n",
      "Iteration 381, loss = 0.41760348\n",
      "Iteration 382, loss = 0.41756138\n",
      "Iteration 383, loss = 0.41746037\n",
      "Iteration 384, loss = 0.41739640\n",
      "Iteration 385, loss = 0.41744084\n",
      "Iteration 386, loss = 0.41727746\n",
      "Iteration 387, loss = 0.41711763\n",
      "Iteration 388, loss = 0.41709712\n",
      "Iteration 389, loss = 0.41702187\n",
      "Iteration 390, loss = 0.41688921\n",
      "Iteration 391, loss = 0.41684883\n",
      "Iteration 392, loss = 0.41670868\n",
      "Iteration 393, loss = 0.41657597\n",
      "Iteration 394, loss = 0.41654181\n",
      "Iteration 395, loss = 0.41648895\n",
      "Iteration 396, loss = 0.41640245\n",
      "Iteration 397, loss = 0.41623709\n",
      "Iteration 398, loss = 0.41620811\n",
      "Iteration 399, loss = 0.41612923\n",
      "Iteration 400, loss = 0.41609797\n",
      "Iteration 401, loss = 0.41601238\n",
      "Iteration 402, loss = 0.41591084\n",
      "Iteration 403, loss = 0.41584914\n",
      "Iteration 404, loss = 0.41571684\n",
      "Iteration 405, loss = 0.41573826\n",
      "Iteration 406, loss = 0.41563090\n",
      "Iteration 407, loss = 0.41551824\n",
      "Iteration 408, loss = 0.41540648\n",
      "Iteration 409, loss = 0.41537859\n",
      "Iteration 410, loss = 0.41532299\n",
      "Iteration 411, loss = 0.41522957\n",
      "Iteration 412, loss = 0.41515097\n",
      "Iteration 413, loss = 0.41505717\n",
      "Iteration 414, loss = 0.41492427\n",
      "Iteration 415, loss = 0.41487046\n",
      "Iteration 416, loss = 0.41479820\n",
      "Iteration 417, loss = 0.41478253\n",
      "Iteration 418, loss = 0.41471321\n",
      "Iteration 419, loss = 0.41465206\n",
      "Iteration 420, loss = 0.41454006\n",
      "Iteration 421, loss = 0.41447780\n",
      "Iteration 422, loss = 0.41437058\n",
      "Iteration 423, loss = 0.41423803\n",
      "Iteration 424, loss = 0.41427785\n",
      "Iteration 425, loss = 0.41415112\n",
      "Iteration 426, loss = 0.41416551\n",
      "Iteration 427, loss = 0.41404827\n",
      "Iteration 428, loss = 0.41398924\n",
      "Iteration 429, loss = 0.41390047\n",
      "Iteration 430, loss = 0.41380359\n",
      "Iteration 431, loss = 0.41374716\n",
      "Iteration 432, loss = 0.41367669\n",
      "Iteration 433, loss = 0.41365456\n",
      "Iteration 434, loss = 0.41364842\n",
      "Iteration 435, loss = 0.41351318\n",
      "Iteration 436, loss = 0.41338721\n",
      "Iteration 437, loss = 0.41336861\n",
      "Iteration 438, loss = 0.41324880\n",
      "Iteration 439, loss = 0.41321912\n",
      "Iteration 440, loss = 0.41311646\n",
      "Iteration 441, loss = 0.41312567\n",
      "Iteration 442, loss = 0.41295950\n",
      "Iteration 443, loss = 0.41301906\n",
      "Iteration 444, loss = 0.41285886\n",
      "Iteration 445, loss = 0.41282658\n",
      "Iteration 446, loss = 0.41275459\n",
      "Iteration 447, loss = 0.41263904\n",
      "Iteration 448, loss = 0.41256159\n",
      "Iteration 449, loss = 0.41249746\n",
      "Iteration 450, loss = 0.41249646\n",
      "Iteration 451, loss = 0.41235370\n",
      "Iteration 452, loss = 0.41232023\n",
      "Iteration 453, loss = 0.41236424\n",
      "Iteration 454, loss = 0.41228093\n",
      "Iteration 455, loss = 0.41223528\n",
      "Iteration 456, loss = 0.41205621\n",
      "Iteration 457, loss = 0.41203050\n",
      "Iteration 458, loss = 0.41196295\n",
      "Iteration 459, loss = 0.41190961\n",
      "Iteration 460, loss = 0.41192364\n",
      "Iteration 461, loss = 0.41174319\n",
      "Iteration 462, loss = 0.41169983\n",
      "Iteration 463, loss = 0.41168076\n",
      "Iteration 464, loss = 0.41161251\n",
      "Iteration 465, loss = 0.41157154\n",
      "Iteration 466, loss = 0.41137761\n",
      "Iteration 467, loss = 0.41139966\n",
      "Iteration 468, loss = 0.41134134\n",
      "Iteration 469, loss = 0.41126736\n",
      "Iteration 470, loss = 0.41120884\n",
      "Iteration 471, loss = 0.41118513\n",
      "Iteration 472, loss = 0.41107378\n",
      "Iteration 473, loss = 0.41112761\n",
      "Iteration 474, loss = 0.41103283\n",
      "Iteration 475, loss = 0.41095904\n",
      "Iteration 476, loss = 0.41091525\n",
      "Iteration 477, loss = 0.41075116\n",
      "Iteration 478, loss = 0.41081122\n",
      "Iteration 479, loss = 0.41072589\n",
      "Iteration 480, loss = 0.41066318\n",
      "Iteration 481, loss = 0.41060174\n",
      "Iteration 482, loss = 0.41059118\n",
      "Iteration 483, loss = 0.41048714\n",
      "Iteration 484, loss = 0.41043557\n",
      "Iteration 485, loss = 0.41043096\n",
      "Iteration 486, loss = 0.41030870\n",
      "Iteration 487, loss = 0.41024501\n",
      "Iteration 488, loss = 0.41021738\n",
      "Iteration 489, loss = 0.41011383\n",
      "Iteration 490, loss = 0.41013670\n",
      "Iteration 491, loss = 0.41012359\n",
      "Iteration 492, loss = 0.40997329\n",
      "Iteration 493, loss = 0.40999426\n",
      "Iteration 494, loss = 0.40984717\n",
      "Iteration 495, loss = 0.40978858\n",
      "Iteration 496, loss = 0.40978035\n",
      "Iteration 497, loss = 0.40972330\n",
      "Iteration 498, loss = 0.40967104\n",
      "Iteration 499, loss = 0.40962080\n",
      "Iteration 500, loss = 0.40949901\n",
      "Iteration 1, loss = 1.93788236\n",
      "Iteration 2, loss = 1.80417442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.69649460\n",
      "Iteration 4, loss = 1.60642668\n",
      "Iteration 5, loss = 1.52822842\n",
      "Iteration 6, loss = 1.45912803\n",
      "Iteration 7, loss = 1.39767241\n",
      "Iteration 8, loss = 1.34282502\n",
      "Iteration 9, loss = 1.29379726\n",
      "Iteration 10, loss = 1.24973247\n",
      "Iteration 11, loss = 1.20981867\n",
      "Iteration 12, loss = 1.17360233\n",
      "Iteration 13, loss = 1.14036249\n",
      "Iteration 14, loss = 1.10978429\n",
      "Iteration 15, loss = 1.08148592\n",
      "Iteration 16, loss = 1.05519036\n",
      "Iteration 17, loss = 1.03068619\n",
      "Iteration 18, loss = 1.00779893\n",
      "Iteration 19, loss = 0.98636253\n",
      "Iteration 20, loss = 0.96619369\n",
      "Iteration 21, loss = 0.94723031\n",
      "Iteration 22, loss = 0.92926110\n",
      "Iteration 23, loss = 0.91225592\n",
      "Iteration 24, loss = 0.89608868\n",
      "Iteration 25, loss = 0.88073399\n",
      "Iteration 26, loss = 0.86622500\n",
      "Iteration 27, loss = 0.85251194\n",
      "Iteration 28, loss = 0.83961057\n",
      "Iteration 29, loss = 0.82742419\n",
      "Iteration 30, loss = 0.81592099\n",
      "Iteration 31, loss = 0.80503952\n",
      "Iteration 32, loss = 0.79472529\n",
      "Iteration 33, loss = 0.78496349\n",
      "Iteration 34, loss = 0.77571267\n",
      "Iteration 35, loss = 0.76691682\n",
      "Iteration 36, loss = 0.75855969\n",
      "Iteration 37, loss = 0.75059542\n",
      "Iteration 38, loss = 0.74299892\n",
      "Iteration 39, loss = 0.73573879\n",
      "Iteration 40, loss = 0.72882287\n",
      "Iteration 41, loss = 0.72215428\n",
      "Iteration 42, loss = 0.71578668\n",
      "Iteration 43, loss = 0.70965343\n",
      "Iteration 44, loss = 0.70376541\n",
      "Iteration 45, loss = 0.69812270\n",
      "Iteration 46, loss = 0.69265627\n",
      "Iteration 47, loss = 0.68740433\n",
      "Iteration 48, loss = 0.68234591\n",
      "Iteration 49, loss = 0.67743363\n",
      "Iteration 50, loss = 0.67271379\n",
      "Iteration 51, loss = 0.66810609\n",
      "Iteration 52, loss = 0.66369929\n",
      "Iteration 53, loss = 0.65938749\n",
      "Iteration 54, loss = 0.65515820\n",
      "Iteration 55, loss = 0.65111239\n",
      "Iteration 56, loss = 0.64713594\n",
      "Iteration 57, loss = 0.64327144\n",
      "Iteration 58, loss = 0.63951581\n",
      "Iteration 59, loss = 0.63585449\n",
      "Iteration 60, loss = 0.63225084\n",
      "Iteration 61, loss = 0.62877111\n",
      "Iteration 62, loss = 0.62535404\n",
      "Iteration 63, loss = 0.62198337\n",
      "Iteration 64, loss = 0.61872285\n",
      "Iteration 65, loss = 0.61550829\n",
      "Iteration 66, loss = 0.61237177\n",
      "Iteration 67, loss = 0.60930176\n",
      "Iteration 68, loss = 0.60625136\n",
      "Iteration 69, loss = 0.60326312\n",
      "Iteration 70, loss = 0.60035872\n",
      "Iteration 71, loss = 0.59753044\n",
      "Iteration 72, loss = 0.59466470\n",
      "Iteration 73, loss = 0.59191409\n",
      "Iteration 74, loss = 0.58918223\n",
      "Iteration 75, loss = 0.58650892\n",
      "Iteration 76, loss = 0.58390187\n",
      "Iteration 77, loss = 0.58130454\n",
      "Iteration 78, loss = 0.57876927\n",
      "Iteration 79, loss = 0.57619714\n",
      "Iteration 80, loss = 0.57376265\n",
      "Iteration 81, loss = 0.57130882\n",
      "Iteration 82, loss = 0.56892564\n",
      "Iteration 83, loss = 0.56655339\n",
      "Iteration 84, loss = 0.56422153\n",
      "Iteration 85, loss = 0.56192717\n",
      "Iteration 86, loss = 0.55969169\n",
      "Iteration 87, loss = 0.55744098\n",
      "Iteration 88, loss = 0.55524083\n",
      "Iteration 89, loss = 0.55310067\n",
      "Iteration 90, loss = 0.55098902\n",
      "Iteration 91, loss = 0.54887137\n",
      "Iteration 92, loss = 0.54683307\n",
      "Iteration 93, loss = 0.54482912\n",
      "Iteration 94, loss = 0.54281262\n",
      "Iteration 95, loss = 0.54086271\n",
      "Iteration 96, loss = 0.53897572\n",
      "Iteration 97, loss = 0.53706733\n",
      "Iteration 98, loss = 0.53520280\n",
      "Iteration 99, loss = 0.53335229\n",
      "Iteration 100, loss = 0.53164929\n",
      "Iteration 101, loss = 0.52980169\n",
      "Iteration 102, loss = 0.52811191\n",
      "Iteration 103, loss = 0.52641629\n",
      "Iteration 104, loss = 0.52477147\n",
      "Iteration 105, loss = 0.52310558\n",
      "Iteration 106, loss = 0.52146960\n",
      "Iteration 107, loss = 0.51989288\n",
      "Iteration 108, loss = 0.51835185\n",
      "Iteration 109, loss = 0.51686817\n",
      "Iteration 110, loss = 0.51535934\n",
      "Iteration 111, loss = 0.51393309\n",
      "Iteration 112, loss = 0.51247642\n",
      "Iteration 113, loss = 0.51107576\n",
      "Iteration 114, loss = 0.50969340\n",
      "Iteration 115, loss = 0.50836551\n",
      "Iteration 116, loss = 0.50700888\n",
      "Iteration 117, loss = 0.50578243\n",
      "Iteration 118, loss = 0.50450989\n",
      "Iteration 119, loss = 0.50320231\n",
      "Iteration 120, loss = 0.50200111\n",
      "Iteration 121, loss = 0.50082973\n",
      "Iteration 122, loss = 0.49963275\n",
      "Iteration 123, loss = 0.49843918\n",
      "Iteration 124, loss = 0.49732945\n",
      "Iteration 125, loss = 0.49619528\n",
      "Iteration 126, loss = 0.49515063\n",
      "Iteration 127, loss = 0.49408621\n",
      "Iteration 128, loss = 0.49301362\n",
      "Iteration 129, loss = 0.49200939\n",
      "Iteration 130, loss = 0.49099908\n",
      "Iteration 131, loss = 0.48999626\n",
      "Iteration 132, loss = 0.48903122\n",
      "Iteration 133, loss = 0.48804827\n",
      "Iteration 134, loss = 0.48716791\n",
      "Iteration 135, loss = 0.48623911\n",
      "Iteration 136, loss = 0.48536180\n",
      "Iteration 137, loss = 0.48447835\n",
      "Iteration 138, loss = 0.48356710\n",
      "Iteration 139, loss = 0.48268929\n",
      "Iteration 140, loss = 0.48189556\n",
      "Iteration 141, loss = 0.48109109\n",
      "Iteration 142, loss = 0.48028664\n",
      "Iteration 143, loss = 0.47948077\n",
      "Iteration 144, loss = 0.47870011\n",
      "Iteration 145, loss = 0.47798808\n",
      "Iteration 146, loss = 0.47719417\n",
      "Iteration 147, loss = 0.47642331\n",
      "Iteration 148, loss = 0.47571312\n",
      "Iteration 149, loss = 0.47496989\n",
      "Iteration 150, loss = 0.47427504\n",
      "Iteration 151, loss = 0.47362164\n",
      "Iteration 152, loss = 0.47294228\n",
      "Iteration 153, loss = 0.47222082\n",
      "Iteration 154, loss = 0.47159872\n",
      "Iteration 155, loss = 0.47100137\n",
      "Iteration 156, loss = 0.47034785\n",
      "Iteration 157, loss = 0.46967961\n",
      "Iteration 158, loss = 0.46907025\n",
      "Iteration 159, loss = 0.46849776\n",
      "Iteration 160, loss = 0.46795746\n",
      "Iteration 161, loss = 0.46737031\n",
      "Iteration 162, loss = 0.46674714\n",
      "Iteration 163, loss = 0.46620059\n",
      "Iteration 164, loss = 0.46557169\n",
      "Iteration 165, loss = 0.46503764\n",
      "Iteration 166, loss = 0.46453210\n",
      "Iteration 167, loss = 0.46401640\n",
      "Iteration 168, loss = 0.46350414\n",
      "Iteration 169, loss = 0.46296259\n",
      "Iteration 170, loss = 0.46246347\n",
      "Iteration 171, loss = 0.46196243\n",
      "Iteration 172, loss = 0.46142787\n",
      "Iteration 173, loss = 0.46096134\n",
      "Iteration 174, loss = 0.46051817\n",
      "Iteration 175, loss = 0.46003260\n",
      "Iteration 176, loss = 0.45953844\n",
      "Iteration 177, loss = 0.45913360\n",
      "Iteration 178, loss = 0.45868651\n",
      "Iteration 179, loss = 0.45816351\n",
      "Iteration 180, loss = 0.45778063\n",
      "Iteration 181, loss = 0.45732442\n",
      "Iteration 182, loss = 0.45689710\n",
      "Iteration 183, loss = 0.45650146\n",
      "Iteration 184, loss = 0.45604663\n",
      "Iteration 185, loss = 0.45564249\n",
      "Iteration 186, loss = 0.45524673\n",
      "Iteration 187, loss = 0.45485867\n",
      "Iteration 188, loss = 0.45444761\n",
      "Iteration 189, loss = 0.45406001\n",
      "Iteration 190, loss = 0.45368393\n",
      "Iteration 191, loss = 0.45334719\n",
      "Iteration 192, loss = 0.45292369\n",
      "Iteration 193, loss = 0.45255841\n",
      "Iteration 194, loss = 0.45222520\n",
      "Iteration 195, loss = 0.45180176\n",
      "Iteration 196, loss = 0.45146434\n",
      "Iteration 197, loss = 0.45111734\n",
      "Iteration 198, loss = 0.45079820\n",
      "Iteration 199, loss = 0.45041925\n",
      "Iteration 200, loss = 0.45006703\n",
      "Iteration 201, loss = 0.44980454\n",
      "Iteration 202, loss = 0.44942485\n",
      "Iteration 203, loss = 0.44905798\n",
      "Iteration 204, loss = 0.44877348\n",
      "Iteration 205, loss = 0.44844359\n",
      "Iteration 206, loss = 0.44814306\n",
      "Iteration 207, loss = 0.44782516\n",
      "Iteration 208, loss = 0.44754292\n",
      "Iteration 209, loss = 0.44722468\n",
      "Iteration 210, loss = 0.44690575\n",
      "Iteration 211, loss = 0.44664593\n",
      "Iteration 212, loss = 0.44632954\n",
      "Iteration 213, loss = 0.44601884\n",
      "Iteration 214, loss = 0.44575619\n",
      "Iteration 215, loss = 0.44554876\n",
      "Iteration 216, loss = 0.44518979\n",
      "Iteration 217, loss = 0.44494819\n",
      "Iteration 218, loss = 0.44463656\n",
      "Iteration 219, loss = 0.44434889\n",
      "Iteration 220, loss = 0.44408524\n",
      "Iteration 221, loss = 0.44376667\n",
      "Iteration 222, loss = 0.44346564\n",
      "Iteration 223, loss = 0.44330145\n",
      "Iteration 224, loss = 0.44303608\n",
      "Iteration 225, loss = 0.44278882\n",
      "Iteration 226, loss = 0.44252445\n",
      "Iteration 227, loss = 0.44219670\n",
      "Iteration 228, loss = 0.44198718\n",
      "Iteration 229, loss = 0.44179394\n",
      "Iteration 230, loss = 0.44149867\n",
      "Iteration 231, loss = 0.44123755\n",
      "Iteration 232, loss = 0.44102999\n",
      "Iteration 233, loss = 0.44081513\n",
      "Iteration 234, loss = 0.44052483\n",
      "Iteration 235, loss = 0.44031759\n",
      "Iteration 236, loss = 0.44006134\n",
      "Iteration 237, loss = 0.43983217\n",
      "Iteration 238, loss = 0.43958306\n",
      "Iteration 239, loss = 0.43934452\n",
      "Iteration 240, loss = 0.43915569\n",
      "Iteration 241, loss = 0.43895090\n",
      "Iteration 242, loss = 0.43871313\n",
      "Iteration 243, loss = 0.43857684\n",
      "Iteration 244, loss = 0.43831308\n",
      "Iteration 245, loss = 0.43802842\n",
      "Iteration 246, loss = 0.43787752\n",
      "Iteration 247, loss = 0.43768993\n",
      "Iteration 248, loss = 0.43745446\n",
      "Iteration 249, loss = 0.43728026\n",
      "Iteration 250, loss = 0.43703032\n",
      "Iteration 251, loss = 0.43677208\n",
      "Iteration 252, loss = 0.43656790\n",
      "Iteration 253, loss = 0.43643734\n",
      "Iteration 254, loss = 0.43623855\n",
      "Iteration 255, loss = 0.43602057\n",
      "Iteration 256, loss = 0.43585928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.43563432\n",
      "Iteration 258, loss = 0.43539943\n",
      "Iteration 259, loss = 0.43526222\n",
      "Iteration 260, loss = 0.43503350\n",
      "Iteration 261, loss = 0.43488995\n",
      "Iteration 262, loss = 0.43470419\n",
      "Iteration 263, loss = 0.43451277\n",
      "Iteration 264, loss = 0.43430946\n",
      "Iteration 265, loss = 0.43411773\n",
      "Iteration 266, loss = 0.43394491\n",
      "Iteration 267, loss = 0.43376049\n",
      "Iteration 268, loss = 0.43359423\n",
      "Iteration 269, loss = 0.43342559\n",
      "Iteration 270, loss = 0.43320188\n",
      "Iteration 271, loss = 0.43313978\n",
      "Iteration 272, loss = 0.43289569\n",
      "Iteration 273, loss = 0.43284506\n",
      "Iteration 274, loss = 0.43262047\n",
      "Iteration 275, loss = 0.43236104\n",
      "Iteration 276, loss = 0.43223371\n",
      "Iteration 277, loss = 0.43209189\n",
      "Iteration 278, loss = 0.43199887\n",
      "Iteration 279, loss = 0.43174208\n",
      "Iteration 280, loss = 0.43163384\n",
      "Iteration 281, loss = 0.43140497\n",
      "Iteration 282, loss = 0.43131564\n",
      "Iteration 283, loss = 0.43113840\n",
      "Iteration 284, loss = 0.43097286\n",
      "Iteration 285, loss = 0.43086955\n",
      "Iteration 286, loss = 0.43066871\n",
      "Iteration 287, loss = 0.43054604\n",
      "Iteration 288, loss = 0.43034062\n",
      "Iteration 289, loss = 0.43025396\n",
      "Iteration 290, loss = 0.43007831\n",
      "Iteration 291, loss = 0.42992049\n",
      "Iteration 292, loss = 0.42971426\n",
      "Iteration 293, loss = 0.42967345\n",
      "Iteration 294, loss = 0.42944524\n",
      "Iteration 295, loss = 0.42931308\n",
      "Iteration 296, loss = 0.42914287\n",
      "Iteration 297, loss = 0.42905534\n",
      "Iteration 298, loss = 0.42889521\n",
      "Iteration 299, loss = 0.42879252\n",
      "Iteration 300, loss = 0.42862789\n",
      "Iteration 301, loss = 0.42847303\n",
      "Iteration 302, loss = 0.42833592\n",
      "Iteration 303, loss = 0.42816745\n",
      "Iteration 304, loss = 0.42803524\n",
      "Iteration 305, loss = 0.42787391\n",
      "Iteration 306, loss = 0.42787053\n",
      "Iteration 307, loss = 0.42766525\n",
      "Iteration 308, loss = 0.42756477\n",
      "Iteration 309, loss = 0.42736583\n",
      "Iteration 310, loss = 0.42727982\n",
      "Iteration 311, loss = 0.42710484\n",
      "Iteration 312, loss = 0.42694772\n",
      "Iteration 313, loss = 0.42686416\n",
      "Iteration 314, loss = 0.42672375\n",
      "Iteration 315, loss = 0.42655300\n",
      "Iteration 316, loss = 0.42648834\n",
      "Iteration 317, loss = 0.42634649\n",
      "Iteration 318, loss = 0.42628919\n",
      "Iteration 319, loss = 0.42607941\n",
      "Iteration 320, loss = 0.42598952\n",
      "Iteration 321, loss = 0.42583133\n",
      "Iteration 322, loss = 0.42572661\n",
      "Iteration 323, loss = 0.42558922\n",
      "Iteration 324, loss = 0.42545023\n",
      "Iteration 325, loss = 0.42534392\n",
      "Iteration 326, loss = 0.42518446\n",
      "Iteration 327, loss = 0.42515674\n",
      "Iteration 328, loss = 0.42499527\n",
      "Iteration 329, loss = 0.42488964\n",
      "Iteration 330, loss = 0.42472021\n",
      "Iteration 331, loss = 0.42468526\n",
      "Iteration 332, loss = 0.42459189\n",
      "Iteration 333, loss = 0.42443108\n",
      "Iteration 334, loss = 0.42429103\n",
      "Iteration 335, loss = 0.42419636\n",
      "Iteration 336, loss = 0.42410021\n",
      "Iteration 337, loss = 0.42401948\n",
      "Iteration 338, loss = 0.42387230\n",
      "Iteration 339, loss = 0.42378473\n",
      "Iteration 340, loss = 0.42361855\n",
      "Iteration 341, loss = 0.42353969\n",
      "Iteration 342, loss = 0.42335968\n",
      "Iteration 343, loss = 0.42330828\n",
      "Iteration 344, loss = 0.42311583\n",
      "Iteration 345, loss = 0.42311821\n",
      "Iteration 346, loss = 0.42290510\n",
      "Iteration 347, loss = 0.42282815\n",
      "Iteration 348, loss = 0.42278963\n",
      "Iteration 349, loss = 0.42264421\n",
      "Iteration 350, loss = 0.42267608\n",
      "Iteration 351, loss = 0.42241218\n",
      "Iteration 352, loss = 0.42240021\n",
      "Iteration 353, loss = 0.42226996\n",
      "Iteration 354, loss = 0.42214477\n",
      "Iteration 355, loss = 0.42203734\n",
      "Iteration 356, loss = 0.42188561\n",
      "Iteration 357, loss = 0.42183596\n",
      "Iteration 358, loss = 0.42173845\n",
      "Iteration 359, loss = 0.42170628\n",
      "Iteration 360, loss = 0.42156797\n",
      "Iteration 361, loss = 0.42150345\n",
      "Iteration 362, loss = 0.42137809\n",
      "Iteration 363, loss = 0.42119247\n",
      "Iteration 364, loss = 0.42111599\n",
      "Iteration 365, loss = 0.42106332\n",
      "Iteration 366, loss = 0.42097704\n",
      "Iteration 367, loss = 0.42085409\n",
      "Iteration 368, loss = 0.42071386\n",
      "Iteration 369, loss = 0.42070103\n",
      "Iteration 370, loss = 0.42054368\n",
      "Iteration 371, loss = 0.42047612\n",
      "Iteration 372, loss = 0.42044635\n",
      "Iteration 373, loss = 0.42032518\n",
      "Iteration 374, loss = 0.42026467\n",
      "Iteration 375, loss = 0.42012048\n",
      "Iteration 376, loss = 0.41994922\n",
      "Iteration 377, loss = 0.41995133\n",
      "Iteration 378, loss = 0.41981178\n",
      "Iteration 379, loss = 0.41969427\n",
      "Iteration 380, loss = 0.41965369\n",
      "Iteration 381, loss = 0.41963629\n",
      "Iteration 382, loss = 0.41946137\n",
      "Iteration 383, loss = 0.41940247\n",
      "Iteration 384, loss = 0.41938123\n",
      "Iteration 385, loss = 0.41927142\n",
      "Iteration 386, loss = 0.41910483\n",
      "Iteration 387, loss = 0.41908403\n",
      "Iteration 388, loss = 0.41898334\n",
      "Iteration 389, loss = 0.41887196\n",
      "Iteration 390, loss = 0.41870442\n",
      "Iteration 391, loss = 0.41868696\n",
      "Iteration 392, loss = 0.41862547\n",
      "Iteration 393, loss = 0.41848474\n",
      "Iteration 394, loss = 0.41842272\n",
      "Iteration 395, loss = 0.41833557\n",
      "Iteration 396, loss = 0.41825376\n",
      "Iteration 397, loss = 0.41820801\n",
      "Iteration 398, loss = 0.41818553\n",
      "Iteration 399, loss = 0.41801600\n",
      "Iteration 400, loss = 0.41787768\n",
      "Iteration 401, loss = 0.41781770\n",
      "Iteration 402, loss = 0.41777039\n",
      "Iteration 403, loss = 0.41764021\n",
      "Iteration 404, loss = 0.41762465\n",
      "Iteration 405, loss = 0.41753300\n",
      "Iteration 406, loss = 0.41735532\n",
      "Iteration 407, loss = 0.41738512\n",
      "Iteration 408, loss = 0.41734228\n",
      "Iteration 409, loss = 0.41716196\n",
      "Iteration 410, loss = 0.41706919\n",
      "Iteration 411, loss = 0.41698252\n",
      "Iteration 412, loss = 0.41700455\n",
      "Iteration 413, loss = 0.41690806\n",
      "Iteration 414, loss = 0.41680838\n",
      "Iteration 415, loss = 0.41680190\n",
      "Iteration 416, loss = 0.41661018\n",
      "Iteration 417, loss = 0.41655681\n",
      "Iteration 418, loss = 0.41644830\n",
      "Iteration 419, loss = 0.41635154\n",
      "Iteration 420, loss = 0.41632705\n",
      "Iteration 421, loss = 0.41617530\n",
      "Iteration 422, loss = 0.41616608\n",
      "Iteration 423, loss = 0.41607270\n",
      "Iteration 424, loss = 0.41599079\n",
      "Iteration 425, loss = 0.41592790\n",
      "Iteration 426, loss = 0.41587834\n",
      "Iteration 427, loss = 0.41582408\n",
      "Iteration 428, loss = 0.41576945\n",
      "Iteration 429, loss = 0.41564587\n",
      "Iteration 430, loss = 0.41565546\n",
      "Iteration 431, loss = 0.41548793\n",
      "Iteration 432, loss = 0.41544213\n",
      "Iteration 433, loss = 0.41533730\n",
      "Iteration 434, loss = 0.41524925\n",
      "Iteration 435, loss = 0.41520666\n",
      "Iteration 436, loss = 0.41511999\n",
      "Iteration 437, loss = 0.41505686\n",
      "Iteration 438, loss = 0.41505460\n",
      "Iteration 439, loss = 0.41496920\n",
      "Iteration 440, loss = 0.41486154\n",
      "Iteration 441, loss = 0.41479860\n",
      "Iteration 442, loss = 0.41473193\n",
      "Iteration 443, loss = 0.41463932\n",
      "Iteration 444, loss = 0.41454433\n",
      "Iteration 445, loss = 0.41447584\n",
      "Iteration 446, loss = 0.41444846\n",
      "Iteration 447, loss = 0.41444812\n",
      "Iteration 448, loss = 0.41432309\n",
      "Iteration 449, loss = 0.41419985\n",
      "Iteration 450, loss = 0.41416144\n",
      "Iteration 451, loss = 0.41408598\n",
      "Iteration 452, loss = 0.41401277\n",
      "Iteration 453, loss = 0.41399295\n",
      "Iteration 454, loss = 0.41390339\n",
      "Iteration 455, loss = 0.41387616\n",
      "Iteration 456, loss = 0.41377885\n",
      "Iteration 457, loss = 0.41369586\n",
      "Iteration 458, loss = 0.41363061\n",
      "Iteration 459, loss = 0.41359173\n",
      "Iteration 460, loss = 0.41351444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93637315\n",
      "Iteration 2, loss = 1.80303300\n",
      "Iteration 3, loss = 1.69584059\n",
      "Iteration 4, loss = 1.60615739\n",
      "Iteration 5, loss = 1.52835870\n",
      "Iteration 6, loss = 1.45930444\n",
      "Iteration 7, loss = 1.39783399\n",
      "Iteration 8, loss = 1.34286597\n",
      "Iteration 9, loss = 1.29368811\n",
      "Iteration 10, loss = 1.24950310\n",
      "Iteration 11, loss = 1.20953343\n",
      "Iteration 12, loss = 1.17322558\n",
      "Iteration 13, loss = 1.13994319\n",
      "Iteration 14, loss = 1.10930760\n",
      "Iteration 15, loss = 1.08094093\n",
      "Iteration 16, loss = 1.05462835\n",
      "Iteration 17, loss = 1.03007639\n",
      "Iteration 18, loss = 1.00718110\n",
      "Iteration 19, loss = 0.98572101\n",
      "Iteration 20, loss = 0.96555996\n",
      "Iteration 21, loss = 0.94655465\n",
      "Iteration 22, loss = 0.92863558\n",
      "Iteration 23, loss = 0.91161878\n",
      "Iteration 24, loss = 0.89551484\n",
      "Iteration 25, loss = 0.88020184\n",
      "Iteration 26, loss = 0.86567489\n",
      "Iteration 27, loss = 0.85192647\n",
      "Iteration 28, loss = 0.83895437\n",
      "Iteration 29, loss = 0.82672012\n",
      "Iteration 30, loss = 0.81519376\n",
      "Iteration 31, loss = 0.80431085\n",
      "Iteration 32, loss = 0.79406602\n",
      "Iteration 33, loss = 0.78436085\n",
      "Iteration 34, loss = 0.77514102\n",
      "Iteration 35, loss = 0.76640193\n",
      "Iteration 36, loss = 0.75802292\n",
      "Iteration 37, loss = 0.75009669\n",
      "Iteration 38, loss = 0.74249055\n",
      "Iteration 39, loss = 0.73521348\n",
      "Iteration 40, loss = 0.72828851\n",
      "Iteration 41, loss = 0.72159934\n",
      "Iteration 42, loss = 0.71522197\n",
      "Iteration 43, loss = 0.70908881\n",
      "Iteration 44, loss = 0.70319989\n",
      "Iteration 45, loss = 0.69753558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.69210933\n",
      "Iteration 47, loss = 0.68686628\n",
      "Iteration 48, loss = 0.68181183\n",
      "Iteration 49, loss = 0.67695239\n",
      "Iteration 50, loss = 0.67222379\n",
      "Iteration 51, loss = 0.66765655\n",
      "Iteration 52, loss = 0.66322721\n",
      "Iteration 53, loss = 0.65893727\n",
      "Iteration 54, loss = 0.65478136\n",
      "Iteration 55, loss = 0.65072322\n",
      "Iteration 56, loss = 0.64678663\n",
      "Iteration 57, loss = 0.64292729\n",
      "Iteration 58, loss = 0.63921989\n",
      "Iteration 59, loss = 0.63556016\n",
      "Iteration 60, loss = 0.63200358\n",
      "Iteration 61, loss = 0.62851484\n",
      "Iteration 62, loss = 0.62508286\n",
      "Iteration 63, loss = 0.62175996\n",
      "Iteration 64, loss = 0.61848812\n",
      "Iteration 65, loss = 0.61530857\n",
      "Iteration 66, loss = 0.61214083\n",
      "Iteration 67, loss = 0.60909470\n",
      "Iteration 68, loss = 0.60604644\n",
      "Iteration 69, loss = 0.60310493\n",
      "Iteration 70, loss = 0.60017382\n",
      "Iteration 71, loss = 0.59729308\n",
      "Iteration 72, loss = 0.59451115\n",
      "Iteration 73, loss = 0.59170735\n",
      "Iteration 74, loss = 0.58897139\n",
      "Iteration 75, loss = 0.58630079\n",
      "Iteration 76, loss = 0.58366179\n",
      "Iteration 77, loss = 0.58107242\n",
      "Iteration 78, loss = 0.57850204\n",
      "Iteration 79, loss = 0.57599048\n",
      "Iteration 80, loss = 0.57350562\n",
      "Iteration 81, loss = 0.57108562\n",
      "Iteration 82, loss = 0.56866970\n",
      "Iteration 83, loss = 0.56631458\n",
      "Iteration 84, loss = 0.56397589\n",
      "Iteration 85, loss = 0.56168582\n",
      "Iteration 86, loss = 0.55943875\n",
      "Iteration 87, loss = 0.55722088\n",
      "Iteration 88, loss = 0.55503326\n",
      "Iteration 89, loss = 0.55286975\n",
      "Iteration 90, loss = 0.55077506\n",
      "Iteration 91, loss = 0.54868945\n",
      "Iteration 92, loss = 0.54664044\n",
      "Iteration 93, loss = 0.54465254\n",
      "Iteration 94, loss = 0.54267440\n",
      "Iteration 95, loss = 0.54073605\n",
      "Iteration 96, loss = 0.53882144\n",
      "Iteration 97, loss = 0.53696564\n",
      "Iteration 98, loss = 0.53516666\n",
      "Iteration 99, loss = 0.53333916\n",
      "Iteration 100, loss = 0.53160601\n",
      "Iteration 101, loss = 0.52988448\n",
      "Iteration 102, loss = 0.52815446\n",
      "Iteration 103, loss = 0.52648842\n",
      "Iteration 104, loss = 0.52489273\n",
      "Iteration 105, loss = 0.52325543\n",
      "Iteration 106, loss = 0.52172123\n",
      "Iteration 107, loss = 0.52018412\n",
      "Iteration 108, loss = 0.51867494\n",
      "Iteration 109, loss = 0.51718551\n",
      "Iteration 110, loss = 0.51576583\n",
      "Iteration 111, loss = 0.51430840\n",
      "Iteration 112, loss = 0.51291840\n",
      "Iteration 113, loss = 0.51158809\n",
      "Iteration 114, loss = 0.51027126\n",
      "Iteration 115, loss = 0.50892108\n",
      "Iteration 116, loss = 0.50765321\n",
      "Iteration 117, loss = 0.50639375\n",
      "Iteration 118, loss = 0.50516675\n",
      "Iteration 119, loss = 0.50398701\n",
      "Iteration 120, loss = 0.50278732\n",
      "Iteration 121, loss = 0.50156831\n",
      "Iteration 122, loss = 0.50047451\n",
      "Iteration 123, loss = 0.49937436\n",
      "Iteration 124, loss = 0.49825863\n",
      "Iteration 125, loss = 0.49722218\n",
      "Iteration 126, loss = 0.49618435\n",
      "Iteration 127, loss = 0.49514147\n",
      "Iteration 128, loss = 0.49412851\n",
      "Iteration 129, loss = 0.49316468\n",
      "Iteration 130, loss = 0.49217049\n",
      "Iteration 131, loss = 0.49125448\n",
      "Iteration 132, loss = 0.49030808\n",
      "Iteration 133, loss = 0.48937072\n",
      "Iteration 134, loss = 0.48849594\n",
      "Iteration 135, loss = 0.48760445\n",
      "Iteration 136, loss = 0.48678547\n",
      "Iteration 137, loss = 0.48590242\n",
      "Iteration 138, loss = 0.48509330\n",
      "Iteration 139, loss = 0.48424816\n",
      "Iteration 140, loss = 0.48345564\n",
      "Iteration 141, loss = 0.48268403\n",
      "Iteration 142, loss = 0.48194886\n",
      "Iteration 143, loss = 0.48114903\n",
      "Iteration 144, loss = 0.48041630\n",
      "Iteration 145, loss = 0.47967982\n",
      "Iteration 146, loss = 0.47899567\n",
      "Iteration 147, loss = 0.47828016\n",
      "Iteration 148, loss = 0.47760896\n",
      "Iteration 149, loss = 0.47687893\n",
      "Iteration 150, loss = 0.47624387\n",
      "Iteration 151, loss = 0.47552569\n",
      "Iteration 152, loss = 0.47495905\n",
      "Iteration 153, loss = 0.47428211\n",
      "Iteration 154, loss = 0.47370194\n",
      "Iteration 155, loss = 0.47302899\n",
      "Iteration 156, loss = 0.47244225\n",
      "Iteration 157, loss = 0.47182901\n",
      "Iteration 158, loss = 0.47129716\n",
      "Iteration 159, loss = 0.47071567\n",
      "Iteration 160, loss = 0.47012376\n",
      "Iteration 161, loss = 0.46957664\n",
      "Iteration 162, loss = 0.46901886\n",
      "Iteration 163, loss = 0.46849729\n",
      "Iteration 164, loss = 0.46794561\n",
      "Iteration 165, loss = 0.46743771\n",
      "Iteration 166, loss = 0.46699019\n",
      "Iteration 167, loss = 0.46641904\n",
      "Iteration 168, loss = 0.46593168\n",
      "Iteration 169, loss = 0.46542503\n",
      "Iteration 170, loss = 0.46489958\n",
      "Iteration 171, loss = 0.46449063\n",
      "Iteration 172, loss = 0.46396125\n",
      "Iteration 173, loss = 0.46349976\n",
      "Iteration 174, loss = 0.46307258\n",
      "Iteration 175, loss = 0.46256827\n",
      "Iteration 176, loss = 0.46215805\n",
      "Iteration 177, loss = 0.46169753\n",
      "Iteration 178, loss = 0.46128343\n",
      "Iteration 179, loss = 0.46085674\n",
      "Iteration 180, loss = 0.46036265\n",
      "Iteration 181, loss = 0.45998541\n",
      "Iteration 182, loss = 0.45958880\n",
      "Iteration 183, loss = 0.45923180\n",
      "Iteration 184, loss = 0.45882140\n",
      "Iteration 185, loss = 0.45836972\n",
      "Iteration 186, loss = 0.45798044\n",
      "Iteration 187, loss = 0.45759873\n",
      "Iteration 188, loss = 0.45719974\n",
      "Iteration 189, loss = 0.45687559\n",
      "Iteration 190, loss = 0.45652906\n",
      "Iteration 191, loss = 0.45616177\n",
      "Iteration 192, loss = 0.45579633\n",
      "Iteration 193, loss = 0.45542412\n",
      "Iteration 194, loss = 0.45505076\n",
      "Iteration 195, loss = 0.45473721\n",
      "Iteration 196, loss = 0.45439083\n",
      "Iteration 197, loss = 0.45401366\n",
      "Iteration 198, loss = 0.45364588\n",
      "Iteration 199, loss = 0.45337186\n",
      "Iteration 200, loss = 0.45307603\n",
      "Iteration 201, loss = 0.45280494\n",
      "Iteration 202, loss = 0.45236495\n",
      "Iteration 203, loss = 0.45204857\n",
      "Iteration 204, loss = 0.45170442\n",
      "Iteration 205, loss = 0.45147854\n",
      "Iteration 206, loss = 0.45113614\n",
      "Iteration 207, loss = 0.45086505\n",
      "Iteration 208, loss = 0.45057385\n",
      "Iteration 209, loss = 0.45023587\n",
      "Iteration 210, loss = 0.44998993\n",
      "Iteration 211, loss = 0.44970325\n",
      "Iteration 212, loss = 0.44941653\n",
      "Iteration 213, loss = 0.44913109\n",
      "Iteration 214, loss = 0.44885061\n",
      "Iteration 215, loss = 0.44853244\n",
      "Iteration 216, loss = 0.44827194\n",
      "Iteration 217, loss = 0.44805690\n",
      "Iteration 218, loss = 0.44775099\n",
      "Iteration 219, loss = 0.44750317\n",
      "Iteration 220, loss = 0.44718544\n",
      "Iteration 221, loss = 0.44695871\n",
      "Iteration 222, loss = 0.44671403\n",
      "Iteration 223, loss = 0.44641594\n",
      "Iteration 224, loss = 0.44620877\n",
      "Iteration 225, loss = 0.44594855\n",
      "Iteration 226, loss = 0.44575001\n",
      "Iteration 227, loss = 0.44543896\n",
      "Iteration 228, loss = 0.44514424\n",
      "Iteration 229, loss = 0.44498295\n",
      "Iteration 230, loss = 0.44466960\n",
      "Iteration 231, loss = 0.44455464\n",
      "Iteration 232, loss = 0.44426586\n",
      "Iteration 233, loss = 0.44401402\n",
      "Iteration 234, loss = 0.44384044\n",
      "Iteration 235, loss = 0.44358744\n",
      "Iteration 236, loss = 0.44336897\n",
      "Iteration 237, loss = 0.44316678\n",
      "Iteration 238, loss = 0.44293300\n",
      "Iteration 239, loss = 0.44270621\n",
      "Iteration 240, loss = 0.44246727\n",
      "Iteration 241, loss = 0.44222277\n",
      "Iteration 242, loss = 0.44207874\n",
      "Iteration 243, loss = 0.44181145\n",
      "Iteration 244, loss = 0.44162594\n",
      "Iteration 245, loss = 0.44145751\n",
      "Iteration 246, loss = 0.44118701\n",
      "Iteration 247, loss = 0.44101297\n",
      "Iteration 248, loss = 0.44081825\n",
      "Iteration 249, loss = 0.44057742\n",
      "Iteration 250, loss = 0.44041641\n",
      "Iteration 251, loss = 0.44019259\n",
      "Iteration 252, loss = 0.44004598\n",
      "Iteration 253, loss = 0.43982730\n",
      "Iteration 254, loss = 0.43964437\n",
      "Iteration 255, loss = 0.43949935\n",
      "Iteration 256, loss = 0.43927760\n",
      "Iteration 257, loss = 0.43900441\n",
      "Iteration 258, loss = 0.43889386\n",
      "Iteration 259, loss = 0.43873520\n",
      "Iteration 260, loss = 0.43861160\n",
      "Iteration 261, loss = 0.43829176\n",
      "Iteration 262, loss = 0.43812519\n",
      "Iteration 263, loss = 0.43799730\n",
      "Iteration 264, loss = 0.43779516\n",
      "Iteration 265, loss = 0.43766343\n",
      "Iteration 266, loss = 0.43749952\n",
      "Iteration 267, loss = 0.43726163\n",
      "Iteration 268, loss = 0.43706395\n",
      "Iteration 269, loss = 0.43688704\n",
      "Iteration 270, loss = 0.43674850\n",
      "Iteration 271, loss = 0.43658304\n",
      "Iteration 272, loss = 0.43649989\n",
      "Iteration 273, loss = 0.43628399\n",
      "Iteration 274, loss = 0.43605629\n",
      "Iteration 275, loss = 0.43585463\n",
      "Iteration 276, loss = 0.43571414\n",
      "Iteration 277, loss = 0.43561016\n",
      "Iteration 278, loss = 0.43542876\n",
      "Iteration 279, loss = 0.43532789\n",
      "Iteration 280, loss = 0.43508604\n",
      "Iteration 281, loss = 0.43501886\n",
      "Iteration 282, loss = 0.43477944\n",
      "Iteration 283, loss = 0.43464363\n",
      "Iteration 284, loss = 0.43449173\n",
      "Iteration 285, loss = 0.43437148\n",
      "Iteration 286, loss = 0.43416706\n",
      "Iteration 287, loss = 0.43407314\n",
      "Iteration 288, loss = 0.43389181\n",
      "Iteration 289, loss = 0.43376125\n",
      "Iteration 290, loss = 0.43358199\n",
      "Iteration 291, loss = 0.43347093\n",
      "Iteration 292, loss = 0.43337610\n",
      "Iteration 293, loss = 0.43316985\n",
      "Iteration 294, loss = 0.43298214\n",
      "Iteration 295, loss = 0.43292488\n",
      "Iteration 296, loss = 0.43278444\n",
      "Iteration 297, loss = 0.43256217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 298, loss = 0.43245912\n",
      "Iteration 299, loss = 0.43228877\n",
      "Iteration 300, loss = 0.43220969\n",
      "Iteration 301, loss = 0.43204809\n",
      "Iteration 302, loss = 0.43194321\n",
      "Iteration 303, loss = 0.43176054\n",
      "Iteration 304, loss = 0.43170433\n",
      "Iteration 305, loss = 0.43154487\n",
      "Iteration 306, loss = 0.43135846\n",
      "Iteration 307, loss = 0.43126338\n",
      "Iteration 308, loss = 0.43106333\n",
      "Iteration 309, loss = 0.43098511\n",
      "Iteration 310, loss = 0.43082036\n",
      "Iteration 311, loss = 0.43071804\n",
      "Iteration 312, loss = 0.43061693\n",
      "Iteration 313, loss = 0.43044004\n",
      "Iteration 314, loss = 0.43028147\n",
      "Iteration 315, loss = 0.43030261\n",
      "Iteration 316, loss = 0.43006802\n",
      "Iteration 317, loss = 0.43003321\n",
      "Iteration 318, loss = 0.42985959\n",
      "Iteration 319, loss = 0.42969668\n",
      "Iteration 320, loss = 0.42963245\n",
      "Iteration 321, loss = 0.42945027\n",
      "Iteration 322, loss = 0.42932712\n",
      "Iteration 323, loss = 0.42918480\n",
      "Iteration 324, loss = 0.42906710\n",
      "Iteration 325, loss = 0.42908671\n",
      "Iteration 326, loss = 0.42882860\n",
      "Iteration 327, loss = 0.42874350\n",
      "Iteration 328, loss = 0.42868674\n",
      "Iteration 329, loss = 0.42858115\n",
      "Iteration 330, loss = 0.42840505\n",
      "Iteration 331, loss = 0.42830213\n",
      "Iteration 332, loss = 0.42817490\n",
      "Iteration 333, loss = 0.42809320\n",
      "Iteration 334, loss = 0.42795531\n",
      "Iteration 335, loss = 0.42789813\n",
      "Iteration 336, loss = 0.42771644\n",
      "Iteration 337, loss = 0.42763565\n",
      "Iteration 338, loss = 0.42746858\n",
      "Iteration 339, loss = 0.42738621\n",
      "Iteration 340, loss = 0.42731892\n",
      "Iteration 341, loss = 0.42723651\n",
      "Iteration 342, loss = 0.42712928\n",
      "Iteration 343, loss = 0.42698164\n",
      "Iteration 344, loss = 0.42692468\n",
      "Iteration 345, loss = 0.42681979\n",
      "Iteration 346, loss = 0.42669769\n",
      "Iteration 347, loss = 0.42651911\n",
      "Iteration 348, loss = 0.42648946\n",
      "Iteration 349, loss = 0.42630521\n",
      "Iteration 350, loss = 0.42630212\n",
      "Iteration 351, loss = 0.42610991\n",
      "Iteration 352, loss = 0.42607745\n",
      "Iteration 353, loss = 0.42596724\n",
      "Iteration 354, loss = 0.42582413\n",
      "Iteration 355, loss = 0.42573589\n",
      "Iteration 356, loss = 0.42564240\n",
      "Iteration 357, loss = 0.42555899\n",
      "Iteration 358, loss = 0.42552401\n",
      "Iteration 359, loss = 0.42529712\n",
      "Iteration 360, loss = 0.42519988\n",
      "Iteration 361, loss = 0.42511300\n",
      "Iteration 362, loss = 0.42502787\n",
      "Iteration 363, loss = 0.42502407\n",
      "Iteration 364, loss = 0.42486775\n",
      "Iteration 365, loss = 0.42477175\n",
      "Iteration 366, loss = 0.42465806\n",
      "Iteration 367, loss = 0.42456716\n",
      "Iteration 368, loss = 0.42449687\n",
      "Iteration 369, loss = 0.42437752\n",
      "Iteration 370, loss = 0.42435339\n",
      "Iteration 371, loss = 0.42416316\n",
      "Iteration 372, loss = 0.42409294\n",
      "Iteration 373, loss = 0.42401115\n",
      "Iteration 374, loss = 0.42393621\n",
      "Iteration 375, loss = 0.42382842\n",
      "Iteration 376, loss = 0.42372012\n",
      "Iteration 377, loss = 0.42364669\n",
      "Iteration 378, loss = 0.42351873\n",
      "Iteration 379, loss = 0.42337292\n",
      "Iteration 380, loss = 0.42339107\n",
      "Iteration 381, loss = 0.42323983\n",
      "Iteration 382, loss = 0.42327369\n",
      "Iteration 383, loss = 0.42309323\n",
      "Iteration 384, loss = 0.42300148\n",
      "Iteration 385, loss = 0.42290178\n",
      "Iteration 386, loss = 0.42279057\n",
      "Iteration 387, loss = 0.42277788\n",
      "Iteration 388, loss = 0.42270923\n",
      "Iteration 389, loss = 0.42255347\n",
      "Iteration 390, loss = 0.42251704\n",
      "Iteration 391, loss = 0.42248359\n",
      "Iteration 392, loss = 0.42227834\n",
      "Iteration 393, loss = 0.42220735\n",
      "Iteration 394, loss = 0.42215849\n",
      "Iteration 395, loss = 0.42210531\n",
      "Iteration 396, loss = 0.42194199\n",
      "Iteration 397, loss = 0.42191444\n",
      "Iteration 398, loss = 0.42183241\n",
      "Iteration 399, loss = 0.42173484\n",
      "Iteration 400, loss = 0.42162104\n",
      "Iteration 401, loss = 0.42161936\n",
      "Iteration 402, loss = 0.42148790\n",
      "Iteration 403, loss = 0.42143179\n",
      "Iteration 404, loss = 0.42138318\n",
      "Iteration 405, loss = 0.42127367\n",
      "Iteration 406, loss = 0.42117519\n",
      "Iteration 407, loss = 0.42110490\n",
      "Iteration 408, loss = 0.42100420\n",
      "Iteration 409, loss = 0.42091528\n",
      "Iteration 410, loss = 0.42088371\n",
      "Iteration 411, loss = 0.42078483\n",
      "Iteration 412, loss = 0.42069327\n",
      "Iteration 413, loss = 0.42063195\n",
      "Iteration 414, loss = 0.42055662\n",
      "Iteration 415, loss = 0.42046495\n",
      "Iteration 416, loss = 0.42037493\n",
      "Iteration 417, loss = 0.42031238\n",
      "Iteration 418, loss = 0.42026211\n",
      "Iteration 419, loss = 0.42008941\n",
      "Iteration 420, loss = 0.42004256\n",
      "Iteration 421, loss = 0.42001860\n",
      "Iteration 422, loss = 0.41994473\n",
      "Iteration 423, loss = 0.41986414\n",
      "Iteration 424, loss = 0.41974998\n",
      "Iteration 425, loss = 0.41973608\n",
      "Iteration 426, loss = 0.41962338\n",
      "Iteration 427, loss = 0.41952603\n",
      "Iteration 428, loss = 0.41950581\n",
      "Iteration 429, loss = 0.41948084\n",
      "Iteration 430, loss = 0.41930563\n",
      "Iteration 431, loss = 0.41920261\n",
      "Iteration 432, loss = 0.41922783\n",
      "Iteration 433, loss = 0.41905523\n",
      "Iteration 434, loss = 0.41907442\n",
      "Iteration 435, loss = 0.41901218\n",
      "Iteration 436, loss = 0.41896437\n",
      "Iteration 437, loss = 0.41882659\n",
      "Iteration 438, loss = 0.41873974\n",
      "Iteration 439, loss = 0.41867866\n",
      "Iteration 440, loss = 0.41857498\n",
      "Iteration 441, loss = 0.41855767\n",
      "Iteration 442, loss = 0.41848392\n",
      "Iteration 443, loss = 0.41838084\n",
      "Iteration 444, loss = 0.41836074\n",
      "Iteration 445, loss = 0.41825454\n",
      "Iteration 446, loss = 0.41819202\n",
      "Iteration 447, loss = 0.41807350\n",
      "Iteration 448, loss = 0.41813966\n",
      "Iteration 449, loss = 0.41800620\n",
      "Iteration 450, loss = 0.41792618\n",
      "Iteration 451, loss = 0.41780206\n",
      "Iteration 452, loss = 0.41777849\n",
      "Iteration 453, loss = 0.41767048\n",
      "Iteration 454, loss = 0.41763000\n",
      "Iteration 455, loss = 0.41754830\n",
      "Iteration 456, loss = 0.41748371\n",
      "Iteration 457, loss = 0.41746114\n",
      "Iteration 458, loss = 0.41732987\n",
      "Iteration 459, loss = 0.41723949\n",
      "Iteration 460, loss = 0.41721593\n",
      "Iteration 461, loss = 0.41712972\n",
      "Iteration 462, loss = 0.41712668\n",
      "Iteration 463, loss = 0.41706173\n",
      "Iteration 464, loss = 0.41697985\n",
      "Iteration 465, loss = 0.41689241\n",
      "Iteration 466, loss = 0.41681548\n",
      "Iteration 467, loss = 0.41678145\n",
      "Iteration 468, loss = 0.41676170\n",
      "Iteration 469, loss = 0.41665283\n",
      "Iteration 470, loss = 0.41655026\n",
      "Iteration 471, loss = 0.41654286\n",
      "Iteration 472, loss = 0.41645574\n",
      "Iteration 473, loss = 0.41637237\n",
      "Iteration 474, loss = 0.41634801\n",
      "Iteration 475, loss = 0.41627960\n",
      "Iteration 476, loss = 0.41619579\n",
      "Iteration 477, loss = 0.41613597\n",
      "Iteration 478, loss = 0.41606971\n",
      "Iteration 479, loss = 0.41599270\n",
      "Iteration 480, loss = 0.41596788\n",
      "Iteration 481, loss = 0.41586876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.93809418\n",
      "Iteration 2, loss = 1.80345693\n",
      "Iteration 3, loss = 1.69530858\n",
      "Iteration 4, loss = 1.60490544\n",
      "Iteration 5, loss = 1.52655062\n",
      "Iteration 6, loss = 1.45737158\n",
      "Iteration 7, loss = 1.39589874\n",
      "Iteration 8, loss = 1.34109520\n",
      "Iteration 9, loss = 1.29220184\n",
      "Iteration 10, loss = 1.24828097\n",
      "Iteration 11, loss = 1.20858085\n",
      "Iteration 12, loss = 1.17244198\n",
      "Iteration 13, loss = 1.13932339\n",
      "Iteration 14, loss = 1.10880962\n",
      "Iteration 15, loss = 1.08056583\n",
      "Iteration 16, loss = 1.05434955\n",
      "Iteration 17, loss = 1.02991566\n",
      "Iteration 18, loss = 1.00708417\n",
      "Iteration 19, loss = 0.98573972\n",
      "Iteration 20, loss = 0.96567146\n",
      "Iteration 21, loss = 0.94675706\n",
      "Iteration 22, loss = 0.92885825\n",
      "Iteration 23, loss = 0.91187456\n",
      "Iteration 24, loss = 0.89575031\n",
      "Iteration 25, loss = 0.88045551\n",
      "Iteration 26, loss = 0.86593134\n",
      "Iteration 27, loss = 0.85225327\n",
      "Iteration 28, loss = 0.83932532\n",
      "Iteration 29, loss = 0.82716197\n",
      "Iteration 30, loss = 0.81563030\n",
      "Iteration 31, loss = 0.80475687\n",
      "Iteration 32, loss = 0.79444452\n",
      "Iteration 33, loss = 0.78467037\n",
      "Iteration 34, loss = 0.77537865\n",
      "Iteration 35, loss = 0.76652939\n",
      "Iteration 36, loss = 0.75810466\n",
      "Iteration 37, loss = 0.75010253\n",
      "Iteration 38, loss = 0.74248626\n",
      "Iteration 39, loss = 0.73518707\n",
      "Iteration 40, loss = 0.72824060\n",
      "Iteration 41, loss = 0.72160010\n",
      "Iteration 42, loss = 0.71520966\n",
      "Iteration 43, loss = 0.70912762\n",
      "Iteration 44, loss = 0.70320820\n",
      "Iteration 45, loss = 0.69756309\n",
      "Iteration 46, loss = 0.69212803\n",
      "Iteration 47, loss = 0.68687730\n",
      "Iteration 48, loss = 0.68179464\n",
      "Iteration 49, loss = 0.67691192\n",
      "Iteration 50, loss = 0.67217092\n",
      "Iteration 51, loss = 0.66760328\n",
      "Iteration 52, loss = 0.66314050\n",
      "Iteration 53, loss = 0.65881335\n",
      "Iteration 54, loss = 0.65464283\n",
      "Iteration 55, loss = 0.65057153\n",
      "Iteration 56, loss = 0.64658576\n",
      "Iteration 57, loss = 0.64275061\n",
      "Iteration 58, loss = 0.63898065\n",
      "Iteration 59, loss = 0.63530921\n",
      "Iteration 60, loss = 0.63172867\n",
      "Iteration 61, loss = 0.62821554\n",
      "Iteration 62, loss = 0.62479363\n",
      "Iteration 63, loss = 0.62145219\n",
      "Iteration 64, loss = 0.61818333\n",
      "Iteration 65, loss = 0.61494933\n",
      "Iteration 66, loss = 0.61176939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67, loss = 0.60865993\n",
      "Iteration 68, loss = 0.60568181\n",
      "Iteration 69, loss = 0.60267526\n",
      "Iteration 70, loss = 0.59975887\n",
      "Iteration 71, loss = 0.59688074\n",
      "Iteration 72, loss = 0.59405008\n",
      "Iteration 73, loss = 0.59127944\n",
      "Iteration 74, loss = 0.58853332\n",
      "Iteration 75, loss = 0.58582229\n",
      "Iteration 76, loss = 0.58317894\n",
      "Iteration 77, loss = 0.58055818\n",
      "Iteration 78, loss = 0.57795956\n",
      "Iteration 79, loss = 0.57544731\n",
      "Iteration 80, loss = 0.57295573\n",
      "Iteration 81, loss = 0.57047080\n",
      "Iteration 82, loss = 0.56808587\n",
      "Iteration 83, loss = 0.56568813\n",
      "Iteration 84, loss = 0.56329935\n",
      "Iteration 85, loss = 0.56098972\n",
      "Iteration 86, loss = 0.55870371\n",
      "Iteration 87, loss = 0.55646690\n",
      "Iteration 88, loss = 0.55424590\n",
      "Iteration 89, loss = 0.55205276\n",
      "Iteration 90, loss = 0.54991200\n",
      "Iteration 91, loss = 0.54778931\n",
      "Iteration 92, loss = 0.54572669\n",
      "Iteration 93, loss = 0.54366880\n",
      "Iteration 94, loss = 0.54171223\n",
      "Iteration 95, loss = 0.53970611\n",
      "Iteration 96, loss = 0.53775877\n",
      "Iteration 97, loss = 0.53588180\n",
      "Iteration 98, loss = 0.53397398\n",
      "Iteration 99, loss = 0.53215584\n",
      "Iteration 100, loss = 0.53034084\n",
      "Iteration 101, loss = 0.52854513\n",
      "Iteration 102, loss = 0.52683502\n",
      "Iteration 103, loss = 0.52512698\n",
      "Iteration 104, loss = 0.52346112\n",
      "Iteration 105, loss = 0.52180398\n",
      "Iteration 106, loss = 0.52022416\n",
      "Iteration 107, loss = 0.51864619\n",
      "Iteration 108, loss = 0.51708506\n",
      "Iteration 109, loss = 0.51554417\n",
      "Iteration 110, loss = 0.51404624\n",
      "Iteration 111, loss = 0.51263567\n",
      "Iteration 112, loss = 0.51120038\n",
      "Iteration 113, loss = 0.50977524\n",
      "Iteration 114, loss = 0.50835169\n",
      "Iteration 115, loss = 0.50701469\n",
      "Iteration 116, loss = 0.50570159\n",
      "Iteration 117, loss = 0.50439611\n",
      "Iteration 118, loss = 0.50316980\n",
      "Iteration 119, loss = 0.50187744\n",
      "Iteration 120, loss = 0.50068550\n",
      "Iteration 121, loss = 0.49944781\n",
      "Iteration 122, loss = 0.49829089\n",
      "Iteration 123, loss = 0.49714462\n",
      "Iteration 124, loss = 0.49604177\n",
      "Iteration 125, loss = 0.49492617\n",
      "Iteration 126, loss = 0.49386268\n",
      "Iteration 127, loss = 0.49279231\n",
      "Iteration 128, loss = 0.49173480\n",
      "Iteration 129, loss = 0.49069778\n",
      "Iteration 130, loss = 0.48971699\n",
      "Iteration 131, loss = 0.48870902\n",
      "Iteration 132, loss = 0.48772355\n",
      "Iteration 133, loss = 0.48678919\n",
      "Iteration 134, loss = 0.48590568\n",
      "Iteration 135, loss = 0.48494370\n",
      "Iteration 136, loss = 0.48406681\n",
      "Iteration 137, loss = 0.48320104\n",
      "Iteration 138, loss = 0.48230661\n",
      "Iteration 139, loss = 0.48146748\n",
      "Iteration 140, loss = 0.48068667\n",
      "Iteration 141, loss = 0.47981116\n",
      "Iteration 142, loss = 0.47903612\n",
      "Iteration 143, loss = 0.47827085\n",
      "Iteration 144, loss = 0.47748405\n",
      "Iteration 145, loss = 0.47670915\n",
      "Iteration 146, loss = 0.47601675\n",
      "Iteration 147, loss = 0.47522590\n",
      "Iteration 148, loss = 0.47453666\n",
      "Iteration 149, loss = 0.47381581\n",
      "Iteration 150, loss = 0.47311747\n",
      "Iteration 151, loss = 0.47245401\n",
      "Iteration 152, loss = 0.47179881\n",
      "Iteration 153, loss = 0.47111357\n",
      "Iteration 154, loss = 0.47048187\n",
      "Iteration 155, loss = 0.46984218\n",
      "Iteration 156, loss = 0.46924209\n",
      "Iteration 157, loss = 0.46862212\n",
      "Iteration 158, loss = 0.46804660\n",
      "Iteration 159, loss = 0.46738221\n",
      "Iteration 160, loss = 0.46680393\n",
      "Iteration 161, loss = 0.46626682\n",
      "Iteration 162, loss = 0.46565959\n",
      "Iteration 163, loss = 0.46514961\n",
      "Iteration 164, loss = 0.46455910\n",
      "Iteration 165, loss = 0.46400721\n",
      "Iteration 166, loss = 0.46350743\n",
      "Iteration 167, loss = 0.46298164\n",
      "Iteration 168, loss = 0.46245724\n",
      "Iteration 169, loss = 0.46191153\n",
      "Iteration 170, loss = 0.46144351\n",
      "Iteration 171, loss = 0.46094995\n",
      "Iteration 172, loss = 0.46048376\n",
      "Iteration 173, loss = 0.45997364\n",
      "Iteration 174, loss = 0.45953035\n",
      "Iteration 175, loss = 0.45903561\n",
      "Iteration 176, loss = 0.45863511\n",
      "Iteration 177, loss = 0.45810297\n",
      "Iteration 178, loss = 0.45766215\n",
      "Iteration 179, loss = 0.45725411\n",
      "Iteration 180, loss = 0.45679969\n",
      "Iteration 181, loss = 0.45640889\n",
      "Iteration 182, loss = 0.45591280\n",
      "Iteration 183, loss = 0.45549049\n",
      "Iteration 184, loss = 0.45507599\n",
      "Iteration 185, loss = 0.45477474\n",
      "Iteration 186, loss = 0.45436930\n",
      "Iteration 187, loss = 0.45390005\n",
      "Iteration 188, loss = 0.45348399\n",
      "Iteration 189, loss = 0.45315072\n",
      "Iteration 190, loss = 0.45275195\n",
      "Iteration 191, loss = 0.45236347\n",
      "Iteration 192, loss = 0.45199863\n",
      "Iteration 193, loss = 0.45165099\n",
      "Iteration 194, loss = 0.45126800\n",
      "Iteration 195, loss = 0.45094081\n",
      "Iteration 196, loss = 0.45060863\n",
      "Iteration 197, loss = 0.45025380\n",
      "Iteration 198, loss = 0.44989059\n",
      "Iteration 199, loss = 0.44957280\n",
      "Iteration 200, loss = 0.44925057\n",
      "Iteration 201, loss = 0.44893886\n",
      "Iteration 202, loss = 0.44855085\n",
      "Iteration 203, loss = 0.44824086\n",
      "Iteration 204, loss = 0.44794047\n",
      "Iteration 205, loss = 0.44757521\n",
      "Iteration 206, loss = 0.44729271\n",
      "Iteration 207, loss = 0.44692091\n",
      "Iteration 208, loss = 0.44674247\n",
      "Iteration 209, loss = 0.44633539\n",
      "Iteration 210, loss = 0.44606295\n",
      "Iteration 211, loss = 0.44579154\n",
      "Iteration 212, loss = 0.44550246\n",
      "Iteration 213, loss = 0.44523226\n",
      "Iteration 214, loss = 0.44487226\n",
      "Iteration 215, loss = 0.44459931\n",
      "Iteration 216, loss = 0.44432427\n",
      "Iteration 217, loss = 0.44411740\n",
      "Iteration 218, loss = 0.44373097\n",
      "Iteration 219, loss = 0.44353863\n",
      "Iteration 220, loss = 0.44325662\n",
      "Iteration 221, loss = 0.44299878\n",
      "Iteration 222, loss = 0.44273745\n",
      "Iteration 223, loss = 0.44246115\n",
      "Iteration 224, loss = 0.44218648\n",
      "Iteration 225, loss = 0.44190415\n",
      "Iteration 226, loss = 0.44164008\n",
      "Iteration 227, loss = 0.44141991\n",
      "Iteration 228, loss = 0.44114520\n",
      "Iteration 229, loss = 0.44093817\n",
      "Iteration 230, loss = 0.44071473\n",
      "Iteration 231, loss = 0.44044536\n",
      "Iteration 232, loss = 0.44029675\n",
      "Iteration 233, loss = 0.43996944\n",
      "Iteration 234, loss = 0.43972946\n",
      "Iteration 235, loss = 0.43945688\n",
      "Iteration 236, loss = 0.43921588\n",
      "Iteration 237, loss = 0.43906328\n",
      "Iteration 238, loss = 0.43883120\n",
      "Iteration 239, loss = 0.43856489\n",
      "Iteration 240, loss = 0.43837609\n",
      "Iteration 241, loss = 0.43818197\n",
      "Iteration 242, loss = 0.43794285\n",
      "Iteration 243, loss = 0.43773342\n",
      "Iteration 244, loss = 0.43746165\n",
      "Iteration 245, loss = 0.43721390\n",
      "Iteration 246, loss = 0.43702482\n",
      "Iteration 247, loss = 0.43679913\n",
      "Iteration 248, loss = 0.43662053\n",
      "Iteration 249, loss = 0.43638378\n",
      "Iteration 250, loss = 0.43623356\n",
      "Iteration 251, loss = 0.43604862\n",
      "Iteration 252, loss = 0.43577165\n",
      "Iteration 253, loss = 0.43565283\n",
      "Iteration 254, loss = 0.43536580\n",
      "Iteration 255, loss = 0.43521346\n",
      "Iteration 256, loss = 0.43498838\n",
      "Iteration 257, loss = 0.43482152\n",
      "Iteration 258, loss = 0.43466952\n",
      "Iteration 259, loss = 0.43444155\n",
      "Iteration 260, loss = 0.43428318\n",
      "Iteration 261, loss = 0.43400732\n",
      "Iteration 262, loss = 0.43392744\n",
      "Iteration 263, loss = 0.43367593\n",
      "Iteration 264, loss = 0.43353483\n",
      "Iteration 265, loss = 0.43331703\n",
      "Iteration 266, loss = 0.43312290\n",
      "Iteration 267, loss = 0.43304932\n",
      "Iteration 268, loss = 0.43280266\n",
      "Iteration 269, loss = 0.43256942\n",
      "Iteration 270, loss = 0.43243031\n",
      "Iteration 271, loss = 0.43227913\n",
      "Iteration 272, loss = 0.43211172\n",
      "Iteration 273, loss = 0.43187013\n",
      "Iteration 274, loss = 0.43178866\n",
      "Iteration 275, loss = 0.43160825\n",
      "Iteration 276, loss = 0.43139086\n",
      "Iteration 277, loss = 0.43125829\n",
      "Iteration 278, loss = 0.43108355\n",
      "Iteration 279, loss = 0.43098004\n",
      "Iteration 280, loss = 0.43075767\n",
      "Iteration 281, loss = 0.43058976\n",
      "Iteration 282, loss = 0.43041226\n",
      "Iteration 283, loss = 0.43022967\n",
      "Iteration 284, loss = 0.43010752\n",
      "Iteration 285, loss = 0.42996048\n",
      "Iteration 286, loss = 0.42971528\n",
      "Iteration 287, loss = 0.42965808\n",
      "Iteration 288, loss = 0.42950219\n",
      "Iteration 289, loss = 0.42934807\n",
      "Iteration 290, loss = 0.42917136\n",
      "Iteration 291, loss = 0.42900585\n",
      "Iteration 292, loss = 0.42890271\n",
      "Iteration 293, loss = 0.42880255\n",
      "Iteration 294, loss = 0.42862174\n",
      "Iteration 295, loss = 0.42844590\n",
      "Iteration 296, loss = 0.42826177\n",
      "Iteration 297, loss = 0.42813033\n",
      "Iteration 298, loss = 0.42801166\n",
      "Iteration 299, loss = 0.42788215\n",
      "Iteration 300, loss = 0.42773211\n",
      "Iteration 301, loss = 0.42761181\n",
      "Iteration 302, loss = 0.42745140\n",
      "Iteration 303, loss = 0.42731068\n",
      "Iteration 304, loss = 0.42714921\n",
      "Iteration 305, loss = 0.42699942\n",
      "Iteration 306, loss = 0.42687985\n",
      "Iteration 307, loss = 0.42676514\n",
      "Iteration 308, loss = 0.42655500\n",
      "Iteration 309, loss = 0.42639865\n",
      "Iteration 310, loss = 0.42633351\n",
      "Iteration 311, loss = 0.42618293\n",
      "Iteration 312, loss = 0.42598791\n",
      "Iteration 313, loss = 0.42587474\n",
      "Iteration 314, loss = 0.42576927\n",
      "Iteration 315, loss = 0.42568156\n",
      "Iteration 316, loss = 0.42555509\n",
      "Iteration 317, loss = 0.42538420\n",
      "Iteration 318, loss = 0.42531100\n",
      "Iteration 319, loss = 0.42521696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 320, loss = 0.42504216\n",
      "Iteration 321, loss = 0.42488803\n",
      "Iteration 322, loss = 0.42469651\n",
      "Iteration 323, loss = 0.42465388\n",
      "Iteration 324, loss = 0.42453469\n",
      "Iteration 325, loss = 0.42439976\n",
      "Iteration 326, loss = 0.42430985\n",
      "Iteration 327, loss = 0.42414912\n",
      "Iteration 328, loss = 0.42406729\n",
      "Iteration 329, loss = 0.42390672\n",
      "Iteration 330, loss = 0.42381485\n",
      "Iteration 331, loss = 0.42358461\n",
      "Iteration 332, loss = 0.42354598\n",
      "Iteration 333, loss = 0.42340224\n",
      "Iteration 334, loss = 0.42325524\n",
      "Iteration 335, loss = 0.42313335\n",
      "Iteration 336, loss = 0.42304843\n",
      "Iteration 337, loss = 0.42293100\n",
      "Iteration 338, loss = 0.42287752\n",
      "Iteration 339, loss = 0.42272754\n",
      "Iteration 340, loss = 0.42264481\n",
      "Iteration 341, loss = 0.42250806\n",
      "Iteration 342, loss = 0.42236919\n",
      "Iteration 343, loss = 0.42230701\n",
      "Iteration 344, loss = 0.42218142\n",
      "Iteration 345, loss = 0.42204408\n",
      "Iteration 346, loss = 0.42194080\n",
      "Iteration 347, loss = 0.42182229\n",
      "Iteration 348, loss = 0.42175722\n",
      "Iteration 349, loss = 0.42159652\n",
      "Iteration 350, loss = 0.42155564\n",
      "Iteration 351, loss = 0.42138558\n",
      "Iteration 352, loss = 0.42126411\n",
      "Iteration 353, loss = 0.42108916\n",
      "Iteration 354, loss = 0.42104688\n",
      "Iteration 355, loss = 0.42095636\n",
      "Iteration 356, loss = 0.42080923\n",
      "Iteration 357, loss = 0.42075370\n",
      "Iteration 358, loss = 0.42066602\n",
      "Iteration 359, loss = 0.42058784\n",
      "Iteration 360, loss = 0.42042747\n",
      "Iteration 361, loss = 0.42031963\n",
      "Iteration 362, loss = 0.42016058\n",
      "Iteration 363, loss = 0.42011398\n",
      "Iteration 364, loss = 0.42002476\n",
      "Iteration 365, loss = 0.42000454\n",
      "Iteration 366, loss = 0.41977506\n",
      "Iteration 367, loss = 0.41971455\n",
      "Iteration 368, loss = 0.41964436\n",
      "Iteration 369, loss = 0.41959109\n",
      "Iteration 370, loss = 0.41951997\n",
      "Iteration 371, loss = 0.41930897\n",
      "Iteration 372, loss = 0.41924726\n",
      "Iteration 373, loss = 0.41912469\n",
      "Iteration 374, loss = 0.41900769\n",
      "Iteration 375, loss = 0.41896068\n",
      "Iteration 376, loss = 0.41888066\n",
      "Iteration 377, loss = 0.41876385\n",
      "Iteration 378, loss = 0.41870626\n",
      "Iteration 379, loss = 0.41857532\n",
      "Iteration 380, loss = 0.41856797\n",
      "Iteration 381, loss = 0.41844260\n",
      "Iteration 382, loss = 0.41825869\n",
      "Iteration 383, loss = 0.41819648\n",
      "Iteration 384, loss = 0.41818090\n",
      "Iteration 385, loss = 0.41804084\n",
      "Iteration 386, loss = 0.41797476\n",
      "Iteration 387, loss = 0.41785168\n",
      "Iteration 388, loss = 0.41772604\n",
      "Iteration 389, loss = 0.41765399\n",
      "Iteration 390, loss = 0.41760711\n",
      "Iteration 391, loss = 0.41752864\n",
      "Iteration 392, loss = 0.41741357\n",
      "Iteration 393, loss = 0.41728245\n",
      "Iteration 394, loss = 0.41725184\n",
      "Iteration 395, loss = 0.41707834\n",
      "Iteration 396, loss = 0.41700370\n",
      "Iteration 397, loss = 0.41700435\n",
      "Iteration 398, loss = 0.41685547\n",
      "Iteration 399, loss = 0.41678268\n",
      "Iteration 400, loss = 0.41668466\n",
      "Iteration 401, loss = 0.41664933\n",
      "Iteration 402, loss = 0.41649612\n",
      "Iteration 403, loss = 0.41645056\n",
      "Iteration 404, loss = 0.41629716\n",
      "Iteration 405, loss = 0.41628139\n",
      "Iteration 406, loss = 0.41614849\n",
      "Iteration 407, loss = 0.41608828\n",
      "Iteration 408, loss = 0.41602366\n",
      "Iteration 409, loss = 0.41596047\n",
      "Iteration 410, loss = 0.41583270\n",
      "Iteration 411, loss = 0.41585166\n",
      "Iteration 412, loss = 0.41567088\n",
      "Iteration 413, loss = 0.41567660\n",
      "Iteration 414, loss = 0.41556092\n",
      "Iteration 415, loss = 0.41546889\n",
      "Iteration 416, loss = 0.41538892\n",
      "Iteration 417, loss = 0.41525252\n",
      "Iteration 418, loss = 0.41521819\n",
      "Iteration 419, loss = 0.41514286\n",
      "Iteration 420, loss = 0.41508871\n",
      "Iteration 421, loss = 0.41494536\n",
      "Iteration 422, loss = 0.41498223\n",
      "Iteration 423, loss = 0.41486529\n",
      "Iteration 424, loss = 0.41479985\n",
      "Iteration 425, loss = 0.41471046\n",
      "Iteration 426, loss = 0.41454731\n",
      "Iteration 427, loss = 0.41445965\n",
      "Iteration 428, loss = 0.41441300\n",
      "Iteration 429, loss = 0.41436988\n",
      "Iteration 430, loss = 0.41424861\n",
      "Iteration 431, loss = 0.41432105\n",
      "Iteration 432, loss = 0.41407333\n",
      "Iteration 433, loss = 0.41406440\n",
      "Iteration 434, loss = 0.41395025\n",
      "Iteration 435, loss = 0.41386773\n",
      "Iteration 436, loss = 0.41378429\n",
      "Iteration 437, loss = 0.41378921\n",
      "Iteration 438, loss = 0.41360536\n",
      "Iteration 439, loss = 0.41354871\n",
      "Iteration 440, loss = 0.41351843\n",
      "Iteration 441, loss = 0.41342415\n",
      "Iteration 442, loss = 0.41337698\n",
      "Iteration 443, loss = 0.41332307\n",
      "Iteration 444, loss = 0.41319246\n",
      "Iteration 445, loss = 0.41319738\n",
      "Iteration 446, loss = 0.41311958\n",
      "Iteration 447, loss = 0.41301120\n",
      "Iteration 448, loss = 0.41292420\n",
      "Iteration 449, loss = 0.41293777\n",
      "Iteration 450, loss = 0.41280855\n",
      "Iteration 451, loss = 0.41269275\n",
      "Iteration 452, loss = 0.41271717\n",
      "Iteration 453, loss = 0.41255882\n",
      "Iteration 454, loss = 0.41252095\n",
      "Iteration 455, loss = 0.41249820\n",
      "Iteration 456, loss = 0.41244752\n",
      "Iteration 457, loss = 0.41242603\n",
      "Iteration 458, loss = 0.41229587\n",
      "Iteration 459, loss = 0.41218547\n",
      "Iteration 460, loss = 0.41225057\n",
      "Iteration 461, loss = 0.41210057\n",
      "Iteration 462, loss = 0.41201787\n",
      "Iteration 463, loss = 0.41191121\n",
      "Iteration 464, loss = 0.41192637\n",
      "Iteration 465, loss = 0.41181895\n",
      "Iteration 466, loss = 0.41171608\n",
      "Iteration 467, loss = 0.41162360\n",
      "Iteration 468, loss = 0.41166461\n",
      "Iteration 469, loss = 0.41156295\n",
      "Iteration 470, loss = 0.41152739\n",
      "Iteration 471, loss = 0.41138673\n",
      "Iteration 472, loss = 0.41133206\n",
      "Iteration 473, loss = 0.41132686\n",
      "Iteration 474, loss = 0.41119523\n",
      "Iteration 475, loss = 0.41110417\n",
      "Iteration 476, loss = 0.41113510\n",
      "Iteration 477, loss = 0.41100303\n",
      "Iteration 478, loss = 0.41097538\n",
      "Iteration 479, loss = 0.41092473\n",
      "Iteration 480, loss = 0.41085205\n",
      "Iteration 481, loss = 0.41076771\n",
      "Iteration 482, loss = 0.41074709\n",
      "Iteration 483, loss = 0.41072792\n",
      "Iteration 484, loss = 0.41057265\n",
      "Iteration 485, loss = 0.41048003\n",
      "Iteration 486, loss = 0.41050354\n",
      "Iteration 487, loss = 0.41038878\n",
      "Iteration 488, loss = 0.41030880\n",
      "Iteration 489, loss = 0.41029329\n",
      "Iteration 490, loss = 0.41022089\n",
      "Iteration 491, loss = 0.41012408\n",
      "Iteration 492, loss = 0.41012140\n",
      "Iteration 493, loss = 0.41013667\n",
      "Iteration 494, loss = 0.40996154\n",
      "Iteration 495, loss = 0.41000497\n",
      "Iteration 496, loss = 0.40993291\n",
      "Iteration 497, loss = 0.40987505\n",
      "Iteration 498, loss = 0.40972855\n",
      "Iteration 499, loss = 0.40969222\n",
      "Iteration 500, loss = 0.40968333\n",
      "Iteration 1, loss = 2.04696955\n",
      "Iteration 2, loss = 1.63961951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 3, loss = 1.37500066\n",
      "Iteration 4, loss = 1.18385908\n",
      "Iteration 5, loss = 1.04201871\n",
      "Iteration 6, loss = 0.93549874\n",
      "Iteration 7, loss = 0.85315900\n",
      "Iteration 8, loss = 0.78885656\n",
      "Iteration 9, loss = 0.73787045\n",
      "Iteration 10, loss = 0.69658968\n",
      "Iteration 11, loss = 0.66268529\n",
      "Iteration 12, loss = 0.63420088\n",
      "Iteration 13, loss = 0.61000634\n",
      "Iteration 14, loss = 0.58871851\n",
      "Iteration 15, loss = 0.57056580\n",
      "Iteration 16, loss = 0.55450253\n",
      "Iteration 17, loss = 0.54051455\n",
      "Iteration 18, loss = 0.52856931\n",
      "Iteration 19, loss = 0.51770159\n",
      "Iteration 20, loss = 0.50812694\n",
      "Iteration 21, loss = 0.49964477\n",
      "Iteration 22, loss = 0.49217127\n",
      "Iteration 23, loss = 0.48532432\n",
      "Iteration 24, loss = 0.47928728\n",
      "Iteration 25, loss = 0.47398616\n",
      "Iteration 26, loss = 0.46899041\n",
      "Iteration 27, loss = 0.46467577\n",
      "Iteration 28, loss = 0.46051060\n",
      "Iteration 29, loss = 0.45683864\n",
      "Iteration 30, loss = 0.45333210\n",
      "Iteration 31, loss = 0.45062609\n",
      "Iteration 32, loss = 0.44788768\n",
      "Iteration 33, loss = 0.44527925\n",
      "Iteration 34, loss = 0.44290149\n",
      "Iteration 35, loss = 0.44059149\n",
      "Iteration 36, loss = 0.43836437\n",
      "Iteration 37, loss = 0.43682440\n",
      "Iteration 38, loss = 0.43526018\n",
      "Iteration 39, loss = 0.43361635\n",
      "Iteration 40, loss = 0.43191963\n",
      "Iteration 41, loss = 0.43050043\n",
      "Iteration 42, loss = 0.42904839\n",
      "Iteration 43, loss = 0.42756446\n",
      "Iteration 44, loss = 0.42640024\n",
      "Iteration 45, loss = 0.42514733\n",
      "Iteration 46, loss = 0.42417251\n",
      "Iteration 47, loss = 0.42317844\n",
      "Iteration 48, loss = 0.42219479\n",
      "Iteration 49, loss = 0.42142310\n",
      "Iteration 50, loss = 0.42044513\n",
      "Iteration 51, loss = 0.41942614\n",
      "Iteration 52, loss = 0.41852250\n",
      "Iteration 53, loss = 0.41801394\n",
      "Iteration 54, loss = 0.41691220\n",
      "Iteration 55, loss = 0.41650925\n",
      "Iteration 56, loss = 0.41576521\n",
      "Iteration 57, loss = 0.41487465\n",
      "Iteration 58, loss = 0.41425244\n",
      "Iteration 59, loss = 0.41393604\n",
      "Iteration 60, loss = 0.41299980\n",
      "Iteration 61, loss = 0.41220354\n",
      "Iteration 62, loss = 0.41198079\n",
      "Iteration 63, loss = 0.41130767\n",
      "Iteration 64, loss = 0.41079674\n",
      "Iteration 65, loss = 0.41000313\n",
      "Iteration 66, loss = 0.40958825\n",
      "Iteration 67, loss = 0.40898610\n",
      "Iteration 68, loss = 0.40836955\n",
      "Iteration 69, loss = 0.40786984\n",
      "Iteration 70, loss = 0.40729257\n",
      "Iteration 71, loss = 0.40710449\n",
      "Iteration 72, loss = 0.40671103\n",
      "Iteration 73, loss = 0.40675352\n",
      "Iteration 74, loss = 0.40577620\n",
      "Iteration 75, loss = 0.40521883\n",
      "Iteration 76, loss = 0.40488963\n",
      "Iteration 77, loss = 0.40457211\n",
      "Iteration 78, loss = 0.40427989\n",
      "Iteration 79, loss = 0.40371832\n",
      "Iteration 80, loss = 0.40344113\n",
      "Iteration 81, loss = 0.40322834\n",
      "Iteration 82, loss = 0.40271950\n",
      "Iteration 83, loss = 0.40239868\n",
      "Iteration 84, loss = 0.40227978\n",
      "Iteration 85, loss = 0.40149652\n",
      "Iteration 86, loss = 0.40141041\n",
      "Iteration 87, loss = 0.40112949\n",
      "Iteration 88, loss = 0.40091846\n",
      "Iteration 89, loss = 0.40014906\n",
      "Iteration 90, loss = 0.40000181\n",
      "Iteration 91, loss = 0.39974309\n",
      "Iteration 92, loss = 0.39954453\n",
      "Iteration 93, loss = 0.39924639\n",
      "Iteration 94, loss = 0.39896277\n",
      "Iteration 95, loss = 0.39862740\n",
      "Iteration 96, loss = 0.39867016\n",
      "Iteration 97, loss = 0.39829532\n",
      "Iteration 98, loss = 0.39827313\n",
      "Iteration 99, loss = 0.39752040\n",
      "Iteration 100, loss = 0.39735416\n",
      "Iteration 101, loss = 0.39705254\n",
      "Iteration 102, loss = 0.39709854\n",
      "Iteration 103, loss = 0.39667633\n",
      "Iteration 104, loss = 0.39647322\n",
      "Iteration 105, loss = 0.39602868\n",
      "Iteration 106, loss = 0.39602761\n",
      "Iteration 107, loss = 0.39589964\n",
      "Iteration 108, loss = 0.39537872\n",
      "Iteration 109, loss = 0.39546175\n",
      "Iteration 110, loss = 0.39500134\n",
      "Iteration 111, loss = 0.39491819\n",
      "Iteration 112, loss = 0.39504074\n",
      "Iteration 113, loss = 0.39443832\n",
      "Iteration 114, loss = 0.39453038\n",
      "Iteration 115, loss = 0.39389196\n",
      "Iteration 116, loss = 0.39402444\n",
      "Iteration 117, loss = 0.39402309\n",
      "Iteration 118, loss = 0.39365468\n",
      "Iteration 119, loss = 0.39352464\n",
      "Iteration 120, loss = 0.39320571\n",
      "Iteration 121, loss = 0.39306146\n",
      "Iteration 122, loss = 0.39305045\n",
      "Iteration 123, loss = 0.39267153\n",
      "Iteration 124, loss = 0.39247173\n",
      "Iteration 125, loss = 0.39246092\n",
      "Iteration 126, loss = 0.39229758\n",
      "Iteration 127, loss = 0.39202032\n",
      "Iteration 128, loss = 0.39153369\n",
      "Iteration 129, loss = 0.39176672\n",
      "Iteration 130, loss = 0.39142273\n",
      "Iteration 131, loss = 0.39117122\n",
      "Iteration 132, loss = 0.39122235\n",
      "Iteration 133, loss = 0.39088539\n",
      "Iteration 134, loss = 0.39091588\n",
      "Iteration 135, loss = 0.39043454\n",
      "Iteration 136, loss = 0.39062224\n",
      "Iteration 137, loss = 0.39037165\n",
      "Iteration 138, loss = 0.39026037\n",
      "Iteration 139, loss = 0.39013062\n",
      "Iteration 140, loss = 0.38984772\n",
      "Iteration 141, loss = 0.39009991\n",
      "Iteration 142, loss = 0.38965488\n",
      "Iteration 143, loss = 0.38975384\n",
      "Iteration 144, loss = 0.38939987\n",
      "Iteration 145, loss = 0.38946262\n",
      "Iteration 146, loss = 0.38919153\n",
      "Iteration 147, loss = 0.38917021\n",
      "Iteration 148, loss = 0.38897564\n",
      "Iteration 149, loss = 0.38884153\n",
      "Iteration 150, loss = 0.38874633\n",
      "Iteration 151, loss = 0.38854425\n",
      "Iteration 152, loss = 0.38850113\n",
      "Iteration 153, loss = 0.38829586\n",
      "Iteration 154, loss = 0.38812102\n",
      "Iteration 155, loss = 0.38823573\n",
      "Iteration 156, loss = 0.38799610\n",
      "Iteration 157, loss = 0.38791875\n",
      "Iteration 158, loss = 0.38796783\n",
      "Iteration 159, loss = 0.38750825\n",
      "Iteration 160, loss = 0.38740989\n",
      "Iteration 161, loss = 0.38757787\n",
      "Iteration 162, loss = 0.38731210\n",
      "Iteration 163, loss = 0.38735878\n",
      "Iteration 164, loss = 0.38734971\n",
      "Iteration 165, loss = 0.38703591\n",
      "Iteration 166, loss = 0.38694856\n",
      "Iteration 167, loss = 0.38673316\n",
      "Iteration 168, loss = 0.38668933\n",
      "Iteration 169, loss = 0.38622908\n",
      "Iteration 170, loss = 0.38634292\n",
      "Iteration 171, loss = 0.38632756\n",
      "Iteration 172, loss = 0.38634401\n",
      "Iteration 173, loss = 0.38616968\n",
      "Iteration 174, loss = 0.38608681\n",
      "Iteration 175, loss = 0.38581737\n",
      "Iteration 176, loss = 0.38605562\n",
      "Iteration 177, loss = 0.38585468\n",
      "Iteration 178, loss = 0.38562217\n",
      "Iteration 179, loss = 0.38587518\n",
      "Iteration 180, loss = 0.38561715\n",
      "Iteration 181, loss = 0.38558049\n",
      "Iteration 182, loss = 0.38518268\n",
      "Iteration 183, loss = 0.38531376\n",
      "Iteration 184, loss = 0.38533569\n",
      "Iteration 185, loss = 0.38497328\n",
      "Iteration 186, loss = 0.38510414\n",
      "Iteration 187, loss = 0.38511843\n",
      "Iteration 188, loss = 0.38475094\n",
      "Iteration 189, loss = 0.38481061\n",
      "Iteration 190, loss = 0.38468355\n",
      "Iteration 191, loss = 0.38486013\n",
      "Iteration 192, loss = 0.38462052\n",
      "Iteration 193, loss = 0.38433351\n",
      "Iteration 194, loss = 0.38422670\n",
      "Iteration 195, loss = 0.38440622\n",
      "Iteration 196, loss = 0.38418229\n",
      "Iteration 197, loss = 0.38437323\n",
      "Iteration 198, loss = 0.38413174\n",
      "Iteration 199, loss = 0.38401287\n",
      "Iteration 200, loss = 0.38405846\n",
      "Iteration 201, loss = 0.38379769\n",
      "Iteration 202, loss = 0.38365569\n",
      "Iteration 203, loss = 0.38404067\n",
      "Iteration 204, loss = 0.38362818\n",
      "Iteration 205, loss = 0.38375506\n",
      "Iteration 206, loss = 0.38336910\n",
      "Iteration 207, loss = 0.38337047\n",
      "Iteration 208, loss = 0.38323564\n",
      "Iteration 209, loss = 0.38321881\n",
      "Iteration 210, loss = 0.38336640\n",
      "Iteration 211, loss = 0.38292324\n",
      "Iteration 212, loss = 0.38303143\n",
      "Iteration 213, loss = 0.38304174\n",
      "Iteration 214, loss = 0.38297441\n",
      "Iteration 215, loss = 0.38282621\n",
      "Iteration 216, loss = 0.38264858\n",
      "Iteration 217, loss = 0.38278271\n",
      "Iteration 218, loss = 0.38263953\n",
      "Iteration 219, loss = 0.38249836\n",
      "Iteration 220, loss = 0.38245805\n",
      "Iteration 221, loss = 0.38219311\n",
      "Iteration 222, loss = 0.38229686\n",
      "Iteration 223, loss = 0.38220502\n",
      "Iteration 224, loss = 0.38210084\n",
      "Iteration 225, loss = 0.38193813\n",
      "Iteration 226, loss = 0.38197799\n",
      "Iteration 227, loss = 0.38200499\n",
      "Iteration 228, loss = 0.38166749\n",
      "Iteration 229, loss = 0.38186628\n",
      "Iteration 230, loss = 0.38150425\n",
      "Iteration 231, loss = 0.38163360\n",
      "Iteration 232, loss = 0.38136400\n",
      "Iteration 233, loss = 0.38151478\n",
      "Iteration 234, loss = 0.38130734\n",
      "Iteration 235, loss = 0.38135372\n",
      "Iteration 236, loss = 0.38121576\n",
      "Iteration 237, loss = 0.38130546\n",
      "Iteration 238, loss = 0.38097333\n",
      "Iteration 239, loss = 0.38088066\n",
      "Iteration 240, loss = 0.38075041\n",
      "Iteration 241, loss = 0.38090563\n",
      "Iteration 242, loss = 0.38054890\n",
      "Iteration 243, loss = 0.38093991\n",
      "Iteration 244, loss = 0.38055608\n",
      "Iteration 245, loss = 0.38030044\n",
      "Iteration 246, loss = 0.38057735\n",
      "Iteration 247, loss = 0.38049670\n",
      "Iteration 248, loss = 0.38025587\n",
      "Iteration 249, loss = 0.38012422\n",
      "Iteration 250, loss = 0.37996244\n",
      "Iteration 251, loss = 0.37989309\n",
      "Iteration 252, loss = 0.38007010\n",
      "Iteration 253, loss = 0.37977715\n",
      "Iteration 254, loss = 0.37953423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.37958855\n",
      "Iteration 256, loss = 0.37954868\n",
      "Iteration 257, loss = 0.37942122\n",
      "Iteration 258, loss = 0.37938988\n",
      "Iteration 259, loss = 0.37921817\n",
      "Iteration 260, loss = 0.37894289\n",
      "Iteration 261, loss = 0.37920273\n",
      "Iteration 262, loss = 0.37906701\n",
      "Iteration 263, loss = 0.37909086\n",
      "Iteration 264, loss = 0.37867521\n",
      "Iteration 265, loss = 0.37908292\n",
      "Iteration 266, loss = 0.37869722\n",
      "Iteration 267, loss = 0.37876955\n",
      "Iteration 268, loss = 0.37857757\n",
      "Iteration 269, loss = 0.37856585\n",
      "Iteration 270, loss = 0.37838101\n",
      "Iteration 271, loss = 0.37845037\n",
      "Iteration 272, loss = 0.37854612\n",
      "Iteration 273, loss = 0.37813040\n",
      "Iteration 274, loss = 0.37812535\n",
      "Iteration 275, loss = 0.37805585\n",
      "Iteration 276, loss = 0.37839786\n",
      "Iteration 277, loss = 0.37815414\n",
      "Iteration 278, loss = 0.37780356\n",
      "Iteration 279, loss = 0.37779462\n",
      "Iteration 280, loss = 0.37792621\n",
      "Iteration 281, loss = 0.37763934\n",
      "Iteration 282, loss = 0.37734602\n",
      "Iteration 283, loss = 0.37746044\n",
      "Iteration 284, loss = 0.37735634\n",
      "Iteration 285, loss = 0.37731029\n",
      "Iteration 286, loss = 0.37736224\n",
      "Iteration 287, loss = 0.37732627\n",
      "Iteration 288, loss = 0.37732332\n",
      "Iteration 289, loss = 0.37710294\n",
      "Iteration 290, loss = 0.37701075\n",
      "Iteration 291, loss = 0.37712636\n",
      "Iteration 292, loss = 0.37707218\n",
      "Iteration 293, loss = 0.37710571\n",
      "Iteration 294, loss = 0.37690901\n",
      "Iteration 295, loss = 0.37693731\n",
      "Iteration 296, loss = 0.37683635\n",
      "Iteration 297, loss = 0.37681734\n",
      "Iteration 298, loss = 0.37687489\n",
      "Iteration 299, loss = 0.37653984\n",
      "Iteration 300, loss = 0.37671963\n",
      "Iteration 301, loss = 0.37660563\n",
      "Iteration 302, loss = 0.37624114\n",
      "Iteration 303, loss = 0.37652986\n",
      "Iteration 304, loss = 0.37584626\n",
      "Iteration 305, loss = 0.37617321\n",
      "Iteration 306, loss = 0.37611458\n",
      "Iteration 307, loss = 0.37603437\n",
      "Iteration 308, loss = 0.37586972\n",
      "Iteration 309, loss = 0.37596557\n",
      "Iteration 310, loss = 0.37595938\n",
      "Iteration 311, loss = 0.37582264\n",
      "Iteration 312, loss = 0.37590292\n",
      "Iteration 313, loss = 0.37568199\n",
      "Iteration 314, loss = 0.37545109\n",
      "Iteration 315, loss = 0.37584765\n",
      "Iteration 316, loss = 0.37553201\n",
      "Iteration 317, loss = 0.37555388\n",
      "Iteration 318, loss = 0.37530796\n",
      "Iteration 319, loss = 0.37545706\n",
      "Iteration 320, loss = 0.37543301\n",
      "Iteration 321, loss = 0.37523852\n",
      "Iteration 322, loss = 0.37546538\n",
      "Iteration 323, loss = 0.37510609\n",
      "Iteration 324, loss = 0.37512555\n",
      "Iteration 325, loss = 0.37527443\n",
      "Iteration 326, loss = 0.37495672\n",
      "Iteration 327, loss = 0.37499508\n",
      "Iteration 328, loss = 0.37478796\n",
      "Iteration 329, loss = 0.37487164\n",
      "Iteration 330, loss = 0.37473468\n",
      "Iteration 331, loss = 0.37454564\n",
      "Iteration 332, loss = 0.37488596\n",
      "Iteration 333, loss = 0.37443272\n",
      "Iteration 334, loss = 0.37483019\n",
      "Iteration 335, loss = 0.37452806\n",
      "Iteration 336, loss = 0.37427060\n",
      "Iteration 337, loss = 0.37417647\n",
      "Iteration 338, loss = 0.37449280\n",
      "Iteration 339, loss = 0.37441710\n",
      "Iteration 340, loss = 0.37431204\n",
      "Iteration 341, loss = 0.37414643\n",
      "Iteration 342, loss = 0.37416562\n",
      "Iteration 343, loss = 0.37369968\n",
      "Iteration 344, loss = 0.37409560\n",
      "Iteration 345, loss = 0.37390185\n",
      "Iteration 346, loss = 0.37403367\n",
      "Iteration 347, loss = 0.37396420\n",
      "Iteration 348, loss = 0.37366880\n",
      "Iteration 349, loss = 0.37366494\n",
      "Iteration 350, loss = 0.37347985\n",
      "Iteration 351, loss = 0.37349163\n",
      "Iteration 352, loss = 0.37367813\n",
      "Iteration 353, loss = 0.37337282\n",
      "Iteration 354, loss = 0.37331572\n",
      "Iteration 355, loss = 0.37332313\n",
      "Iteration 356, loss = 0.37337184\n",
      "Iteration 357, loss = 0.37343223\n",
      "Iteration 358, loss = 0.37339460\n",
      "Iteration 359, loss = 0.37330382\n",
      "Iteration 360, loss = 0.37337951\n",
      "Iteration 361, loss = 0.37357023\n",
      "Iteration 362, loss = 0.37303501\n",
      "Iteration 363, loss = 0.37304279\n",
      "Iteration 364, loss = 0.37282641\n",
      "Iteration 365, loss = 0.37286594\n",
      "Iteration 366, loss = 0.37272747\n",
      "Iteration 367, loss = 0.37293509\n",
      "Iteration 368, loss = 0.37262168\n",
      "Iteration 369, loss = 0.37266623\n",
      "Iteration 370, loss = 0.37263260\n",
      "Iteration 371, loss = 0.37270287\n",
      "Iteration 372, loss = 0.37263258\n",
      "Iteration 373, loss = 0.37229556\n",
      "Iteration 374, loss = 0.37258648\n",
      "Iteration 375, loss = 0.37270848\n",
      "Iteration 376, loss = 0.37239018\n",
      "Iteration 377, loss = 0.37229387\n",
      "Iteration 378, loss = 0.37238481\n",
      "Iteration 379, loss = 0.37224551\n",
      "Iteration 380, loss = 0.37228551\n",
      "Iteration 381, loss = 0.37223898\n",
      "Iteration 382, loss = 0.37195915\n",
      "Iteration 383, loss = 0.37228708\n",
      "Iteration 384, loss = 0.37189955\n",
      "Iteration 385, loss = 0.37189509\n",
      "Iteration 386, loss = 0.37187695\n",
      "Iteration 387, loss = 0.37173734\n",
      "Iteration 388, loss = 0.37188718\n",
      "Iteration 389, loss = 0.37170434\n",
      "Iteration 390, loss = 0.37190979\n",
      "Iteration 391, loss = 0.37158906\n",
      "Iteration 392, loss = 0.37161256\n",
      "Iteration 393, loss = 0.37145455\n",
      "Iteration 394, loss = 0.37143006\n",
      "Iteration 395, loss = 0.37153215\n",
      "Iteration 396, loss = 0.37131818\n",
      "Iteration 397, loss = 0.37141873\n",
      "Iteration 398, loss = 0.37123630\n",
      "Iteration 399, loss = 0.37153222\n",
      "Iteration 400, loss = 0.37129678\n",
      "Iteration 401, loss = 0.37133540\n",
      "Iteration 402, loss = 0.37107469\n",
      "Iteration 403, loss = 0.37110455\n",
      "Iteration 404, loss = 0.37087667\n",
      "Iteration 405, loss = 0.37094848\n",
      "Iteration 406, loss = 0.37107897\n",
      "Iteration 407, loss = 0.37106844\n",
      "Iteration 408, loss = 0.37090434\n",
      "Iteration 409, loss = 0.37101825\n",
      "Iteration 410, loss = 0.37057702\n",
      "Iteration 411, loss = 0.37092926\n",
      "Iteration 412, loss = 0.37086906\n",
      "Iteration 413, loss = 0.37052481\n",
      "Iteration 414, loss = 0.37070418\n",
      "Iteration 415, loss = 0.37078569\n",
      "Iteration 416, loss = 0.37058960\n",
      "Iteration 417, loss = 0.37053159\n",
      "Iteration 418, loss = 0.37042608\n",
      "Iteration 419, loss = 0.37059795\n",
      "Iteration 420, loss = 0.37053340\n",
      "Iteration 421, loss = 0.37044906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04913275\n",
      "Iteration 2, loss = 1.63935951\n",
      "Iteration 3, loss = 1.37357957\n",
      "Iteration 4, loss = 1.18194836\n",
      "Iteration 5, loss = 1.04051077\n",
      "Iteration 6, loss = 0.93396811\n",
      "Iteration 7, loss = 0.85184404\n",
      "Iteration 8, loss = 0.78798513\n",
      "Iteration 9, loss = 0.73712806\n",
      "Iteration 10, loss = 0.69622818\n",
      "Iteration 11, loss = 0.66239349\n",
      "Iteration 12, loss = 0.63382321\n",
      "Iteration 13, loss = 0.60962613\n",
      "Iteration 14, loss = 0.58864652\n",
      "Iteration 15, loss = 0.57028250\n",
      "Iteration 16, loss = 0.55443105\n",
      "Iteration 17, loss = 0.54041860\n",
      "Iteration 18, loss = 0.52841595\n",
      "Iteration 19, loss = 0.51751225\n",
      "Iteration 20, loss = 0.50798237\n",
      "Iteration 21, loss = 0.49979251\n",
      "Iteration 22, loss = 0.49235594\n",
      "Iteration 23, loss = 0.48563209\n",
      "Iteration 24, loss = 0.47952567\n",
      "Iteration 25, loss = 0.47414511\n",
      "Iteration 26, loss = 0.46933550\n",
      "Iteration 27, loss = 0.46484402\n",
      "Iteration 28, loss = 0.46102650\n",
      "Iteration 29, loss = 0.45716908\n",
      "Iteration 30, loss = 0.45394360\n",
      "Iteration 31, loss = 0.45077694\n",
      "Iteration 32, loss = 0.44810789\n",
      "Iteration 33, loss = 0.44519921\n",
      "Iteration 34, loss = 0.44305622\n",
      "Iteration 35, loss = 0.44082544\n",
      "Iteration 36, loss = 0.43867503\n",
      "Iteration 37, loss = 0.43683542\n",
      "Iteration 38, loss = 0.43513071\n",
      "Iteration 39, loss = 0.43341335\n",
      "Iteration 40, loss = 0.43179319\n",
      "Iteration 41, loss = 0.43061091\n",
      "Iteration 42, loss = 0.42912763\n",
      "Iteration 43, loss = 0.42798979\n",
      "Iteration 44, loss = 0.42643366\n",
      "Iteration 45, loss = 0.42544542\n",
      "Iteration 46, loss = 0.42452951\n",
      "Iteration 47, loss = 0.42324591\n",
      "Iteration 48, loss = 0.42242351\n",
      "Iteration 49, loss = 0.42160266\n",
      "Iteration 50, loss = 0.42040375\n",
      "Iteration 51, loss = 0.41959580\n",
      "Iteration 52, loss = 0.41898472\n",
      "Iteration 53, loss = 0.41792771\n",
      "Iteration 54, loss = 0.41747552\n",
      "Iteration 55, loss = 0.41667066\n",
      "Iteration 56, loss = 0.41562605\n",
      "Iteration 57, loss = 0.41508459\n",
      "Iteration 58, loss = 0.41468231\n",
      "Iteration 59, loss = 0.41367691\n",
      "Iteration 60, loss = 0.41324186\n",
      "Iteration 61, loss = 0.41272812\n",
      "Iteration 62, loss = 0.41226914\n",
      "Iteration 63, loss = 0.41133961\n",
      "Iteration 64, loss = 0.41074801\n",
      "Iteration 65, loss = 0.41054212\n",
      "Iteration 66, loss = 0.40995539\n",
      "Iteration 67, loss = 0.40906459\n",
      "Iteration 68, loss = 0.40832868\n",
      "Iteration 69, loss = 0.40808287\n",
      "Iteration 70, loss = 0.40724052\n",
      "Iteration 71, loss = 0.40683856\n",
      "Iteration 72, loss = 0.40660915\n",
      "Iteration 73, loss = 0.40606514\n",
      "Iteration 74, loss = 0.40573940\n",
      "Iteration 75, loss = 0.40501969\n",
      "Iteration 76, loss = 0.40479302\n",
      "Iteration 77, loss = 0.40423666\n",
      "Iteration 78, loss = 0.40415443\n",
      "Iteration 79, loss = 0.40350424\n",
      "Iteration 80, loss = 0.40308621\n",
      "Iteration 81, loss = 0.40245468\n",
      "Iteration 82, loss = 0.40202570\n",
      "Iteration 83, loss = 0.40155263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 0.40158890\n",
      "Iteration 85, loss = 0.40102247\n",
      "Iteration 86, loss = 0.40067399\n",
      "Iteration 87, loss = 0.39996975\n",
      "Iteration 88, loss = 0.40004773\n",
      "Iteration 89, loss = 0.39928764\n",
      "Iteration 90, loss = 0.39912560\n",
      "Iteration 91, loss = 0.39860391\n",
      "Iteration 92, loss = 0.39890078\n",
      "Iteration 93, loss = 0.39838142\n",
      "Iteration 94, loss = 0.39791477\n",
      "Iteration 95, loss = 0.39772230\n",
      "Iteration 96, loss = 0.39726467\n",
      "Iteration 97, loss = 0.39688486\n",
      "Iteration 98, loss = 0.39653767\n",
      "Iteration 99, loss = 0.39651863\n",
      "Iteration 100, loss = 0.39588159\n",
      "Iteration 101, loss = 0.39615972\n",
      "Iteration 102, loss = 0.39531417\n",
      "Iteration 103, loss = 0.39515906\n",
      "Iteration 104, loss = 0.39516141\n",
      "Iteration 105, loss = 0.39497642\n",
      "Iteration 106, loss = 0.39448344\n",
      "Iteration 107, loss = 0.39425495\n",
      "Iteration 108, loss = 0.39412765\n",
      "Iteration 109, loss = 0.39363891\n",
      "Iteration 110, loss = 0.39352340\n",
      "Iteration 111, loss = 0.39338098\n",
      "Iteration 112, loss = 0.39291263\n",
      "Iteration 113, loss = 0.39265046\n",
      "Iteration 114, loss = 0.39244402\n",
      "Iteration 115, loss = 0.39248957\n",
      "Iteration 116, loss = 0.39236469\n",
      "Iteration 117, loss = 0.39207259\n",
      "Iteration 118, loss = 0.39188283\n",
      "Iteration 119, loss = 0.39166646\n",
      "Iteration 120, loss = 0.39154639\n",
      "Iteration 121, loss = 0.39095540\n",
      "Iteration 122, loss = 0.39093065\n",
      "Iteration 123, loss = 0.39074256\n",
      "Iteration 124, loss = 0.39043026\n",
      "Iteration 125, loss = 0.39075556\n",
      "Iteration 126, loss = 0.38996072\n",
      "Iteration 127, loss = 0.39022769\n",
      "Iteration 128, loss = 0.38972248\n",
      "Iteration 129, loss = 0.38979535\n",
      "Iteration 130, loss = 0.38971025\n",
      "Iteration 131, loss = 0.38955350\n",
      "Iteration 132, loss = 0.38929585\n",
      "Iteration 133, loss = 0.38900442\n",
      "Iteration 134, loss = 0.38893225\n",
      "Iteration 135, loss = 0.38883605\n",
      "Iteration 136, loss = 0.38863050\n",
      "Iteration 137, loss = 0.38859827\n",
      "Iteration 138, loss = 0.38855813\n",
      "Iteration 139, loss = 0.38814243\n",
      "Iteration 140, loss = 0.38794293\n",
      "Iteration 141, loss = 0.38775547\n",
      "Iteration 142, loss = 0.38784382\n",
      "Iteration 143, loss = 0.38762981\n",
      "Iteration 144, loss = 0.38748406\n",
      "Iteration 145, loss = 0.38717872\n",
      "Iteration 146, loss = 0.38739474\n",
      "Iteration 147, loss = 0.38706842\n",
      "Iteration 148, loss = 0.38702569\n",
      "Iteration 149, loss = 0.38709654\n",
      "Iteration 150, loss = 0.38694220\n",
      "Iteration 151, loss = 0.38661779\n",
      "Iteration 152, loss = 0.38636720\n",
      "Iteration 153, loss = 0.38646299\n",
      "Iteration 154, loss = 0.38666320\n",
      "Iteration 155, loss = 0.38622638\n",
      "Iteration 156, loss = 0.38601673\n",
      "Iteration 157, loss = 0.38580162\n",
      "Iteration 158, loss = 0.38585588\n",
      "Iteration 159, loss = 0.38600198\n",
      "Iteration 160, loss = 0.38573554\n",
      "Iteration 161, loss = 0.38570042\n",
      "Iteration 162, loss = 0.38542121\n",
      "Iteration 163, loss = 0.38540631\n",
      "Iteration 164, loss = 0.38530912\n",
      "Iteration 165, loss = 0.38507634\n",
      "Iteration 166, loss = 0.38509989\n",
      "Iteration 167, loss = 0.38478539\n",
      "Iteration 168, loss = 0.38497181\n",
      "Iteration 169, loss = 0.38469587\n",
      "Iteration 170, loss = 0.38469909\n",
      "Iteration 171, loss = 0.38448464\n",
      "Iteration 172, loss = 0.38423455\n",
      "Iteration 173, loss = 0.38432858\n",
      "Iteration 174, loss = 0.38442948\n",
      "Iteration 175, loss = 0.38405109\n",
      "Iteration 176, loss = 0.38437975\n",
      "Iteration 177, loss = 0.38408804\n",
      "Iteration 178, loss = 0.38380121\n",
      "Iteration 179, loss = 0.38380205\n",
      "Iteration 180, loss = 0.38355388\n",
      "Iteration 181, loss = 0.38380654\n",
      "Iteration 182, loss = 0.38361707\n",
      "Iteration 183, loss = 0.38340676\n",
      "Iteration 184, loss = 0.38334480\n",
      "Iteration 185, loss = 0.38339290\n",
      "Iteration 186, loss = 0.38314884\n",
      "Iteration 187, loss = 0.38307458\n",
      "Iteration 188, loss = 0.38313867\n",
      "Iteration 189, loss = 0.38303266\n",
      "Iteration 190, loss = 0.38327440\n",
      "Iteration 191, loss = 0.38279108\n",
      "Iteration 192, loss = 0.38284407\n",
      "Iteration 193, loss = 0.38279546\n",
      "Iteration 194, loss = 0.38248844\n",
      "Iteration 195, loss = 0.38270462\n",
      "Iteration 196, loss = 0.38241736\n",
      "Iteration 197, loss = 0.38258032\n",
      "Iteration 198, loss = 0.38243956\n",
      "Iteration 199, loss = 0.38258004\n",
      "Iteration 200, loss = 0.38224450\n",
      "Iteration 201, loss = 0.38227502\n",
      "Iteration 202, loss = 0.38205124\n",
      "Iteration 203, loss = 0.38210341\n",
      "Iteration 204, loss = 0.38195892\n",
      "Iteration 205, loss = 0.38192865\n",
      "Iteration 206, loss = 0.38181210\n",
      "Iteration 207, loss = 0.38171656\n",
      "Iteration 208, loss = 0.38177845\n",
      "Iteration 209, loss = 0.38137840\n",
      "Iteration 210, loss = 0.38148322\n",
      "Iteration 211, loss = 0.38123907\n",
      "Iteration 212, loss = 0.38151669\n",
      "Iteration 213, loss = 0.38129051\n",
      "Iteration 214, loss = 0.38123652\n",
      "Iteration 215, loss = 0.38167776\n",
      "Iteration 216, loss = 0.38134009\n",
      "Iteration 217, loss = 0.38101214\n",
      "Iteration 218, loss = 0.38112192\n",
      "Iteration 219, loss = 0.38127520\n",
      "Iteration 220, loss = 0.38110576\n",
      "Iteration 221, loss = 0.38125049\n",
      "Iteration 222, loss = 0.38072312\n",
      "Iteration 223, loss = 0.38083528\n",
      "Iteration 224, loss = 0.38106460\n",
      "Iteration 225, loss = 0.38088387\n",
      "Iteration 226, loss = 0.38076421\n",
      "Iteration 227, loss = 0.38046124\n",
      "Iteration 228, loss = 0.38058651\n",
      "Iteration 229, loss = 0.38037061\n",
      "Iteration 230, loss = 0.38046859\n",
      "Iteration 231, loss = 0.38012166\n",
      "Iteration 232, loss = 0.38041712\n",
      "Iteration 233, loss = 0.38014865\n",
      "Iteration 234, loss = 0.37994337\n",
      "Iteration 235, loss = 0.37991576\n",
      "Iteration 236, loss = 0.37989297\n",
      "Iteration 237, loss = 0.38016660\n",
      "Iteration 238, loss = 0.38005149\n",
      "Iteration 239, loss = 0.37974061\n",
      "Iteration 240, loss = 0.38002919\n",
      "Iteration 241, loss = 0.37971384\n",
      "Iteration 242, loss = 0.37968678\n",
      "Iteration 243, loss = 0.37962438\n",
      "Iteration 244, loss = 0.37946810\n",
      "Iteration 245, loss = 0.37977978\n",
      "Iteration 246, loss = 0.37947892\n",
      "Iteration 247, loss = 0.37938517\n",
      "Iteration 248, loss = 0.37954692\n",
      "Iteration 249, loss = 0.37904591\n",
      "Iteration 250, loss = 0.37947749\n",
      "Iteration 251, loss = 0.37935041\n",
      "Iteration 252, loss = 0.37885079\n",
      "Iteration 253, loss = 0.37930689\n",
      "Iteration 254, loss = 0.37888630\n",
      "Iteration 255, loss = 0.37887503\n",
      "Iteration 256, loss = 0.37887719\n",
      "Iteration 257, loss = 0.37903933\n",
      "Iteration 258, loss = 0.37888238\n",
      "Iteration 259, loss = 0.37870731\n",
      "Iteration 260, loss = 0.37860790\n",
      "Iteration 261, loss = 0.37877400\n",
      "Iteration 262, loss = 0.37861088\n",
      "Iteration 263, loss = 0.37849369\n",
      "Iteration 264, loss = 0.37855995\n",
      "Iteration 265, loss = 0.37840605\n",
      "Iteration 266, loss = 0.37840260\n",
      "Iteration 267, loss = 0.37832661\n",
      "Iteration 268, loss = 0.37810992\n",
      "Iteration 269, loss = 0.37822530\n",
      "Iteration 270, loss = 0.37840615\n",
      "Iteration 271, loss = 0.37784340\n",
      "Iteration 272, loss = 0.37814210\n",
      "Iteration 273, loss = 0.37810901\n",
      "Iteration 274, loss = 0.37763331\n",
      "Iteration 275, loss = 0.37789813\n",
      "Iteration 276, loss = 0.37769622\n",
      "Iteration 277, loss = 0.37748446\n",
      "Iteration 278, loss = 0.37773788\n",
      "Iteration 279, loss = 0.37789856\n",
      "Iteration 280, loss = 0.37753168\n",
      "Iteration 281, loss = 0.37761413\n",
      "Iteration 282, loss = 0.37739771\n",
      "Iteration 283, loss = 0.37762331\n",
      "Iteration 284, loss = 0.37744933\n",
      "Iteration 285, loss = 0.37740368\n",
      "Iteration 286, loss = 0.37756027\n",
      "Iteration 287, loss = 0.37679531\n",
      "Iteration 288, loss = 0.37669625\n",
      "Iteration 289, loss = 0.37696903\n",
      "Iteration 290, loss = 0.37695160\n",
      "Iteration 291, loss = 0.37683818\n",
      "Iteration 292, loss = 0.37749229\n",
      "Iteration 293, loss = 0.37676075\n",
      "Iteration 294, loss = 0.37690767\n",
      "Iteration 295, loss = 0.37632433\n",
      "Iteration 296, loss = 0.37689648\n",
      "Iteration 297, loss = 0.37649352\n",
      "Iteration 298, loss = 0.37640792\n",
      "Iteration 299, loss = 0.37670165\n",
      "Iteration 300, loss = 0.37629167\n",
      "Iteration 301, loss = 0.37623925\n",
      "Iteration 302, loss = 0.37639000\n",
      "Iteration 303, loss = 0.37641255\n",
      "Iteration 304, loss = 0.37625803\n",
      "Iteration 305, loss = 0.37610649\n",
      "Iteration 306, loss = 0.37619954\n",
      "Iteration 307, loss = 0.37611620\n",
      "Iteration 308, loss = 0.37587845\n",
      "Iteration 309, loss = 0.37590848\n",
      "Iteration 310, loss = 0.37560505\n",
      "Iteration 311, loss = 0.37556237\n",
      "Iteration 312, loss = 0.37581060\n",
      "Iteration 313, loss = 0.37579053\n",
      "Iteration 314, loss = 0.37551382\n",
      "Iteration 315, loss = 0.37545374\n",
      "Iteration 316, loss = 0.37551906\n",
      "Iteration 317, loss = 0.37545628\n",
      "Iteration 318, loss = 0.37520526\n",
      "Iteration 319, loss = 0.37533683\n",
      "Iteration 320, loss = 0.37515015\n",
      "Iteration 321, loss = 0.37503734\n",
      "Iteration 322, loss = 0.37500197\n",
      "Iteration 323, loss = 0.37492980\n",
      "Iteration 324, loss = 0.37473594\n",
      "Iteration 325, loss = 0.37487933\n",
      "Iteration 326, loss = 0.37489499\n",
      "Iteration 327, loss = 0.37467262\n",
      "Iteration 328, loss = 0.37472372\n",
      "Iteration 329, loss = 0.37446501\n",
      "Iteration 330, loss = 0.37451873\n",
      "Iteration 331, loss = 0.37442040\n",
      "Iteration 332, loss = 0.37442313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 333, loss = 0.37440570\n",
      "Iteration 334, loss = 0.37417984\n",
      "Iteration 335, loss = 0.37400475\n",
      "Iteration 336, loss = 0.37409183\n",
      "Iteration 337, loss = 0.37370362\n",
      "Iteration 338, loss = 0.37425244\n",
      "Iteration 339, loss = 0.37376809\n",
      "Iteration 340, loss = 0.37397432\n",
      "Iteration 341, loss = 0.37409024\n",
      "Iteration 342, loss = 0.37376583\n",
      "Iteration 343, loss = 0.37363381\n",
      "Iteration 344, loss = 0.37375246\n",
      "Iteration 345, loss = 0.37346871\n",
      "Iteration 346, loss = 0.37327583\n",
      "Iteration 347, loss = 0.37327477\n",
      "Iteration 348, loss = 0.37332667\n",
      "Iteration 349, loss = 0.37330136\n",
      "Iteration 350, loss = 0.37342568\n",
      "Iteration 351, loss = 0.37297994\n",
      "Iteration 352, loss = 0.37346914\n",
      "Iteration 353, loss = 0.37277678\n",
      "Iteration 354, loss = 0.37328255\n",
      "Iteration 355, loss = 0.37314491\n",
      "Iteration 356, loss = 0.37296025\n",
      "Iteration 357, loss = 0.37264234\n",
      "Iteration 358, loss = 0.37251667\n",
      "Iteration 359, loss = 0.37274208\n",
      "Iteration 360, loss = 0.37247330\n",
      "Iteration 361, loss = 0.37244578\n",
      "Iteration 362, loss = 0.37208428\n",
      "Iteration 363, loss = 0.37209290\n",
      "Iteration 364, loss = 0.37213912\n",
      "Iteration 365, loss = 0.37187143\n",
      "Iteration 366, loss = 0.37188784\n",
      "Iteration 367, loss = 0.37214067\n",
      "Iteration 368, loss = 0.37167666\n",
      "Iteration 369, loss = 0.37184364\n",
      "Iteration 370, loss = 0.37132054\n",
      "Iteration 371, loss = 0.37156038\n",
      "Iteration 372, loss = 0.37130380\n",
      "Iteration 373, loss = 0.37147599\n",
      "Iteration 374, loss = 0.37129014\n",
      "Iteration 375, loss = 0.37120913\n",
      "Iteration 376, loss = 0.37101924\n",
      "Iteration 377, loss = 0.37108813\n",
      "Iteration 378, loss = 0.37088078\n",
      "Iteration 379, loss = 0.37101414\n",
      "Iteration 380, loss = 0.37046676\n",
      "Iteration 381, loss = 0.37077827\n",
      "Iteration 382, loss = 0.37074816\n",
      "Iteration 383, loss = 0.37033081\n",
      "Iteration 384, loss = 0.37041954\n",
      "Iteration 385, loss = 0.37085296\n",
      "Iteration 386, loss = 0.37039731\n",
      "Iteration 387, loss = 0.37030430\n",
      "Iteration 388, loss = 0.37031785\n",
      "Iteration 389, loss = 0.37063593\n",
      "Iteration 390, loss = 0.37016233\n",
      "Iteration 391, loss = 0.37022089\n",
      "Iteration 392, loss = 0.36993946\n",
      "Iteration 393, loss = 0.37012101\n",
      "Iteration 394, loss = 0.36993812\n",
      "Iteration 395, loss = 0.36994722\n",
      "Iteration 396, loss = 0.36974446\n",
      "Iteration 397, loss = 0.36975173\n",
      "Iteration 398, loss = 0.36959258\n",
      "Iteration 399, loss = 0.36971173\n",
      "Iteration 400, loss = 0.36989672\n",
      "Iteration 401, loss = 0.36954972\n",
      "Iteration 402, loss = 0.36945719\n",
      "Iteration 403, loss = 0.36919197\n",
      "Iteration 404, loss = 0.36934956\n",
      "Iteration 405, loss = 0.36907139\n",
      "Iteration 406, loss = 0.36959470\n",
      "Iteration 407, loss = 0.36909652\n",
      "Iteration 408, loss = 0.36925956\n",
      "Iteration 409, loss = 0.36956719\n",
      "Iteration 410, loss = 0.36882617\n",
      "Iteration 411, loss = 0.36927415\n",
      "Iteration 412, loss = 0.36896291\n",
      "Iteration 1, loss = 2.04948698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.05206325\n",
      "Iteration 2, loss = 1.64371260\n",
      "Iteration 1, loss = 2.04917752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.64085856\n",
      "Iteration 1, loss = 2.04649585\n",
      "Iteration 2, loss = 1.64146769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04886143\n",
      "Iteration 2, loss = 1.64336918\n",
      "Iteration 1, loss = 2.05002366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.64249348\n",
      "Iteration 1, loss = 2.04974115\n",
      "Iteration 2, loss = 1.64147185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.05125997\n",
      "Iteration 2, loss = 1.64347091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-8c09a3174d74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                            cv=cross_validation)\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \"\"\"\n\u001b[0;32m   1027\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m-> 1028\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 376\u001b[1;33m                             intercept_grads, layer_units)\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    468\u001b[0m                     \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                 },\n\u001b[1;32m--> 470\u001b[1;33m                 args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lbfgs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 601\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    602\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[1;32m--> 178\u001b[1;33m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;31m# Forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;31m# Get loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m    105\u001b[0m                                                  self.coefs_[i])\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# For the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameter_grid = {'activation': ['tanh','identity','logistic','relu'],\n",
    "                  'solver': ['adam','lbfgs','sgd'],\n",
    "                  'hidden_layer_sizes': [3,5,8,13,21,34],\n",
    "                  'verbose': [True]}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "grid_search = GridSearchCV(ANNClassifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8488461893830092\n",
      "Best parameters: {'activation': 'tanh', 'hidden_layer_sizes': (3, 5, 8, 13, 21, 34), 'solver': 'adam', 'verbose': True}\n",
      "Best estimator: MLPClassifier(activation='tanh', hidden_layer_sizes=(3, 5, 8, 13, 21, 34),\n",
      "              max_iter=500, random_state=1, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
